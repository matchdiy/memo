
<center><h1> Full Stack Optimization of Transformer Inference: a Survey </h1> </center>

<center>
<p><a href="https://arxiv.org/pdf/2302.14017.pdf">Paper Link
</a></p>
</center>

# 摘要

最近，先进的神经网络架构设计趋向于Transformer模型。这些模型在计算机视觉、自然语言处理和语音识别等各种应用中取得了卓越的准确性。自从Transformer模型最初被引入以来，这一趋势在过去几年中一直持续存在。然而，最近Transformer模型推断所需的计算和带宽量正在以显著的速度增长，这使得它们在对延迟敏感的应用中的部署变得具有挑战性。因此，越来越多的关注点放在使Transformer模型更加高效上，采用的方法从改变架构设计到开发专用的领域特定加速器不等。在这项工作中，我们对高效Transformer推断的不同方法进行了调查，包括：(i) 分析和分析现有Transformer架构中的瓶颈问题以及它们与之前的卷积模型之间的相似性和差异；(ii) Transformer架构对硬件的影响，包括非线性操作（如Layer Normalization、Softmax和GELU）以及线性操作对硬件设计的影响；(iii) 优化固定Transformer架构的方法；(iv) 在Transformer模型中找到正确的操作映射和调度的挑战；以及(v) 通过使用神经架构搜索来优化Transformer模型的方法。最后，我们通过将调查得出的优化方法应用于Gemmini上，这是一个开源的全栈深度神经网络加速器生成器，并展示了每种方法相对于Gemmini先前的基准结果可以产生的改进。我们发现，除其他外，采用上述方法的全栈协同设计方法可以在Transformer推断中获得高达88.7倍的加速，同时性能下降最小。

# 1 引言

在训练和推断过程中，深度学习模型的参数数量和乘积累加（MAC）操作数量已经扩展到数十亿级别。因此，人们越来越关注高效计算这些模型，并在资源受限的边缘设备上部署这些计算和内存密集型工作负载。这些边缘设备具有严格的能量和内存限制，而利用深度学习模型的相应应用程序通常也具有实时延迟限制。

CPU和GPU都是常用的通用性能计算平台，它们具有无处不在的优势，并能支持各种工作负载和操作。然而，这种灵活性的代价是降低了效率。深度学习模型由少数几个不同的操作组成，这些操作被重复执行数百万次或数十亿次，因此它们通常不需要高水平的灵活性。此外，虽然现代CPU和GPU可以并行执行多个操作，但它们缺乏利用深度学习模型中大量数据重用机会的能力。

对于深度学习，需要快速高效的计算、少量不同操作的使用以及数据重用的机会，这些因素都导致了深度学习的硬件加速器的使用。过去十年间，许多企业级深度学习加速器（如[1, 4, 62, 91, 100, 115, 134, 137, 171, 196, 208]）已经被行业开发并集成到商用硬件中。这与学术界开发的许多研究加速器[34, 37, 39, 40, 59, 69, 70, 81, 169]相呼应。随着硬件加速器的发展，软件框架[3, 32, 98, 167]和编译器[33, 161, 185]用于部署各种深度学习算法也得到了增强和成熟。这些工具使得深度学习算法可以在加速器上执行，并进行映射优化以提高整个深度学习流水线的性能和效率。然而，快速发展的深度学习算法仍然不断对硬件和软件支持提出新的需求，以及它们的联合优化，以满足各种部署约束。

最近，Transformers和大型语言模型在解决各种自然语言处理（NLP）任务中的应用越来越受欢迎[22, 44, 52, 58, 86, 173–175, 177, 190, 198]，这给加速器和框架的设计带来了全新的挑战。特别是由于Transformer模型的规模和运行时复杂性不断增长，人们越来越关注如何提高Transformer推断的效率。然而，与更为知名的卷积神经网络（CNN）架构相比，对于Transformer架构的工作负载特征以及有效运行这些模型所需的设计原则仍然缺乏理解。例如，与传统的以CNN为重点的设计相比，Transformer主要由矩阵乘法（matmul）和内存密集型的非线性操作组成。此外，Transformer模型的计算图和数据流比CNN更复杂，包含更多类型的操作节点，以及更多的数据流分割和连接。所有这些挑战要求我们对当前的硬件和软件解决方案以及Transformer推断的各种设计权衡进行全面分析。进行这样的分析将使我们能够全面了解高效运行Transformer所需的要求。

这项工作的贡献有两个方面：（1）分析Transformer的运行特性，并调查不同的高效Transformer推断方法；（2）通过将调查得出的方法应用于Gemmini [70]，即全栈深度神经网络（DNN）加速器生成器，进行案例研究。这项工作的长期目标是对硬件和软件堆栈中的不同因素进行表征，以优化Transformer推断。

关于我们的第一个贡献，本论文包含了一项调查和分析，涵盖了端到端深度学习推断中不同层次的内容，特别关注了Transformer。具体包括：

* 对Transformer架构的运行特性和瓶颈进行分析和剖析（第2节）。
* 用于Transformer推断的硬件架构，包括Transformer架构中非线性操作对其设计的影响（第3节）。
* 优化策略，如修剪和量化，以进一步提高固定Transformer架构的性能（第4节）。
* 在Transformer架构中的操作映射和调度以及相关挑战（第5节）。
* 通过自动化神经架构搜索过程，设计和调整Transformer架构以提高硬件效率（第6节）。

关于我们的第二个贡献，我们对应用调查方法论进行Transformer部署的案例研究得出了几个关键发现，包括以下内容：

* 最初为CNN工作负载设计的Gemmini并不适合用于Transformer推断的硬件加速器架构。在CNN专用加速器上运行Transformer的主要瓶颈不一定是线性操作，而是在浮点非线性操作以及量化和反量化操作上所花费的时间。除非这些操作得到适当解决，否则硬件利用率可能低于1%（第3.4节和图14）。
* 对于Transformer加速器，更大的累加器大小和较小的临时存储器大小通常更好，而对于CNN加速器则相反。改变加速器架构以纳入这一观察结果可以使延迟优化基准针对CNN的基准测试获得36%的改进（第3.4.3节）。
* 尽管相比于CNN中的卷积需要6个循环，Transformer中的矩阵乘法（matmul）仅需要3个循环，但我们发现为Transformer找到高性能的调度与为CNN找到高性能的调度一样具有挑战性。选择适当的Transformer调度决策涉及大量的决策，最好和最差的解决方案之间的性能差异可高达四个数量级（第5.5.1节和图18、19、20）。
* 在CNN模型中，将批量归一化与相邻的卷积合并是直接的。然而，在Transformer架构中，将层归一化与前面的矩阵乘法（matmul）合并会对映射施加约束，特别是与瓦片大小相关的约束。这需要进一步考虑，因为由于映射约束引起的运行时成本可能在某些情况下超过了操作融合所带来的收益（第5.5.2节和图21、22）。

# 2 TRANSFORMER模型架构和性能瓶颈

本节中，我们从高层次介绍Transformer架构的构建模块开始。首先，在2.1节中，我们讨论了Transformer中使用的多头注意力和前馈模块，以及非线性操作，并介绍了编码器/解码器模型之间的区别。然后，在2.2节中，我们通过算术运算、分析建模以及直接分析每个组件的性能，分析了这些不同模块对硬件性能的影响。

## 2.1 Transformer架构的高级概述

Transformer架构[217]通常由多个Transformer块组成，每个块包括一个多头注意力（MHA）模块和一个前馈（FFN）模块，每个块后面都跟着一个层归一化（LayerNorm）操作和一个残差连接。MHA和FFN的详细计算如 __图1__ 所示，Transformer架构的配置参数（以BERT-Base和BERT-Large使用的值为例）在 __表1__ 中提供。输入到Transformer块的序列由𝑙个标记组成，每个标记由一个𝑑维向量表示，形成一个𝑑×𝑙的矩阵。标记是输入序列的片段，例如，当输入是一个句子时，一个标记可以是一个单词或一个句子片段。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature1.png)

___表1___：Transformer架构的配置参数。以BERT-Base、BERT-Large和GPT-2（最小型）为例提供了参数。请注意，GPT-2与BERT-Base具有相同的参数。序列长度可以是任意数字，只要不超过最大可能的序列长度即可。

| Symbol    | Paramter          | BERT-Base | BERT-Large | GPT-2(smallest) |
|-----------|-------------------|-----------|------------|----------------|
| $N$       | #Layers           | 12        | 24         | 12             |
| $d$       | Model dimension   | 768       | 1024       | 768            |
| $h$       | # Attention Heads | 12        | 16         | 12             |
| $d_{ffn}$ | FFN dimension     | 3072      | 4096       | 3072           |
| $l$       | Sequence length   | -         | -          | -              |

请注意，以上参数仅作为示例，实际应用中可能会根据具体需求进行调整。序列长度可以是任意数字，只要不超过最大可能的序列长度。每个块的隐藏维度为 $𝑑/ℎ$。然后，这些块被传递给ℎ个不同的注意力头，在注意力头中，查询块和键块沿着隐藏维度进行乘法运算，生成一个大小为 $𝑙×𝑙$ 的激活矩阵。该激活矩阵经过Softmax操作（其输出通常被称为注意力分数），并与值块相乘，得到一个隐藏维度为 $𝑑/ℎ$ 的激活。随后，所有注意力头的激活在隐藏维度上进行连接，生成一个隐藏维度为𝑑的单个激活，然后通过最后一个线性层（使用权重矩阵 $𝑊_{out}$）将其投影到相同的维度。最后，在MHA模块中，最后一个线性层的输出经过LayerNorm运算符，然后与残差连接相加，得到MHA模块的输出。

总之，一个MHA模块包括六个线性操作，其中四个是相同的权重到激活的矩阵乘法（即 $𝑊_𝑄$, $𝑊_𝐾$, $𝑊_𝑉$ 和 $𝑊_{out}$ 投影），另外两个是激活到激活的矩阵乘法（Q × K and attention score × V）。在本文中，我们将第一类矩阵乘法称为投影，将第二类矩阵乘法称为激活到激活的矩阵乘法（简称为act-to-act矩阵乘法），因为它们具有不同的运行时行为。

__表2__：Transformer模型中的线性操作。最后一列是矩阵乘法的维度，即𝑚 × 𝑛 × 𝑘表示输入维度为𝑚 × 𝑛和𝑛 × 𝑘，输出维度为𝑚 × 𝑘。请注意，act-to-act matmuls在多头方案中都重复了ℎ次。MHA和FFN的完整计算图在图1中详细说明。

|Module|operation|matmul dim|
|------|---------|----------|
|MHA   |$W_Q$ projection    | $𝑑 × 𝑑 × 𝑙$  |
|MHA   |$W_K$ projection    | $𝑑 × 𝑑 × 𝑙$  |
|MHA   |$W_V$ projection    | $𝑑 × 𝑑 × 𝑙$  |
|MHA   |$Q*K$               | $𝑙 × 𝑑/ℎ × 𝑙$ |
|MHA   |attn.score*value    | $𝑑/ℎ × 𝑙 × 𝑙$ |
|MHA   |$W_{out}$ projection| $𝑑×𝑑×𝑙$       |
|FFN   |$W_1$ projection    | $𝑑_{FFN}×𝑑×𝑙$ |
|FFN   |$W_2 $projection    | $𝑑×𝑑_{FFN}×𝑙$ |

FFN模块（见 __图1__，右侧）是一个相对简单的模块，由两个线性层组成。输入序列首先通过第一个线性层（权重矩阵为𝑊1）从隐藏维度𝑑投影到更高的FFN维度𝑑FFN。随后，投影后的序列通过第二个线性层（权重矩阵为𝑊2）重新投影回原始维度𝑑。通常，𝑑FFN的维度选择比𝑑大4倍，使得𝑊1和𝑊2的宽高比为4:1（例如，在BERT-Base [52]中）。在这两个线性层之间是一个非线性层。通常情况下，GELU [85] 用于这一层 [22, 52, 143, 173, 174]。表2总结了Transformer块中MHA和FFN模块中的所有线性层类型。

请注意，这里的𝑑表示隐藏维度，𝑑FFN表示FFN维度，𝑊1和𝑊2表示权重矩阵。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature2.png)

### 2.1.1 非线性操作

在Transformer网络中进行推理时，存在几种非线性操作，例如Softmax、LayerNorm和GELU，这些操作需要专门的支持或离芯片计算。与线性操作相比，这些非线性操作在整体操作中所占比例相对较小（参见第2.2.2节）。然而，它们在典型硬件上的计算比矩阵乘法更具挑战性，如果处理不当，可能会产生显著的开销。
非线性操作在有效利用时间内存和高效计算方面存在挑战。这是因为它们需要多次遍历所有输入值，这要求这些值在时间内存中保持。如图2（a）所示，Softmax操作涉及到（1）指数运算，（2）沿着序列长度维度对结果求和，以及（3）通过将输入除以求和结果来进行归一化。众所周知，指数函数容易产生数值溢出，因此采用最大值减法技巧 [151] 将表达式exp(𝑥𝑖)/Σexp(𝑥𝑗)转化为exp(𝑥𝑖 − 𝑥𝑚𝑎𝑥)/Σexp(𝑥𝑗 − 𝑥𝑚𝑎𝑥)，其中𝑥𝑚𝑎𝑥是𝑥𝑗的最大值。然而，这需要对输入进行额外的遍历，导致了一种三次遍历的数值稳定实现方式。计算LayerNorm函数也需要在整个输入值上进行多次遍历，如图2（b）所示。在第一次遍历中，必须计算均值。在第二次遍历中，使用这个均值计算标准差。最后，在实际应用归一化的第三次遍历中，需要对每个输入值进行一次除法运算。此外，非线性操作还涉及到操作融合的挑战，操作融合是一种常见的技术，将多个操作合并为一个操作以减少层间通信（参见第5.2.1节）。与许多CNN架构中可以无缝嵌入前后线性操作的批量归一化（BatchNorm）不同，LayerNorm在运行时需要计算输入的均值和方差。因此，要将此操作与前面的矩阵乘法操作融合，必须在减少维度（即计算均值和方差的维度）上累积整个输出矩阵，然后写出结果。这导致不规则的分块维度和较低的数据复用率。因此，在将这些操作与前面的层融合和最大化复用的最佳分块维度之间存在一个非常重要的权衡。对这种权衡的详细分析将在第5.5.2节中提供。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature3.png)

### 2.1.2 编码器和解码器架构

Transformer 架构最初作为机器翻译任务的编码器-解码器模型引入 [217]。在这个设置中，编码器将整个源语言句子作为输入，并通过多个 Transformer 编码器块对其进行处理，提取输入句子的高级特征。然后，这些提取的特征被馈送到解码器中，解码器负责逐个生成目标语言中的标记。它基于来自编码器的源语言特征以及它先前生成的标记 [217]。在后续的工作中，引入了仅编码器和仅解码器的架构，分别仅使用原始编码器-解码器架构中的编码器和解码器组件 [46，174]（图3）。

* __编码器块__
  在仅包含编码器的Transformer模型中[52，143，240]，输入序列一次性通过重复的编码器块进行处理。因此，仅编码器的结构适用于自然语言理解任务[52，143]，例如情感分析[202]或句子相似性分析[28，53，96]，其中整个输入序列被馈送到模型中。在编码器块中，推理由矩阵乘法、逐元素加法和非线性操作组成。MHA模块和FFN模块中的投影层的成本与输入序列长度𝑙成线性关系。然而，MHA模块中的激活-激活矩阵乘法与序列长度呈二次关系（如表2中的query × key和attention score × value行所示）。在第2.2.2节中，我们通过性能分析证明这取决于序列长度：对于较短的序列长度，投影层占主导地位，使得编码器块的整体复杂度为𝑂(𝑙)；然而，对于较长的序列长度，激活-激活矩阵乘法占主导地位，使得整体复杂度为𝑂(𝑙2)。
* __解码器块__
  与仅编码器模型相反，仅包含解码器的模型[22，173，174]由重复的解码器块组成，具有自回归的特性。这意味着给定时间步的输出基于先前时间步的输出。换句话说，模型根据其到目前为止生成的先前标记来预测句子中的标记，并且推理必须逐个输出标记地顺序进行迭代。例如，如果先前生成的序列是“I am a”，模型以此为输入可能预测下一个标记为“student”。然后，在下一个时间步中，模型的输入变为“I am a student”。因此，仅解码器的结构适用于自然语言生成任务。需要注意的是，在仅解码器模型中，在模型开始生成后续标记之前，可以并行处理输入提示标记。在本文中，我们仅考虑开放式生成（即不考虑输入提示）。

与编码器块操作整个输入序列不同，解码器块逐个标记进行推断，因此每个时间步的序列长度为1。在投影层的情况下，每个标记独立于先前生成的标记。因此，投影操作仅应用于输入标记，导致矩阵-向量乘法和固定的计算成本。然而，对于激活-激活矩阵乘法，情况并非如此，因为输入标记不独立于先前生成的标记，而是需要与所有先前生成的标记进行关联。

因此，这些操作的计算复杂度与序列长度成线性关系，这意味着在较大的时间步中处理一个标记所需的计算量比在较小的时间步中处理一个标记更多。需要注意的关键细节是，为了使输入标记与先前生成的所有标记关联，必须存在完整的键和值激活。在生成标记的常见优化技术是在后续迭代中缓存和重用先前生成的标记的中间键和值，从而避免了需要为每次迭代重新计算它们的需求。综上所述，生成完整序列的端到端计算复杂度对于投影层是线性的，而对于另外两个激活-激活矩阵乘法是二次的。Transformer解码器块的端到端计算图也在附录A.6的图27中提供。

### 总结 （第2.1节：Transformer概述）

Transformer由多个Transformer块组成，每个块都包含一个MHA（多头注意力模块）和一个FFN（前馈网络）模块（在每个模块后还包括LayerNorm和残差相加）。MHA模块包含投影层、激活-激活矩阵乘法和Softmax操作。FFN模块由两个投影层组成，它们之间有一个非线性函数。Transformer块有两种类型：编码器块和解码器块。编码器块并行处理整个输入序列，适用于自然语言理解任务。解码器块是自回归的，意味着推理必须针对每个生成的输出标记进行一次，因此通常用于生成型任务。

## 2.2 模型分析

### 2.2.1 工作负载分析

为了评估Transformer中的瓶颈，我们首先对Transformer的编码器和解码器模型进行了浮点运算（FLOPs）的建模，并计算了这些网络的算术强度。算术强度是每个从内存加载的字节可以执行的浮点运算次数。可以通过将总的FLOPs数除以总的访存字节数（也称为MOPs，即内存操作）来计算 __算术强度__[227]：
$$
ArithmeticIntensity = \frac{FLOPs}{MOPs}
$$

__End-to-end FLOPs and MOPs__
在这里，我们假设本地内存足够大，可以在给定操作中完全存储两个矩阵，并且因此计算的算术强度值可以作为可实现的数据重用的上限。在计算FLOPs时，我们还将MAC操作中的乘法和加法分别计算。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature4_5.png)

对于编码器的分析，我们使用了12层BERT-Base模型和24层BERT-Large网络（请参见表格1中的模型配置）；对于解码器，我们使用了12层GPT-2模型架构，该模型与BERT-Base具有相同的模型配置参数。为了分析的目的，在本节中我们忽略了标准BERT模型的最大输入序列长度为512。然后，我们计算了在推理这些模型时需要访问的字节数（MOPs）。我们假设所有操作的精度为8位，这意味着加载一个参数或激活值将需要加载一个字节。对于解码器模型，我们测量了生成给定长度的完整序列所需的总浮点操作数和内存操作数。这些网络在一系列序列长度上的FLOPs和MOPs如 __图4__ 和 __图5__ 所示。可以看到，对于所有模型，FLOPs和MOPs呈超线性增长，特别是在长序列长度的情况下，这是由于act-to-act matmuls在序列长度方面的二次复杂性导致的。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature6.png)

__End-to-end Arithmetic Intensity__
我们通过将推理这些模型时所需的FLOPs数量除以MOPs数量来对算术强度进行建模。BERT-Base、BERT-Large和GPT-2在不同序列长度下的算术强度如 __图6__ 所示。对于BERT-Base和BERT-Large，算术强度在序列长度增加到512之前随着序列长度的增加而增加，然后在更大的序列长度下开始降低。造成这种情况的原因是，如在2.2.2节中将更详细分析的那样，具有比MHA模块更高算术强度的FFN模块（表3）在小序列（__图7__）中占据了总的FLOPs的主导地位。然而，对于较大的序列长度，MHA模块中的act-to-act matmuls的成本随着序列长度的增加呈二次增长，导致端到端模型推理的算术强度降低。
与仅编码器的BERT推理相比，仅解码器的GPT-2推理的算术强度明显较低。这是因为解码器仅由矩阵-向量操作组成，限制了数据重用的机会。尽管如此，对于单个矩阵-向量操作，我们大约每加载一个参数执行一次乘法和加法，因为加载无法在令牌之间共享。这意味着每加载一个参数执行大约2个操作。重要的是要注意，随着序列长度的增加，GPT-2的FLOPs数量较BERT-Base和BERT-Large要少。然而，由于其低算术强度，通常更具挑战性地高效运行其推理，这使得其性能受到内存带宽的限制，与仅编码器的BERT模型相比。这种行为也在[166]中得到了详细描述。

|Sequence Length|Operator|GFLOPs|percent|GMOPs|percent|Arithmetic Intensity|
|---------------|--------|------|-------|-----|-------|--------------------|
|128 |MHA_P      | 7.25| 32%|0.04 | 27%|192.00|
|128 |MHA_ACT    | 0.60|  3%|0.006|  7%| 63.62|
|128 |FFN_P      |14.50| 65%|0.07 | 49%|211.86|
|128 |Other      | 0.08|0.3%|0.02 | 18%|  3.14|
|128 |Total      |22.42|100%|0.14 |100%|159.68|
|512 |MHA_P      |28.99| 30%|0.07 | 16%|438.86|
|512 |MHA_ACT    | 9.62| 10%|0.09 | 20%|101.95|
|512 |FFN_P      |57.98| 60%|0.10 | 25%|558.54|
|512 |Other      | 0.42|0.4%|0.16 | 37%|  2.73|
|512 |Total      |97.02|100%|0.42 |100%|231.0 |
|4096|MHA_P     |231.93 | 18%| 0.33|  3%| 702.17|
|4096|MHA_ACT   |616.02 | 46%| 4.98| 44%| 123.63|
|4096|FFN_P     |463.86 | 35%| 0.43|  4%|1068.52|
|4096|Other     | 11.85 |  1%| 5.47| 49%|   2.16|
|4096|Total     |1323.66|100%|11.22|100%| 117.96|

__表3__:对于BERT-Base编码器，使用128、512和4096个标记的序列长度的每层FLOPs、内存操作（MOPs）和算术强度。在较短的序列长度下，对于FLOPs和MOPs的主要贡献者是MHA和FFN的投影。对于较长的序列长度，act-to-act matmuls占据了更大比例的FLOPs，而这些操作和Softmax占据了大部分的MOPs。对于每个序列长度，act-to-act matmuls的算术强度也低于MHA和FFN中的投影层。

|Operator|GFLOPs|Percent|GMOPs|Percent|Arithmetic Intensity|
|--------|------|-------|-----|-------|--------------------|
|Convolution   |7.26 |99% |0.04| 36%|183.36|
|BatchNorm     |0.03 |0.5%|0.03| 31%|  1.00|
|ReLU          |0.008|0.1%|0.02| 15%|  0.50|
|Other         |0.01 |0.1%|0.02| 18%|  0.53|
|Total(Unfused)|7.31 |100%|0.11|100%| 66.94|
|Total(Fused)  |7.28 |100%|0.06|100%|121.36|

__表4__:ResNet50的每层FLOPs、内存操作（MOPs）和算术强度。卷积操作占据了FLOPs的主要比例，但BatchNorm、ReLU和其他操作在MOPs中也占据了相当大的比例。

__Per-Layer FLOPs, MOPs, and Arithmetic Intensity__
然后，我们评估了BERT-Base编码器的每层FLOPs、MOPs和算术强度与序列长度的关系（见 __表3__）。如表3所示，随着序列长度的增加，act-to-act matmuls消耗的FLOPs和MOPs的比例增加，并且相对于FFN和MHA模块中的投影层，这些操作具有较低的算术强度。这解释了在长序列长度下编码器模型整体算术强度的降低，正如 __图6__ 所观察到的那样。

act-to-act matmuls相对于投影层的低算术强度是因为在这两个操作中，𝑑/ℎ维度相对于投影层的维度（𝑑和𝑑𝐹𝐹𝑁）以及相对于𝑙而言较小，随着序列长度的增加，矩阵维度较小导致算术强度较低，因为每个元素中要执行的操作较少，导致重用减少。当加载和存储act-to-act matmuls所需的大型激活大小时，低算术强度进一步恶化。该激活大小不仅随着序列长度𝑙的平方级增长，而且还乘以头数ℎ，因为在多头方案中，每个头部都有自己的激活（注意力分数）。因此，如附录A.3的表10所示，具有较小头数的假设BERT模型（因此具有较大的𝑑/ℎ维度）将减少MOPs的数量并改善MHA模块中的act-to-act注意力的算术强度。这表明，在设计Transformer架构时，头数可能会在准确性与硬件性能指标之间产生权衡。

此外，__表3__ 还说明了非线性操作（在表中归类为“其他”）消耗了整体FLOPs中的一小部分，但对于较长的序列长度，它们消耗了大部分MOPs。与act-to-act matmuls的情况类似，对于较长的序列长度，Softmax操作中大量的MOPs主要是由于每个注意头需要写出或加载的几个 $𝑙 × 𝑙$ 矩阵。___这也表明，非线性激活函数在处理不当时可能会对整体性能产生显著影响，即使它们在总FLOPs中的贡献微不足道也可能被忽视。___ 我们在附录A.3的表11中对GPT-2解码器进行了类似的每层分析，该分析显示与仅编码器模型相比，在所有层中都存在显著降低的算术强度，这是由于大量的内存操作所导致的。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature7_8.png)

__图7__：在CPU上绘制的BERT-Base编码器与序列长度的计算分解图。对于较小的序列长度，MHA和FFN模块中的投影层主导了模型的延迟。然而，对于较长的序列长度，act-to-act matmuls开始占据主导地位。
__图8__：在CPU上绘制的GPT-2解码器与序列长度的计算分解图。对于较短的序列长度，MHA和FFN模块中的投影层主导了延迟，但对于较长的序列长度，act-to-act matmuls变得更为重要。请注意，非线性操作所占的延迟比编码器推理中更为显著。

与ResNet50的比较。为了提供一个典型卷积神经网络的基准，我们还对ResNet50进行了相应的分析（架构细节可在附录A.2中找到）。表4提供了ResNet50的FLOPs、MOPs和算术强度的分解情况。与序列长度为128的BERT-Base编码器相比（__表3__），没有任何操作融合的ResNet50的FLOPs消耗减少了3.07倍，MOPs减少了1.28倍，从而导致在 __表3__ 中的所有序列长度上的端到端算术强度低于BERT-Base。低算术强度部分是由于ResNet50中的非线性操作所占的FLOPs比例微不足道，但所占的MOPs比例显著，与BERT-Base编码器类似。然而，与Transformer中的非线性操作不同，ResNet50中的这些操作可以直接与之前的矩阵乘法直接融合，以进行推理。特别是ReLU操作可以直接应用于累积输出，而BatchNorm操作实际上可以折叠到之前的卷积中。融合ReLU消除了此操作的MOPs，而折叠BatchNorm消除了此操作所需的FLOPs和MOPs。广义上说，操作融合是指将一个操作（例如矩阵乘法或卷积）的输出值直接用作后续操作（例如ReLU或BatchNorm）的输入，而不需要首先将输出值写入片外存储器。操作融合消除了非线性操作的不必要的内存加载和存储需求，因此进一步改善了端到端的算术强度。如 __表4__ 所示，将这些操作与之前的卷积融合将ResNet-50网络的总体算术强度从66.9提高到121.4。在附录A.4的表12中，我们提供了ResNet50中几个卷积层的FLOPs、MOPs和算术强度的更详细的数据作为参考。

