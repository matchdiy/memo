
<center><h1> Full Stack Optimization of Transformer Inference: a Survey </h1> </center>

<center>
<p><a href="https://arxiv.org/pdf/2302.14017.pdf">Paper Link
</a></p>
</center>

# 摘要

最近，先进的神经网络架构设计趋向于Transformer模型。这些模型在计算机视觉、自然语言处理和语音识别等各种应用中取得了卓越的准确性。自从Transformer模型最初被引入以来，这一趋势在过去几年中一直持续存在。然而，最近Transformer模型推断所需的计算和带宽量正在以显著的速度增长，这使得它们在对延迟敏感的应用中的部署变得具有挑战性。因此，越来越多的关注点放在使Transformer模型更加高效上，采用的方法从改变架构设计到开发专用的领域特定加速器不等。在这项工作中，我们对高效Transformer推断的不同方法进行了调查，包括：(i) 分析和分析现有Transformer架构中的瓶颈问题以及它们与之前的卷积模型之间的相似性和差异；(ii) Transformer架构对硬件的影响，包括非线性操作（如Layer Normalization、Softmax和GELU）以及线性操作对硬件设计的影响；(iii) 优化固定Transformer架构的方法；(iv) 在Transformer模型中找到正确的操作映射和调度的挑战；以及(v) 通过使用神经架构搜索来优化Transformer模型的方法。最后，我们通过将调查得出的优化方法应用于Gemmini上，这是一个开源的全栈深度神经网络加速器生成器，并展示了每种方法相对于Gemmini先前的基准结果可以产生的改进。我们发现，除其他外，采用上述方法的全栈协同设计方法可以在Transformer推断中获得高达88.7倍的加速，同时性能下降最小。

# 1 引言

在训练和推断过程中，深度学习模型的参数数量和乘积累加（MAC）操作数量已经扩展到数十亿级别。因此，人们越来越关注高效计算这些模型，并在资源受限的边缘设备上部署这些计算和内存密集型工作负载。这些边缘设备具有严格的能量和内存限制，而利用深度学习模型的相应应用程序通常也具有实时延迟限制。

CPU和GPU都是常用的通用性能计算平台，它们具有无处不在的优势，并能支持各种工作负载和操作。然而，这种灵活性的代价是降低了效率。深度学习模型由少数几个不同的操作组成，这些操作被重复执行数百万次或数十亿次，因此它们通常不需要高水平的灵活性。此外，虽然现代CPU和GPU可以并行执行多个操作，但它们缺乏利用深度学习模型中大量数据重用机会的能力。

对于深度学习，需要快速高效的计算、少量不同操作的使用以及数据重用的机会，这些因素都导致了深度学习的硬件加速器的使用。过去十年间，许多企业级深度学习加速器（如[1, 4, 62, 91, 100, 115, 134, 137, 171, 196, 208]）已经被行业开发并集成到商用硬件中。这与学术界开发的许多研究加速器[34, 37, 39, 40, 59, 69, 70, 81, 169]相呼应。随着硬件加速器的发展，软件框架[3, 32, 98, 167]和编译器[33, 161, 185]用于部署各种深度学习算法也得到了增强和成熟。这些工具使得深度学习算法可以在加速器上执行，并进行映射优化以提高整个深度学习流水线的性能和效率。然而，快速发展的深度学习算法仍然不断对硬件和软件支持提出新的需求，以及它们的联合优化，以满足各种部署约束。

最近，Transformers和大型语言模型在解决各种自然语言处理（NLP）任务中的应用越来越受欢迎[22, 44, 52, 58, 86, 173–175, 177, 190, 198]，这给加速器和框架的设计带来了全新的挑战。特别是由于Transformer模型的规模和运行时复杂性不断增长，人们越来越关注如何提高Transformer推断的效率。然而，与更为知名的卷积神经网络（CNN）架构相比，对于Transformer架构的工作负载特征以及有效运行这些模型所需的设计原则仍然缺乏理解。例如，与传统的以CNN为重点的设计相比，Transformer主要由矩阵乘法（matmul）和内存密集型的非线性操作组成。此外，Transformer模型的计算图和数据流比CNN更复杂，包含更多类型的操作节点，以及更多的数据流分割和连接。所有这些挑战要求我们对当前的硬件和软件解决方案以及Transformer推断的各种设计权衡进行全面分析。进行这样的分析将使我们能够全面了解高效运行Transformer所需的要求。

这项工作的贡献有两个方面：（1）分析Transformer的运行特性，并调查不同的高效Transformer推断方法；（2）通过将调查得出的方法应用于Gemmini [70]，即全栈深度神经网络（DNN）加速器生成器，进行案例研究。这项工作的长期目标是对硬件和软件堆栈中的不同因素进行表征，以优化Transformer推断。

关于我们的第一个贡献，本论文包含了一项调查和分析，涵盖了端到端深度学习推断中不同层次的内容，特别关注了Transformer。具体包括：

* 对Transformer架构的运行特性和瓶颈进行分析和剖析（第2节）。
* 用于Transformer推断的硬件架构，包括Transformer架构中非线性操作对其设计的影响（第3节）。
* 优化策略，如修剪和量化，以进一步提高固定Transformer架构的性能（第4节）。
* 在Transformer架构中的操作映射和调度以及相关挑战（第5节）。
* 通过自动化神经架构搜索过程，设计和调整Transformer架构以提高硬件效率（第6节）。

关于我们的第二个贡献，我们对应用调查方法论进行Transformer部署的案例研究得出了几个关键发现，包括以下内容：

* 最初为CNN工作负载设计的Gemmini并不适合用于Transformer推断的硬件加速器架构。在CNN专用加速器上运行Transformer的主要瓶颈不一定是线性操作，而是在浮点非线性操作以及量化和反量化操作上所花费的时间。除非这些操作得到适当解决，否则硬件利用率可能低于1%（第3.4节和图14）。
* 对于Transformer加速器，更大的累加器大小和较小的临时存储器大小通常更好，而对于CNN加速器则相反。改变加速器架构以纳入这一观察结果可以使延迟优化基准针对CNN的基准测试获得36%的改进（第3.4.3节）。
* 尽管相比于CNN中的卷积需要6个循环，Transformer中的矩阵乘法（matmul）仅需要3个循环，但我们发现为Transformer找到高性能的调度与为CNN找到高性能的调度一样具有挑战性。选择适当的Transformer调度决策涉及大量的决策，最好和最差的解决方案之间的性能差异可高达四个数量级（第5.5.1节和图18、19、20）。
* 在CNN模型中，将批量归一化与相邻的卷积合并是直接的。然而，在Transformer架构中，将层归一化与前面的矩阵乘法（matmul）合并会对映射施加约束，特别是与瓦片大小相关的约束。这需要进一步考虑，因为由于映射约束引起的运行时成本可能在某些情况下超过了操作融合所带来的收益（第5.5.2节和图21、22）。

# 2 TRANSFORMER模型架构和性能瓶颈

本节中，我们从高层次介绍Transformer架构的构建模块开始。首先，在2.1节中，我们讨论了Transformer中使用的多头注意力和前馈模块，以及非线性操作，并介绍了编码器/解码器模型之间的区别。然后，在2.2节中，我们通过算术运算、分析建模以及直接分析每个组件的性能，分析了这些不同模块对硬件性能的影响。

## 2.1 Transformer架构的高级概述

Transformer架构[217]通常由多个Transformer块组成，每个块包括一个多头注意力（MHA）模块和一个前馈（FFN）模块，每个块后面都跟着一个层归一化（LayerNorm）操作和一个残差连接。MHA和FFN的详细计算如 __图1__ 所示，Transformer架构的配置参数（以BERT-Base和BERT-Large使用的值为例）在 __表1__ 中提供。输入到Transformer块的序列由𝑙个标记组成，每个标记由一个𝑑维向量表示，形成一个𝑑×𝑙的矩阵。标记是输入序列的片段，例如，当输入是一个句子时，一个标记可以是一个单词或一个句子片段。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature1.png)

___表1___：Transformer架构的配置参数。以BERT-Base、BERT-Large和GPT-2（最小型）为例提供了参数。请注意，GPT-2与BERT-Base具有相同的参数。序列长度可以是任意数字，只要不超过最大可能的序列长度即可。

| Symbol    | Paramter          | BERT-Base | BERT-Large | GPT-2(smallest) |
|-----------|-------------------|-----------|------------|----------------|
| $N$       | #Layers           | 12        | 24         | 12             |
| $d$       | Model dimension   | 768       | 1024       | 768            |
| $h$       | # Attention Heads | 12        | 16         | 12             |
| $d_{ffn}$ | FFN dimension     | 3072      | 4096       | 3072           |
| $l$       | Sequence length   | -         | -          | -              |

请注意，以上参数仅作为示例，实际应用中可能会根据具体需求进行调整。序列长度可以是任意数字，只要不超过最大可能的序列长度。每个块的隐藏维度为 $𝑑/ℎ$。然后，这些块被传递给ℎ个不同的注意力头，在注意力头中，查询块和键块沿着隐藏维度进行乘法运算，生成一个大小为 $𝑙×𝑙$ 的激活矩阵。该激活矩阵经过Softmax操作（其输出通常被称为注意力分数），并与值块相乘，得到一个隐藏维度为 $𝑑/ℎ$ 的激活。随后，所有注意力头的激活在隐藏维度上进行连接，生成一个隐藏维度为𝑑的单个激活，然后通过最后一个线性层（使用权重矩阵 $𝑊_{out}$）将其投影到相同的维度。最后，在MHA模块中，最后一个线性层的输出经过LayerNorm运算符，然后与残差连接相加，得到MHA模块的输出。

总之，一个MHA模块包括六个线性操作，其中四个是相同的权重到激活的矩阵乘法（即 $𝑊_𝑄$, $𝑊_𝐾$, $𝑊_𝑉$ 和 $𝑊_{out}$ 投影），另外两个是激活到激活的矩阵乘法（Q × K and attention score × V）。在本文中，我们将第一类矩阵乘法称为投影，将第二类矩阵乘法称为激活到激活的矩阵乘法（简称为act-to-act矩阵乘法），因为它们具有不同的运行时行为。
