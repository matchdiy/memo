
<center><h1> Full Stack Optimization of Transformer Inference: a Survey </h1> </center>

<center>
<p><a href="https://arxiv.org/pdf/2302.14017.pdf">Paper Link
</a></p>
</center>

# 摘要

最近，先进的神经网络架构设计趋向于Transformer模型。这些模型在计算机视觉、自然语言处理和语音识别等各种应用中取得了卓越的准确性。自从Transformer模型最初被引入以来，这一趋势在过去几年中一直持续存在。然而，最近Transformer模型推断所需的计算和带宽量正在以显著的速度增长，这使得它们在对延迟敏感的应用中的部署变得具有挑战性。因此，越来越多的关注点放在使Transformer模型更加高效上，采用的方法从改变架构设计到开发专用的领域特定加速器不等。在这项工作中，我们对高效Transformer推断的不同方法进行了调查，包括：(i) 分析和分析现有Transformer架构中的瓶颈问题以及它们与之前的卷积模型之间的相似性和差异；(ii) Transformer架构对硬件的影响，包括非线性操作（如Layer Normalization、Softmax和GELU）以及线性操作对硬件设计的影响；(iii) 优化固定Transformer架构的方法；(iv) 在Transformer模型中找到正确的操作映射和调度的挑战；以及(v) 通过使用神经架构搜索来优化Transformer模型的方法。最后，我们通过将调查得出的优化方法应用于Gemmini上，这是一个开源的全栈深度神经网络加速器生成器，并展示了每种方法相对于Gemmini先前的基准结果可以产生的改进。我们发现，除其他外，采用上述方法的全栈协同设计方法可以在Transformer推断中获得高达88.7倍的加速，同时性能下降最小。

# 1 引言

在训练和推断过程中，深度学习模型的参数数量和乘积累加（MAC）操作数量已经扩展到数十亿级别。因此，人们越来越关注高效计算这些模型，并在资源受限的边缘设备上部署这些计算和内存密集型工作负载。这些边缘设备具有严格的能量和内存限制，而利用深度学习模型的相应应用程序通常也具有实时延迟限制。

CPU和GPU都是常用的通用性能计算平台，它们具有无处不在的优势，并能支持各种工作负载和操作。然而，这种灵活性的代价是降低了效率。深度学习模型由少数几个不同的操作组成，这些操作被重复执行数百万次或数十亿次，因此它们通常不需要高水平的灵活性。此外，虽然现代CPU和GPU可以并行执行多个操作，但它们缺乏利用深度学习模型中大量数据重用机会的能力。

对于深度学习，需要快速高效的计算、少量不同操作的使用以及数据重用的机会，这些因素都导致了深度学习的硬件加速器的使用。过去十年间，许多企业级深度学习加速器（如[1, 4, 62, 91, 100, 115, 134, 137, 171, 196, 208]）已经被行业开发并集成到商用硬件中。这与学术界开发的许多研究加速器[34, 37, 39, 40, 59, 69, 70, 81, 169]相呼应。随着硬件加速器的发展，软件框架[3, 32, 98, 167]和编译器[33, 161, 185]用于部署各种深度学习算法也得到了增强和成熟。这些工具使得深度学习算法可以在加速器上执行，并进行映射优化以提高整个深度学习流水线的性能和效率。然而，快速发展的深度学习算法仍然不断对硬件和软件支持提出新的需求，以及它们的联合优化，以满足各种部署约束。

最近，Transformers和大型语言模型在解决各种自然语言处理（NLP）任务中的应用越来越受欢迎[22, 44, 52, 58, 86, 173–175, 177, 190, 198]，这给加速器和框架的设计带来了全新的挑战。特别是由于Transformer模型的规模和运行时复杂性不断增长，人们越来越关注如何提高Transformer推断的效率。然而，与更为知名的卷积神经网络（CNN）架构相比，对于Transformer架构的工作负载特征以及有效运行这些模型所需的设计原则仍然缺乏理解。例如，与传统的以CNN为重点的设计相比，Transformer主要由矩阵乘法（matmul）和内存密集型的非线性操作组成。此外，Transformer模型的计算图和数据流比CNN更复杂，包含更多类型的操作节点，以及更多的数据流分割和连接。所有这些挑战要求我们对当前的硬件和软件解决方案以及Transformer推断的各种设计权衡进行全面分析。进行这样的分析将使我们能够全面了解高效运行Transformer所需的要求。

这项工作的贡献有两个方面：（1）分析Transformer的运行特性，并调查不同的高效Transformer推断方法；（2）通过将调查得出的方法应用于Gemmini [70]，即全栈深度神经网络（DNN）加速器生成器，进行案例研究。这项工作的长期目标是对硬件和软件堆栈中的不同因素进行表征，以优化Transformer推断。

关于我们的第一个贡献，本论文包含了一项调查和分析，涵盖了端到端深度学习推断中不同层次的内容，特别关注了Transformer。具体包括：

* 对Transformer架构的运行特性和瓶颈进行分析和剖析（第2节）。
* 用于Transformer推断的硬件架构，包括Transformer架构中非线性操作对其设计的影响（第3节）。
* 优化策略，如修剪和量化，以进一步提高固定Transformer架构的性能（第4节）。
* 在Transformer架构中的操作映射和调度以及相关挑战（第5节）。
* 通过自动化神经架构搜索过程，设计和调整Transformer架构以提高硬件效率（第6节）。

关于我们的第二个贡献，我们对应用调查方法论进行Transformer部署的案例研究得出了几个关键发现，包括以下内容：

* 最初为CNN工作负载设计的Gemmini并不适合用于Transformer推断的硬件加速器架构。在CNN专用加速器上运行Transformer的主要瓶颈不一定是线性操作，而是在浮点非线性操作以及量化和反量化操作上所花费的时间。除非这些操作得到适当解决，否则硬件利用率可能低于1%（第3.4节和图14）。
* 对于Transformer加速器，更大的累加器大小和较小的临时存储器大小通常更好，而对于CNN加速器则相反。改变加速器架构以纳入这一观察结果可以使延迟优化基准针对CNN的基准测试获得36%的改进（第3.4.3节）。
* 尽管相比于CNN中的卷积需要6个循环，Transformer中的矩阵乘法（matmul）仅需要3个循环，但我们发现为Transformer找到高性能的调度与为CNN找到高性能的调度一样具有挑战性。选择适当的Transformer调度决策涉及大量的决策，最好和最差的解决方案之间的性能差异可高达四个数量级（第5.5.1节和图18、19、20）。
* 在CNN模型中，将批量归一化与相邻的卷积合并是直接的。然而，在Transformer架构中，将层归一化与前面的矩阵乘法（matmul）合并会对映射施加约束，特别是与瓦片大小相关的约束。这需要进一步考虑，因为由于映射约束引起的运行时成本可能在某些情况下超过了操作融合所带来的收益（第5.5.2节和图21、22）。

# 2 TRANSFORMER模型架构和性能瓶颈

本节中，我们从高层次介绍Transformer架构的构建模块开始。首先，在2.1节中，我们讨论了Transformer中使用的多头注意力和前馈模块，以及非线性操作，并介绍了编码器/解码器模型之间的区别。然后，在2.2节中，我们通过算术运算、分析建模以及直接分析每个组件的性能，分析了这些不同模块对硬件性能的影响。

## 2.1 Transformer架构的高级概述

Transformer架构[217]通常由多个Transformer块组成，每个块包括一个多头注意力（MHA）模块和一个前馈（FFN）模块，每个块后面都跟着一个层归一化（LayerNorm）操作和一个残差连接。MHA和FFN的详细计算如 __图1__ 所示，Transformer架构的配置参数（以BERT-Base和BERT-Large使用的值为例）在 __表1__ 中提供。输入到Transformer块的序列由𝑙个标记组成，每个标记由一个𝑑维向量表示，形成一个𝑑×𝑙的矩阵。标记是输入序列的片段，例如，当输入是一个句子时，一个标记可以是一个单词或一个句子片段。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature1.png)

___表1___：Transformer架构的配置参数。以BERT-Base、BERT-Large和GPT-2（最小型）为例提供了参数。请注意，GPT-2与BERT-Base具有相同的参数。序列长度可以是任意数字，只要不超过最大可能的序列长度即可。

| Symbol    | Paramter          | BERT-Base | BERT-Large | GPT-2(smallest) |
|-----------|-------------------|-----------|------------|----------------|
| $N$       | #Layers           | 12        | 24         | 12             |
| $d$       | Model dimension   | 768       | 1024       | 768            |
| $h$       | # Attention Heads | 12        | 16         | 12             |
| $d_{ffn}$ | FFN dimension     | 3072      | 4096       | 3072           |
| $l$       | Sequence length   | -         | -          | -              |

请注意，以上参数仅作为示例，实际应用中可能会根据具体需求进行调整。序列长度可以是任意数字，只要不超过最大可能的序列长度。每个块的隐藏维度为 $𝑑/ℎ$。然后，这些块被传递给ℎ个不同的注意力头，在注意力头中，查询块和键块沿着隐藏维度进行乘法运算，生成一个大小为 $𝑙×𝑙$ 的激活矩阵。该激活矩阵经过Softmax操作（其输出通常被称为注意力分数），并与值块相乘，得到一个隐藏维度为 $𝑑/ℎ$ 的激活。随后，所有注意力头的激活在隐藏维度上进行连接，生成一个隐藏维度为𝑑的单个激活，然后通过最后一个线性层（使用权重矩阵 $𝑊_{out}$）将其投影到相同的维度。最后，在MHA模块中，最后一个线性层的输出经过LayerNorm运算符，然后与残差连接相加，得到MHA模块的输出。

总之，一个MHA模块包括六个线性操作，其中四个是相同的权重到激活的矩阵乘法（即 $𝑊_𝑄$, $𝑊_𝐾$, $𝑊_𝑉$ 和 $𝑊_{out}$ 投影），另外两个是激活到激活的矩阵乘法（Q × K and attention score × V）。在本文中，我们将第一类矩阵乘法称为投影，将第二类矩阵乘法称为激活到激活的矩阵乘法（简称为act-to-act矩阵乘法），因为它们具有不同的运行时行为。

__表2__：Transformer模型中的线性操作。最后一列是矩阵乘法的维度，即𝑚 × 𝑛 × 𝑘表示输入维度为𝑚 × 𝑛和𝑛 × 𝑘，输出维度为𝑚 × 𝑘。请注意，act-to-act matmuls在多头方案中都重复了ℎ次。MHA和FFN的完整计算图在图1中详细说明。

|Module|operation|matmul dim|
|------|---------|----------|
|MHA   |$W_Q$ projection    | $𝑑 × 𝑑 × 𝑙$  |
|MHA   |$W_K$ projection    | $𝑑 × 𝑑 × 𝑙$  |
|MHA   |$W_V$ projection    | $𝑑 × 𝑑 × 𝑙$  |
|MHA   |$Q*K$               | $𝑙 × 𝑑/ℎ × 𝑙$ |
|MHA   |attn.score*value    | $𝑑/ℎ × 𝑙 × 𝑙$ |
|MHA   |$W_{out}$ projection| $𝑑×𝑑×𝑙$       |
|FFN   |$W_1$ projection    | $𝑑_{FFN}×𝑑×𝑙$ |
|FFN   |$W_2 $projection    | $𝑑×𝑑_{FFN}×𝑙$ |

FFN模块（见 __图1__，右侧）是一个相对简单的模块，由两个线性层组成。输入序列首先通过第一个线性层（权重矩阵为𝑊1）从隐藏维度𝑑投影到更高的FFN维度𝑑FFN。随后，投影后的序列通过第二个线性层（权重矩阵为𝑊2）重新投影回原始维度𝑑。通常，𝑑FFN的维度选择比𝑑大4倍，使得𝑊1和𝑊2的宽高比为4:1（例如，在BERT-Base [52]中）。在这两个线性层之间是一个非线性层。通常情况下，GELU [85] 用于这一层 [22, 52, 143, 173, 174]。表2总结了Transformer块中MHA和FFN模块中的所有线性层类型。

请注意，这里的𝑑表示隐藏维度，𝑑FFN表示FFN维度，𝑊1和𝑊2表示权重矩阵。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature2.png)

### 2.1.1 非线性操作

在Transformer网络中进行推理时，存在几种非线性操作，例如Softmax、LayerNorm和GELU，这些操作需要专门的支持或离芯片计算。与线性操作相比，这些非线性操作在整体操作中所占比例相对较小（参见第2.2.2节）。然而，它们在典型硬件上的计算比矩阵乘法更具挑战性，如果处理不当，可能会产生显著的开销。
非线性操作在有效利用时间内存和高效计算方面存在挑战。这是因为它们需要多次遍历所有输入值，这要求这些值在时间内存中保持。如图2（a）所示，Softmax操作涉及到（1）指数运算，（2）沿着序列长度维度对结果求和，以及（3）通过将输入除以求和结果来进行归一化。众所周知，指数函数容易产生数值溢出，因此采用最大值减法技巧 [151] 将表达式exp(𝑥𝑖)/Σexp(𝑥𝑗)转化为exp(𝑥𝑖 − 𝑥𝑚𝑎𝑥)/Σexp(𝑥𝑗 − 𝑥𝑚𝑎𝑥)，其中𝑥𝑚𝑎𝑥是𝑥𝑗的最大值。然而，这需要对输入进行额外的遍历，导致了一种三次遍历的数值稳定实现方式。计算LayerNorm函数也需要在整个输入值上进行多次遍历，如图2（b）所示。在第一次遍历中，必须计算均值。在第二次遍历中，使用这个均值计算标准差。最后，在实际应用归一化的第三次遍历中，需要对每个输入值进行一次除法运算。此外，非线性操作还涉及到操作融合的挑战，操作融合是一种常见的技术，将多个操作合并为一个操作以减少层间通信（参见第5.2.1节）。与许多CNN架构中可以无缝嵌入前后线性操作的批量归一化（BatchNorm）不同，LayerNorm在运行时需要计算输入的均值和方差。因此，要将此操作与前面的矩阵乘法操作融合，必须在减少维度（即计算均值和方差的维度）上累积整个输出矩阵，然后写出结果。这导致不规则的分块维度和较低的数据复用率。因此，在将这些操作与前面的层融合和最大化复用的最佳分块维度之间存在一个非常重要的权衡。对这种权衡的详细分析将在第5.5.2节中提供。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature3.png)

### 2.1.2 编码器和解码器架构

Transformer 架构最初作为机器翻译任务的编码器-解码器模型引入 [217]。在这个设置中，编码器将整个源语言句子作为输入，并通过多个 Transformer 编码器块对其进行处理，提取输入句子的高级特征。然后，这些提取的特征被馈送到解码器中，解码器负责逐个生成目标语言中的标记。它基于来自编码器的源语言特征以及它先前生成的标记 [217]。在后续的工作中，引入了仅编码器和仅解码器的架构，分别仅使用原始编码器-解码器架构中的编码器和解码器组件 [46，174]（图3）。

* __编码器块__
  在仅包含编码器的Transformer模型中[52，143，240]，输入序列一次性通过重复的编码器块进行处理。因此，仅编码器的结构适用于自然语言理解任务[52，143]，例如情感分析[202]或句子相似性分析[28，53，96]，其中整个输入序列被馈送到模型中。在编码器块中，推理由矩阵乘法、逐元素加法和非线性操作组成。MHA模块和FFN模块中的投影层的成本与输入序列长度𝑙成线性关系。然而，MHA模块中的激活-激活矩阵乘法与序列长度呈二次关系（如表2中的query × key和attention score × value行所示）。在第2.2.2节中，我们通过性能分析证明这取决于序列长度：对于较短的序列长度，投影层占主导地位，使得编码器块的整体复杂度为𝑂(𝑙)；然而，对于较长的序列长度，激活-激活矩阵乘法占主导地位，使得整体复杂度为𝑂(𝑙2)。
* __解码器块__
  与仅编码器模型相反，仅包含解码器的模型[22，173，174]由重复的解码器块组成，具有自回归的特性。这意味着给定时间步的输出基于先前时间步的输出。换句话说，模型根据其到目前为止生成的先前标记来预测句子中的标记，并且推理必须逐个输出标记地顺序进行迭代。例如，如果先前生成的序列是“I am a”，模型以此为输入可能预测下一个标记为“student”。然后，在下一个时间步中，模型的输入变为“I am a student”。因此，仅解码器的结构适用于自然语言生成任务。需要注意的是，在仅解码器模型中，在模型开始生成后续标记之前，可以并行处理输入提示标记。在本文中，我们仅考虑开放式生成（即不考虑输入提示）。

与编码器块操作整个输入序列不同，解码器块逐个标记进行推断，因此每个时间步的序列长度为1。在投影层的情况下，每个标记独立于先前生成的标记。因此，投影操作仅应用于输入标记，导致矩阵-向量乘法和固定的计算成本。然而，对于激活-激活矩阵乘法，情况并非如此，因为输入标记不独立于先前生成的标记，而是需要与所有先前生成的标记进行关联。

因此，这些操作的计算复杂度与序列长度成线性关系，这意味着在较大的时间步中处理一个标记所需的计算量比在较小的时间步中处理一个标记更多。需要注意的关键细节是，为了使输入标记与先前生成的所有标记关联，必须存在完整的键和值激活。在生成标记的常见优化技术是在后续迭代中缓存和重用先前生成的标记的中间键和值，从而避免了需要为每次迭代重新计算它们的需求。综上所述，生成完整序列的端到端计算复杂度对于投影层是线性的，而对于另外两个激活-激活矩阵乘法是二次的。Transformer解码器块的端到端计算图也在附录A.6的图27中提供。

### 总结 （第2.1节：Transformer概述）

Transformer由多个Transformer块组成，每个块都包含一个MHA（多头注意力模块）和一个FFN（前馈网络）模块（在每个模块后还包括LayerNorm和残差相加）。MHA模块包含投影层、激活-激活矩阵乘法和Softmax操作。FFN模块由两个投影层组成，它们之间有一个非线性函数。Transformer块有两种类型：编码器块和解码器块。编码器块并行处理整个输入序列，适用于自然语言理解任务。解码器块是自回归的，意味着推理必须针对每个生成的输出标记进行一次，因此通常用于生成型任务。

## 2.2 模型分析

### 2.2.1 工作负载分析

为了评估Transformer中的瓶颈，我们首先对Transformer的编码器和解码器模型进行了浮点运算（FLOPs）的建模，并计算了这些网络的算术强度。算术强度是每个从内存加载的字节可以执行的浮点运算次数。可以通过将总的FLOPs数除以总的访存字节数（也称为MOPs，即内存操作）来计算 __算术强度__[227]：
$$
ArithmeticIntensity = \frac{FLOPs}{MOPs}
$$

__End-to-end FLOPs and MOPs__
在这里，我们假设本地内存足够大，可以在给定操作中完全存储两个矩阵，并且因此计算的算术强度值可以作为可实现的数据重用的上限。在计算FLOPs时，我们还将MAC操作中的乘法和加法分别计算。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature4_5.png)

对于编码器的分析，我们使用了12层BERT-Base模型和24层BERT-Large网络（请参见表格1中的模型配置）；对于解码器，我们使用了12层GPT-2模型架构，该模型与BERT-Base具有相同的模型配置参数。为了分析的目的，在本节中我们忽略了标准BERT模型的最大输入序列长度为512。然后，我们计算了在推理这些模型时需要访问的字节数（MOPs）。我们假设所有操作的精度为8位，这意味着加载一个参数或激活值将需要加载一个字节。对于解码器模型，我们测量了生成给定长度的完整序列所需的总浮点操作数和内存操作数。这些网络在一系列序列长度上的FLOPs和MOPs如 __图4__ 和 __图5__ 所示。可以看到，对于所有模型，FLOPs和MOPs呈超线性增长，特别是在长序列长度的情况下，这是由于act-to-act matmuls在序列长度方面的二次复杂性导致的。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature6.png)

__End-to-end Arithmetic Intensity__
我们通过将推理这些模型时所需的FLOPs数量除以MOPs数量来对算术强度进行建模。BERT-Base、BERT-Large和GPT-2在不同序列长度下的算术强度如 __图6__ 所示。对于BERT-Base和BERT-Large，算术强度在序列长度增加到512之前随着序列长度的增加而增加，然后在更大的序列长度下开始降低。造成这种情况的原因是，如在2.2.2节中将更详细分析的那样，具有比MHA模块更高算术强度的FFN模块（表3）在小序列（__图7__）中占据了总的FLOPs的主导地位。然而，对于较大的序列长度，MHA模块中的act-to-act matmuls的成本随着序列长度的增加呈二次增长，导致端到端模型推理的算术强度降低。
与仅编码器的BERT推理相比，仅解码器的GPT-2推理的算术强度明显较低。这是因为解码器仅由矩阵-向量操作组成，限制了数据重用的机会。尽管如此，对于单个矩阵-向量操作，我们大约每加载一个参数执行一次乘法和加法，因为加载无法在令牌之间共享。这意味着每加载一个参数执行大约2个操作。重要的是要注意，随着序列长度的增加，GPT-2的FLOPs数量较BERT-Base和BERT-Large要少。然而，由于其低算术强度，通常更具挑战性地高效运行其推理，这使得其性能受到内存带宽的限制，与仅编码器的BERT模型相比。这种行为也在[166]中得到了详细描述。

|Sequence Length|Operator|GFLOPs|percent|GMOPs|percent|Arithmetic Intensity|
|---------------|--------|------|-------|-----|-------|--------------------|
|128 |MHA_P      | 7.25| 32%|0.04 | 27%|192.00|
|128 |MHA_ACT    | 0.60|  3%|0.006|  7%| 63.62|
|128 |FFN_P      |14.50| 65%|0.07 | 49%|211.86|
|128 |Other      | 0.08|0.3%|0.02 | 18%|  3.14|
|128 |Total      |22.42|100%|0.14 |100%|159.68|
|512 |MHA_P      |28.99| 30%|0.07 | 16%|438.86|
|512 |MHA_ACT    | 9.62| 10%|0.09 | 20%|101.95|
|512 |FFN_P      |57.98| 60%|0.10 | 25%|558.54|
|512 |Other      | 0.42|0.4%|0.16 | 37%|  2.73|
|512 |Total      |97.02|100%|0.42 |100%|231.0 |
|4096|MHA_P     |231.93 | 18%| 0.33|  3%| 702.17|
|4096|MHA_ACT   |616.02 | 46%| 4.98| 44%| 123.63|
|4096|FFN_P     |463.86 | 35%| 0.43|  4%|1068.52|
|4096|Other     | 11.85 |  1%| 5.47| 49%|   2.16|
|4096|Total     |1323.66|100%|11.22|100%| 117.96|

__表3__:对于BERT-Base编码器，使用128、512和4096个标记的序列长度的每层FLOPs、内存操作（MOPs）和算术强度。在较短的序列长度下，对于FLOPs和MOPs的主要贡献者是MHA和FFN的投影。对于较长的序列长度，act-to-act matmuls占据了更大比例的FLOPs，而这些操作和Softmax占据了大部分的MOPs。对于每个序列长度，act-to-act matmuls的算术强度也低于MHA和FFN中的投影层。

|Operator|GFLOPs|Percent|GMOPs|Percent|Arithmetic Intensity|
|--------|------|-------|-----|-------|--------------------|
|Convolution   |7.26 |99% |0.04| 36%|183.36|
|BatchNorm     |0.03 |0.5%|0.03| 31%|  1.00|
|ReLU          |0.008|0.1%|0.02| 15%|  0.50|
|Other         |0.01 |0.1%|0.02| 18%|  0.53|
|Total(Unfused)|7.31 |100%|0.11|100%| 66.94|
|Total(Fused)  |7.28 |100%|0.06|100%|121.36|

__表4__:ResNet50的每层FLOPs、内存操作（MOPs）和算术强度。卷积操作占据了FLOPs的主要比例，但BatchNorm、ReLU和其他操作在MOPs中也占据了相当大的比例。

__Per-Layer FLOPs, MOPs, and Arithmetic Intensity__
然后，我们评估了BERT-Base编码器的每层FLOPs、MOPs和算术强度与序列长度的关系（见 __表3__）。如表3所示，随着序列长度的增加，act-to-act matmuls消耗的FLOPs和MOPs的比例增加，并且相对于FFN和MHA模块中的投影层，这些操作具有较低的算术强度。这解释了在长序列长度下编码器模型整体算术强度的降低，正如 __图6__ 所观察到的那样。

act-to-act matmuls相对于投影层的低算术强度是因为在这两个操作中，𝑑/ℎ维度相对于投影层的维度（𝑑和𝑑𝐹𝐹𝑁）以及相对于𝑙而言较小，随着序列长度的增加，矩阵维度较小导致算术强度较低，因为每个元素中要执行的操作较少，导致重用减少。当加载和存储act-to-act matmuls所需的大型激活大小时，低算术强度进一步恶化。该激活大小不仅随着序列长度𝑙的平方级增长，而且还乘以头数ℎ，因为在多头方案中，每个头部都有自己的激活（注意力分数）。因此，如附录A.3的表10所示，具有较小头数的假设BERT模型（因此具有较大的𝑑/ℎ维度）将减少MOPs的数量并改善MHA模块中的act-to-act注意力的算术强度。这表明，在设计Transformer架构时，头数可能会在准确性与硬件性能指标之间产生权衡。

此外，__表3__ 还说明了非线性操作（在表中归类为“其他”）消耗了整体FLOPs中的一小部分，但对于较长的序列长度，它们消耗了大部分MOPs。与act-to-act matmuls的情况类似，对于较长的序列长度，Softmax操作中大量的MOPs主要是由于每个注意头需要写出或加载的几个 $𝑙 × 𝑙$ 矩阵。___这也表明，非线性激活函数在处理不当时可能会对整体性能产生显著影响，即使它们在总FLOPs中的贡献微不足道也可能被忽视。___ 我们在附录A.3的表11中对GPT-2解码器进行了类似的每层分析，该分析显示与仅编码器模型相比，在所有层中都存在显著降低的算术强度，这是由于大量的内存操作所导致的。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature7_8.png)

__图7__：在CPU上绘制的BERT-Base编码器与序列长度的计算分解图。对于较小的序列长度，MHA和FFN模块中的投影层主导了模型的延迟。然而，对于较长的序列长度，act-to-act matmuls开始占据主导地位。
__图8__：在CPU上绘制的GPT-2解码器与序列长度的计算分解图。对于较短的序列长度，MHA和FFN模块中的投影层主导了延迟，但对于较长的序列长度，act-to-act matmuls变得更为重要。请注意，非线性操作所占的延迟比编码器推理中更为显著。

__与ResNet50的比较__,为了提供一个典型卷积神经网络的基准，我们还对ResNet50进行了相应的分析（架构细节可在附录A.2中找到）。表4提供了ResNet50的FLOPs、MOPs和算术强度的分解情况。与序列长度为128的BERT-Base编码器相比（__表3__），没有任何操作融合的ResNet50的FLOPs消耗减少了3.07倍，MOPs减少了1.28倍，从而导致在 __表3__ 中的所有序列长度上的端到端算术强度低于BERT-Base。低算术强度部分是由于ResNet50中的非线性操作所占的FLOPs比例微不足道，但所占的MOPs比例显著，与BERT-Base编码器类似。然而，与Transformer中的非线性操作不同，ResNet50中的这些操作可以直接与之前的矩阵乘法直接融合，以进行推理。特别是ReLU操作可以直接应用于累积输出，而BatchNorm操作实际上可以折叠到之前的卷积中。融合ReLU消除了此操作的MOPs，而折叠BatchNorm消除了此操作所需的FLOPs和MOPs。广义上说，操作融合是指将一个操作（例如矩阵乘法或卷积）的输出值直接用作后续操作（例如ReLU或BatchNorm）的输入，而不需要首先将输出值写入片外存储器。操作融合消除了非线性操作的不必要的内存加载和存储需求，因此进一步改善了端到端的算术强度。如 __表4__ 所示，将这些操作与之前的卷积融合将ResNet-50网络的总体算术强度从66.9提高到121.4。在附录A.4的表12中，我们提供了ResNet50中几个卷积层的FLOPs、MOPs和算术强度的更详细的数据作为参考。

请注意，算术强度提供了一个大致估计，用于确定在理想情况下不同模型和操作中可以进行多少数据重用。在第3.3节中，我们将讨论分析建模如何通过考虑硬件细节来提供更准确、非理想的估计。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature9.png)

__图9__：在CPU上绘制了BERT-Base和BERT-Large编码器以及GPT-2解码器的归一化延迟与序列长度的关系，归一化基准为序列长度为128的BERT-Base的延迟。对于仅编码器和仅解码器网络，延迟与序列长度呈二次方关系。此外，对于具有相同模型结构的仅编码器和仅解码器网络，由于其算术强度降低，仅解码器网络的延迟显著更长。

### 2.2.2 Profiling

为了分析通用硬件上Transformer工作负载中的瓶颈，我们对Intel Gold 6242 CPU上的Transformer推断进行了分析。我们对仅编码器的BERT-Base和仅解码器的GPT-2的工作负载延迟进行了分析。

__延迟分解__，__图7__ 和 __图8__ 分别展示了在CPU上，对于BERT-Base和GPT-2，延迟分解如何随着序列长度的变化而变化。这些分解说明，在较短的序列长度（例如128-512）中，大部分计算在FFN模块的投影层中进行，而大多数MHA计算在投影层中进行。然而，随着序列长度的增加，活动到活动的矩阵乘法开始占主导地位，因为它们与序列长度的增长都呈二次方关系。

__端到端延迟__，图9展示了BERT-Base、BERT-Large和GPT-2在不同序列长度下的归一化延迟。显然，无论是在每个序列长度上，GPT-2的延迟都远远超过BERT-Base或BERT-Large的延迟，尽管BERT-Base和GPT-2在模型配置和端到端FLOPs上基本相同（如图4所示）。这主要是由于矩阵-向量操作的较低算术强度，这在图6中得到了突出展示。具有较高算术强度的模型可以在相同（甚至可能更多）的FLOPs下运行得更快，而具有较低算术强度的模型则更慢。这些观察结果证实了我们的发现，即解码推断是一个受限于内存而不是计算的问题。我们在第4.3.3节重新讨论这个问题，讨论一些现有的加速解码过程的方法。

### 总结（第2.2节 模型分析）

* 对于所有Transformer模型，由于act-to-act矩阵乘法的二次复杂度，FLOPs和归一化延迟都随着序列长度超线性增长。然而，对于较小的序列长度，这种趋势不太明显，因为整体计算的主要贡献者是FFN模块，其与序列长度线性相关，而不是MHA模块（图4和9）。
* 对于仅编码器模型，随着序列长度增加，算术强度首先增加，但对于较大的序列，它会减小，因为MHA模块（特别是具有较低算术强度的act-to-act矩阵乘法）成为总计算的主要贡献者（图6）。
* 仅解码器模型的算术强度明显低于仅编码器模型，导致相同序列长度的端到端延迟显著增加。这是因为解码器模型涉及到具有有限数据重用的矩阵-向量操作，使其受限于内存带宽而不是计算（图6和9）。
* 矩阵乘法在仅编码器模型和仅解码器模型中消耗了超过99％的FLOPs，而非线性操作在总体FLOPs中所占比例相对较小。然而，非线性操作的算术强度非常低，特别是对于大的序列长度，这是由于它们需要加载和存储大量激活值所致。

# 3 硬件设计

到目前为止，在第2节中，我们对Transformer架构的运行时特性和瓶颈进行了分析。现在我们将重点转向高效Transformer推理的全栈解决方案，首先是高效硬件的设计。第2.3节将讨论硬件设计的方法和技术，包括量化、低精度计算、专用加速器和内存优化等。然后，我们将在第3节中介绍一些优化技术，包括模型剪枝、稀疏性、分布式推理和并行计算等。最后，在第4节中，我们将探讨高效Transformer推理的软件优化策略，包括操作融合、内存优化、并发执行和量化感知训练等。通过综合应用这些硬件和软件优化技术，可以实现高效的Transformer推理，提高性能并减少资源消耗。

第3.1节概述了在DNN中使用领域特定加速器的理由，以及大多数DNN加速器中使用的基本架构和数据流。第3.2节重点介绍了在加速Transformer方面的现有工作。然后，第3.3节使用分析模型进行了分析，评估了Transformer在典型加速器上的运行情况。最后，第3.4节提供了一个案例研究，展示了构建一个典型Transformer加速器的过程。总体而言，本节提供了相关的性能分析，并从全栈的角度为所选硬件决策提供了理论依据。需要注意的是，本文只关注高效地推断DNN模型，对于设计用于高效模型训练的硬件不在本文的范围内。

## 3.1 典型的DNN加速器概述

典型的深度学习加速器通常包括几个关键组件，如[27]所述：

* 外部DRAM用于存储完整网络的权重和激活值，其容量需要足够大以容纳所有模型的权重和激活值。
* 较小的内部存储器，这里称为全局缓冲区，其容量需要足够大以容纳一部分权重和输入数据，以供处理元件（PEs）使用。
* 一组处理元件（PEs），每个PE都具有执行MAC（Multiply and Accumulate）操作的能力，并且通常包含一个或多个称为寄存器文件（RFs）的小型局部存储器，这些存储器的每次访问能耗较低于全局缓冲区。
* 一个内部网络互连（NoC），用于在处理元件之间传输数据。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature10.png)

__图10__ 展示了一个典型的DNN加速器的结构。全局缓冲区被设计为能够容纳足够数量的权重和激活值，以实现数据重用并限制与外部DRAM之间的传输次数。处理元件（PEs）中的局部存储器用于在可能的情况下提供局部数据重用，以减少对全局缓冲区的访问。在没有重用的情况下，MAC（Multiply and Accumulate）操作需要加载三个参数：要相乘的两个输入值以及当前部分和（即输出矩阵中给定位置的部分累积值），然后将输出值存回内存。这很重要，因为从能量的角度来看，内存读写的代价相比之下要高出数个数量级。例如，对于某一特定技术，从局部缓冲区读取的代价大约是一个MAC操作的6倍，而从外部DRAM读取的代价大约是200倍。因此，利用重用机会是至关重要的，以减少昂贵的内存访问次数。

为了最大化数据重用，有两种广泛采用的数据流类别，即时间和空间数据流[27, 38, 206]。时间数据流包含一组由中央控制的PEs，它们从全局缓冲区加载数据并执行请求的ALU（算术逻辑单元）操作，然后将数据写回全局缓冲区。这些PEs不包含局部存储器，也没有PE之间的通信或数据移动。在这种类型的数据流中，数据重用只能通过在全局缓冲区中重用权重或部分和来实现。时间数据流的示例包括单指令多数据（SIMD）和单指令多线程（SIMT）执行单元。这些类型的单元通常在CPU和GPU中用于向量处理。在时间架构中，全连接层和卷积层都被映射为矩阵乘法操作。

在空间数据流中，处理单元（PEs）可以进行通信和数据传输，以利用数据重用，而无需重复从全局缓冲区读取数据。这些PEs通常包含本地寄存器文件（RFs），用于在本地保存权重或部分和以提高数据重用，并且可以通过在相邻PE之间传递数据来实现额外的重用。空间数据流在基于FPGA和ASIC的加速器中广泛使用，特别适用于卷积网络[38]。这些数据流允许在多个维度上进行数据重用，从而大幅减少所需的内存访问。为了最大程度地在空间数据流中实现重用，采用了几种不同的重用方案：

* 权重定点数据流通过将权重保留在PE中的本地RFs中，并通过输入进行流式传输，从而最小化了权重矩阵所需的读取次数[72]；
* 输出定点数据流通过在PE的本地RFs中累积输出来最小化读取和写入部分和的能量消耗[59]；
* 无本地重用数据流中每个PE都没有RF，并利用没有RF的面积节省来分配更大的全局缓冲区[34]；
* 行定点数据流通过在一行PE中保持一行权重静止，流式传输输入，并流式传输部分和，最大程度地提高了部分和和权重的重用[35]。

需要注意的是，由于DNN由一系列层组成，因此还可以融合操作以进一步利用跨多个层的数据重用。我们建议读者参考[206]的第V节，[49]的第IV-A和IV-B节，以及[27]的第III-A到III-C节，以获得有关DNN加速器的基本架构和典型加速器数据流的更全面的调查和比较。

__总结3.1节__ 典型的深度学习加速器包括用于存储部分模型权重和输入的芯片内存，以及能够执行MAC（Multiply and Accumulate）运算的处理元素（PEs）阵列。外部DRAM用于存储完整网络的权重和激活值，而内部的片上网络（NoC）可用于在PE之间传输数据。深度学习加速器通常旨在利用时间数据流或空间数据流中的一种，以实现数据重用。空间数据流重用方案包括权重固定数据流，它将权重存储在PE的本地存储器中以提高重用性。

## 3.2 为Transform适配DNN加速器

在设计适用于Transformer的DNN加速器或调整现有的CNN加速器时，有几个关键考虑因素。

首先，CNN和Transformer之间的一个区别是由于算术强度和矩阵维度的差异，这些模型在每个内存层次的最佳大小以及内存带宽需求方面也不同。因此，在设计DNN加速器时需要考虑适应Transformer模型的内存层次结构和带宽要求。

另一个考虑因素是在推断过程中如何计算非线性函数，这对硬件设计提出了额外的挑战。这些操作要么需要专门的芯片内计算支持，要么需要转移到CPU上执行。在第3.4节中，论文将概述非线性操作可能对推断产生瓶颈的情况，尽管它们在模型的FLOPs中占比较小的比例，特别是如果它们必须转移到CPU上执行。一些用于Transformer推断的加速器包含用于非线性函数的专门后处理单元。然而，添加支持这些操作的额外单元也会增加加速器的面积。在支持这些操作的芯片内执行和加速器面积之间存在权衡，在第3.4节中将进一步探讨这个问题。此外，设计硬件既要高效地支持所需的非线性操作（如Softmax和LayerNorm），还要支持未来DNN中的新非线性操作，这也是具有挑战性的。

还有一些关于数据路径设计的考虑因素，取决于加速器是针对MHA模块还是针对端到端Transformer推断进行设计。专为MHA模块设计的加速器旨在匹配MHA模块的数据流，其中所有操作被"融合"在一起，从而具有较少的灵活性，但通过减少所需的内存访问次数来提高性能[64, 79, 80, 223, 237, 253]。操作融合是指直接使用一个操作（如矩阵乘法）的输出值作为后续操作（如Softmax层）的输入，而不将中间值写入芯片外存储器。针对MHA模块的几个加速器开发了专用的数据路径，其中分别针对查询×键、Softmax和注意力得分×值操作提供了单独的单元，以更好地利用操作级别的融合。相比之下，用于端到端Transformer推断的加速器通常采用与Gemmini [70]类似的结构（在第3.4节中有更详细的介绍），设计上更加灵活，通过在更通用的矩阵乘法引擎中单独执行各个操作。这些加速器也在尽可能地融合操作以提高性能（例如，在写出之前直接对矩阵乘法的累积输出应用Softmax）。然而，整个图级别的数据流并不像MHA专用加速器那样在硬件中硬编码。

在两种情况下，还需要考虑非线性函数单元的数据流放置。这是因为正如我们在2.2节中所示，非线性操作通常具有较高的MOP数，尽管其FLOPs计数较小，因此通过操作融合（如ResNet50的情况）可以改善整体的算术强度。对于针对MHA模块的加速器，为了利用MHA模块中的操作级融合，必须适当地放置Softmax单元，使其可以在查询×键乘法之后和注意力得分×值乘法之前进行计算。例如，[64]将Softmax单元放置在查询×键和注意力得分×值乘法的专用单元之间，并在单独的硬件模块中计算LayerNorm。将功能单元放置以支持操作融合提供了更高的效率，但代价是较少的灵活性，因为架构现在对操作级的数据流做出了假设。

__总结__ (Sec. 3.2. 适应Transformer的DNN加速器)：Transformer和CNN的加速器在内存层次结构的最佳尺寸和内存带宽要求方面存在差异。用于MHA模块的加速器往往设计了硬件化的数据路径以利用操作融合。而端到端的Transformer加速器通常不会围绕MHA模块中的图级数据流设计其数据路径。Transformer加速器往往会加入后处理单元，以在芯片上高效计算非线性函数。

## 3.3 分析建模

分析建模是在推断DNN基准测试时识别瓶颈的有用工具，它可以快速估计在目标硬件平台上的运行时行为。在设计阶段，分析基准工作负载的运行时行为以及硬件架构变化对性能的潜在影响通常很困难。这与第2.2.2节中在实际硬件上（例如，CPU）上进行的性能分析不同。在性能分析困难或不可行的情况下，分析建模可以提供估计值，快速指导设计决策。

在这里，我们开发了一个分析模型，以展示它在理解Transformer推理在硬件加速器上的性能分解中的用处。我们的分析模型基于Gemmini驱动的架构[70]，将在第3.4.1节中详细介绍。其结构如图11所示，并包括可调参数。该模型包括局部存储器、用于计算瓦片矩阵乘法的PE阵列，并依赖外部存储器来存储所有模型参数。性能估计假设计算时间和存储器操作时间可以完全重叠，并且每个操作的总时间是两者中的最大值。请注意，假设在临时存储器中使用双缓冲，以确保计算与内存读写尽可能重叠。模型结构类似于典型的DNN加速器，但需要特别注意的是，模型中包含的特殊功能单元（SFU）能够计算所有所需的非线性操作，因此没有必要在外部计算这些操作。模型还假设PE阵列的延迟为𝑊个周期，其中𝑊是PE阵列的宽度，并且SFU每个向量的延迟为1个周期。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature10_11.png)

__延迟分解和端到端延迟__（Latency Breakdown and End-to-end Latency）：分析建模的一个有用场景是获取估计的延迟分解和端到端运行时延迟。以BERT-Base和BERT-Large编码器以及GPT-2解码器为例，我们应用分析建模，在假设所有矩阵操作均采用方形切片且没有操作融合的情况下进行分析（即，每个操作都需要从外部存储器读取输入并将输出刷新出来）。在附录A.5中，我们提供了BERT-Base和GPT-2的延迟分解（分别为图30和图31）以及所有模型在不同序列长度下的端到端运行时延迟（图32）。总体而言，分析模型的结果显示与第2.2.2节中在CPU上进行的性能分析结果相似的运行时延迟缩放和分解趋势，只是细节上略有差异。请注意，分析模型的设计是假设硬件架构与CPU架构不同，因此对于不同的硬件平台，运行时行为不一定相同。附录A.5中还提供了与ResNet50上的分析建模结果的比较，更多细节可在附录中找到。

__非理想算术强度__（Non-ideal Arithmetic Intensity）：与第2.2节中的分析类似，算术强度在理想情况下提供了不同操作的数据重用程度的粗略估计。然而，在现实世界的场景中，例如由于矩阵大小超过本地缓存容量而需要进行分块操作时，算术强度将进一步降低。在这种情况下，分析建模可以通过考虑硬件细节提供更准确的估计，即非理想算术强度。为了考虑分块效应，我们在分析建模中计算了从DRAM到L2内存的流量，但不计算从L2到Scratchpad的流量，以避免重复计算。此外，我们假设在非线性操作之前有32位的输出精度，因为已知对这些操作的低输入精度（例如8位）可能会导致相当大的精度降低[111]。对于BERT-Base编码器在序列长度为128、512和4096时的不同操作，我们提供了非理想算术强度的数据，详见表5。

与我们在第2.2.1节中讨论的理想算术强度（图6）相比，即序列长度为每个128、512和4096分别为160、231和118，我们观察到非理想算术强度显著降低。这是由于分块效应以及在非线性操作之前必须加载和存储的大型32位输出激活值所致。随着序列长度的增加，差距变得更大（序列长度4096的降低幅度高达2.5倍），其中加载和存储中间值的影响更加明显。这与ResNet50的情况不同，其非理想算术强度为121.312，与理想算术强度122.172的差异不大。这也表明，尽管Transformers的理想算术强度通常高于ResNet50，但在所有序列长度下，Transformers的实际算术强度却较低。

|Sequence Length|𝑊𝑄 𝑊𝐾 𝑊𝑉 projections|Q × K|Attn. score × V|𝑊out projection|𝑊1 projection|𝑊2 projection|Total|
|-|-|-|-|-|-|-|-|-|
| 128|170.670| 25.400|  63.750| 128.000| 130.723| 186.182| 106.110|
| 512|341.333| 29.882| 102.300| 204.800| 211.862| 409.6  | 111.122|
|4096|409.6  | 30.788| 118.710| 227.556| 236.308| 512.000|  47.067|

__表5__: BERT-Base编码器在序列长度为128、512和4096个标记的情况下的非理想算术强度。由于在非线性操作之前使用了32位的输出精度，并且受到内存大小的限制，因此非理想算术强度低于理想算术强度（在表3中提供）。请注意，具有相同操作维度的𝑊𝑄，𝑊𝐾，𝑊𝑉投影和𝑊out投影之间的非理想算术强度差异是由于输出精度的差异 - 𝑊out之后是非线性操作，因此它使用32位而不是8位。

摘要（第3.3节 分析建模）：分析建模是在目标硬件平台上识别深度神经网络推断的瓶颈和运行特性的有用工具。在设计阶段，实际硬件上的性能分析可能很困难，但分析是必要的，以便进行设计决策，因此这种技术尤其有用。我们提供了使用分析建模来获得延迟分解和非理想算术强度的示例。具体而言，我们证明了当考虑到硬件限制和实施细节时，与理想情况相比，Transformer的非理想算术强度可以进一步降低（最高可降低2.5倍）。

## 3.3 Case Study: 构建Transformer加速器

现在，我们将通过一个更加实际的例子来说明熟悉用于卷积、基于视觉的工作负载的主流加速器的架构师如何设计最先进的Transformer加速器。尽管第3.3节中的分析模型提供了Transformer推理的理想延迟和运行时预测，但在实际的硬件加速器中达到理想的性能和效率可能需要相当大的工程努力，我们在这里进行探讨。我们从一个由Gemmini [70]加速器生成器生成的相当典型的CNN加速器开始，该加速器主要针对类似ResNet50的工作负载进行了优化，然后我们讨论了我们对该加速器及其软件堆栈所进行的更改，以有效地支持诸如BERT之类的Transformer工作负载。一些用于端到端Transformer推理的加速器采用了与Gemmini和我们的分析模型类似的结构，并且还包含用于非线性函数的专用后处理单元[107, 148, 166, 209]。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature12_13.png)

### 3.4.1 Baseline Accelerator

我们首先使用Gemmini加速器生成器生成一个相当典型的CNN加速器，如图12所示。该加速器使用一个16×16的同步阵列执行矩阵乘法，采用了权重固定的数据流。在执行卷积操作时，输出通道和输入通道的维度会进行空间展开。8位整数权重和输入存储在256 kB的本地划分存储器中，32位的部分和存储在双口64 kB的累加器SRAM中，用于执行矩阵相加。当DNN层的大小超过本地划分存储器的容量时，它们将使用与CPU和其他加速器共享的外部L2缓存和DRAM。主机CPU将这些层进行切片以计算完整的输出。

虽然大多数CNN的FLOP用于计算矩阵乘法或卷积，我们的基准Gemmini生成的加速器还包含外围电路来执行ReLU和最大池化操作，以及整数-浮点乘法器，用于将32位部分和缩放为8位输入，以便在CNN的下一层中使用。对这些操作的原生支持非常重要，因为它消除了在DRAM或外部缓存（其中CPU可以执行这些操作）和本地划分存储器（Gemmini存储其矩阵操作数的位置）之间昂贵的数据传输的需求。

最后，需要注意的是，这个基准CNN加速器不包含任何特定于Transformer的功能。特别是，它不支持非线性归一化操作，如LayerNorm或Softmax。也没有支持GELU，它是一种相对昂贵的非线性激活函数，通常使用昂贵的查找表实现。相反，这个基准设计是为量化整数CNN推断而设计和优化的典型示例。它在端到端的CNN工作负载，如ResNet50 [82]、SqueezeNet [93]或MobileNetV2 [188]上实现了实时或接近实时的性能，但在诸如BERT之类的Transformer工作负载上，性能受限，因为需要在CPU上执行诸如GELU、LayerNorm和Softmax等操作。

### 3.4.2 Performance Bottlenecks

我们的基准CNN加速器在执行BERT推理时，其功能单元的利用率远低于1%。虽然单个矩阵乘法（matmul）的利用率达到了74%，但加速器不原生支持的操作，如LayerNorm，由于必须由CPU执行，会显著降低性能。实际上，图14显示96%的执行时间花在了非矩阵乘法操作上。需要注意的是，在我们的Transformer推理中，超过99%的FLOP是用于矩阵乘法的MAC运算，因此除非进行进一步的优化，否则基准加速器的每个操作的运行时间与理论上的理想情况相差很远。

此外，我们的基准加速器将GELU和Softmax操作卸载到主机CPU上，使用浮点单元执行这些操作。如图15所示，浮点加法器或乘法器的能耗比整数相应部件高出数个数量级。在我们的基准CNN加速器中，矩阵乘法使用INT8输入进行计算，但为了在CPU上执行浮点激活函数，必须在矩阵乘法操作之间进行去量化和重新量化操作，进一步增加了能耗和延迟开销。

最后，专用硬件加速器的内存层次结构通常需要根据正在运行的工作负载进行精心调整。CNN主要执行卷积操作，其算术强度非常高，而Transformer主要执行矩阵乘法操作，通常使用较小和/或矩形矩阵，其算术强度显著较低且具有不同的最佳划分策略。例如，在表3中观察到MHA模块的低算术强度。这表明我们基准CNN加速器的内存层次结构和内存带宽应该重新调整，以实现更高效的Transformer推理。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature14.png)

### 3.4.3 Memory Hierarchy

Transformer的矩阵乘法操作（尤其是激活到激活的矩阵乘法）与CNN中的卷积层具有非常不同的形状和算术强度，如表3和表4所示。如图13所示，仅仅调整输入/权重缓冲区和32位部分累加器的大小就可以显著提高BERT的矩阵乘法操作的性能。较大的累加器可以实现更高的输出重用，这对于Transformer中的一些矩阵乘法操作更为适用。特别是查询-键（query × key）矩阵乘法操作具有𝑙 × 𝑙的输出激活矩阵，对于较长的序列长度，其大小要比𝑙 × 𝑑/ℎ的输入查询和键矩阵大得多。增加累加器的缓冲区大小可以改善这些操作的输出重用效果。

基于这一观察，我们将基准加速器的共享输入/权重缓冲区的大小从256 kB减小到64 kB，并将部分和累加器的大小从64 kB增加到256 kB。这不会增加总的SRAM容量，几乎不会改变加速器的总面积。然而，这些变化使总的矩阵乘法延迟大幅减少了36%。

### 3.4.4 Hardware-Software Co-Design

如3.3节所述，矩阵乘法是Transformer工作负载中的主要核心算子，但即使在优化基准CNN加速器的矩阵乘法性能后，其利用率仍未超过1%。这是由于CPU卸载的非线性操作的开销。图14显示，实际上只有1%的时间用于矩阵乘法，其余时间用于浮点非线性激活、归一化以及量化和去量化操作，因为它们被卸载到了CPU上。

为了减轻运行时量化和去量化的开销，我们将基准Transformer工作负载从简单的BERT实现切换到了一种名为I-BERT的仅使用整数的变体 [111]。有关量化和I-BERT的更多细节将在4.1节和4.3节中进行详细介绍，但I-BERT的主要思想是使用整数多项式逼近替换浮点非线性操作（如GELU和Softmax），以使它们在专用硬件加速器中实现更快、更廉价。

为了融入I-BERT，我们在基准CNN加速器中添加了新的整数实现，用于I-BERT的GELU、LayerNorm和Softmax变体。累加器中的32位矩阵乘法结果被送入新添加的“归一化单元”，该单元计算和减少操作，包括求和、平方和、最大值等，这些操作被LayerNorm和Softmax使用。需要多次读取累加器才能计算出这些操作中的所有减少结果。例如，在计算方差之前，首先计算总和。然后，最后一次读取累加器中的矩阵乘法结果，供一组16个激活单元并行计算I-BERT的GELU、LayerNorm或Softmax变体。

有了这些新功能，相对于基准加速器的初始性能，整体的端到端BERT推断性能提高了39.6倍。正如图14所示，计算瓶颈再次变成了矩阵乘法，而不是归一化或激活函数；而且这种趋势在不同的序列长度下都持续存在。由于非线性浮点操作被I-BERT的整数多项式逼近所取代，因此量化和去量化不再必要。此外，GELU操作现在可以轻松与前面的矩阵乘法合并，从而成为一个流水线操作。使用ASAP7 PDK [45]进行合成时，新的硬件单元仅增加了加速器的

总面积消耗14%，而GELU、LayerNorm和Softmax操作仅增加了BERT推断的能量消耗9.3%。

总结起来，如3.3节所示，在理想情况下，非线性操作对Transformer加速器的总FLOPs、面积或功耗并不会有很大的增加。然而，在实际情况下可能不是这样，特别是如果计算被卸载到CPU上，会导致非常重要的延迟影响。我们证明了使用I-BERT对LayerNorm、Softmax和GELU的实现可以解决这个问题，这只会将Transformer加速器的面积增加5-15%，并增加总延迟8%。

基于CNN架构的基准Gemmini加速器在运行Transformer时遇到了性能问题，原因如下：

1. 在CPU上运行的非矩阵乘操作占总运行时间的96%，为加速器创建了瓶颈。
2. 浮点数激活函数需要重复的量化和反量化操作，增加了延迟和能量消耗。
3. Transformer推理的低算术强度特性使其对非优化的内存层次结构更加敏感。

为了解决这些问题，进行了以下改进：

1. 减少了scratchpad容量，并增加了累加器大小。这个调整提高了输出的重用性和内存效率。
2. 基准加速器切换到了I-BERT，即BERT的整数版本，用于近似浮点数激活。这个改变消除了与非线性操作相关的量化开销。
3. 添加了特殊的规范化单元和激活单元，用于将GELU、LayerNorm和Softmax计算从CPU中卸载。

这些改进有效地缓解了由非矩阵乘操作引起的瓶颈，并实现了显著的39.6倍性能提升。需要注意的是，虽然Transformer中的非线性操作对于FLOPs、面积或功耗的贡献可能不大，但它们对延迟仍然会产生明显的影响。

# 4 模型优化

对于已经设计和训练好的DNN模型来说，一个重要的问题是是否仍然有可能通过将模型调整为更适合硬件平台的格式来在算法上提高模型的效率。在本节中，我们将讨论流行的现成模型优化方法，即量化和稀疏化（剪枝），分别在第4.1节和第4.2节中。然后，在第4.3节中，我们概述了针对Transformer特定的优化方法，以提高Transformer特有特性（如注意力和非线性操作）的性能。

## 4.1 Quantization

DNN模型通常使用高精度浮点计算进行训练。然而，在推断阶段通常不需要高精度算术。量化是一种通过使用较低位数的表示（通常是固定点表示，如8位整数(INT8)）来压缩DNN模型的过程，而不是使用32位或16位浮点表示（FP32或FP16）。


![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature15.png)

量化为高效的DNN推断提供了多种好处。降低精度的一个明显优势是减少内存消耗。例如，将模型权重从FP32量化为INT8可以使模型大小减小4倍。这样可以减少离片存储和带宽，而无需对DNN加速器进行任何修改。此外，量化激活还可以减少中间部分结果的内存流量和存储需求。
内存层次结构也可以根据精度差异进行重构，可以通过存储更多的参数（因为现在每个参数消耗的存储空间更少）来实现更大的本地重用，或者使用更小的内部缓冲区同时保持相同数量的本地数据重用。
量化模型权重和激活的第二个优势是ALU（算术逻辑单元）和相应PE（处理单元）的尺寸、延迟和能量消耗的减小。一般而言，浮点ALU在面积、延迟和能量消耗方面比整数ALU效率低下。这是因为浮点PE需要乘法运算，添加指数，并使用指数进行左移操作，以获得单个乘法运算的最终结果，而定点PE只需要一个乘法单元。因此，现代GPU和TPU通常包含INT8处理路径[43, 100]，可以从量化中获得显著的好处。例如，如图15所示，与FP32相比，执行INT8加法的能效可以提高约30倍，面积效率提高约120倍。

量化的另一个关键应用是在只支持整数计算的硬件上部署模型。一些用于低成本和功耗高效的嵌入式设备（例如ARM Cortex-M内核[12]和GAP-8[66]）的边缘处理器不包含专用的浮点单元。在这些处理器上部署模型时，不仅必须对模型权重和激活进行量化，而且所有计算都必须只使用整数算术进行。否则，部署将不可能或由于需要在离片处理非整数操作而导致显着的开销。这将导致数据传输到通用主机处理器的额外延迟和能量消耗。将整个推断过程使用整数算术进行的量化技术称为仅整数量化[97, 110, 111, 132, 241]。我们在第3.4.4节中已经讨论过，仅整数量化使Gemmini的端到端推断延迟降低了39.6倍。

量化方法可以广泛分为均匀量化和非均匀量化，取决于它们如何映射值。均匀量化将浮点域分割为均匀间隔的区间，并将每个区间映射到单个固定点值。这可以通过简单的算术规则获得：

$$
𝑄(𝑟) = Int(𝑟/𝑠) + 𝑍
$$

量化是一种在DNN推断中提高效率的重要技术。其中，𝑄代表量化操作，𝑟代表浮点数值，𝑆是缩放因子，𝑍是偏移因子。非均匀量化不要求间隔均匀分布。通过将更多的量化区间分配给重要区域，通常可以实现更好的压缩率，非均匀量化可以更准确地捕捉浮点域中的原始数据分布。然而，在通用计算硬件上高效部署非均匀量化模型通常更具挑战性。因此，由于其简单性和与硬件的有效映射，目前均匀量化是主流方法。

尽管较低位量化可以实现更好的压缩率，但过度降低精度可能会显著降低模型的准确性。因此，在减少精度以提高性能的同时，保持模型准确性之间的平衡至关重要。缓解这个问题的一种有前景的策略是混合精度量化。根据以往的研究[55、195、224、229]，模型中的不同层对量化具有不同的敏感性，因此将较高位精度分配给更敏感的层是至关重要的。在使用混合精度对Transformer进行量化方面，一些值得注意的工作包括使用Hessian信息（即曲率）作为敏感性的代理的Q-BERT [195]，以及应用强化学习（RL）学习每层适当位精度的HAT [222]。

对预训练的Transformer模型进行量化时的另一个挑战是激活值中的异常值[117]。均匀量化试图将从最小可能值到最大可能值的范围划分为多个区间，但这可能导致性能显著降低。这是因为更多的值被映射到相同的量化值（即分辨率降低），这是由于异常值扩展了每个量化区间的范围。非均匀量化可以解决异常值问题[246]，但也提出了一种均匀量化方案，将较大的位精度分配给包含异常值的激活值[51]。此外，最近引入的FP8精度[156]提供了在设置指数位精度时的额外自由度，因此被发现是一种适用于具有异常值的模型量化的合适解决方案[121]。有关此主题的更多信息，请参阅[27]的III-F部分，[49]的IV-C-3部分，[13]的3.1部分，以及[71]进行更全面的软件级方法调查。

__总结__(第4.1节：量化)：量化是通过降低模型参数和/或激活的精度来压缩DNN模型的一种方法。量化的直接好处是减少内存消耗，从而实现减少芯片外存储和带宽，并设计更高效的内存层次结构。此外，通过低位精度算术，量化可以减少ALU和相应PE的大小、延迟和能量消耗。在某些情况下，量化还可以使DNN模型能够部署在仅支持整数运算的硬件单元中，否则可能无法实现或需要在芯片外处理非整数运算，导致额外的开销。尽管许多DNN模型对一定程度的量化噪声具有鲁棒性，但在更低位精度（例如INT4甚至更低）下，需要特定的算法层面的改进来防止准确性降低。特别是对于无需准确性降低的预训练Transformer进行量化，必须进行特殊考虑，因为它们已知具有异常激活值。

## 4.2 稀疏（Sparsity）

稀疏性（又称为剪枝）是减少深度学习推理所需计算量的另一种常见方法。通过引入稀疏性，可以使深度神经网络模型变得稀疏，即去除冗余/不敏感的参数。研究表明，在成功训练模型之后，可以删除许多参数而不会导致质量下降。通过训练大型模型，然后通过剪枝压缩模型，可以实现比从头开始训练压缩模型更好的精度[133]。这可能是因为训练开始时具有冗余参数可能使损失函数的优化更加容易[139]，或者与获得“中彩票”的可能性增加有关[67]。

从广义上讲，剪枝可以分为两个分支：非结构化剪枝和结构化剪枝。非结构化剪枝允许对参数和特征图进行任意模式的稀疏化。理论上，它可以在不降低精度的情况下实现显著的计算节约[136]。然而，非结构化剪枝在硬件上的有效利用可能具有挑战性。为了有效地存储数据而不存储空（即零）参数，需要使用压缩的存储格式。此外，计算单元必须进行调整，以便能够直接处理压缩的数据。否则，在计算之前必须对参数进行解压缩，然后在计算之后重新压缩，这会导致额外的开销。因此，普通的深度神经网络加速器可能无法有效地利用非结构化稀疏性模式。

结构化剪枝通过严格地移除结构化参数集来避免这些限制。例如，在Transformer中，可以结构性剪枝线性层、注意力头[155]甚至整个层[63, 186]。最近的研究进一步将不同架构组件的结构化剪枝集成到单个框架中（例如，在MHA模块中剪枝注意力头，在FFN模块中剪枝过滤器）[88, 123, 145, 233]。这种结构化剪枝方法可以立即得到比原始模型更小的稠密矩阵乘积，无需压缩的存储格式或特殊的硬件支持，即可实现内存减少和延迟改善。然而，与非结构化剪枝相比，压缩率可能不够好。已经证明，最先进的非结构化剪枝方法[120]可以在BERT[52]中剪枝多达90%的参数，而在MNLI基准测试[226]上没有性能下降，而要达到相同的性能，则需要最先进的结构化剪枝方法[233]来剪枝多达70%的参数。

尽管前面提到的剪枝方法属于权重剪枝，但激活剪枝（即动态剪枝）也可以应用于在运行时动态检测和清零不重要的激活。在Transformer推理中，激活剪枝的一种流行分支是标记剪枝[74, 108, 113, 150, 223]，它在每个Transformer层中检测并丢弃不太重要的标记，以与推理的其余部分区分开。其基本原理是并非所有标记（例如，NLP任务中的单词）都是理解输入序列含义所必需的。通过减少Transformer需要处理的序列长度，这些方法已经证明在NLP基准测试[181, 220]中可以将所需的总计算量减少约30-50%，而不会明显降低准确性。

然而，加速此类动态稀疏模式可能是一项挑战，因为它需要检测逻辑来实时确定非零元素的位置。因此，在许多情况下，动态稀疏性需要同时设计算法和硬件。

无论使用哪种剪枝方法，主要关注的问题是确定哪些权重应该保留，哪些应该移除，以提高神经网络的效率而不损害其性能。剪枝Transformer的常见方法包括以下内容：

* Magnitude pruning [68]是一种技术，它使用每个权重的绝对值作为其重要性的代理。它在训练过程中剪枝具有最小幅度的权重。这种方法的理论基础是较小的权重对模型的最终结果贡献较小。
* Movement pruning [124, 189]是一种技术，它考虑了在微调过程中权重的变化，并为在微调过程中远离零点的权重分配更大的重要性分数。随着微调过程的进行，该技术将更大的重要性分数分配给权重发生较大变化的权重。相对于仅使用权重大小的剪枝方法，该技术在使用预训练和微调方案（例如BERT [52]）训练的模型上被发现更加有效，因为它更好地捕捉了在微调过程中权重的重要性。
* First-order pruning [155]使用相对于损失的梯度流入权重或一组权重作为评估模型准确性重要性的代理。该方法将梯度视为将参数置零对损失的影响的指示器。该方案进一步改进[88]，使用权重大小和梯度的乘积作为重要性的代理，因为它可能更准确地估计了将权重置零对模型的影响。
* Second-order pruning [120, 123, 245]使用相对于损失的权重或一组权重的海森矩阵作为重要性度量的代理。与一阶信息相比，二阶信息通常被认为更准确地指示了移除权重的效果。然而，由于海森矩阵的规模很大，与权重数量呈二次增长，因此需要采用适当且可扩展的近似方法，通常使用随机数值线性代数中的算法[57, 242]。

修剪的主要优点之一是减少内存占用。通过结构化修剪，内存效率的提升是直接的，它直接减少了矩阵乘法的大小和/或数量。相比之下，非结构化修剪通常需要使用稀疏编码（也称为稀疏存储格式）来压缩和存储稀疏数据。这些方法通过使用元数据来编码矩阵中非零条目的位置来减少内存使用量。稀疏编码可以降低芯片外存储器的消耗以及相应的内存访问量。它们还可以减小芯片上所需的存储空间，从而允许更小的缓冲区或者增加重用。这是因为，尽管相同数量的数据可以存储在缓冲区中，但编码数据对应于完整输入张量的更大比例。

修剪还可以通过消除不必要的计算来降低能量消耗和延迟。与之前所描述的情况类似，通过结构化修剪相对容易实现这一点，但非结构化修剪需要特殊技术来识别和绕过涉及空元素的计算。这可以涉及识别和跳过单个元素或整个空向量。一些检测和跳过方法通过不执行涉及空元素的操作来节省能量。也就是说，处理元素不必用于空计算，从而避免能量消耗。其他方法通过为跳过的处理元素分配不同的有效计算来进一步降低延迟，而不是让它们在无效的计算周期中空闲。此外，为了在非结构化稀疏矩阵乘法中保持处理元素的利用率，可能还需要进行负载均衡。由于零的分布可能在处理元素之间不平衡，一些处理元素可能需要比其他处理元素更长的执行时间，导致其他处理元素的空闲等待时间。一些工作已经使用负载均衡来加速具有非结构化稀疏性的神经网络。

我们建议对稀疏编码方法更详细的概述感兴趣的读者参考[47]的第V节和[27]的第III节。此外，[47]提供了对利用各种稀疏模式的硬件架构的总体概述。

__总结__（第4.2节 稀疏性）：稀疏性（或剪枝）是另一种广泛使用的方法，通过去除冗余或不重要的权重和激活，来减少超参数化DNN模型的推理成本。与量化类似，剪枝有助于减少片外存储器消耗和相应的内存流量，以及能量消耗和延迟。剪枝可以广泛分为权重剪枝和激活剪枝。权重剪枝又可以进一步分为无结构剪枝（允许任意稀疏模式）和结构剪枝（对稀疏模式施加额外约束）。虽然结构剪枝可以在内存、能量消耗和延迟方面带来好处，而无需额外的硬件支持，但它的压缩率通常较低于无结构剪枝。激活剪枝在推理过程中修剪冗余的激活，对Transformer模型尤为有效。然而，这需要在运行时支持动态检测和清零不重要的激活。

## 4.3 Transformer特定的优化方法

使用量化和剪枝等现成的优化方法可以带来显著的性能优势。然而，还有其他针对Transformer架构特定定制的优化策略，通过充分利用Transformer内部的特性来进一步优化Transformer的推断过程。在这里，我们将回顾一些重要的Transformer特定优化技术，以进一步优化Transformer的推断性能。

### 4.3.1 加速注意力机制

许多工作旨在优化MHA模块中的注意力机制。回想一下，在长序列中，MHA模块中的矩阵乘法所需的时间随序列长度的增加呈二次增长，正如在第2.2.2节中所述。因此，对于长序列，计算注意力成为总体运行时间的主要部分。更高效地计算注意力网络的一种常见方法是令牌剪枝。这涉及移除不重要的令牌，以减小有效的序列长度，正如在第4.2节中讨论的那样。在实时识别和丢弃不重要的令牌方面，已经提出了几种硬件-软件协同设计的方法。在SpAtten [223]中，令牌根据其在输入句子中受到其他令牌关注的程度进行排名，被关注较少的令牌被剪枝。该方法的基本理念是，一个词被关注得越多，在推断过程中它就越重要。为了实现高效，使用了一个顶部𝑘的硬件引擎，根据它们的注意力分数过滤掉低重要性的令牌。DTA-Trans [237]进一步引入了两层方案，第一轮确定应该剪枝哪些令牌，第二轮根据其重要性进一步确定每个剩余令牌的位精度分配。
加速注意力的另一种方法是利用注意力得分激活的动态稀疏模式 [79, 80, 131, 147, 172, 194, 204, 253]。合理的假设是，许多查询令牌和键令牌的组合在语义上是没有意义的，因此与此组合相关的注意力得分将接近于零。通过利用这种稀疏性，可以在避免关联的act-to-act matmuls（即查询×键或注意力得分×值）的同时保持推断准确性，从而减少计算成本。然而，这需要专门的硬件逻辑来实时检测和加速这些动态稀疏模式。例如，在ELSA [80]中，提出了一种逼近键和查询向量之间的角度相似性的数据路径，从而可以预测它们的点积是否可能为零。该方法能够提前剪枝与给定查询向量相关性较低的关键向量。Sanger框架 [147]建议在计算注意力得分之前对查询和键值进行量化，因为这

将使得注意力得分的不重要条目为零，如果这些值没有被量化的话，它们本来会接近于零。类似地，DOTA [172]提出利用低秩（因此更小）的查询和键值的矩阵乘法作为代理，来近似将注意力得分的条目置零。LeOPArd [131]在查询×键乘法中使用位串行计算，以便在不达到预定阈值时提前终止计算。
值得注意的是，加速注意力机制需要硬件支持，因为它使得像顶部𝑘 [223, 237]、角度逼近 [80]、聚类 [204]和多精度计算 [13, 147, 172, 253] 这样的操作成为可能，这些操作对于检测注意力得分的动态稀疏模式是必要的。此外，还需要专门的硬件支持来利用（主要是非结构化的）动态稀疏性以跳过计算。例如，Sanger [147]通过分割和打包使用负载重平衡，配备了一个自定义数据路径，同时提供对采样稠密矩阵乘法和稀疏矩阵乘法的支持。

### 4.3.2 非线性操作

正如在2.1.1节中讨论的，Transformer架构包含多个非线性函数，在高效硬件设计中引入专门用于计算这些操作的硬件模块可能是一个可行的解决方案。然而，对于硬件设计来说，这可能会带来相当大的开销，尤其是针对低端设备。因此，已经提出了各种解决方案来避免构建专用的硬件模块来解决这个问题。一个常见的解决方案是函数逼近 [107, 111, 132, 203, 223]，它旨在近似非线性函数的精确值，以获得既好又计算效率高的近似值。例如，Keller等人 [107]使用Softermax [203]函数，该函数使用基于2的逼近，将Softmax操作中指数计算的基数从𝑒改为2，从而简化了硬件实现。Softermax [203]还采用了在线归一化 [157]，从而将数值稳定的Softmax计算所需的传递次数从3减少到2。

I-BERT [111]提供了一个更通用的逼近算法，用二阶多项式逼近非线性操作。这不仅简化了操作，还使得它们可以只使用整数算术进行计算。SpAtten [223]采用了类似的方法，使用了一个五阶泰勒级数逼近来计算Softmax，如[160]中所述。I-ViT [132]进一步扩展了这个思路，使用硬件友好的位移操作来高效计算视觉Transformer推断中的非线性操作。虽然主要关注的是Softmax的指数运算的逼近，但其他工作 [19, 209, 225] 也利用了log sum-exp技巧来避免除法操作，这是另一个在硬件中实现可能复杂的操作 [60]。

另一种广泛采用的方法是查找表，它存储给定输入范围的预计算输出值。在这种情况下，如果给定输入，就会输出存储在查找表中对应的值，从而无需评估函数。使用查找表加速非线性函数的方法并不是一个新概念，其起源可以追溯到Transformer或DNN架构出现之前 [50, 212]。因此，最近的方法更加关注减小查找表的大小以节省面积和延迟。例如，𝐴 3 [79]将指数运算分解为两个较小精度的指数运算的乘积，从而用两个较小的查找表替换

一个较大的查找表。NN-LUT [244]使用一个单隐藏层网络来近似非线性操作，并将网络的数值近似存储在查找表中，从而避免了执行网络的需求。

### 4.3.3 解码加速

如2.2节所讨论的，由于低硬件利用率和算术强度，Transformer的生成推断解码过程可能导致显著的推断延迟。由于近年来大型语言模型 [2, 22, 44, 213] 的发展引起了对生成任务的日益关注，因此优化解码过程的延迟至关重要。减少推断延迟的一种方法是通过提前退出来跳过不必要的计算。这种方法通过在中间层终止推断并使用中间隐藏状态进行预测，而不是等到最后一层，动态地调整每个标记生成的解码器的深度。虽然这是一种在编码器模型中广为研究的技术 [192, 234]，但CALM [191]最近才将这种方法扩展到解码器模型。在解码任务中的一个主要挑战是，与编码任务不同，一个标记的生成依赖于所有先前标记的激活状态，这是由于注意力机制所致。如果一个先前的标记被提前退出，那么跳过的层就没有可以参考的内容。为了解决这个问题，CALM提出了“状态传播”，它将退出之前的最后一层的激活状态复制到所有跳过的层。这对生成质量的影响很小。另一个最近的尝试是协同使用多个不同大小的模型 [31, 112]。其基本动机是，大多数简单单词的生成可以由更快、更不精确的小型模型来完成。偶尔，当小型模型无法准确预测一个单词时，它会切换到更大的模型进行更准确的预测。这种方法不仅使大模型的执行频率降低，还使其能够进行非自回归（即标记级并行）执行，因为它可以接收从小型模型生成的所有标记并并行处理它们，从而更有效地利用硬件资源。Big Little Decoder [112]在各种模型和生成任务中展示了约2倍的推断延迟缩短，而不损害生成质量。

### 4.3.4 选择使用哪种优化方法

到目前为止，我们已经讨论了可以应用于Transformer架构的各种优化技术。重要的是要注意，这些优化方法的很大一部分依赖于底层硬件的支持。因此，在选择要使用的优化技术时，必须采用整体的视角，同时考虑底层硬件的特性，包括硬件和软件堆栈。特别是，加速器是否在同一数据通路中支持MHA和FFN模块，还是包含每个模块的单独数据通路，这可能对可以执行的优化产生重大影响。

具有统一数据通路的加速器往往追求更一般化的优化，可以同时应用于MHA和FFN模块，或者至少不需要改变数据通路，以至于不能计算其他模块。例如，一些支持MHA和FFN模块的加速器采用了通用的静态剪枝方法来优化权重矩阵 [65, 166, 209]，但不旨在利用注意力特定的剪枝方法，如动态稀疏性。然而，如果MHA和FFN模块在不同的数据通路中计算，或者PE可以重新配置，那么可以分别针对MHA和FFN模块进行更奇特的优化。例如，FABNet [64] 通过为MHA和FFN模块采用独立的数据通路，利用了只能应用于FFN模块的静态稀疏模式。FTRANS [127] 通过整合可重配置的PE，既可以处理MHA工作负载，又可以处理FFN工作负载，并为MHA和FFN模块应用不同的优化方法。然而，使用单独的数据通路或可重配置的PE会增加额外的开销，与使用通用的统一数据通路相比。因此，需要权衡面积开销和使用更激进优化所带来的性能收益之间的折衷。

__摘要__（第4.3节：Transformer特定的优化）：尽管通用的现成优化方法也可以改善高效的Transformer推理，但已经进行了大量研究，以设计利用Transformer特定特征的优化策略。一个机会是优化MHA模块中的注意力机制，其运行时成本与序列长度呈二次增长。例如，动态剪枝被广泛应用于利用注意力分数激活的稀疏性质。此外，还应考虑非线性操作的高效计算。为了减少为非线性操作实现专用硬件单元的硬件成本，提出了函数近似和查找表方法作为可行的替代方案。最后，模型优化方法还应该了解底层硬件架构和数据通路。对于MHA和FFN模块使用单独的数据通路可能会增加面积开销，但与为两个模块使用单一数据通路相比，可以实现更激进的优化。

# 5 将TRANSFORMER映射奥硬件

为了在目标硬件架构上执行Transformer块，我们必须将其映射为能够执行所需计算和通信的硬件指令。在映射过程中所做的选择对性能起着重要作用。然而，可能映射的空间大小使得找到最佳映射变得困难，并且需要使用经过深思熟虑的探索、启发式或基于学习的方法。

在本节中，我们在第5.1节中介绍了映射问题；并在第5.2节中讨论了有效执行Transformer所需的关键映射决策。我们在第5.3节概述了现有映射技术的分类，类似地，在第5.4节中讨论了不同映射的性能建模技术。最后，在第5.5节中介绍了针对映射器的Transformer特定考虑因素。

## 5.1 What are Mappings?

映射或调度被定义为在特定目标硬件架构上执行一组操作的硬件指令序列。对于像Gemmini这样的系统级阵列加速器，这些硬件指令可能包括在特定数据流和加载/存储下执行的稠密矩阵乘法，以及用于在片外DRAM和本地SRAM之间传输数据的加载/存储指令。映射将列出完整的数据和存储指令序列，最终目标是生成可在硬件上执行的源代码或编译后的二进制文件。

对于某些操作，可能存在多个有效的映射，其中有效性指的是执行每个映射时的正确性保证。具体而言，对同一问题应用不同的映射决策可能会导致有效但不同的映射。我们将映射决策及其产生的映射的总空间称为映射空间（mapspace）。有关各个映射决策的详细信息将在接下来的第5.2节中讨论。

不出所料，两个不同的有效映射可能在端到端性能方面存在差异，例如延迟、带宽和能量消耗。因此，映射或调度框架的目标通常是获得给定软件操作和期望性能指标的帕累托最优的有效映射。对于某些操作，找到一个好的映射是不必要的，因为问题很简单，映射空间很小，或者因为该操作本身不是性能瓶颈，不值得费力进行精心的调度。然而，在DNNs（包括Transformer）的核心计算操作中，由于庞大的映射空间和整体模型执行速度提升的潜力，映射问题既具有挑战性又具有回报性。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature16.png)
__图16__：在典型的DNN加速器上将卷积操作映射到CNN中的可视化示意图。卷积操作被表示为一个六重嵌套循环，不包括批次维度。循环置换涉及确定应该以何种顺序执行每个循环级别，并涉及与加速器本地内存或片外DRAM之间的内存访问。时空映射确定了应该使用加速器硬件资源并行执行的循环级别。切片因子是每个循环级别的循环边界，其中每个维度可以通过切片划分为多个子循环。如示例所示，输入通道大小维度（C）使用切片因子为8进行切片，因此分为两个子循环，其中循环变量为𝑐0和𝑐1。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature17.png)
__图17__：在典型的DNN加速器上将Transformer编码器/解码器中的矩阵乘法操作映射的可视化示意图。矩阵乘法被表示为一个三重嵌套循环。循环置换涉及确定应该以何种顺序执行每个循环级别，并涉及与加速器本地内存或片外DRAM之间的内存访问。时空映射确定了应该使用加速器硬件资源并行执行的循环级别。切片因子是每个循环级别的循环边界，其中每个维度可以通过切片划分为多个子循环。如示例所示，输出列维度（𝑁）使用切片因子为4进行切片，因此分为两个子循环，其中循环变量为𝑛0和𝑛1。正如我们将在5.5.1节中讨论的那样，尽管矩阵乘法只有3个嵌套循环而不是像卷积那样的6个循环，但找到最优映射仍可能具有挑战性。

图16和17分别展示了CNN和Transformer中关键操作及其可能的映射示例。如示例所示，嵌套循环的每个级别必须满足以下要求：（1）分配以从DRAM或本地加速器内存中获取数据进行执行；（2）分配以在空间（即并行）或时间（即顺序）上执行，如果加速器包含空间并行化的计算资源；（3）分配以作为一个循环执行还是分割为多个子循环（如果是多个子循环，则使用哪些切片因子）。特别是对于Gemmini的情况，空间映射涉及决定将哪些循环级别分配给N×N的PE系统阵列网格进行执行。

## 5.2 关键的映射决策是什么？

映射过程分为两个步骤。首先，图形在图级别上转化为一组张量操作。这可能涉及将连续的操作融合在一起、稀疏化张量，并决定适当的量化策略。然后，将每个结果张量操作进行调度，将其转化为适当大小的硬件指令。

### 5.2.1 Graph Level

图级别的调度涉及改变计算图的结构，而不仅仅是图中各个节点表示的张量操作的执行计划。常见的改变包括以下几点：

* 层融合或操作融合是指将多个层（例如，矩阵乘法后跟归一化层）合并为单个张量操作，以便在加速器上进行调度和运行。这减少了层间通信，因为一个层的结果可以作为输入保留在芯片上，而无需写入主存储器后再读取，但会增加层内通信。正如在5.5节中将看到的，与CNN架构相比，层融合可能不会带来太多的延迟改进，因为静态融合机会不像将卷积与批归一化层融合那样直观。对于Transformer来说，可以在同一个内核中组合多个操作，但这可能会增加层内通信的程度，使这种方法变得不可行。此外，这也可能依赖于目标硬件平台。
* 张量的动态稀疏化是指基于激活映射进行修剪决策的过程。常见的动态稀疏化方法包括使用局部敏感哈希将可能较小的点积置零。这可以显著减少操作所需的算术运算数量，正如在4.2节中讨论的那样。这些优化方法高度依赖于数据，因为它们需要访问激活值，因此不能总是预先估计[103]。因此，关于稀疏感知映射的研究结果相对较少，而且大部分只覆盖给定稀疏度下的操作级别映射。
* 张量的静态稀疏化是指修剪决策独立于激活值，并在静态环境下确定的过程。正如在4.2节中讨论的那样，有各种方法可以用于静态稀疏化。一般来说，结构化稀疏化能够提供较高的加速效果，但通常会导致非常重要的精度降低，而非结构化稀疏化可以提供更好的精度保持。
* 静态张量稀疏化是指修剪决策独立于激活值并在静态环境下确定的过程。正如在第4.2节中讨论的，有各种方法用于静态稀疏化。一般来说，结构化稀疏化能够提供较高的加速效果，但通常会导致非常重要的精度降低，而非结构化稀疏化则能够在极度稀疏的情况下保持较高的精度，但很难进行加速。然而，后者将变得越来越重要，因为它减少了内存流量，而内存流量已成为功耗的主要瓶颈。

### 5.2.2 Operator Level

操作级别的调度步骤将张量操作分解为在给定架构上运行的一组任务。这包含了几个不同的步骤，每个步骤都需要程序员做出决策。其中包括：

* 将操作分成可以适应内存层次结构不同层的瓦片；瓦片的维度是一个选择（例如，图17中的瓦片大小）。
* 确定计算的数据流，即执行瓦片的顺序以及保持静止或在处理器之间移动的张量。这可以编码为一个循环排序问题，其中最内层的循环对应于保持静止的张量的轴（例如，图16中的任何循环排列）。
* 决定要并行化哪些轴，以及要串行运行哪些轴，我们称之为时空映射。
* 决定如何交错通信和计算，以最小化延迟。例如，双缓冲可以将存储器划分为两半，其中一半由处理器用于计算，而另一半加载来自存储器的数据。
* 将算术指令映射到硬件指令。对于某些架构，这可能只是将适当大小的矩阵乘法（通过划分瓦片实现）替换为调用适当的ISA（指令集架构）指令。对于其他架构，它可能涉及在不同的矢量指令之间选择，这可能会影响矢量化的轴的选择以及结果的时空映射。

更详细的描述可以在[128]中找到。

在映射空间中选择的点会对性能产生重大影响，可能相差数个数量级，正如我们将在第5.5.1节中讨论的那样。因此，硬件映射器的目标是在给定的硬件目标上选择一个点，以最小化能量、能耗延迟积（EDP）、延迟等某种成本。然而，映射空间的大小使得探索变得困难。例如，仅考虑瓦片、时空映射和循环排序（数据流），对于一个BERT注意力层来说，可能的映射数量可以超过10^12。因此，映射器的设计和选择在理论和实践中受到了重视。

此外，最佳映射在不同的硬件架构上可能会有显著差异，而对于某一组硬件参数有效的映射在其他情况下可能效果不佳[103]。这显著增加了在协同设计环境中进行映射的难度，因为必须为每对神经网络和硬件架构计算一个映射。

__总结__（第5.2节.关键映射决策）：将Transformer映射到硬件需要在图形和操作级别上做出决策。这些决策范围从选择简单的数值或分类参数到对正在运行的程序进行结构修改。所需的决策空间是巨大的，随着每个可能的决策组合地呈指数级增长，但选择空间中的一个好的点可以显著影响性能。

## 5.3 寻找高性能的映射

为了处理庞大的搜索空间，许多针对加速器的映射技术[84, 92, 102, 129, 163, 239]和完整的编译器框架[3, 32, 33, 114, 161, 167, 185]已经被开发出来。下面对它们进行简要讨论。

### 5.3.1 映射策略

为了应对庞大的搜索空间，映射算法专注于映射网络到架构所需步骤的子空间，并只对其中的一部分步骤进行决策。

__图级别调度器__：大多数现有的机器学习编译框架（例如，XLA [185]、TensorRT [161]、TVM [33]、Glow [184]和CuDNN [41]）针对图级别优化，如操作融合、资源分配、图分区、图重写等。已经开发了大量的操作融合技术[10, 247, 252]，以优化跨DNN层之间的数据重用映射。其中，已经提出了一些针对Transformer的特定操作融合技术[42, 168]。特别地，[42]对Softmax层进行了分解，并动态地将GPU内核与MHA块中的前后矩阵乘法层进行融合。相关地，[168]表明，在GPU上将LayerNorm层融合和将小矩阵乘法组合成大矩阵乘法对性能改进是有益的。在[104]中，引入了一种优化的DNN加速器数据流，以有效地将关键的矩阵乘法和Softmax层在MHA中进行融合。为了了解Gemmini加速器中操作融合的权衡，我们进行了一个案例研究，并将其分析结果包含在第5.5节中。

__操作级别的映射器__：在第5.2.2节中，我们讨论了关于划分、数据流和时空映射的决策可能导致巨大的搜索空间，并且在ML加速器中选择合适的点是实现高效利用的关键。在给定子空间范围内，映射器通常根据它们进行决策的方式可以分为三个一般类别：穷举搜索、基于反馈的搜索和约束优化。__表6__ 总结了利用不同技术导航映射空间的现有映射器。

![Tux, the Linux mascot](/transformer/papers/images/SurveyTable6.png)

穷举搜索方法[29, 48, 163, 239]采用各种采样策略，要么穷举地探索映射空间中的大量点，要么从映射空间中随机采样。为了降低穷举搜索的成本，这一类别中的映射器通常依赖开发者的启发式规则来修剪映射空间，并利用轻量级性能模型在合理的时间内比较所有有效的映射以找到最佳映射。这种方法的缺点有两个：一方面，穷举搜索往往非常昂贵，特别是对于更复杂的目标工作负载和硬件架构；另一方面，这个昂贵的过程对于任何目标工作负载或加速器架构的变化都会重复进行，而没有利用任何先前的知识。

基于反馈的方法使用机器学习算法或其他统计方法[7, 33, 99, 179]，要么提高成本模型的准确性，要么直接使用黑盒调整来搜索解决方案。虽然这些方法有可能准确地学习调度空间，但它们的计算成本很高，既需要评估足够多的调度以学习模型，也需要付出学习算法的成本。因此，这些方法通常适用于已有的硬件或分析模型，其中大规模的测量是可行的。

约束优化方法与穷举搜索和基于学习的算法不同，它们将调度问题形式化为一个数值优化问题，以确定在给定一组约束和目标函数下的变量赋值。流行的技术，如混合整数规划（MIP），已经证明了它们解决大规模和复杂问题的适用性。特别是，多面体转换已经利用基于约束优化的方法进行自动向量化和循环划分[5, 6, 15, 21, 75, 116, 165]。这些多面体优化关注测试变换的可行性，并提供信息来引导迭代搜索。另一方面，[92, 129]利用ML工作负载和硬件中的规律性，将映射问题形式化为优化问题，
然后可以直接由现成的求解器解决。

__总结__（5.3 寻找高性能映射）：针对在加速器和通用处理器上映射DNN的挑战，已经开发出了一套全面的策略。最初针对CNN开发的技术也可以应用于Transformer，因为关键操作也是张量代数操作。在图层级别上，运算符融合是一种重要的优化技术，它对庞大的映射空间进行编码，以决定如何重叠层的执行。在操作级别上，映射策略可以广泛分类为搜索策略（随机或基于反馈）或优化或启发式策略。

## 5.4 映射的性能建模

性能模型可以为不同映射提供性能反馈，而无需在实际硬件上执行映射或在开发中的加速器上运行模拟。它们可以显著降低映射器的评估成本，并用作性能代理，以优化映射。不同的性能模型提供不同的精确度水平、运行时成本、目标工作负载范围和与各种映射算法的兼容性。性能模型的选择取决于映射器和目标工作负载。

对于Transformer，映射器可以使用领域特定的多项式模型 [92] 和分析模型 [122, 146, 153, 163] 来快速比较不同映射。这些模型利用张量代数工作负载中已知的迭代空间边界以及静态可分析的数据访问模式来估计性能。数学形式表达的多项式模型也可以直接用作基于优化的映射器的目标。

另一类流行的性能模型涉及数据驱动的机器学习模型 [33, 84, 106]。这些模型不是通过分析建立性能模型来表达已知的映射关系，而是通过训练模型来学习从输入映射到性能输出的映射关系。这种方法的优点是可以处理更复杂的映射关系和非线性特性，但需要大量的训练数据和计算资源。

根据具体的映射任务和硬件目标，选择适合的性能模型对于提高映射器的性能优化效果非常重要。

生成映射的性能模型可能无法在实际的加速器上达到最优（甚至良好）性能，因为这些模型可能无法准确捕捉硬件实现的差异。基于真实硬件实现的循环精确软件模型可以提供更高的准确性 [187, 218]，还可以使用诸如Firesim [105] 的平台进行FPGA仿真，用于模拟正在开发的硬件。然而，这些平台需要更多的问题维度和映射描述以外的信息，它们需要一系列明确的指令。

生成这些指令流需要考虑大量的边界情况。例如，对于matmul的简单切片操作，可以在Timeloop [163] 中表示为一行切片大小，需要插入指令来指定不同级别的内存层次之间的内存移动，并且需要为矩阵维度不能被切片大小整除时生成边界情况的代码。此外，协同设计过程要求这个过程可以自动化。换句话说，每个映射必须能够自动转换为代码。

因此，代码生成工具 [33, 126, 180] 用于将映射实现到硬件（或模拟器）上。许多这些工具不仅集成了硬件后端的规范，还包括映射决策算法，通常针对该硬件目标进行调优。这些工具还可用于神经架构搜索（NAS），以获得给定硬件架构的准确性能数据，以指导自动化的DNN架构设计（有关详细信息，请参见第6.1节）。然而，这些工具很难适应代码设计框架，其中映射空间和硬件目标都可能变化。

为了解决这个问题，开发了用户可调度语言，如Halide [179]、TVM [33]、Rise/Elevate [78]和Exo [95]。这些工具以要执行的计算的描述和映射空间中的一个点作为输入。它们通常通过一组重写规则来定义，表示某些变换，如循环的拆分和重新排列、用ISA指令替换适当的循环以及循环融合。这些语言还允许用户指定和定制硬件指令集，并通过将这些点表示为一系列重写规则来无缝地将找到的调度转换为可执行代码 [158, 249]。

__总结__（第5.4节 映射性能建模）：对于在新架构上运行的Transformer来说，性能估计对于找到最优算法、映射和硬件组合至关重要。有各种开源性能模型可用于估计在硬件上的映射性能，包括领域特定的分析模型、数据驱动的机器学习模型和循环精确模型。选择适用于Transformer的性能模型取决于目标工作负载大小、硬件复杂度和开发阶段。此外，还有许多成熟的代码生成工具可用于优化Transformer设计，以适配现成的硬件平台。

## 5.5 Transformer与CNN的映射比较

在第5.3节中讨论的先前工作主要集中在将CNN映射到加速器或通用硬件上。与CNN类似，Transformer的大部分周期都花在了来自MHA和FFN模块的矩阵乘法上。实质上，现有的CNN映射器可以轻松扩展到调度Transformer的矩阵乘法。然而，正如我们在第3.4.2节中讨论的那样，Transformer块包括LayerNorm和Softmax操作，在某些实际情况下可能具有计算上的复杂性（这也被[42, 168]观察到）。反过来，这些操作的存在对于调度前后的矩阵乘法施加了约束。这导致了对Transformer整体进行调度优化的问题更加复杂。在本小节中：

* 我们比较了Transformer块与CNN的映射空间（第5.5.1节）。
* 我们深入探讨了由于LayerNorm和Softmax的存在而导致的Transformer矩阵操作调度复杂性增加的问题（第5.5.2节）。

### 5.5.1 Transformer的地图空间特征化

我们通过实证方法对代表性的Transformer和CNN架构的合法映射搜索空间进行了特征化。为此，我们选择了BERT [52]和ResNet50 [82]。通过Timeloop映射器 [163]，共搜索了10万个随机有效映射，并使用Timeloop模型测量了估计的延迟和能耗。目标空间硬件架构是Gemmini系统发生器 [70]。假设BERT和ResNet50模型均为8位整数量化。我们假设输入序列长度为512，这是BERT-Base和BERT-Large模型的典型假设。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature18.png)
图18：（左）BERT和（右）ResNet50的地图空间比较。显示了10万个随机抽样的有效映射的分布。这两个分布都显示出与最佳（最小）观察值相比高达四个数量级差异的EDP范围。两个分布都没有明显偏向较低或较高相对EDP的情况。总体而言，我们发现BERT的矩阵乘法和ResNet50的卷积的地图空间在大小上相似，并且它们的分布形状没有明显差异。这表明，在BERT矩阵乘法调度的情况下，使用蛮力或随机搜索的挑战与ResNet50操作符的情况一样困难。

__EDP分布分析 (EDP Distribution Analysis)__。图18展示了BERT（左）和ResNet50（右）的地图空间比较。

对于BERT的MHA模块，图中的结果对应于执行查询、键和值投影的矩阵乘法的映射。对于FFN，我们选择𝑊1投影中的矩阵乘法，该投影将隐藏维度扩展到其值的四倍。对于ResNet50，我们选择了具有不同内核大小和形状的卷积操作，包括1×1、3×3和7×7卷积内核。具体而言，我们在ResNet50的conv1层中使用了7×7内核卷积，输出通道大小为64，步幅为2，还使用了输出通道大小为512和2048的3×3内核卷积和1×1内核卷积，它们属于最后的卷积层conv5_x。这四个特定的卷积操作在通道和内核大小上有所不同，合理地代表了在ResNet50中找到的卷积操作。

从我们的地图空间分析中，我们观察到BERT和ResNet50的地图空间都具有类似的潜在能耗-延迟乘积（EDP）值范围，这是通过随机抽样映射得出的结论。考虑到每个架构的运算符之间的差异，它们本身的EDP值分布也在很大程度上相似，即具有较低EDP值的帕累托最优映射的比例对于BERT和ResNet50的运算符来说都很小。作为替代的可视化方式，图19比较了相同的10万个随机映射的经验累积分布函数（CDF）。在这里，我们密切观察在接近最优映射的区域附近的CDF之间的差异。我们观察到，在3×3、512卷积中，相对EDP值对应的第十百分位数为7.44，在1×1、2048卷积中为12.06。BERT的MHA和FFN投影的矩阵乘法的第十百分位数分别为9.42和9.84。此外，我们还考察了相对EDP值小于观察到的最小值的3倍的映射的百分比。这个百分比代表了可以被安全标记为接近最优的映射数量的粗略上限，而差异则表示在搜索不同运算符的最优映射时的相对困难程度。我们发现，在3×3、512内核中，有1.58%的映射在这个EDP范围内，而在1×1、2048内核中有2.62%的映射。对于BERT，MHA投影的矩阵乘法显示有1.70%的映射相对EDP值小于3，在FFN矩阵乘法中有1.48%的映射在这个范围内。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature19_20.png)
__图20__：ResNet50运算符的随机有效映射分布，与Transformer的MHA和FFN矩阵乘法的分布进行比较，其中matmul的大小经过调整，使得总的MAC数目相等。MHA matmul的维度被校准为240 × 240 × 512，FFN 𝑊1投影matmul的维度也类似地设置为480 × 120 × 512，其中𝑑FFN = 4 × 𝑑。与图18进行比较，我们可以看到ResNet卷积和BERT matmul的EDP分布仍然相似。这意味着即使考虑到MAC总数的差异，BERT matmul的mapspace的相对EDP范围与CNN卷积核的mapspace一样广阔。

总体而言，从随机抽样的有效映射中分析能耗-延迟积（EDP）的分布表明，相较于卷积操作而言，BERT的矩阵乘法（matmul）在划分和重排序的循环层级方面更少，但在调度方面与卷积操作一样具有挑战性。图形和操作级别的调度对于CNN推理的端到端性能和效率产生了显著影响，同样的调度适配性对于Transformer的矩阵操作也同样重要。

__固定总MAC数的EDP分布分析__：作为对mapspace特征化的额外分析，我们进一步强制固定总的MAC数。这使得Transformer和ResNet50运算符的映射结果分布可以更加公平地进行比较。我们仍然假设Transformer的输入序列长度为512，前馈网络的扩展比例为隐藏维度大小的4倍。为了保持MAC数目相等，我们计算出与ResNet50 conv1层的7×7卷积核和输出通道维度64相同总MAC数的隐藏维度大小。对于MHA中的查询投影的matmul，相应的隐藏维度大小为240。类似地，对于FFN块中的𝑊1投影的matmul，相应的隐藏维度大小为120。为了阐明合成的BERT层与实际的ResNet50卷积之间的比较，我们在图20中绘制了相应的映射分布对。即使在强制使MAC数目相等的情况下，我们可以看到BERT matmul和ResNet50卷积之间的相对EDP值的范围是相似的。这一发现进一步凸显了在Transformer模型中的matmul调度问题的复杂性。

### 5.5.2 LayerNorm和Softmax的调度复杂性

尽管我们发现Transformer中的matmul已经是非平凡的目标，很难在DNN加速器上获得高效的执行调度，但是存在于不同矩阵操作之间的几个非线性操作，包括LayerNorm和Softmax，进一步增加了问题的复杂性。在追求更激进的优化时，一种吸引人的策略是将相对高算术强度的matmul与随后的低算术强度归一化操作（如LayerNorm和Softmax）进行融合。这在处理量化工作负载时尤其吸引人，因为等待归一化的部分和通常比最终归一化输出具有更高的位宽。熟悉CNN类型加速器的架构师可能会觉得这特别直观，因为卷积通常与ReLU或最大池化操作进行融合。

类似地，对于Transformer编码器，我们可以重叠归一化操作和之前的matmul的执行，但这只有在有额外硬件支持和对matmul执行调度的适当约束条件下才能实现。为了实现完全隐藏非线性操作的延迟，必须最大化matmul的任一输出维度的切片因子大小，以便行/列立即就绪并存储在Gemmini累加器高速缓存中，用于计算均值和标准差。我们将这种替代的调度方法称为融合优化调度。

另一方面，在内存受限的边缘设备中，这种策略（有些令人费解地）是适得其反的。在Transformer中找到的归一化操作通常需要在片上局部内存中保留长向量的数据，然后才能产生任何归一化的输出元素。此外，当将matmul与这些归一化操作融合时，通常需要使用尴尬的matmul切片形状。这些尴尬的切片形状通常在任一维度上要大得多，而不是方形的，这种偏斜的切片形状往往会导致算术强度远远降低。这极大地降低了matmul的性能，并且可能增加总的内存访问量，即使考虑到当禁用融合时，必须将未归一化的部分和高位宽的数据发送到外部内存。
在 __图21__ 中，我们更深入地研究了针对BERT matmul的融合优化调度的性能影响。我们考虑隐藏维度为768且具有12个注意力头的BERT-Base编码器。默认情况下，我们假设序列长度为512。我们采用16×16的权重固定的Systolic阵列Gemmini作为目标硬件，并具有用于非线性操作（激活、归一化等）的I-BERT实现的自定义硬件单元，如3.4.4节所述。通过Timeloop[163]估算了每个相邻的matmul和LayerNorm/Softmax操作的总延迟。

重叠计算的机会包括：(1) MHA的查询 × 关键matmul和随后的Softmax；(2) MHA的𝑊out投影和随后的LayerNorm；以及(3) FFN的𝑊2投影和随后的LayerNorm。我们比较了两种调度策略。在第一种策略中，我们使用Gemmini的默认基于启发式的调度器，对每个matmul维度在本地SRAM级别上贪婪地最大化循环切片因子。在这种方法中，我们不尝试将matmul计算与后续的非线性操作重叠，这意味着matmul被独立调度，就像它单独执行一样。在图21中，我们将这种方法称为非融合调度。第二种策略是上述的融合优化调度。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature21.png)
__图21__：融合优化调度对BERT MHA的影响，可以隐藏LayerNorm和Softmax的延迟。结果基于BERT-Base架构和Gemmini加速器。（左侧）假设输入序列长度为512，并且累加器SRAM大小从128kB增加到256kB。隐藏Softmax延迟可以将组合的matmul和Softmax延迟减少78％。然而，将𝑊out投影与LayerNorm重叠可能会对总延迟产生负面影响或改善，这取决于累加器大小。总体而言，对MHA中的两个matmul进行融合优化调度可分别提高128kB和256kB累加器大小的延迟23％和52％。（右侧）输入序列长度增加到4096。同样，我们可以看到将查询×关键matmul与Softmax重叠可以将延迟降低22％。总体而言，将MHA中的两个matmul与非线性操作进行融合可以提高21％的延迟。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature22.png)
__图22__：融合优化调度对BERT FFN matmul的影响，可以隐藏LayerNorm操作的延迟。输入序列长度从512变化到4096。我们观察到，在这两种情况下，融合优化调度会使总延迟增加27％。这促使我们有必要仔细评估在系统阵列上链接matmul和LayerNorm执行的影响，因为映射约束的影响可能超过非线性操作的延迟隐藏带来的收益。

左图展示了图21中MHA块中matmul和非线性操作融合对累加器SRAM大小的影响。在这个实验中，虽然用于输入激活和权重的芯片内临时存储器大小固定为256kB，但输出激活累加器大小从128kB增加到256kB。我们注意到两个发现：首先，在MHA块内，无论累加器大小如何，将查询×键matmul与每个注意力头的Softmax融合可以减少延迟。特别地，我们看到Softmax的延迟与matmul相比较大，占据总周期的约78％，而隐藏这种延迟可以显著减少总延迟。与此同时，查询×键matmul的延迟相对于附加调度约束而言几乎没有变化，主要是因为matmul的内部维度较小（对于BERT-Base，𝑑/𝑙 = 64）。另一方面，来自融合优化调度的映射约束严重影响了与后续LayerNorm融合的𝑊out projection matmul的执行延迟，导致延迟比非融合调度高出83％。然而，一旦累加器大小加倍，matmul调度的性能损失就得到了缓解。256kB的累加器SRAM大小可以在缓冲区中存储更多的部分和，而不是溢出到DRAM，从而将总延迟减少了4％。

在图21的右图中，我们进一步研究了MHA块的序列长度对融合优化调度的影响。在这里，序列长度从512增加到4096，这影响了MHA块中matmul、Softmax和LayerNorm的循环比例。特别地，需要注意的是，查询×键matmul和Softmax计算的大小与序列长度呈二次方关系，而其他matmul和LayerNorm呈线性关系。当将查询×键matmul与后续的Softmax融合时，映射约束会恶化matmul的性能，即使使用更大（256kB）的累加器大小。这是因为随着查询×键matmul的维度增加和强制的分块因子，调度器无法避免在DRAM级别上进行分块。然而，通过重叠Softmax操作，从而消除加载和存储中间激活的需求（这与序列长度的二次方成比例），可以抵消查询×键矩阵的延迟增加，从而将总延迟减少22％。

另一方面，图22显示了FFN 𝑊2 projection中matmul和LayerNorm

重叠的结果。即使在更大的累加器大小和两种序列长度下，我们始终观察到融合优化调度会使总延迟增加27％。结合之前的发现，我们发现融合优化调度的延迟改进取决于累加器SRAM大小和序列长度。此外，我们发现，在BERT-Base规模下，重叠MHA查询×键和随后的Softmax是一致有利的，但是链式连接FFN 𝑊2 projection matmul和LayerNorm是一致不利的。这与之前对Transformer的GPU内核融合研究[42, 168]形成了对比，并强调了当针对不同风格的自定义硬件设计（包括Gemmini加速器）时，针对Transformer的matmul的调度变得更加复杂。

__总结__（以下是本节的要点总结）：

* Transformer matmul的调度与CNN卷积操作一样具有挑战性。两者的映射空间具有相似的相对EDP分布和近似最优映射的百分比。尽管Transformer matmul的循环层次较少，但蛮力或随机调度并不比CNN卷积更简单。
* LayerNorm和Softmax等非线性操作增加了Transformer matmul调度问题的复杂性。这些非线性操作的延迟可以通过将其计算与前面的matmul融合来隐藏。这需要额外的硬件支持，并对matmul的调度施加了约束，如第3.4.4节所述。
* fusion-optimized调度是否能改善端到端的延迟取决于Transformer和底层硬件参数。特别是我们观察到：（1）芯片上SRAM缓冲区大小；和（2）序列长度都很重要。
* 我们一致观察到，在MHA块中，将query × key matmul的执行与Softmax重叠可以将延迟降低高达78％，与在系统阵列加速器上分别执行这两个操作相比。另一方面，将FFN 𝑊2投影与后续的LayerNorm重叠调度会导致性能下降27％。

这些见解强调了调度Transformer matmul的复杂性，并强调了需要仔细考虑硬件参数和融合的具体操作以实现最佳性能。

# 6 使用神经架构搜索来调整Transformer架构

神经架构搜索（NAS）是一种自动化的过程，用于搜索最佳的神经网络架构。它被广泛应用于改进各种深度学习模型的性能和效率，包括Transformer模型。

到目前为止，我们已经深入探讨了DNN推理的整个技术栈方面，重点关注了Transformer架构，从硬件层面到优化和调度策略，以提高其推理性能。在DNN的整个技术栈优化中，另一个重要方向显然是优化DNN的架构本身，并为特定的硬件平台进行定制。在本节中，我们将主要关注自动化神经架构搜索（NAS）作为设计DNN的方法。第6.1节将提供NAS的一般概述，然后第6.2节将探讨面向硬件的NAS方法。这两个子节主要关注CNN的NAS技术，因为NAS最初是在Transformer之前引入并进行了广泛研究的。然而，我们认为提供全面的概述和背景有助于理解NAS。在第6.3节中，将讨论针对Transformer架构的NAS方法。最后，在第6.4节中，将提供一个在目标硬件架构上优化Transformer推理的NAS方法的案例研究。

## 6.1 神经架构搜索

通常，DNN架构的设计和训练是为了在给定任务中达到最高的准确性，而不一定考虑目标硬件或推理延迟、内存和功耗要求。然而，通常存在多种不同的DNN架构变体，它们在准确性上相同，但在硬件性能上更好。

在这个领域有丰富的文献。其中一些值得注意的作品包括MobileBERT [205]，这是最早的尝试之一，它采用瓶颈结构设计了一个更轻薄的Transformer版本；Lite Transformer [232]提出了长短距离注意力，其中一组头部被替换为卷积操作，以更有效地捕捉短距离上下文；SqueezeBERT [94]将分组卷积结合到Transformer架构中，以减小模型大小和推理延迟。这种方法不限于自然语言处理，类似的模型也在计算机视觉(CV) [24, 36, 130, 152]和语音识别 [23, 76, 109]等领域中提出。

寻找这些DNN架构通常非常困难，因为搜索空间是指数级的，即使不考虑底层硬件平台。即使对于在DNN架构设计方面有经验的人，架构变化对准确性和运行时性能的影响也往往很难预测。因此，已经提出了自动化的NAS方法来为给定的约束条件调整DNN架构。然而，需要注意的是，NAS方法通常需要大量的计算和试验才能找到一个候选架构。例如，在早期的NAS工作中，寻找一个优化的CNN花费了22,400个GPU小时。此外，NAS方法尚未完全自动化，它们通常需要手动调整搜索空间。

广义上讲，NAS框架包括三个主要组件：搜索空间、搜索方法和评估方法 [18, 61]。搜索空间包括一组有效的操作（例如卷积、池化、激活等）及其连接方式，这些定义了有效的DNN架构，可以从中抽取候选模型。为了限制搜索空间并提高搜索效率，通常需要先前的知识和人的直觉来指导好的DNN设计。搜索方法定义了如何探索搜索空间。显然，穷举搜索是不可行的。因此，关键是要有方法快速探索搜索空间并采样候选架构。评估方法是一种评

估候选架构在未见数据上表现如何的方式。最简单的方法是在完成完整的训练过程之后评估所有候选架构。然而，这会产生巨大的开销，因此通常会使用更高效的方法来估计性能，作为最终准确性的代理。图23示意性地展示了这些不同的组件。下面，我们将更详细地讨论每个组件。请注意，本节的主要目的不是对现有工作进行全面调查，而是从实践者的角度提供更广泛的方法论概述，以改进NAS。我们建议读者参考[18, 61, 183, 193]以获取更全面的NAS调查。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature23.png)
__图23__：NAS框架的一般结构示意图，候选的DNN架构根据搜索方法从搜索空间中进行采样，然后对其进行评估。评估结果由搜索方法用于指导对搜索空间中的架构进行更好的探索。

### 6.1.1 搜索空间

NAS的搜索空间定义了一组有效的DNN架构，用于NAS框架的搜索。设计合适的搜索空间至关重要，因为它的大小和覆盖范围直接影响NAS框架的最终结果。一种朴素的搜索空间设计原则是逐层搜索[26, 77, 254]，其中每一层（或操作）可以独立于其他层进行搜索。例如，在[254]中，RNN控制器模型生成一个序列，描述了每个层来构建候选的DNN架构。

然而，逐层搜索空间往往面临着搜索空间大小的问题，随着候选架构的深度呈指数级增长，这可能降低搜索效率和最终性能。基于单元的搜索[54, 141, 170, 250, 255]可以缓解这个缺点，通过搜索单元（即由多个层组成的块或模块），而不是整个架构，然后可以重复堆叠这些单元以组成一个架构。这是受到许多成功的手动设计的DNN架构的启发，这些架构由重复的单元或具有相似结构的块组成[82,89]。

NASNet [255]是最早提出搜索两种类型的单元的工作之一：正常单元，它在不改变空间分辨率的情况下多次堆叠；还有减少单元，它在固定数量的重复正常单元之后插入，以减少空间维度朝向输出层。与同一作者提出的先前逐层搜索方法相比，这显著减少了搜索时间7倍。同样，基于单元的搜索空间通过对有效DNN架构施加额外的结构约束，显著减少了搜索空间（因为单元比整个网络小得多），因此在后续的工作中被广泛采用[142, 182]。

### 6.1.2 搜索方法

由于NAS的搜索空间通常太大，无法进行穷举搜索，因此需要高效的搜索方法来确保整体性能。在NAS的早期工作中，使用基于强化学习（RL）的方法作为搜索方法[16, 170, 250, 254, 255]（图24a）。在高层次上，基于RL的NAS框架包含控制器（即RL代理），控制器采取采样DNN架构的动作，训练后的评估准确性作为奖励信号馈送到控制器中，以改进其采样策略。控制器可以使用不同的RL算法进行训练，如策略梯度[254]或Q-learning[16]。
NAS的另一种搜索策略是进化搜索[141, 182]（图24b）。在这种方法中，首先初始化一组不同的DNN架构，然后通过添加、删除或更改层来进行变异，根据它们在每个进化步骤中的验证准确性进行评估和选择。这生成了下一个进化步骤的新种群。进化搜索的搜索成本可能非常高，因为它要求在每个进化步骤中验证种群中的所有DNN。因此，通常会与各种降低验证成本的方法相结合，例如权重共享。这些将在6.1.3节中详细讨论。
前面提到的方法可以看作是在离散搜索空间上的黑盒优化问题。由于搜索空间的离散性和可调节参数的大量性质，搜索成本可能变得非常高。这一问题进一步加剧了单个RL或进化迭代的长时间评估，通常需要从头开始进行训练。例如，基于RL的NASNet[255]和基于进化搜索的AmoebaNet[182]需要几千个GPU小时进行端到端搜索[183]。相比之下，DARTS[142]提出了搜索空间的连续松弛，使其能够通过基于梯度的优化方法高效地探索和优化搜索空间（图24c）。实质上，DARTS引入了一个可训练权重，允许多个操作的加权平均，而不是选择单个操作。这个权重可以在训练过程中与其他模型参数一起训练，并最终收敛到对某个操作更有利的状态。这种方法将搜索成本从之前的RL或进化搜索方法的数千个GPU小时减少到几个小时。由于搜索效率高，基于梯度的搜索已成为许多NAS框架的流行选择[219, 228]。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature24.png)

### 6.1.3 权重共享和超网络

NAS方法面临的主要挑战之一是训练成本过高。为了解决这个问题，ENAS [170] 提出了权重共享的方法。ENAS将DNN模型视为有向无环图，其中节点表示具有自己可训练权重的计算单元，边表示信息从一个节点流向另一个节点。然后，可以将单个候选DNN看作是更大的超参数化超网络（supernet）的子网络。这重新定义了NAS作为从supernet中采样共享权重的好的子网络的过程。
一旦训练完supernet，就可以对其子网络进行采样和评估，而无需单独训练每个子网络。这显著降低了评估每个候选架构的计算成本。

权重共享允许多个子网络共享相同的权重集，从而实现对搜索空间的高效探索。在搜索过程中，子网络的架构是通过选择supernet中的特定路径来确定的，而与所选路径相关联的权重被微调以优化性能。

ENAS引入了一个控制器网络，用于生成架构决策，例如在supernet中选择哪些路径，以及一个共享的权重矩阵，该矩阵被所有子网络使用。控制器网络与supernet一起使用强化学习进行联合训练，以学习采样出性能良好的架构。

另一种应对高训练成本的方法是使用超网络。在这种方法中，通过将搜索空间中所有可能的子网络组合在一起，创建一个单个的大型网络，称为超网络。超网络包括可能在搜索过程中采样的所有架构变化。通过训练和评估超网络，可以估计不同子网络的性能，而无需显式地单独训练每个子网络。这样可以更高效地探索搜索空间。

权重共享和超网络是NAS中缓解训练和评估个别候选架构的计算负担的技术。它们通过重用权重或在综合网络表示上评估性能，实现了更快速和更具成本效益的最佳架构搜索。无需从头开始训练模型即可评估。这极大地降低了整体搜索成本。

这种方法，也被称为基于超网络的NAS，被许多后续算法采用[17, 25, 26, 77, 142, 228, 243]。特别是，Single Path One-Shot NAS [77]通过堆叠选择块构建超网络。选择块由多个操作选择（例如，具有不同内核大小的卷积或跳过操作）组成，每次只能选择一个操作。对于每个训练步骤，通过均匀采样选择块中的一个操作，获得不同子网络并进行训练，期望以完全相等的方式训练具有不同选择排列的所有子网络。训练完成后，应用进化算法从超网络中搜索最佳子网络，而无需付出从头开始训练的昂贵代价。

然而，从完全训练的超网络中获得的子网络的准确性通常低于以独立方式从头开始训练的相同模型架构[17]。因此，通常需要重新训练发现的子网络架构。为解决这个问题，Once-For-All [25]提出了渐进收缩算法，而BigNAS [243]提出了三明治规则和原地蒸馏。这两种方法的目标都是以一种方式训练超网络，使其子网络能够在没有额外训练过程的情况下达到良好的准确性（即与从头开始训练的对应模型相当的准确性）。这些方法在实践中具有很高的价值，因为可以对子网络进行采样（例如，通过进化搜索）并立即部署。

### 6.1.4 评估方法

为了对候选架构的“好坏”进行排名，需要使用验证数据集来评估抽样的架构。早期的NAS算法[16, 254]会对抽样的架构进行完全训练直到收敛，但对于大型数据集来说这是不可行的。在将NAS应用于更大规模的任务时，通常采用一种被广泛接受的策略：使用较小的数据集（例如计算机视觉中的CIFAR-10）发现准确的细胞架构，然后将其应用于构建适用于更大数据集（例如ImageNet）的更大模型[140, 142, 149, 182, 210]。这里的前提是针对一个任务进行优化的DNN架构可以在类似领域的其他任务中良好地迁移。然而，最近的一些NAS研究对这一前提提出了质疑[26]。基于超网络的NAS算法可以是避免使用代理任务的良好替代方案[17, 25, 26, 77, 142, 228, 243]。这些算法只需要进行一次超网络训练迭代，可以直接在大规模数据集上进行，而不需要过多的计算资源。

### 总结（6.1 NAS）

神经架构搜索（NAS）是手动设计高效DNN的一种有前景的替代方案。NAS包括：

* 搜索空间，定义有效的候选架构
* 搜索方法，定义如何高效地探索搜索空间；
* 评估方法，用于评估候选架构的优劣。

尽管NAS具有潜力，但它也带来了一系列挑战，通常需要手动调整搜索空间，并且在时间和资源方面可能代价高昂。为了解决这个问题，NAS领域的许多最新进展都集中在提高搜索效率上。值得注意的方法包括：

* 基于细胞的搜索，限制搜索空间的大小
* 搜索空间的连续放松，允许高效的基于梯度的优化方法
* 在候选架构之间共享权重
* 用于评估候选架构性能的更快评估方法。

## 6.2 Hardware-aware NAS

硬件感知的NAS旨在优化DNN的准确性以及在目标硬件平台上的各种性能指标（如延迟、能耗或内存使用量）。其中一个关键问题是如何将这些指标纳入学习过程中。快速测量候选模型的延迟或能耗通常是困难的。因此，文献中的大多数工作只考虑FLOPs或参数总数。然而，正如前面在第2.2节中讨论的那样，FLOPs并不一定与延迟或能耗相关。因此，引入了多种硬件感知的NAS框架，直接考虑延迟，或者使用近似度量来衡量延迟（例如，测量单个层的延迟并累积它们以近似总延迟，而不是测量端到端的运行时间）。在这里，我们讨论将硬件性能纳入NAS框架的流行策略。有关硬件感知NAS技术及其算法细节的更详尽调查，请参阅[18]。

最直接的方法是直接测量硬件性能，并将其作为NAS框架的附加优化目标[138, 210]。例如，MNasNet [210]将现有的基于RL的NAS框架扩展到多目标优化设置。它旨在在目标平台上将准确性最大化，同时将延迟限制在低于某个目标延迟的范围内。它将这两个目标（准确性和延迟）结合为一个单一目标，通过加权乘积来实现。这个修改后的目标被作为更新控制器的奖励提供。通过直接优化目标平台上的延迟，MNasNet找到的DNN架构比MobileNetV2 [188]和NASNet [255]在Pixel手机上具有相当的ImageNet分类准确性，速度快约2倍。另一个值得注意的工作是针对资源受限微控制器搜索DNN的MCUNet [138]。与GPU或移动设备不同，微控制器用于微小IoT设备的内存和存储容量有限。因此，设计适应其紧凑内存预算的模型至关重要。MCUNet将其基于超网的NAS框架 [17, 77]与TinyEngine相结合，后者是作为该项目的一部分由作者开发的轻量级推理引擎。通过这种方式，MCUNet为每个进化步骤采样子网络，并将其提供给TinyEngine进行内存优化调度和测量最佳内存使用情况。

然而，由于可用设备数量有限，直接测量硬件性能可能很慢且无法并行化 [238]。此外，不可能预先测量搜索空间中所有可能DNN的硬件性能 [228]。为了克服这个问题，一些硬件感知的NAS方法采用了基于操作的查找表 [219, 228, 238]。查找表仅包含单个操作的预测性能数据，而不是存储端到端的硬件性能，这些数据可以相加以估计给定DNN的整体性能。例如，在FBNet [228]中，从查找表估计的延迟数被用作其基于梯度的NAS框架中的正则化项，以惩罚在目标硬件设备上慢速的操作。

最后，一些硬件感知的NAS框架依赖于轻量级的预测模型，可以快速预测给定DNN的硬件性能数值。例如，ProxylessNAS [26]训练了一个模型，该模型以DNN配置（如操作类型、输入和输出形状，以及其他操作属性如内核大小）作为输入，并输出在目标硬件平台上的估计延迟。

__总结__（第6.2节 硬件感知的NAS）：硬件效率指标可以与NAS损失函数结合，找到同时考虑准确性和延迟（或类似指标）的架构。尽管在真实硬件环境中直接测量性能指标是最准确的方法，但它可能较慢且难以并行化。相反，可以使用基于操作的查找表或训练简单预测模型的方法来高精度估计硬件性能。

|Name|Domain|SearchSpace|SearchMethod|WeightSharing|
|----|------|-----------|------------|-------------|
|Evolved Tfm. [200]|NLP|Cell | EA    | ×           |
|HAT [222]         |NLP|Layer| EA    | OFA [25]    |
|NAS-BERT [235]    |NLP|Layer| EA    | SPOS [77]   |
|Primer [201]      |NLP|Cell | EA    | ×           |
|Autoformer [230]  |CV |Layer| EA    | SPOS [77]   |
|GLiT [30]         |CV |Layer| EA    | SPOS [77]   |
|ViT-ResNAS [135]  |CV |Layer| EA    | SPOS [77]   |
|NAS-ViT [73]      |CV |Layer| EA    | BigNAS [243]|
|BurgerFormer [236]|CV |Layer| EA    | SPOS [77]   |

__表7__: 针对Transformer的特定NAS技术的现有文献总结如下。SPOS代表单路径一次性（Single Path One-Shot）[77]，OFA代表一次适用于全部（Once-for-All）[25]。

## 6.3 Transformer专用 NAS

早期的NAS研究主要集中在CNN模型上，主要应用于计算机视觉任务。然而，随着Transformer架构的引入和成熟，它不仅在自然语言处理任务中取得了最先进的结果，还在其他任务中表现出色，因此一些研究开始探索使用NAS方法寻找更高效的替代方案。由于Transformer架构最初是为自然语言处理任务开发的，因此最早用于Transformer的NAS工作主要集中在这个领域。

Evolved Transformer [200]是最早尝试将NAS应用于搜索更好的Transformer架构的工作之一，它使用了进化搜索算法。受NASNet [255]的启发，Evolved Transformer采用了基于单元的搜索空间，用于搜索两个单元结构。这些单元结构中的每一个都可以堆叠多次，形成编码器和解码器的Transformer架构。单元结构包含多个块的堆叠，每个块都有自己的超参数，例如操作类型、归一化类型和维度，这些超参数可以进行搜索。主要挑战在于NLP任务需要更长的训练和评估时间（例如，流行的WMT 2014 En-De翻译基准包含超过300万个句对）。此外，与之前的CNN作品[140, 142, 149, 182, 210]发现CIFAR-10是更大的ImageNet的合理代理不同，这些NLP任务通常没有良好的较小代理任务。为了解决这个问题，Evolved Transformer提出通过在较少的步骤内未能达到关键适应度的结构早停止的方式，动态分配资源给更有前途的架构。

由于在NLP任务上训练Transformer的计算成本较高，权重共享和基于超网络的NAS成为流行的选择。HAT [222]将Once-for-All [25]方案扩展到Transformer架构，训练一个超网络，从中可以采样不同深度、头数和维度的子网络。此外，HAT是硬件感知的，它使用多层延迟预测模型直接优化延迟和准确性。HAT共享了Once-for-All的优点，允许通过进化搜索采样子网络，并立即部署到目标硬件设备上而无需重新训练。

NAS-BERT [235]是另一种基于超网络的Transformer NAS方法，它扩展了Single Path One-Shot [77]。与前面的方法不同，NAS-BERT提出了一种可以应用于仅编码器的预训练阶段的NAS方法，以便对

下游任务不加偏见。为了避免在重型预训练任务中直接在大型超网络上进行架构搜索的巨大成本，NAS-BERT采用了两种新技术：（1）块级训练，将整个超网络分割为多个连续的Transformer层块，然后分别训练它们；（2）渐进收缩，根据验证损失动态地修剪表现较差的子网络。Primer [201]是用于自回归语言建模的更高效的仅解码器Transformer的搜索方法。与大多数NAS方法将模型视为从NAS搜索空间中选择的多个操作的连接不同，Primer将其视为由细粒度的Tensorflow (TF)原语操作组成的单个有效的TF程序，例如加法、指数、卷积等。使用进化搜索，它的目标是搜索一个TF程序，定义一个解码器块，可以多次堆叠以形成自回归语言模型。希望这样做可以最小化在设计搜索空间时的归纳偏差，因为操作的可能集合和它们的连接性不再由人类专家预先确定。为了减少自回归预训练的巨大计算成本，Primer引入了Evolved Transformer的hurdles思想。此外，它使用相对较小的LM1B数据集作为代理任务来发现模型架构，然后将其迁移到更大的目标任务，如PG-19 [176]和C4 [178]。

最初为自然语言处理任务开发的Transformer架构已经被应用于计算机视觉领域。这些被称为Vision Transformers（ViTs）[56, 144, 215]的模型在各种计算机视觉应用中表现出优于流行的CNN架构的性能，推动了研究朝着自动设计更好的ViT架构的NAS技术的发展。然而，由于架构的相似性，这些工作在针对NLP的Transformer的NAS方法学上有很多共同之处。例如，Autoformer [230]和ViT-ResNAS [135]是Single Path One-Shot [77]在ViT搜索空间上的扩展，包括每个Transformer层的深度、隐藏维度和头数。Burgerformer [236]更进一步考虑了微观设计，即操作类型、激活函数和归一化方式。NASViT扩展了BigNAS [243]和AlphaNet [221]，应用了“三明治采样法”来训练超网络。GLiT [30]提出了一种层次化的NAS方案，用于搜索混合卷积-注意力结构。它在NAS的第一阶段确定了每个层中的卷积和多头注意力头的数量，并在第二阶段确定了详细的超参数，如维度。

上述大多数用于Transformer架构的NAS方法（无论是用于NLP还是CV应用）的一个显著特点是它们使用基于超网络的权重共享方法，如表7所总结的。这可能是由于训练Transformer架构的巨大计算成本。使用基于超网络的NAS方法可以限制可以发现的架构范围，因为它对搜索空间施加了很大的约束，可能阻止了独特或创新架构的发现。因此，有必要探索更好的方法来平衡NAS技术的灵活性和效率。

__总结__（6.3 Transformer-specific NAS）：现有的NAS框架已经被扩展以设计更高效的Transformer架构。由于训练Transformer模型的巨大计算成本，尤其是与无监督预训练方法结合时，大多数现有方法都严重依赖权重共享方案和进化搜索。Transformer特定的NAS面临的一个关键挑战是现有的工作主要局限于调整相对较简单的超参数，如隐藏维度、层数和头的数量。然而，这可能排除了发现新型Transformer变体的可能性。

## 6.4 在Transformer上运行NAS和协同设计的案例研究

到目前为止，我们已经讨论了NAS的一般概念，其在硬件感知场景中的应用以及其在Transformer架构中的扩展。在这里，我们进行一个案例研究，以展示将NAS应用于Gemmini上的Transformer推断所带来的性能提升，旨在优化准确性以及延迟和能源等硬件成本。

### 6.4.1 实验设置

作为基准架构，我们使用一个6层的Transformer架构，其他模型配置与BERT-Base或GPT-2相同（详见表1）。我们考虑语言建模任务，并在WikiText-2 [154]基准数据集上对一个随机初始化的模型进行训练，该数据集包含37k个训练示例和4k个验证示例，并使用语言建模的训练目标。为了评估模型性能，我们使用验证示例的困惑度作为衡量标准，其中较低的分数表示更好的性能。独立的基准模型使用Adam优化器进行了50个epoch的训练，并采用线性学习率调度，峰值学习率范围为{5, 2, 1, 0.5} × 10−5。训练示例被连接起来，以达到最大序列长度为512，并使用批量大小为16的批量处理。

对于NAS，我们采用BigNAS风格的策略来训练一个超网络，然后使用进化算法从完全训练的超网络中搜索子网络。NAS的搜索空间包括层数𝑙、头数ℎ、隐藏维度𝑑和FFN维度𝑑FFN的各种组合（详见表8）。对于超网络的训练，我们使用与独立训练相同的训练超参数，只是在每次训练迭代中，我们随机选择四个子网络：最大可能的、最小可能的和两个随机选择的子网络。然后，使用三明治规则执行模型参数更新，即对这四个子网络的反向传播路径收集的梯度进行平均。

对于进化搜索，我们初始化一个由40个子网络组成的种群，并进行40轮进化迭代。在每次迭代中，收集每个子网络在目标硬件上的验证困惑度和能量延迟积（EDP），只保留 Pareto 最优的子网络。在这里，我们使用EDP作为单个硬件成本度量，因为它可以将多目标优化问题转化为单目标优化问题，将延迟和能量结合在一个度量指标中。然后，使用突变概率为0.2对保留的子网络进行突变，以填充下一次迭代的种群。为了测量硬件成本，我们使用基于查找表的方法快速评估每个子网络在目标硬件上的延迟和能量消耗，而不是使用可能耗时的RTL（寄存器传输逻辑）仿真。查找表中的条目是通过Timeloop [163]仿

真获得的，该仿真提供了每个操作的模拟延迟和能量数据。然后，通过对每个操作的成本求和来估计端到端的延迟和能量。在进化搜索之后，使用RTL模拟器对 Pareto 最优子网络进行评估，以获得更精确的延迟估计。对于能量度量，我们继续使用Timeloop的数据，因为通过RTL测量能量消耗在技术上具有挑战性。

对于目标硬件，我们使用Gemmini，并根据第3.4节中的优化措施配置Gemmini，其中包括专用的标准化单元，用于在芯片上运行非线性操作。我们根据第3.4.3节的见解，将Gemmini的Scratchpad大小配置为64kB，累加器大小配置为256kB，以最大化累加器大小。

### 6.4.2 实验结果

我们在图25中展示了NAS Pareto前沿的延迟和能量结果（蓝色曲线），其中每个点对应于通过上述进化搜索算法找到的不同Transformer架构。此外，我们还将从头开始训练的基线6层Transformer模型作为参考（×标记）。所有的EDP值都与基线EDP进行了归一化处理。请注意，基线模型对应于搜索空间中最大的Transformer架构（详见表8）。

首先，我们在图26中展示了基于EDP的进化搜索过程的结果。从图中可以看出，NAS框架使我们能够获得具有更好硬件成本与困惑度权衡的多个Transformer架构。也就是说，它找到了具有相似或甚至更好困惑度的架构，相比具有较小硬件成本的基线模型。作为示例，我们选择了具有最低EDP且困惑度损失小于+0.1的架构，其EDP为3.6×10^9，困惑度为22.51。架构参数列在表9中。这个架构展示了多样化搜索空间的重要性，因为每层的注意力头数在6到12之间变化，全连接层的维度在768到2560之间变化。通过能够按层改变这些参数，可以发现比如果为每层固定这些参数时更多的 Pareto 最优架构。

在图25中，我们将延迟和能量分开，并用RTL值替代延迟值。正如可以看到的，与从头开始训练的基线Transformer相比，可以实现1.4倍的延迟减少，困惑度仅降低0.1个点。如果可以容忍困惑度降低约1个点，延迟可以减少2.4倍，甚至可以使用更先进的架构搜索技术进一步减少。关于能量方面，可以在困惑度降低0.1个点的情况下实现1.6倍的改进，如果允许困惑度增加1个点，则可以实现4.4倍的改进。综合考虑，只需困惑度降低0.1个点，就可以将EDP减少2.2倍，而困惑度降低1个点，则可以将EDP减少10.6倍。这些示例展示了协同设计的能力，使从业者能够选择最符合其需求的组合。重要的是要注意，这代表了我们在特定硬件平台上运行的一次协同设计方法，结果可能因目标硬件和优化目标而异。

__总结__（第6.4节，Transformer的NAS案例研究）：本案例研究使用基于超网络的NAS方法来采样多样化的架构，然后通过进化搜索来发现在困惑度和能耗-延迟产品（一种运行效率度量）之间进行权衡的帕累托最优架构。许多发现的架构在经过优化的Gemmini硬件加速器上运行时，与从头开始训练的基准模型相比，具有显著的延迟和能耗改进。使用NAS来探索搜索空间的重要性得到了强调，因为许多性能良好的架构使用了多样化的层配置。在WikiText-2语言建模基准上进行训练时，这些技术找到了在容忍0.1个困惑度下降的情况下减少2.2倍能耗-延迟产品（EDP）的Transformer架构，并且在容忍1个困惑度下降的情况下减少10.6倍。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature25.png)
__图25__：（左）在我们优化的Gemmini硬件配置上通过进化搜索找到的Transformer架构的延迟-困惑度图和（右）能耗-困惑度图。与图26类似，更低的困惑度表示更好的性能，我们绘制线条来说明+0.1和+1个困惑度降低。

![Tux, the Linux mascot](/transformer/papers/images/SurveyFeature26.png)
__图26__：在我们的Gemmini硬件配置上通过进化搜索找到的Transformer架构的EDP-困惑度图。较低的困惑度表示经过训练的模型性能更好。为了更好地进行比较，我们还绘制了线条来说明+0.1和+1个困惑度降低。

在使用WikiText-2语言建模基准进行训练时，这些技术找到了Transformer架构，其EDP降低了2.2倍，同时容忍了0.1个困惑度的退化，并且与基准相比，容忍了1个困惑度的退化，达到了10.6倍的性能提升。

# 7 结论

Transformer架构[217]彻底改变了自然语言理解领域[52, 125, 143, 173, 174, 177, 240]，并随着近期数百亿参数的大语言模型的发展而进一步加速[22, 44, 58, 86, 175, 190, 198]。该架构还被扩展到包括计算机视觉[24, 36, 130, 152]和语音识别[23, 76, 109]在内的广泛领域。尽管Transformer模型已经显示出显著的性能改进，但其日益增长的规模和运行时复杂性对于高效推理提出了重大挑战。尽管能够实现快速高效的深度学习计算的DNN加速器可以成为可行的解决方案，但与CNN架构相比，对于Transformer工作负载的运行时特性和瓶颈以及有效运行这些模型所需的设计原则的理解仍然有限。

在本文中，我们对Transformer工作负载进行了全面的分析，以更好地了解运行时特性，并识别运行在商品硬件和加速器上的Transformer的性能瓶颈（第2节）。此外，我们对当前的硬件和软件解决方案进行了广泛的调查，旨在识别Transformer全栈部署中的潜在优化机会。具体而言，我们的调查涵盖以下主题：

* 用于Transformer推理的硬件架构设计，包括非线性操作对硬件加速器设计的影响（第3节）；
* 优化策略，例如修剪和量化，可应用于固定的Transformer架构以获得更好的性能（第4节）；
* Transformer架构中操作的映射和调度及相关挑战（第5节）；
* 使用自动化NAS设计更高效的Transformer架构并使其适应目标硬件（第6.1节）。

该研究的主要发现包括：

* 尽管FLOPs数量较小，但如果在设计面向特定领域的加速器时未正确考虑非线性操作，Transformer中的非线性操作（如Softmax和LayerNorm）可能对整体性能产生很大影响。与CNN中的BatchNorm操作在推理过程中可以吸收到之前的卷积层中不同，计算Softmax和LayerNorm等非线性操作还需要计算运行时统计信息。
* 与CNN的硬件设计可能不完全相同，例如，在Transformer应用中，增加累加器大小以实现更高的输出重用率可以显著提高Gemmini的性能。
* Transformer中的矩阵乘法调度似乎比CNN中的卷积调度更简单，因为矩阵乘法调度涉及3个循环，而卷积涉及6个循环。然而，我们观察到，矩阵乘法调度涉及类似数量的决策点和广泛的性能结果范围，因此与调度卷积一样具有挑战性。
* 将LayerNorm与Transformer架构中的前置矩阵乘法融合会对映射产生多个约束，尤其是与瓦片大小相关的约束。因此，在决定是否融合操作时必须仔细考虑，与通常认为融合操作通常有益的观点相反。

最后，在整篇论文中，我们进行了案例研究，以量化全栈Transformer推理中协同设计和协同优化技术的优势。总体而言，结果显示相较于没有考虑全栈因素的朴素实现，EDP改善了88.7倍，并且没有明显的性能损失。

* 在第3.4节中，我们应用硬件设计技术，以避免将不支持的操作卸载到主机CPU时产生的高通信开销。Gemmini最初设计用于CNN工作负载，使其能够在芯片上执行Softmax、LayerNorm和GELU需要额外的改动。我们通过实现专用的归一化单元来支持Softmax和LayerNorm，这带来了5-15%的面积成本和8%的延迟增加。然而，通过使用I-BERT中提出的多项式逼近在芯片上运行非线性操作，这种额外开销得到了补偿。结合内存层次结构的再平衡，这提供了净的39.6倍的延迟减少。
* 在第6.4节中，我们运行了NAS，以在流行的语言建模任务中权衡EDP和困惑度来搜索 Pareto-最优的Transformer架构。我们使用Timeloop模拟的数据估计了大量搜索空间中各种架构的成本，并指导了自动化的NAS过程。如图25所示，总贡献为2.24倍的EDP减少，没有明显的困惑度下降，和10.56倍的EDP减少与1个困惑度下降。我们使用Timeloop的模拟数据来估计大规模搜索空间中各种架构的成本，并指导自动NAS过程。如图25所示，总体贡献为EDP降低2.24倍，而感知困惑度无明显下降；以及EDP降低10.56倍，困惑度下降1个单位。

我们预计我们深入的分析和结果，以及本文中呈现的全面调查，将促进对Transformer推断的进一步理解，并从各个角度优化其推断效率。我们相信，这将使Transformer能够充分发挥其潜力，并将其应用范围扩展到比目前更广泛的领域。
我们预计我们深入的分析和结果，以及本文提出的全面调查将有助于进一步理解Transformer推理并从多个角度优化其推理效率。我们相信这将使Transformer发

# 8 致谢

我们对Meta以及Michael Anderson、Satish Nadathur和Summer Deng的慷慨支持表示感谢，还要感谢Google Cloud、Google TRC团队，特别是Jonathan Caton、David Patterson教授和Jing Li。Keutzer教授的实验室受到Intel公司、Intel VLAB团队、Intel One-API卓越中心的赞助，以及BDD和BAIR的资助。Sehoon Kim要感谢韩国高级研究基金会（KFAS）的支持。Amir Gholami获得了三星SAIT的资助。Michael W. Mahoney还要感谢J.P.摩根大通教师研究奖以及能源部、国家科学基金会和海军研究办公室的支持。我们的结论并不一定反映赞助机构的立场或政策，并且不应推断出任何官方认可。
