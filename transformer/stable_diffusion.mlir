/// pipeline_option : tops-hlir-pipeline{}
/// pipeline_option_flag : 

/// IR Dump After hlir::HlirFirstPass ///


module attributes {dtu_hlir.executable_id = 1 : i64, dtu_hlir.ktype = 2 : i64, dtu_hlir.layout = "NCHW", dtu_hlir.overflow_clamp = true, dtu_hlir.target = "dorado_3pg"} {
  func @main(%arg0: tensor<2x4x64x64xf32>, %arg1: tensor<1xf32>, %arg2: tensor<2x77x1024xf32>) -> tensor<2x4x64x64xf32> {
    %0 = dtu_hlir.constant  {node_name = "conv_in.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x4x3x3xf32>
    %1 = dtu_hlir.constant  {node_name = "conv_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %2 = dtu_hlir.constant  {node_name = "time_embedding.linear_1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf32>
    %3 = dtu_hlir.constant  {node_name = "time_embedding.linear_1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %4 = dtu_hlir.constant  {node_name = "time_embedding.linear_2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %5 = dtu_hlir.constant  {node_name = "time_embedding.linear_2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %6 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %7 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %8 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %9 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %10 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %11 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %12 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %13 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %14 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %15 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %16 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %17 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %18 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %19 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %20 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %21 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %22 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %23 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %24 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %25 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %26 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %27 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %28 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %29 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %30 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %31 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %32 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1280xf32>
    %33 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %34 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %35 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %36 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %37 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %38 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1280xf32>
    %39 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %40 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %41 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %42 = dtu_hlir.constant  {node_name = "down_blocks.0.downsamplers.0.conv.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %43 = dtu_hlir.constant  {node_name = "down_blocks.0.downsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %44 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %45 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %46 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %47 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %48 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %49 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %50 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %51 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %52 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %53 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %54 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %55 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %56 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %57 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %58 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %59 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %60 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %61 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %62 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %63 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %64 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %65 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %66 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %67 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %68 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x320x3x3xf32>
    %69 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %70 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1280xf32>
    %71 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %72 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %73 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %74 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x320x1x1xf32>
    %75 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %76 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %77 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %78 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1280xf32>
    %79 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %80 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %81 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %82 = dtu_hlir.constant  {node_name = "down_blocks.1.downsamplers.0.conv.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %83 = dtu_hlir.constant  {node_name = "down_blocks.1.downsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %84 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %85 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %86 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %87 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %88 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %89 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %90 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %91 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %92 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %93 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %94 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %95 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %96 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %97 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %98 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %99 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %100 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %101 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %102 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %103 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %104 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %105 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %106 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %107 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %108 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x640x3x3xf32>
    %109 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %110 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %111 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %112 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %113 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %114 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x640x1x1xf32>
    %115 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %116 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %117 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %118 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %119 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %120 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %121 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %122 = dtu_hlir.constant  {node_name = "down_blocks.2.downsamplers.0.conv.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %123 = dtu_hlir.constant  {node_name = "down_blocks.2.downsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %124 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %125 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %126 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %127 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %128 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %129 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %130 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %131 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %132 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %133 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %134 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %135 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %136 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x3x3xf32>
    %137 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %138 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %139 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %140 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %141 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %142 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x1x1xf32>
    %143 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %144 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x3x3xf32>
    %145 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %146 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %147 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %148 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %149 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %150 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x1x1xf32>
    %151 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %152 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x3x3xf32>
    %153 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %154 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %155 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %156 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %157 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %158 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x1x1xf32>
    %159 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %160 = dtu_hlir.constant  {node_name = "up_blocks.0.upsamplers.0.conv.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %161 = dtu_hlir.constant  {node_name = "up_blocks.0.upsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %162 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %163 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %164 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %165 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %166 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %167 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %168 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %169 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %170 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %171 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %172 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %173 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %174 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %175 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %176 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %177 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %178 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %179 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %180 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %181 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %182 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %183 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %184 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %185 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %186 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %187 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %188 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %189 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %190 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %191 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %192 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %193 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %194 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %195 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %196 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %197 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %198 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x3x3xf32>
    %199 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %200 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %201 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %202 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %203 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %204 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x1x1xf32>
    %205 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %206 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x3x3xf32>
    %207 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %208 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %209 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %210 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %211 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %212 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x2560x1x1xf32>
    %213 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %214 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1920x3x3xf32>
    %215 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %216 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %217 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %218 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %219 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %220 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1920x1x1xf32>
    %221 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %222 = dtu_hlir.constant  {node_name = "up_blocks.1.upsamplers.0.conv.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %223 = dtu_hlir.constant  {node_name = "up_blocks.1.upsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %224 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %225 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %226 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %227 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %228 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %229 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %230 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %231 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %232 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %233 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %234 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %235 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %236 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %237 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %238 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %239 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %240 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %241 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %242 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %243 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %244 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %245 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %246 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %247 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %248 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %249 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %250 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %251 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %252 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %253 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %254 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %255 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %256 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %257 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %258 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %259 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %260 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1920x3x3xf32>
    %261 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %262 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1280xf32>
    %263 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %264 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %265 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %266 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1920x1x1xf32>
    %267 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %268 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1280x3x3xf32>
    %269 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %270 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1280xf32>
    %271 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %272 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %273 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %274 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1280x1x1xf32>
    %275 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %276 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x960x3x3xf32>
    %277 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %278 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1280xf32>
    %279 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %280 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %281 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %282 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x960x1x1xf32>
    %283 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %284 = dtu_hlir.constant  {node_name = "up_blocks.2.upsamplers.0.conv.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640x3x3xf32>
    %285 = dtu_hlir.constant  {node_name = "up_blocks.2.upsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %286 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %287 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %288 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %289 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %290 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %291 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %292 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %293 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %294 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %295 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %296 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %297 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %298 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %299 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %300 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %301 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %302 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %303 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %304 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %305 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %306 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %307 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %308 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %309 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %310 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %311 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %312 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %313 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %314 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %315 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %316 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %317 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %318 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %319 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %320 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %321 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %322 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x960x3x3xf32>
    %323 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %324 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1280xf32>
    %325 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %326 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %327 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %328 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x960x1x1xf32>
    %329 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %330 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x640x3x3xf32>
    %331 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %332 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1280xf32>
    %333 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %334 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %335 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %336 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x640x1x1xf32>
    %337 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %338 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x640x3x3xf32>
    %339 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %340 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1280xf32>
    %341 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %342 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320x3x3xf32>
    %343 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %344 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv_shortcut.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x640x1x1xf32>
    %345 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %346 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %347 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %348 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %349 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %350 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %351 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %352 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %353 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %354 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %355 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %356 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %357 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %358 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %359 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %360 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %361 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %362 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %363 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %364 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %365 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %366 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.time_emb_proj.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %367 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.time_emb_proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %368 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280x3x3xf32>
    %369 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %370 = dtu_hlir.constant  {node_name = "conv_out.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<4x320x3x3xf32>
    %371 = dtu_hlir.constant  {node_name = "conv_out.bias", node_type = "Constant"} dense<[-0.0025138855, 0.00184345245, 3.44514847E-4, 0.00167942047]> : tensor<4xf32>
    %372 = dtu_hlir.constant  {node_name = "onnx::Mul_9836", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %373 = dtu_hlir.constant  {node_name = "onnx::Add_9837", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %374 = dtu_hlir.constant  {node_name = "onnx::Mul_9838", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %375 = dtu_hlir.constant  {node_name = "onnx::Add_9839", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %376 = dtu_hlir.constant  {node_name = "onnx::Mul_9840", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %377 = dtu_hlir.constant  {node_name = "onnx::Add_9841", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %378 = dtu_hlir.constant  {node_name = "onnx::MatMul_9842", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %379 = dtu_hlir.constant  {node_name = "onnx::MatMul_9843", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %380 = dtu_hlir.constant  {node_name = "onnx::MatMul_9844", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %381 = dtu_hlir.constant  {node_name = "onnx::MatMul_9845", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %382 = dtu_hlir.constant  {node_name = "onnx::Concat_9846", node_type = "Constant"} dense<5> : tensor<1xi64>
    %383 = dtu_hlir.constant  {node_name = "onnx::Mul_9849", node_type = "Constant"} dense<1.250000e-01> : tensor<f32>
    %384 = dtu_hlir.constant  {node_name = "onnx::Mul_9850", node_type = "Constant"} dense<0.000000e+00> : tensor<f32>
    %385 = dtu_hlir.constant  {node_name = "onnx::MatMul_9852", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %386 = dtu_hlir.constant  {node_name = "onnx::MatMul_9853", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %387 = dtu_hlir.constant  {node_name = "onnx::MatMul_9854", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %388 = dtu_hlir.constant  {node_name = "onnx::MatMul_9855", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %389 = dtu_hlir.constant  {node_name = "onnx::MatMul_9862", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %390 = dtu_hlir.constant  {node_name = "onnx::MatMul_9863", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf32>
    %391 = dtu_hlir.constant  {node_name = "onnx::MatMul_9864", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf32>
    %392 = dtu_hlir.constant  {node_name = "onnx::MatMul_9865", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %393 = dtu_hlir.constant  {node_name = "onnx::Mul_9866", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %394 = dtu_hlir.constant  {node_name = "onnx::Add_9867", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %395 = dtu_hlir.constant  {node_name = "onnx::Mul_9868", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %396 = dtu_hlir.constant  {node_name = "onnx::Add_9869", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %397 = dtu_hlir.constant  {node_name = "onnx::Mul_9870", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %398 = dtu_hlir.constant  {node_name = "onnx::Add_9871", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %399 = dtu_hlir.constant  {node_name = "onnx::MatMul_9872", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %400 = dtu_hlir.constant  {node_name = "onnx::MatMul_9873", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %401 = dtu_hlir.constant  {node_name = "onnx::MatMul_9874", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %402 = dtu_hlir.constant  {node_name = "onnx::MatMul_9875", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %403 = dtu_hlir.constant  {node_name = "onnx::MatMul_9882", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %404 = dtu_hlir.constant  {node_name = "onnx::MatMul_9883", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %405 = dtu_hlir.constant  {node_name = "onnx::MatMul_9884", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %406 = dtu_hlir.constant  {node_name = "onnx::MatMul_9885", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %407 = dtu_hlir.constant  {node_name = "onnx::MatMul_9892", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %408 = dtu_hlir.constant  {node_name = "onnx::MatMul_9893", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf32>
    %409 = dtu_hlir.constant  {node_name = "onnx::MatMul_9894", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf32>
    %410 = dtu_hlir.constant  {node_name = "onnx::MatMul_9895", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %411 = dtu_hlir.constant  {node_name = "onnx::Mul_9896", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %412 = dtu_hlir.constant  {node_name = "onnx::Add_9897", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %413 = dtu_hlir.constant  {node_name = "onnx::Mul_9898", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %414 = dtu_hlir.constant  {node_name = "onnx::Add_9899", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %415 = dtu_hlir.constant  {node_name = "onnx::Mul_9900", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %416 = dtu_hlir.constant  {node_name = "onnx::Add_9901", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %417 = dtu_hlir.constant  {node_name = "onnx::MatMul_9902", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %418 = dtu_hlir.constant  {node_name = "onnx::MatMul_9903", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %419 = dtu_hlir.constant  {node_name = "onnx::MatMul_9904", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %420 = dtu_hlir.constant  {node_name = "onnx::MatMul_9905", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %421 = dtu_hlir.constant  {node_name = "onnx::Concat_9906", node_type = "Constant"} dense<10> : tensor<1xi64>
    %422 = dtu_hlir.constant  {node_name = "onnx::MatMul_9912", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %423 = dtu_hlir.constant  {node_name = "onnx::MatMul_9913", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %424 = dtu_hlir.constant  {node_name = "onnx::MatMul_9914", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %425 = dtu_hlir.constant  {node_name = "onnx::MatMul_9915", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %426 = dtu_hlir.constant  {node_name = "onnx::MatMul_9922", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %427 = dtu_hlir.constant  {node_name = "onnx::MatMul_9923", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf32>
    %428 = dtu_hlir.constant  {node_name = "onnx::MatMul_9924", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf32>
    %429 = dtu_hlir.constant  {node_name = "onnx::MatMul_9925", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %430 = dtu_hlir.constant  {node_name = "onnx::Mul_9926", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %431 = dtu_hlir.constant  {node_name = "onnx::Add_9927", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %432 = dtu_hlir.constant  {node_name = "onnx::Mul_9928", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %433 = dtu_hlir.constant  {node_name = "onnx::Add_9929", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %434 = dtu_hlir.constant  {node_name = "onnx::Mul_9930", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %435 = dtu_hlir.constant  {node_name = "onnx::Add_9931", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %436 = dtu_hlir.constant  {node_name = "onnx::MatMul_9932", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %437 = dtu_hlir.constant  {node_name = "onnx::MatMul_9933", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %438 = dtu_hlir.constant  {node_name = "onnx::MatMul_9934", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %439 = dtu_hlir.constant  {node_name = "onnx::MatMul_9935", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %440 = dtu_hlir.constant  {node_name = "onnx::MatMul_9942", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %441 = dtu_hlir.constant  {node_name = "onnx::MatMul_9943", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %442 = dtu_hlir.constant  {node_name = "onnx::MatMul_9944", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %443 = dtu_hlir.constant  {node_name = "onnx::MatMul_9945", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %444 = dtu_hlir.constant  {node_name = "onnx::MatMul_9952", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %445 = dtu_hlir.constant  {node_name = "onnx::MatMul_9953", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf32>
    %446 = dtu_hlir.constant  {node_name = "onnx::MatMul_9954", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf32>
    %447 = dtu_hlir.constant  {node_name = "onnx::MatMul_9955", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %448 = dtu_hlir.constant  {node_name = "onnx::Mul_9956", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %449 = dtu_hlir.constant  {node_name = "onnx::Add_9957", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %450 = dtu_hlir.constant  {node_name = "onnx::Mul_9958", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %451 = dtu_hlir.constant  {node_name = "onnx::Add_9959", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %452 = dtu_hlir.constant  {node_name = "onnx::Mul_9960", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %453 = dtu_hlir.constant  {node_name = "onnx::Add_9961", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %454 = dtu_hlir.constant  {node_name = "onnx::MatMul_9962", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %455 = dtu_hlir.constant  {node_name = "onnx::MatMul_9963", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %456 = dtu_hlir.constant  {node_name = "onnx::MatMul_9964", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %457 = dtu_hlir.constant  {node_name = "onnx::MatMul_9965", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %458 = dtu_hlir.constant  {node_name = "onnx::Concat_9966", node_type = "Constant"} dense<20> : tensor<1xi64>
    %459 = dtu_hlir.constant  {node_name = "onnx::MatMul_9972", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %460 = dtu_hlir.constant  {node_name = "onnx::MatMul_9973", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %461 = dtu_hlir.constant  {node_name = "onnx::MatMul_9974", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %462 = dtu_hlir.constant  {node_name = "onnx::MatMul_9975", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %463 = dtu_hlir.constant  {node_name = "onnx::MatMul_9982", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %464 = dtu_hlir.constant  {node_name = "onnx::MatMul_9983", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf32>
    %465 = dtu_hlir.constant  {node_name = "onnx::MatMul_9984", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf32>
    %466 = dtu_hlir.constant  {node_name = "onnx::MatMul_9985", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %467 = dtu_hlir.constant  {node_name = "onnx::Mul_9986", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %468 = dtu_hlir.constant  {node_name = "onnx::Add_9987", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %469 = dtu_hlir.constant  {node_name = "onnx::Mul_9988", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %470 = dtu_hlir.constant  {node_name = "onnx::Add_9989", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %471 = dtu_hlir.constant  {node_name = "onnx::Mul_9990", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %472 = dtu_hlir.constant  {node_name = "onnx::Add_9991", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %473 = dtu_hlir.constant  {node_name = "onnx::MatMul_9992", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %474 = dtu_hlir.constant  {node_name = "onnx::MatMul_9993", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %475 = dtu_hlir.constant  {node_name = "onnx::MatMul_9994", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %476 = dtu_hlir.constant  {node_name = "onnx::MatMul_9995", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %477 = dtu_hlir.constant  {node_name = "onnx::MatMul_10002", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %478 = dtu_hlir.constant  {node_name = "onnx::MatMul_10003", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %479 = dtu_hlir.constant  {node_name = "onnx::MatMul_10004", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %480 = dtu_hlir.constant  {node_name = "onnx::MatMul_10005", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %481 = dtu_hlir.constant  {node_name = "onnx::MatMul_10012", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %482 = dtu_hlir.constant  {node_name = "onnx::MatMul_10013", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf32>
    %483 = dtu_hlir.constant  {node_name = "onnx::MatMul_10014", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf32>
    %484 = dtu_hlir.constant  {node_name = "onnx::MatMul_10015", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %485 = dtu_hlir.constant  {node_name = "onnx::Mul_10016", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %486 = dtu_hlir.constant  {node_name = "onnx::Add_10017", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %487 = dtu_hlir.constant  {node_name = "onnx::Mul_10018", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %488 = dtu_hlir.constant  {node_name = "onnx::Add_10019", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %489 = dtu_hlir.constant  {node_name = "onnx::Mul_10020", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %490 = dtu_hlir.constant  {node_name = "onnx::Add_10021", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %491 = dtu_hlir.constant  {node_name = "onnx::Mul_10022", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %492 = dtu_hlir.constant  {node_name = "onnx::Add_10023", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %493 = dtu_hlir.constant  {node_name = "onnx::Mul_10024", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %494 = dtu_hlir.constant  {node_name = "onnx::Add_10025", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %495 = dtu_hlir.constant  {node_name = "onnx::Mul_10026", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %496 = dtu_hlir.constant  {node_name = "onnx::Add_10027", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %497 = dtu_hlir.constant  {node_name = "onnx::Mul_10028", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %498 = dtu_hlir.constant  {node_name = "onnx::Add_10029", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %499 = dtu_hlir.constant  {node_name = "onnx::MatMul_10030", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %500 = dtu_hlir.constant  {node_name = "onnx::MatMul_10031", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %501 = dtu_hlir.constant  {node_name = "onnx::MatMul_10032", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %502 = dtu_hlir.constant  {node_name = "onnx::MatMul_10033", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %503 = dtu_hlir.constant  {node_name = "onnx::MatMul_10040", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %504 = dtu_hlir.constant  {node_name = "onnx::MatMul_10041", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %505 = dtu_hlir.constant  {node_name = "onnx::MatMul_10042", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %506 = dtu_hlir.constant  {node_name = "onnx::MatMul_10043", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %507 = dtu_hlir.constant  {node_name = "onnx::MatMul_10050", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %508 = dtu_hlir.constant  {node_name = "onnx::MatMul_10051", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf32>
    %509 = dtu_hlir.constant  {node_name = "onnx::MatMul_10052", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf32>
    %510 = dtu_hlir.constant  {node_name = "onnx::MatMul_10053", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %511 = dtu_hlir.constant  {node_name = "onnx::Mul_10054", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %512 = dtu_hlir.constant  {node_name = "onnx::Add_10055", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %513 = dtu_hlir.constant  {node_name = "onnx::Mul_10056", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %514 = dtu_hlir.constant  {node_name = "onnx::Add_10057", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %515 = dtu_hlir.constant  {node_name = "onnx::Mul_10058", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %516 = dtu_hlir.constant  {node_name = "onnx::Add_10059", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %517 = dtu_hlir.constant  {node_name = "onnx::Mul_10060", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %518 = dtu_hlir.constant  {node_name = "onnx::Add_10061", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %519 = dtu_hlir.constant  {node_name = "onnx::Mul_10062", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %520 = dtu_hlir.constant  {node_name = "onnx::Add_10063", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %521 = dtu_hlir.constant  {node_name = "onnx::Mul_10064", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %522 = dtu_hlir.constant  {node_name = "onnx::Add_10065", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %523 = dtu_hlir.constant  {node_name = "onnx::Mul_10066", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %524 = dtu_hlir.constant  {node_name = "onnx::Add_10067", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %525 = dtu_hlir.constant  {node_name = "onnx::Mul_10068", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %526 = dtu_hlir.constant  {node_name = "onnx::Add_10069", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %527 = dtu_hlir.constant  {node_name = "onnx::Resize_10070", node_type = "Constant"} dense<[1.000000e+00, 1.000000e+00, 2.000000e+00, 2.000000e+00]> : tensor<4xf32>
    %528 = dtu_hlir.constant  {node_name = "onnx::Mul_10071", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %529 = dtu_hlir.constant  {node_name = "onnx::Add_10072", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %530 = dtu_hlir.constant  {node_name = "onnx::Mul_10073", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %531 = dtu_hlir.constant  {node_name = "onnx::Add_10074", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %532 = dtu_hlir.constant  {node_name = "onnx::Mul_10075", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %533 = dtu_hlir.constant  {node_name = "onnx::Add_10076", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %534 = dtu_hlir.constant  {node_name = "onnx::MatMul_10077", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %535 = dtu_hlir.constant  {node_name = "onnx::MatMul_10078", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %536 = dtu_hlir.constant  {node_name = "onnx::MatMul_10079", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %537 = dtu_hlir.constant  {node_name = "onnx::MatMul_10080", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %538 = dtu_hlir.constant  {node_name = "onnx::MatMul_10087", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %539 = dtu_hlir.constant  {node_name = "onnx::MatMul_10088", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %540 = dtu_hlir.constant  {node_name = "onnx::MatMul_10089", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %541 = dtu_hlir.constant  {node_name = "onnx::MatMul_10090", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %542 = dtu_hlir.constant  {node_name = "onnx::MatMul_10097", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %543 = dtu_hlir.constant  {node_name = "onnx::MatMul_10098", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf32>
    %544 = dtu_hlir.constant  {node_name = "onnx::MatMul_10099", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf32>
    %545 = dtu_hlir.constant  {node_name = "onnx::MatMul_10100", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %546 = dtu_hlir.constant  {node_name = "onnx::Mul_10101", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %547 = dtu_hlir.constant  {node_name = "onnx::Add_10102", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x1x1xf32>
    %548 = dtu_hlir.constant  {node_name = "onnx::Mul_10103", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %549 = dtu_hlir.constant  {node_name = "onnx::Add_10104", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %550 = dtu_hlir.constant  {node_name = "onnx::Mul_10105", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %551 = dtu_hlir.constant  {node_name = "onnx::Add_10106", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %552 = dtu_hlir.constant  {node_name = "onnx::MatMul_10107", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %553 = dtu_hlir.constant  {node_name = "onnx::MatMul_10108", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %554 = dtu_hlir.constant  {node_name = "onnx::MatMul_10109", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %555 = dtu_hlir.constant  {node_name = "onnx::MatMul_10110", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %556 = dtu_hlir.constant  {node_name = "onnx::MatMul_10117", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %557 = dtu_hlir.constant  {node_name = "onnx::MatMul_10118", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %558 = dtu_hlir.constant  {node_name = "onnx::MatMul_10119", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %559 = dtu_hlir.constant  {node_name = "onnx::MatMul_10120", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %560 = dtu_hlir.constant  {node_name = "onnx::MatMul_10127", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %561 = dtu_hlir.constant  {node_name = "onnx::MatMul_10128", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf32>
    %562 = dtu_hlir.constant  {node_name = "onnx::MatMul_10129", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf32>
    %563 = dtu_hlir.constant  {node_name = "onnx::MatMul_10130", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %564 = dtu_hlir.constant  {node_name = "onnx::Mul_10131", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1920x1x1xf32>
    %565 = dtu_hlir.constant  {node_name = "onnx::Add_10132", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1920x1x1xf32>
    %566 = dtu_hlir.constant  {node_name = "onnx::Mul_10133", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %567 = dtu_hlir.constant  {node_name = "onnx::Add_10134", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %568 = dtu_hlir.constant  {node_name = "onnx::Mul_10135", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %569 = dtu_hlir.constant  {node_name = "onnx::Add_10136", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %570 = dtu_hlir.constant  {node_name = "onnx::MatMul_10137", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %571 = dtu_hlir.constant  {node_name = "onnx::MatMul_10138", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %572 = dtu_hlir.constant  {node_name = "onnx::MatMul_10139", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %573 = dtu_hlir.constant  {node_name = "onnx::MatMul_10140", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %574 = dtu_hlir.constant  {node_name = "onnx::MatMul_10147", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %575 = dtu_hlir.constant  {node_name = "onnx::MatMul_10148", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %576 = dtu_hlir.constant  {node_name = "onnx::MatMul_10149", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %577 = dtu_hlir.constant  {node_name = "onnx::MatMul_10150", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf32>
    %578 = dtu_hlir.constant  {node_name = "onnx::MatMul_10157", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %579 = dtu_hlir.constant  {node_name = "onnx::MatMul_10158", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf32>
    %580 = dtu_hlir.constant  {node_name = "onnx::MatMul_10159", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf32>
    %581 = dtu_hlir.constant  {node_name = "onnx::MatMul_10160", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf32>
    %582 = dtu_hlir.constant  {node_name = "onnx::Mul_10162", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1920x1x1xf32>
    %583 = dtu_hlir.constant  {node_name = "onnx::Add_10163", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1920x1x1xf32>
    %584 = dtu_hlir.constant  {node_name = "onnx::Mul_10164", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %585 = dtu_hlir.constant  {node_name = "onnx::Add_10165", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %586 = dtu_hlir.constant  {node_name = "onnx::Mul_10166", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %587 = dtu_hlir.constant  {node_name = "onnx::Add_10167", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %588 = dtu_hlir.constant  {node_name = "onnx::MatMul_10168", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %589 = dtu_hlir.constant  {node_name = "onnx::MatMul_10169", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %590 = dtu_hlir.constant  {node_name = "onnx::MatMul_10170", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %591 = dtu_hlir.constant  {node_name = "onnx::MatMul_10171", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %592 = dtu_hlir.constant  {node_name = "onnx::MatMul_10178", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %593 = dtu_hlir.constant  {node_name = "onnx::MatMul_10179", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %594 = dtu_hlir.constant  {node_name = "onnx::MatMul_10180", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %595 = dtu_hlir.constant  {node_name = "onnx::MatMul_10181", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %596 = dtu_hlir.constant  {node_name = "onnx::MatMul_10188", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %597 = dtu_hlir.constant  {node_name = "onnx::MatMul_10189", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf32>
    %598 = dtu_hlir.constant  {node_name = "onnx::MatMul_10190", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf32>
    %599 = dtu_hlir.constant  {node_name = "onnx::MatMul_10191", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %600 = dtu_hlir.constant  {node_name = "onnx::Mul_10192", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %601 = dtu_hlir.constant  {node_name = "onnx::Add_10193", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x1x1xf32>
    %602 = dtu_hlir.constant  {node_name = "onnx::Mul_10194", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %603 = dtu_hlir.constant  {node_name = "onnx::Add_10195", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %604 = dtu_hlir.constant  {node_name = "onnx::Mul_10196", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %605 = dtu_hlir.constant  {node_name = "onnx::Add_10197", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %606 = dtu_hlir.constant  {node_name = "onnx::MatMul_10198", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %607 = dtu_hlir.constant  {node_name = "onnx::MatMul_10199", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %608 = dtu_hlir.constant  {node_name = "onnx::MatMul_10200", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %609 = dtu_hlir.constant  {node_name = "onnx::MatMul_10201", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %610 = dtu_hlir.constant  {node_name = "onnx::MatMul_10208", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %611 = dtu_hlir.constant  {node_name = "onnx::MatMul_10209", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %612 = dtu_hlir.constant  {node_name = "onnx::MatMul_10210", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %613 = dtu_hlir.constant  {node_name = "onnx::MatMul_10211", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %614 = dtu_hlir.constant  {node_name = "onnx::MatMul_10218", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %615 = dtu_hlir.constant  {node_name = "onnx::MatMul_10219", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf32>
    %616 = dtu_hlir.constant  {node_name = "onnx::MatMul_10220", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf32>
    %617 = dtu_hlir.constant  {node_name = "onnx::MatMul_10221", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %618 = dtu_hlir.constant  {node_name = "onnx::Mul_10222", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<960x1x1xf32>
    %619 = dtu_hlir.constant  {node_name = "onnx::Add_10223", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<960x1x1xf32>
    %620 = dtu_hlir.constant  {node_name = "onnx::Mul_10224", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %621 = dtu_hlir.constant  {node_name = "onnx::Add_10225", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %622 = dtu_hlir.constant  {node_name = "onnx::Mul_10226", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %623 = dtu_hlir.constant  {node_name = "onnx::Add_10227", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %624 = dtu_hlir.constant  {node_name = "onnx::MatMul_10228", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %625 = dtu_hlir.constant  {node_name = "onnx::MatMul_10229", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %626 = dtu_hlir.constant  {node_name = "onnx::MatMul_10230", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %627 = dtu_hlir.constant  {node_name = "onnx::MatMul_10231", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %628 = dtu_hlir.constant  {node_name = "onnx::MatMul_10238", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %629 = dtu_hlir.constant  {node_name = "onnx::MatMul_10239", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %630 = dtu_hlir.constant  {node_name = "onnx::MatMul_10240", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %631 = dtu_hlir.constant  {node_name = "onnx::MatMul_10241", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf32>
    %632 = dtu_hlir.constant  {node_name = "onnx::MatMul_10248", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %633 = dtu_hlir.constant  {node_name = "onnx::MatMul_10249", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf32>
    %634 = dtu_hlir.constant  {node_name = "onnx::MatMul_10250", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf32>
    %635 = dtu_hlir.constant  {node_name = "onnx::MatMul_10251", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf32>
    %636 = dtu_hlir.constant  {node_name = "onnx::Mul_10253", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<960x1x1xf32>
    %637 = dtu_hlir.constant  {node_name = "onnx::Add_10254", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<960x1x1xf32>
    %638 = dtu_hlir.constant  {node_name = "onnx::Mul_10255", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %639 = dtu_hlir.constant  {node_name = "onnx::Add_10256", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %640 = dtu_hlir.constant  {node_name = "onnx::Mul_10257", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %641 = dtu_hlir.constant  {node_name = "onnx::Add_10258", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %642 = dtu_hlir.constant  {node_name = "onnx::MatMul_10259", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %643 = dtu_hlir.constant  {node_name = "onnx::MatMul_10260", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %644 = dtu_hlir.constant  {node_name = "onnx::MatMul_10261", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %645 = dtu_hlir.constant  {node_name = "onnx::MatMul_10262", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %646 = dtu_hlir.constant  {node_name = "onnx::MatMul_10269", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %647 = dtu_hlir.constant  {node_name = "onnx::MatMul_10270", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %648 = dtu_hlir.constant  {node_name = "onnx::MatMul_10271", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %649 = dtu_hlir.constant  {node_name = "onnx::MatMul_10272", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %650 = dtu_hlir.constant  {node_name = "onnx::MatMul_10279", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %651 = dtu_hlir.constant  {node_name = "onnx::MatMul_10280", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf32>
    %652 = dtu_hlir.constant  {node_name = "onnx::MatMul_10281", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf32>
    %653 = dtu_hlir.constant  {node_name = "onnx::MatMul_10282", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %654 = dtu_hlir.constant  {node_name = "onnx::Mul_10283", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %655 = dtu_hlir.constant  {node_name = "onnx::Add_10284", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %656 = dtu_hlir.constant  {node_name = "onnx::Mul_10285", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %657 = dtu_hlir.constant  {node_name = "onnx::Add_10286", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %658 = dtu_hlir.constant  {node_name = "onnx::Mul_10287", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %659 = dtu_hlir.constant  {node_name = "onnx::Add_10288", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %660 = dtu_hlir.constant  {node_name = "onnx::MatMul_10289", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %661 = dtu_hlir.constant  {node_name = "onnx::MatMul_10290", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %662 = dtu_hlir.constant  {node_name = "onnx::MatMul_10291", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %663 = dtu_hlir.constant  {node_name = "onnx::MatMul_10292", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %664 = dtu_hlir.constant  {node_name = "onnx::MatMul_10299", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %665 = dtu_hlir.constant  {node_name = "onnx::MatMul_10300", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %666 = dtu_hlir.constant  {node_name = "onnx::MatMul_10301", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %667 = dtu_hlir.constant  {node_name = "onnx::MatMul_10302", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %668 = dtu_hlir.constant  {node_name = "onnx::MatMul_10309", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %669 = dtu_hlir.constant  {node_name = "onnx::MatMul_10310", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf32>
    %670 = dtu_hlir.constant  {node_name = "onnx::MatMul_10311", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf32>
    %671 = dtu_hlir.constant  {node_name = "onnx::MatMul_10312", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %672 = dtu_hlir.constant  {node_name = "onnx::Mul_10313", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %673 = dtu_hlir.constant  {node_name = "onnx::Add_10314", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640x1x1xf32>
    %674 = dtu_hlir.constant  {node_name = "onnx::Mul_10315", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %675 = dtu_hlir.constant  {node_name = "onnx::Add_10316", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %676 = dtu_hlir.constant  {node_name = "onnx::Mul_10317", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %677 = dtu_hlir.constant  {node_name = "onnx::Add_10318", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %678 = dtu_hlir.constant  {node_name = "onnx::MatMul_10319", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %679 = dtu_hlir.constant  {node_name = "onnx::MatMul_10320", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %680 = dtu_hlir.constant  {node_name = "onnx::MatMul_10321", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %681 = dtu_hlir.constant  {node_name = "onnx::MatMul_10322", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %682 = dtu_hlir.constant  {node_name = "onnx::MatMul_10329", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %683 = dtu_hlir.constant  {node_name = "onnx::MatMul_10330", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %684 = dtu_hlir.constant  {node_name = "onnx::MatMul_10331", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %685 = dtu_hlir.constant  {node_name = "onnx::MatMul_10332", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf32>
    %686 = dtu_hlir.constant  {node_name = "onnx::MatMul_10339", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %687 = dtu_hlir.constant  {node_name = "onnx::MatMul_10340", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf32>
    %688 = dtu_hlir.constant  {node_name = "onnx::MatMul_10341", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf32>
    %689 = dtu_hlir.constant  {node_name = "onnx::MatMul_10342", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf32>
    %690 = dtu_hlir.constant  {node_name = "onnx::Mul_10343", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %691 = dtu_hlir.constant  {node_name = "onnx::Add_10344", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320x1x1xf32>
    %692 = "dtu_hlir.reshape"(%382) {node_name = "Identity_0-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %693 = "dtu_hlir.reshape"(%384) {node_name = "Identity_1-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %694 = "dtu_hlir.reshape"(%383) {node_name = "Identity_2-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %695 = "dtu_hlir.reshape"(%382) {node_name = "Identity_3-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %696 = "dtu_hlir.reshape"(%382) {node_name = "Identity_4-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %697 = "dtu_hlir.reshape"(%382) {node_name = "Identity_5-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %698 = "dtu_hlir.reshape"(%382) {node_name = "Identity_6-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %699 = "dtu_hlir.reshape"(%384) {node_name = "Identity_7-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %700 = "dtu_hlir.reshape"(%383) {node_name = "Identity_8-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %701 = "dtu_hlir.reshape"(%382) {node_name = "Identity_9-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %702 = "dtu_hlir.reshape"(%382) {node_name = "Identity_10-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %703 = "dtu_hlir.reshape"(%382) {node_name = "Identity_11-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %704 = "dtu_hlir.reshape"(%382) {node_name = "Identity_12-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %705 = "dtu_hlir.reshape"(%384) {node_name = "Identity_13-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %706 = "dtu_hlir.reshape"(%383) {node_name = "Identity_14-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %707 = "dtu_hlir.reshape"(%382) {node_name = "Identity_15-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %708 = "dtu_hlir.reshape"(%382) {node_name = "Identity_16-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %709 = "dtu_hlir.reshape"(%382) {node_name = "Identity_17-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %710 = "dtu_hlir.reshape"(%382) {node_name = "Identity_18-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %711 = "dtu_hlir.reshape"(%384) {node_name = "Identity_19-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %712 = "dtu_hlir.reshape"(%383) {node_name = "Identity_20-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %713 = "dtu_hlir.reshape"(%382) {node_name = "Identity_21-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %714 = "dtu_hlir.reshape"(%382) {node_name = "Identity_22-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %715 = "dtu_hlir.reshape"(%382) {node_name = "Identity_23-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %716 = "dtu_hlir.reshape"(%382) {node_name = "Identity_24-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %717 = "dtu_hlir.reshape"(%384) {node_name = "Identity_25-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %718 = "dtu_hlir.reshape"(%383) {node_name = "Identity_26-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %719 = "dtu_hlir.reshape"(%382) {node_name = "Identity_27-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %720 = "dtu_hlir.reshape"(%382) {node_name = "Identity_28-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %721 = "dtu_hlir.reshape"(%382) {node_name = "Identity_29-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %722 = "dtu_hlir.reshape"(%382) {node_name = "Identity_30-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %723 = "dtu_hlir.reshape"(%384) {node_name = "Identity_31-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %724 = "dtu_hlir.reshape"(%383) {node_name = "Identity_32-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %725 = "dtu_hlir.reshape"(%382) {node_name = "Identity_33-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %726 = "dtu_hlir.reshape"(%382) {node_name = "Identity_34-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %727 = "dtu_hlir.reshape"(%382) {node_name = "Identity_35-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %728 = "dtu_hlir.reshape"(%527) {node_name = "Identity_36-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<4xf32>) -> tensor<4xf32>
    %729 = "dtu_hlir.reshape"(%421) {node_name = "Identity_37-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %730 = "dtu_hlir.reshape"(%384) {node_name = "Identity_38-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %731 = "dtu_hlir.reshape"(%383) {node_name = "Identity_39-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %732 = "dtu_hlir.reshape"(%421) {node_name = "Identity_40-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %733 = "dtu_hlir.reshape"(%421) {node_name = "Identity_41-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %734 = "dtu_hlir.reshape"(%421) {node_name = "Identity_42-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %735 = "dtu_hlir.reshape"(%421) {node_name = "Identity_43-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %736 = "dtu_hlir.reshape"(%384) {node_name = "Identity_44-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %737 = "dtu_hlir.reshape"(%383) {node_name = "Identity_45-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %738 = "dtu_hlir.reshape"(%421) {node_name = "Identity_46-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %739 = "dtu_hlir.reshape"(%421) {node_name = "Identity_47-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %740 = "dtu_hlir.reshape"(%421) {node_name = "Identity_48-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %741 = "dtu_hlir.reshape"(%421) {node_name = "Identity_49-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %742 = "dtu_hlir.reshape"(%384) {node_name = "Identity_50-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %743 = "dtu_hlir.reshape"(%383) {node_name = "Identity_51-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %744 = "dtu_hlir.reshape"(%421) {node_name = "Identity_52-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %745 = "dtu_hlir.reshape"(%421) {node_name = "Identity_53-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %746 = "dtu_hlir.reshape"(%421) {node_name = "Identity_54-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %747 = "dtu_hlir.reshape"(%421) {node_name = "Identity_55-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %748 = "dtu_hlir.reshape"(%384) {node_name = "Identity_56-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %749 = "dtu_hlir.reshape"(%383) {node_name = "Identity_57-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %750 = "dtu_hlir.reshape"(%421) {node_name = "Identity_58-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %751 = "dtu_hlir.reshape"(%421) {node_name = "Identity_59-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %752 = "dtu_hlir.reshape"(%421) {node_name = "Identity_60-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %753 = "dtu_hlir.reshape"(%421) {node_name = "Identity_61-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %754 = "dtu_hlir.reshape"(%384) {node_name = "Identity_62-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %755 = "dtu_hlir.reshape"(%383) {node_name = "Identity_63-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %756 = "dtu_hlir.reshape"(%421) {node_name = "Identity_64-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %757 = "dtu_hlir.reshape"(%421) {node_name = "Identity_65-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %758 = "dtu_hlir.reshape"(%421) {node_name = "Identity_66-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %759 = "dtu_hlir.reshape"(%421) {node_name = "Identity_67-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %760 = "dtu_hlir.reshape"(%384) {node_name = "Identity_68-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %761 = "dtu_hlir.reshape"(%383) {node_name = "Identity_69-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %762 = "dtu_hlir.reshape"(%421) {node_name = "Identity_70-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %763 = "dtu_hlir.reshape"(%421) {node_name = "Identity_71-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %764 = "dtu_hlir.reshape"(%421) {node_name = "Identity_72-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %765 = "dtu_hlir.reshape"(%527) {node_name = "Identity_73-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<4xf32>) -> tensor<4xf32>
    %766 = "dtu_hlir.reshape"(%458) {node_name = "Identity_74-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %767 = "dtu_hlir.reshape"(%384) {node_name = "Identity_75-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %768 = "dtu_hlir.reshape"(%383) {node_name = "Identity_76-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %769 = "dtu_hlir.reshape"(%458) {node_name = "Identity_77-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %770 = "dtu_hlir.reshape"(%458) {node_name = "Identity_78-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %771 = "dtu_hlir.reshape"(%458) {node_name = "Identity_79-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %772 = "dtu_hlir.reshape"(%458) {node_name = "Identity_80-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %773 = "dtu_hlir.reshape"(%384) {node_name = "Identity_81-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %774 = "dtu_hlir.reshape"(%383) {node_name = "Identity_82-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %775 = "dtu_hlir.reshape"(%458) {node_name = "Identity_83-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %776 = "dtu_hlir.reshape"(%458) {node_name = "Identity_84-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %777 = "dtu_hlir.reshape"(%458) {node_name = "Identity_85-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %778 = "dtu_hlir.reshape"(%458) {node_name = "Identity_86-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %779 = "dtu_hlir.reshape"(%384) {node_name = "Identity_87-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %780 = "dtu_hlir.reshape"(%383) {node_name = "Identity_88-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %781 = "dtu_hlir.reshape"(%458) {node_name = "Identity_89-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %782 = "dtu_hlir.reshape"(%458) {node_name = "Identity_90-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %783 = "dtu_hlir.reshape"(%458) {node_name = "Identity_91-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %784 = "dtu_hlir.reshape"(%458) {node_name = "Identity_92-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %785 = "dtu_hlir.reshape"(%384) {node_name = "Identity_93-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %786 = "dtu_hlir.reshape"(%383) {node_name = "Identity_94-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %787 = "dtu_hlir.reshape"(%458) {node_name = "Identity_95-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %788 = "dtu_hlir.reshape"(%458) {node_name = "Identity_96-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %789 = "dtu_hlir.reshape"(%458) {node_name = "Identity_97-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %790 = "dtu_hlir.reshape"(%458) {node_name = "Identity_98-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %791 = "dtu_hlir.reshape"(%384) {node_name = "Identity_99-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %792 = "dtu_hlir.reshape"(%383) {node_name = "Identity_100-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %793 = "dtu_hlir.reshape"(%458) {node_name = "Identity_101-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %794 = "dtu_hlir.reshape"(%458) {node_name = "Identity_102-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %795 = "dtu_hlir.reshape"(%458) {node_name = "Identity_103-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %796 = "dtu_hlir.reshape"(%458) {node_name = "Identity_104-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %797 = "dtu_hlir.reshape"(%384) {node_name = "Identity_105-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %798 = "dtu_hlir.reshape"(%383) {node_name = "Identity_106-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %799 = "dtu_hlir.reshape"(%458) {node_name = "Identity_107-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %800 = "dtu_hlir.reshape"(%458) {node_name = "Identity_108-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %801 = "dtu_hlir.reshape"(%458) {node_name = "Identity_109-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %802 = "dtu_hlir.reshape"(%458) {node_name = "Identity_110-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %803 = "dtu_hlir.reshape"(%384) {node_name = "Identity_111-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %804 = "dtu_hlir.reshape"(%383) {node_name = "Identity_112-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %805 = "dtu_hlir.reshape"(%458) {node_name = "Identity_113-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %806 = "dtu_hlir.reshape"(%458) {node_name = "Identity_114-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %807 = "dtu_hlir.reshape"(%458) {node_name = "Identity_115-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %808 = "dtu_hlir.reshape"(%458) {node_name = "Identity_116-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %809 = "dtu_hlir.reshape"(%384) {node_name = "Identity_117-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %810 = "dtu_hlir.reshape"(%383) {node_name = "Identity_118-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %811 = "dtu_hlir.reshape"(%458) {node_name = "Identity_119-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %812 = "dtu_hlir.reshape"(%458) {node_name = "Identity_120-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %813 = "dtu_hlir.reshape"(%458) {node_name = "Identity_121-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %814 = "dtu_hlir.reshape"(%458) {node_name = "Identity_122-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %815 = "dtu_hlir.reshape"(%384) {node_name = "Identity_123-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %816 = "dtu_hlir.reshape"(%383) {node_name = "Identity_124-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %817 = "dtu_hlir.reshape"(%458) {node_name = "Identity_125-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %818 = "dtu_hlir.reshape"(%458) {node_name = "Identity_126-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %819 = "dtu_hlir.reshape"(%458) {node_name = "Identity_127-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %820 = "dtu_hlir.reshape"(%458) {node_name = "Identity_128-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %821 = "dtu_hlir.reshape"(%384) {node_name = "Identity_129-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %822 = "dtu_hlir.reshape"(%383) {node_name = "Identity_130-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %823 = "dtu_hlir.reshape"(%458) {node_name = "Identity_131-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %824 = "dtu_hlir.reshape"(%458) {node_name = "Identity_132-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %825 = "dtu_hlir.reshape"(%458) {node_name = "Identity_133-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %826 = "dtu_hlir.reshape"(%458) {node_name = "Identity_134-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %827 = "dtu_hlir.reshape"(%384) {node_name = "Identity_135-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %828 = "dtu_hlir.reshape"(%383) {node_name = "Identity_136-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %829 = "dtu_hlir.reshape"(%458) {node_name = "Identity_137-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %830 = "dtu_hlir.reshape"(%458) {node_name = "Identity_138-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %831 = "dtu_hlir.reshape"(%458) {node_name = "Identity_139-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %832 = "dtu_hlir.reshape"(%458) {node_name = "Identity_140-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %833 = "dtu_hlir.reshape"(%384) {node_name = "Identity_141-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %834 = "dtu_hlir.reshape"(%383) {node_name = "Identity_142-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %835 = "dtu_hlir.reshape"(%458) {node_name = "Identity_143-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %836 = "dtu_hlir.reshape"(%458) {node_name = "Identity_144-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %837 = "dtu_hlir.reshape"(%421) {node_name = "Identity_145-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %838 = "dtu_hlir.reshape"(%384) {node_name = "Identity_146-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %839 = "dtu_hlir.reshape"(%383) {node_name = "Identity_147-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %840 = "dtu_hlir.reshape"(%421) {node_name = "Identity_148-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %841 = "dtu_hlir.reshape"(%421) {node_name = "Identity_149-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %842 = "dtu_hlir.reshape"(%421) {node_name = "Identity_150-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %843 = "dtu_hlir.reshape"(%421) {node_name = "Identity_151-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %844 = "dtu_hlir.reshape"(%384) {node_name = "Identity_152-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %845 = "dtu_hlir.reshape"(%383) {node_name = "Identity_153-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %846 = "dtu_hlir.reshape"(%421) {node_name = "Identity_154-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %847 = "dtu_hlir.reshape"(%421) {node_name = "Identity_155-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %848 = "dtu_hlir.reshape"(%421) {node_name = "Identity_156-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %849 = "dtu_hlir.reshape"(%421) {node_name = "Identity_157-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %850 = "dtu_hlir.reshape"(%384) {node_name = "Identity_158-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %851 = "dtu_hlir.reshape"(%383) {node_name = "Identity_159-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %852 = "dtu_hlir.reshape"(%421) {node_name = "Identity_160-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %853 = "dtu_hlir.reshape"(%421) {node_name = "Identity_161-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %854 = "dtu_hlir.reshape"(%421) {node_name = "Identity_162-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %855 = "dtu_hlir.reshape"(%421) {node_name = "Identity_163-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %856 = "dtu_hlir.reshape"(%384) {node_name = "Identity_164-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %857 = "dtu_hlir.reshape"(%383) {node_name = "Identity_165-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %858 = "dtu_hlir.reshape"(%421) {node_name = "Identity_166-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %859 = "dtu_hlir.reshape"(%421) {node_name = "Identity_167-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %860 = "dtu_hlir.reshape"(%382) {node_name = "Identity_168-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %861 = "dtu_hlir.reshape"(%384) {node_name = "Identity_169-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %862 = "dtu_hlir.reshape"(%383) {node_name = "Identity_170-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %863 = "dtu_hlir.reshape"(%382) {node_name = "Identity_171-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %864 = "dtu_hlir.reshape"(%382) {node_name = "Identity_172-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %865 = "dtu_hlir.reshape"(%382) {node_name = "Identity_173-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %866 = "dtu_hlir.reshape"(%382) {node_name = "Identity_174-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %867 = "dtu_hlir.reshape"(%384) {node_name = "Identity_175-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %868 = "dtu_hlir.reshape"(%383) {node_name = "Identity_176-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %869 = "dtu_hlir.reshape"(%382) {node_name = "Identity_177-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %870 = "dtu_hlir.reshape"(%382) {node_name = "Identity_178-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %871 = "dtu_hlir.reshape"(%382) {node_name = "Identity_179-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %872 = "dtu_hlir.reshape"(%382) {node_name = "Identity_180-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %873 = "dtu_hlir.reshape"(%384) {node_name = "Identity_181-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %874 = "dtu_hlir.reshape"(%383) {node_name = "Identity_182-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<f32>
    %875 = "dtu_hlir.reshape"(%382) {node_name = "Identity_183-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %876 = "dtu_hlir.reshape"(%382) {node_name = "Identity_184-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %877 = "dtu_hlir.reshape"(%382) {node_name = "Identity_185-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %878 = "dtu_hlir.reshape"(%382) {node_name = "Identity_186-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %879 = "dtu_hlir.reshape"(%382) {node_name = "Identity_187-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %880 = "dtu_hlir.reshape"(%382) {node_name = "Identity_188-0", node_type = "Identity", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %881 = "dtu_hlir.shape"(%arg0) {end = 2147483647 : i64, node_name = "Shape_189-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4x64x64xf32>) -> tensor<4xi64>
    %882 = dtu_hlir.constant  {node_name = "Constant_190-0", node_type = "Constant"} dense<0> : tensor<i64>
    %883 = "dtu_hlir.gather"(%881, %882) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_191-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %884 = dtu_hlir.constant  {node_name = "Constant_192-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %885 = "dtu_hlir.unsqueeze"(%883, %884) {node_name = "Unsqueeze_193-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %886 = "dtu_hlir.concatenate"(%885) {dimension = 0 : i64, node_name = "Concat_194-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %887 = dtu_hlir.constant  {node_name = "Constant_195-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %888 = "dtu_hlir.dynamic_reshape"(%886, %887) {allowzero = 0 : i64, node_name = "Reshape_196-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %889 = "dtu_hlir.shape"(%888) {end = 2147483647 : i64, node_name = "Shape_197-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %890 = dtu_hlir.constant  {node_name = "ConstantOfShape_198-Const-0", node_type = "ConstantOfShape"} dense<1> : tensor<1xi64>
    %891 = "dtu_hlir.dynamic_broadcast_in_dim"(%890, %889) {node_name = "ConstantOfShape_198-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %892 = dtu_hlir.constant  {node_name = "Constant_199-0", node_type = "Constant"} dense<-1> : tensor<i64>
    %893 = "dtu_hlir.broadcast_in_dim"(%892) {node_name = "Mul_200-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<1xi64>
    %894 = "dtu_hlir.mul"(%891, %893) {node_name = "Mul_200-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %895 = "dtu_hlir.compare"(%888, %894) {comparison_direction = "EQ", node_name = "Equal_201-0", node_type = "Equal", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %896 = "dtu_hlir.select"(%895, %891, %888) {node_name = "Where_202-0", node_type = "Where", tif_quantize_type = "f16"} : (tensor<1xi1>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %897 = "dtu_hlir.dynamic_broadcast_in_dim"(%arg1, %896) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Expand_203-0", node_type = "Expand", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<1xi64>) -> tensor<2xf32>
    %898 = dtu_hlir.constant  {node_name = "Constant_204-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %899 = "dtu_hlir.unsqueeze"(%897, %898) {node_name = "Unsqueeze_205-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2xf32>, tensor<1xi64>) -> tensor<2x1xf32>
    %900 = "dtu_hlir.convert"(%899) {node_name = "Cast_206-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<2x1xf32>) -> tensor<2x1xf32>
    %901 = dtu_hlir.constant  {node_name = "Constant_207-0", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1x160xf32>
    %902 = "dtu_hlir.mul"(%900, %901) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_208-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1xf32>, tensor<1x160xf32>) -> tensor<2x160xf32>
    %903 = "dtu_hlir.sine"(%902) {node_name = "Sin_209-0", node_type = "Sin", tif_quantize_type = "f16"} : (tensor<2x160xf32>) -> tensor<2x160xf32>
    %904 = "dtu_hlir.cos"(%902) {node_name = "Cos_210-0", node_type = "Cos", tif_quantize_type = "f16"} : (tensor<2x160xf32>) -> tensor<2x160xf32>
    %905 = "dtu_hlir.concatenate"(%903, %904) {dimension = 1 : i64, node_name = "Concat_211-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x160xf32>, tensor<2x160xf32>) -> tensor<2x320xf32>
    %906 = "dtu_hlir.slice"(%905) {limit_indices = dense<[2, 320]> : tensor<2xi64>, node_name = "Slice_216-0", node_type = "Slice", start_indices = dense<[0, 160]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<2x320xf32>) -> tensor<2x160xf32>
    %907 = "dtu_hlir.slice"(%905) {limit_indices = dense<[2, 160]> : tensor<2xi64>, node_name = "Slice_221-0", node_type = "Slice", start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<2x320xf32>) -> tensor<2x160xf32>
    %908 = "dtu_hlir.concatenate"(%906, %907) {dimension = 1 : i64, node_name = "Concat_222-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x160xf32>, tensor<2x160xf32>) -> tensor<2x320xf32>
    %909 = "dtu_hlir.convert"(%908) {node_name = "Cast_223-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<2x320xf32>) -> tensor<2x320xf32>
    %910 = "dtu_hlir.transpose"(%2) {node_name = "Gemm_224-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x320xf32>) -> tensor<320x1280xf32>
    %911 = "dtu_hlir.gemm"(%909, %910, %3) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_224-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x320xf32>, tensor<320x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %912 = "dtu_hlir.sigmoid"(%911) {node_name = "Sigmoid_225-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %913 = "dtu_hlir.mul"(%911, %912) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_226-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %914 = "dtu_hlir.transpose"(%4) {node_name = "Gemm_227-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %915 = "dtu_hlir.gemm"(%913, %914, %5) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_227-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %916 = "dtu_hlir.conv_bias"(%arg0, %0, %1) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_228-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x4x64x64xf32>, tensor<320x4x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %917 = dtu_hlir.constant  {node_name = "Constant_229-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %918 = "dtu_hlir.dynamic_reshape"(%916, %917) {allowzero = 0 : i64, node_name = "Reshape_230-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %919 = dtu_hlir.constant  {node_name = "Constant_231-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %920 = dtu_hlir.constant  {node_name = "Constant_232-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %921 = "dtu_hlir.instance_norm"(%918, %919, %920) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_233-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %922 = "dtu_hlir.shape"(%916) {end = 2147483647 : i64, node_name = "Shape_234-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %923 = "dtu_hlir.dynamic_reshape"(%921, %922) {allowzero = 0 : i64, node_name = "Reshape_235-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %924 = "dtu_hlir.mul"(%923, %372) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_236-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %925 = "dtu_hlir.add"(%924, %373) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_237-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %926 = "dtu_hlir.sigmoid"(%925) {node_name = "Sigmoid_238-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %927 = "dtu_hlir.mul"(%925, %926) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_239-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %928 = "dtu_hlir.conv_bias"(%927, %30, %31) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_240-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %929 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_241-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %930 = "dtu_hlir.mul"(%915, %929) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_242-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %931 = "dtu_hlir.transpose"(%32) {node_name = "Gemm_243-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<320x1280xf32>) -> tensor<1280x320xf32>
    %932 = "dtu_hlir.gemm"(%930, %931, %33) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_243-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x320xf32>, tensor<320xf32>) -> tensor<2x320xf32>
    %933 = dtu_hlir.constant  {node_name = "Constant_244-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %934 = "dtu_hlir.unsqueeze"(%932, %933) {node_name = "Unsqueeze_245-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320xf32>, tensor<1xi64>) -> tensor<2x320x1xf32>
    %935 = dtu_hlir.constant  {node_name = "Constant_246-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %936 = "dtu_hlir.unsqueeze"(%934, %935) {node_name = "Unsqueeze_247-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320x1xf32>, tensor<1xi64>) -> tensor<2x320x1x1xf32>
    %937 = "dtu_hlir.add"(%928, %936) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_248-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %938 = dtu_hlir.constant  {node_name = "Constant_249-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %939 = "dtu_hlir.dynamic_reshape"(%937, %938) {allowzero = 0 : i64, node_name = "Reshape_250-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %940 = dtu_hlir.constant  {node_name = "Constant_251-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %941 = dtu_hlir.constant  {node_name = "Constant_252-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %942 = "dtu_hlir.instance_norm"(%939, %940, %941) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_253-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %943 = "dtu_hlir.shape"(%937) {end = 2147483647 : i64, node_name = "Shape_254-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %944 = "dtu_hlir.dynamic_reshape"(%942, %943) {allowzero = 0 : i64, node_name = "Reshape_255-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %945 = "dtu_hlir.mul"(%944, %374) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_256-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %946 = "dtu_hlir.add"(%945, %375) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_257-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %947 = "dtu_hlir.sigmoid"(%946) {node_name = "Sigmoid_258-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %948 = "dtu_hlir.mul"(%946, %947) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_259-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %949 = "dtu_hlir.conv_bias"(%948, %34, %35) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_260-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %950 = "dtu_hlir.add"(%916, %949) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_261-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %951 = dtu_hlir.constant  {node_name = "Constant_262-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %952 = "dtu_hlir.broadcast_in_dim"(%951) {node_name = "Div_263-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x320x64x64xf32>
    %953 = "dtu_hlir.div"(%950, %952) {node_name = "Div_263-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %954 = "dtu_hlir.shape"(%953) {end = 2147483647 : i64, node_name = "Shape_264-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %955 = dtu_hlir.constant  {node_name = "Constant_265-0", node_type = "Constant"} dense<0> : tensor<i64>
    %956 = "dtu_hlir.gather"(%954, %955) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_266-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %957 = "dtu_hlir.shape"(%953) {end = 2147483647 : i64, node_name = "Shape_267-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %958 = dtu_hlir.constant  {node_name = "Constant_268-0", node_type = "Constant"} dense<2> : tensor<i64>
    %959 = "dtu_hlir.gather"(%957, %958) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_269-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %960 = "dtu_hlir.shape"(%953) {end = 2147483647 : i64, node_name = "Shape_270-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %961 = dtu_hlir.constant  {node_name = "Constant_271-0", node_type = "Constant"} dense<3> : tensor<i64>
    %962 = "dtu_hlir.gather"(%960, %961) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_272-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %963 = dtu_hlir.constant  {node_name = "Constant_273-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %964 = "dtu_hlir.dynamic_reshape"(%953, %963) {allowzero = 0 : i64, node_name = "Reshape_274-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %965 = dtu_hlir.constant  {node_name = "Constant_275-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %966 = dtu_hlir.constant  {node_name = "Constant_276-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %967 = "dtu_hlir.instance_norm"(%964, %965, %966) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_277-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %968 = "dtu_hlir.shape"(%953) {end = 2147483647 : i64, node_name = "Shape_278-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %969 = "dtu_hlir.dynamic_reshape"(%967, %968) {allowzero = 0 : i64, node_name = "Reshape_279-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %970 = "dtu_hlir.mul"(%969, %376) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_280-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %971 = "dtu_hlir.add"(%970, %377) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_281-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %972 = "dtu_hlir.shape"(%971) {end = 2147483647 : i64, node_name = "Shape_282-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %973 = dtu_hlir.constant  {node_name = "Constant_283-0", node_type = "Constant"} dense<1> : tensor<i64>
    %974 = "dtu_hlir.gather"(%972, %973) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_284-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %975 = "dtu_hlir.transpose"(%971) {node_name = "Transpose_285-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x64x64x320xf32>
    %976 = "dtu_hlir.mul"(%959, %962) {node_name = "Mul_286-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %977 = dtu_hlir.constant  {node_name = "Constant_287-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %978 = "dtu_hlir.unsqueeze"(%956, %977) {node_name = "Unsqueeze_288-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %979 = dtu_hlir.constant  {node_name = "Constant_289-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %980 = "dtu_hlir.unsqueeze"(%976, %979) {node_name = "Unsqueeze_290-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %981 = dtu_hlir.constant  {node_name = "Constant_291-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %982 = "dtu_hlir.unsqueeze"(%974, %981) {node_name = "Unsqueeze_292-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %983 = "dtu_hlir.concatenate"(%978, %980, %982) {dimension = 0 : i64, node_name = "Concat_293-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %984 = "dtu_hlir.dynamic_reshape"(%975, %983) {allowzero = 0 : i64, node_name = "Reshape_294-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %985 = "dtu_hlir.dot_general"(%984, %378) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_295-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %986 = "dtu_hlir.add"(%6, %985) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_296-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %987 = dtu_hlir.constant  {node_name = "ReduceMean_297-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %988 = "dtu_hlir.reshape"(%987) {node_name = "ReduceMean_297-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %989 = "dtu_hlir.reduce"(%986, %988) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_297-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_297-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %990 = dtu_hlir.constant  {node_name = "ReduceMean_297-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %991 = "dtu_hlir.unsqueeze"(%989, %990) {node_name = "ReduceMean_297-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %992 = dtu_hlir.constant  {node_name = "ReduceMean_297-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %993 = "dtu_hlir.broadcast_in_dim"(%992) {node_name = "ReduceMean_297-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %994 = "dtu_hlir.div"(%991, %993) {node_name = "ReduceMean_297-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %995 = "dtu_hlir.sub"(%986, %994) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_298-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %996 = dtu_hlir.constant  {node_name = "Constant_299-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %997 = "dtu_hlir.broadcast_in_dim"(%996) {node_name = "Pow_300-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %998 = "dtu_hlir.pow"(%995, %997) {node_name = "Pow_300-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %999 = dtu_hlir.constant  {node_name = "ReduceMean_301-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1000 = "dtu_hlir.reshape"(%999) {node_name = "ReduceMean_301-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1001 = "dtu_hlir.reduce"(%998, %1000) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_301-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_301-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1002 = dtu_hlir.constant  {node_name = "ReduceMean_301-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1003 = "dtu_hlir.unsqueeze"(%1001, %1002) {node_name = "ReduceMean_301-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1004 = dtu_hlir.constant  {node_name = "ReduceMean_301-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1005 = "dtu_hlir.broadcast_in_dim"(%1004) {node_name = "ReduceMean_301-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1006 = "dtu_hlir.div"(%1003, %1005) {node_name = "ReduceMean_301-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1007 = dtu_hlir.constant  {node_name = "Constant_302-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %1008 = "dtu_hlir.broadcast_in_dim"(%1007) {node_name = "Add_303-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %1009 = "dtu_hlir.add"(%1006, %1008) {node_name = "Add_303-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1010 = "dtu_hlir.sqrt"(%1009) {node_name = "Sqrt_304-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1011 = "dtu_hlir.div"(%995, %1010) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_305-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1012 = "dtu_hlir.mul"(%1011, %11) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_306-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1013 = "dtu_hlir.add"(%1012, %12) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_307-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1014 = "dtu_hlir.dot_general"(%1013, %379) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_308-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1015 = "dtu_hlir.dot_general"(%1013, %380) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_309-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1016 = "dtu_hlir.dot_general"(%1013, %381) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_310-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1017 = "dtu_hlir.shape"(%1014) {end = 2147483647 : i64, node_name = "Shape_311-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1018 = dtu_hlir.constant  {node_name = "Constant_312-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1019 = "dtu_hlir.gather"(%1017, %1018) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_313-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1020 = "dtu_hlir.shape"(%1014) {end = 2147483647 : i64, node_name = "Shape_314-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1021 = dtu_hlir.constant  {node_name = "Constant_315-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1022 = "dtu_hlir.gather"(%1020, %1021) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_316-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1023 = "dtu_hlir.shape"(%1014) {end = 2147483647 : i64, node_name = "Shape_317-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1024 = dtu_hlir.constant  {node_name = "Constant_318-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1025 = "dtu_hlir.gather"(%1023, %1024) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_319-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1026 = dtu_hlir.constant  {node_name = "Constant_320-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1027 = "dtu_hlir.div"(%1025, %1026) {node_name = "Div_321-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1028 = "dtu_hlir.convert"(%1027) {node_name = "Cast_322-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1029 = "dtu_hlir.convert"(%1028) {node_name = "Cast_323-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1030 = dtu_hlir.constant  {node_name = "Constant_324-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1031 = "dtu_hlir.unsqueeze"(%1019, %1030) {node_name = "Unsqueeze_325-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1032 = dtu_hlir.constant  {node_name = "Constant_326-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1033 = "dtu_hlir.unsqueeze"(%1022, %1032) {node_name = "Unsqueeze_327-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1034 = dtu_hlir.constant  {node_name = "Constant_328-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1035 = "dtu_hlir.unsqueeze"(%1029, %1034) {node_name = "Unsqueeze_329-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1036 = "dtu_hlir.concatenate"(%1031, %1033, %382, %1035) {dimension = 0 : i64, node_name = "Concat_330-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1037 = "dtu_hlir.dynamic_reshape"(%1014, %1036) {allowzero = 0 : i64, node_name = "Reshape_331-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1038 = "dtu_hlir.transpose"(%1037) {node_name = "Transpose_332-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1039 = dtu_hlir.constant  {node_name = "Constant_333-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1040 = "dtu_hlir.mul"(%1019, %1039) {node_name = "Mul_334-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1041 = dtu_hlir.constant  {node_name = "Constant_335-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1042 = "dtu_hlir.div"(%1025, %1041) {node_name = "Div_336-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1043 = "dtu_hlir.convert"(%1042) {node_name = "Cast_337-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1044 = "dtu_hlir.convert"(%1043) {node_name = "Cast_338-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1045 = dtu_hlir.constant  {node_name = "Constant_339-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1046 = "dtu_hlir.unsqueeze"(%1040, %1045) {node_name = "Unsqueeze_340-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1047 = dtu_hlir.constant  {node_name = "Constant_341-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1048 = "dtu_hlir.unsqueeze"(%1022, %1047) {node_name = "Unsqueeze_342-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1049 = dtu_hlir.constant  {node_name = "Constant_343-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1050 = "dtu_hlir.unsqueeze"(%1044, %1049) {node_name = "Unsqueeze_344-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1051 = "dtu_hlir.concatenate"(%1046, %1048, %1050) {dimension = 0 : i64, node_name = "Concat_345-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1052 = "dtu_hlir.dynamic_reshape"(%1038, %1051) {allowzero = 0 : i64, node_name = "Reshape_346-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1053 = "dtu_hlir.shape"(%1015) {end = 2147483647 : i64, node_name = "Shape_347-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1054 = dtu_hlir.constant  {node_name = "Constant_348-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1055 = "dtu_hlir.gather"(%1053, %1054) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_349-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1056 = "dtu_hlir.shape"(%1015) {end = 2147483647 : i64, node_name = "Shape_350-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1057 = dtu_hlir.constant  {node_name = "Constant_351-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1058 = "dtu_hlir.gather"(%1056, %1057) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_352-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1059 = "dtu_hlir.shape"(%1015) {end = 2147483647 : i64, node_name = "Shape_353-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1060 = dtu_hlir.constant  {node_name = "Constant_354-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1061 = "dtu_hlir.gather"(%1059, %1060) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_355-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1062 = dtu_hlir.constant  {node_name = "Constant_356-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1063 = "dtu_hlir.div"(%1061, %1062) {node_name = "Div_357-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1064 = "dtu_hlir.convert"(%1063) {node_name = "Cast_358-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1065 = "dtu_hlir.convert"(%1064) {node_name = "Cast_359-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1066 = dtu_hlir.constant  {node_name = "Constant_360-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1067 = "dtu_hlir.unsqueeze"(%1055, %1066) {node_name = "Unsqueeze_361-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1068 = dtu_hlir.constant  {node_name = "Constant_362-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1069 = "dtu_hlir.unsqueeze"(%1058, %1068) {node_name = "Unsqueeze_363-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1070 = dtu_hlir.constant  {node_name = "Constant_364-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1071 = "dtu_hlir.unsqueeze"(%1065, %1070) {node_name = "Unsqueeze_365-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1072 = "dtu_hlir.concatenate"(%1067, %1069, %880, %1071) {dimension = 0 : i64, node_name = "Concat_366-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1073 = "dtu_hlir.dynamic_reshape"(%1015, %1072) {allowzero = 0 : i64, node_name = "Reshape_367-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1074 = "dtu_hlir.transpose"(%1073) {node_name = "Transpose_368-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1075 = dtu_hlir.constant  {node_name = "Constant_369-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1076 = "dtu_hlir.mul"(%1055, %1075) {node_name = "Mul_370-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1077 = dtu_hlir.constant  {node_name = "Constant_371-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1078 = "dtu_hlir.div"(%1061, %1077) {node_name = "Div_372-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1079 = "dtu_hlir.convert"(%1078) {node_name = "Cast_373-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1080 = "dtu_hlir.convert"(%1079) {node_name = "Cast_374-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1081 = dtu_hlir.constant  {node_name = "Constant_375-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1082 = "dtu_hlir.unsqueeze"(%1076, %1081) {node_name = "Unsqueeze_376-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1083 = dtu_hlir.constant  {node_name = "Constant_377-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1084 = "dtu_hlir.unsqueeze"(%1058, %1083) {node_name = "Unsqueeze_378-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1085 = dtu_hlir.constant  {node_name = "Constant_379-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1086 = "dtu_hlir.unsqueeze"(%1080, %1085) {node_name = "Unsqueeze_380-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1087 = "dtu_hlir.concatenate"(%1082, %1084, %1086) {dimension = 0 : i64, node_name = "Concat_381-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1088 = "dtu_hlir.dynamic_reshape"(%1074, %1087) {allowzero = 0 : i64, node_name = "Reshape_382-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1089 = "dtu_hlir.shape"(%1016) {end = 2147483647 : i64, node_name = "Shape_383-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1090 = dtu_hlir.constant  {node_name = "Constant_384-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1091 = "dtu_hlir.gather"(%1089, %1090) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_385-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1092 = "dtu_hlir.shape"(%1016) {end = 2147483647 : i64, node_name = "Shape_386-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1093 = dtu_hlir.constant  {node_name = "Constant_387-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1094 = "dtu_hlir.gather"(%1092, %1093) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_388-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1095 = "dtu_hlir.shape"(%1016) {end = 2147483647 : i64, node_name = "Shape_389-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1096 = dtu_hlir.constant  {node_name = "Constant_390-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1097 = "dtu_hlir.gather"(%1095, %1096) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_391-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1098 = dtu_hlir.constant  {node_name = "Constant_392-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1099 = "dtu_hlir.div"(%1097, %1098) {node_name = "Div_393-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1100 = "dtu_hlir.convert"(%1099) {node_name = "Cast_394-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1101 = "dtu_hlir.convert"(%1100) {node_name = "Cast_395-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1102 = dtu_hlir.constant  {node_name = "Constant_396-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1103 = "dtu_hlir.unsqueeze"(%1091, %1102) {node_name = "Unsqueeze_397-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1104 = dtu_hlir.constant  {node_name = "Constant_398-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1105 = "dtu_hlir.unsqueeze"(%1094, %1104) {node_name = "Unsqueeze_399-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1106 = dtu_hlir.constant  {node_name = "Constant_400-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1107 = "dtu_hlir.unsqueeze"(%1101, %1106) {node_name = "Unsqueeze_401-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1108 = "dtu_hlir.concatenate"(%1103, %1105, %879, %1107) {dimension = 0 : i64, node_name = "Concat_402-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1109 = "dtu_hlir.dynamic_reshape"(%1016, %1108) {allowzero = 0 : i64, node_name = "Reshape_403-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1110 = "dtu_hlir.transpose"(%1109) {node_name = "Transpose_404-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1111 = dtu_hlir.constant  {node_name = "Constant_405-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1112 = "dtu_hlir.mul"(%1091, %1111) {node_name = "Mul_406-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1113 = dtu_hlir.constant  {node_name = "Constant_407-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1114 = "dtu_hlir.div"(%1097, %1113) {node_name = "Div_408-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1115 = "dtu_hlir.convert"(%1114) {node_name = "Cast_409-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1116 = "dtu_hlir.convert"(%1115) {node_name = "Cast_410-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1117 = dtu_hlir.constant  {node_name = "Constant_411-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1118 = "dtu_hlir.unsqueeze"(%1112, %1117) {node_name = "Unsqueeze_412-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1119 = dtu_hlir.constant  {node_name = "Constant_413-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1120 = "dtu_hlir.unsqueeze"(%1094, %1119) {node_name = "Unsqueeze_414-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1121 = dtu_hlir.constant  {node_name = "Constant_415-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1122 = "dtu_hlir.unsqueeze"(%1116, %1121) {node_name = "Unsqueeze_416-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1123 = "dtu_hlir.concatenate"(%1118, %1120, %1122) {dimension = 0 : i64, node_name = "Concat_417-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1124 = "dtu_hlir.dynamic_reshape"(%1110, %1123) {allowzero = 0 : i64, node_name = "Reshape_418-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1125 = "dtu_hlir.shape"(%1052) {end = 2147483647 : i64, node_name = "Shape_419-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1126 = dtu_hlir.constant  {node_name = "Constant_420-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1127 = "dtu_hlir.gather"(%1125, %1126) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_421-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1128 = "dtu_hlir.shape"(%1052) {end = 2147483647 : i64, node_name = "Shape_422-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1129 = dtu_hlir.constant  {node_name = "Constant_423-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1130 = "dtu_hlir.gather"(%1128, %1129) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_424-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1131 = "dtu_hlir.shape"(%1088) {end = 2147483647 : i64, node_name = "Shape_425-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1132 = dtu_hlir.constant  {node_name = "Constant_426-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1133 = "dtu_hlir.gather"(%1131, %1132) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_427-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1134 = dtu_hlir.constant  {node_name = "Constant_428-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1135 = "dtu_hlir.unsqueeze"(%1127, %1134) {node_name = "Unsqueeze_429-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1136 = dtu_hlir.constant  {node_name = "Constant_430-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1137 = "dtu_hlir.unsqueeze"(%1130, %1136) {node_name = "Unsqueeze_431-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1138 = dtu_hlir.constant  {node_name = "Constant_432-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1139 = "dtu_hlir.unsqueeze"(%1133, %1138) {node_name = "Unsqueeze_433-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1140 = "dtu_hlir.concatenate"(%1135, %1137, %1139) {dimension = 0 : i64, node_name = "Concat_434-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1141 = dtu_hlir.constant  {node_name = "ConstantOfShape_435-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %1142 = "dtu_hlir.dynamic_broadcast_in_dim"(%1141, %1140) {node_name = "ConstantOfShape_435-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x4096xf32>
    %1143 = "dtu_hlir.transpose"(%1088) {node_name = "Transpose_436-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<10x64x4096xf32>
    %1144 = "dtu_hlir.dot_general"(%1052, %1143) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_437-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x4096xf32>) -> tensor<10x4096x4096xf32>
    %1145 = "dtu_hlir.broadcast_in_dim"(%383) {node_name = "Mul_438-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %1146 = "dtu_hlir.mul"(%1144, %1145) {node_name = "Mul_438-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1147 = "dtu_hlir.broadcast_in_dim"(%384) {node_name = "Mul_439-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %1148 = "dtu_hlir.mul"(%1142, %1147) {node_name = "Mul_439-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1149 = "dtu_hlir.add"(%1146, %1148) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_440-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1150 = "dtu_hlir.softmax"(%1149) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_441-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1151 = "dtu_hlir.convert"(%1150) {node_name = "Cast_442-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1152 = "dtu_hlir.dot_general"(%1151, %1124) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_443-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x64xf32>) -> tensor<10x4096x64xf32>
    %1153 = "dtu_hlir.shape"(%1152) {end = 2147483647 : i64, node_name = "Shape_444-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1154 = dtu_hlir.constant  {node_name = "Constant_445-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1155 = "dtu_hlir.gather"(%1153, %1154) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_446-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1156 = "dtu_hlir.shape"(%1152) {end = 2147483647 : i64, node_name = "Shape_447-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1157 = dtu_hlir.constant  {node_name = "Constant_448-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1158 = "dtu_hlir.gather"(%1156, %1157) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_449-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1159 = "dtu_hlir.shape"(%1152) {end = 2147483647 : i64, node_name = "Shape_450-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1160 = dtu_hlir.constant  {node_name = "Constant_451-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1161 = "dtu_hlir.gather"(%1159, %1160) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_452-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1162 = dtu_hlir.constant  {node_name = "Constant_453-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1163 = "dtu_hlir.div"(%1155, %1162) {node_name = "Div_454-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1164 = "dtu_hlir.convert"(%1163) {node_name = "Cast_455-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1165 = "dtu_hlir.convert"(%1164) {node_name = "Cast_456-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1166 = dtu_hlir.constant  {node_name = "Constant_457-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1167 = "dtu_hlir.unsqueeze"(%1165, %1166) {node_name = "Unsqueeze_458-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1168 = dtu_hlir.constant  {node_name = "Constant_459-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1169 = "dtu_hlir.unsqueeze"(%1158, %1168) {node_name = "Unsqueeze_460-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1170 = dtu_hlir.constant  {node_name = "Constant_461-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1171 = "dtu_hlir.unsqueeze"(%1161, %1170) {node_name = "Unsqueeze_462-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1172 = "dtu_hlir.concatenate"(%1167, %878, %1169, %1171) {dimension = 0 : i64, node_name = "Concat_463-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1173 = "dtu_hlir.dynamic_reshape"(%1152, %1172) {allowzero = 0 : i64, node_name = "Reshape_464-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %1174 = "dtu_hlir.transpose"(%1173) {node_name = "Transpose_465-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %1175 = dtu_hlir.constant  {node_name = "Constant_466-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1176 = "dtu_hlir.div"(%1155, %1175) {node_name = "Div_467-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1177 = "dtu_hlir.convert"(%1176) {node_name = "Cast_468-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1178 = "dtu_hlir.convert"(%1177) {node_name = "Cast_469-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1179 = dtu_hlir.constant  {node_name = "Constant_470-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1180 = "dtu_hlir.mul"(%1161, %1179) {node_name = "Mul_471-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1181 = dtu_hlir.constant  {node_name = "Constant_472-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1182 = "dtu_hlir.unsqueeze"(%1178, %1181) {node_name = "Unsqueeze_473-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1183 = dtu_hlir.constant  {node_name = "Constant_474-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1184 = "dtu_hlir.unsqueeze"(%1158, %1183) {node_name = "Unsqueeze_475-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1185 = dtu_hlir.constant  {node_name = "Constant_476-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1186 = "dtu_hlir.unsqueeze"(%1180, %1185) {node_name = "Unsqueeze_477-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1187 = "dtu_hlir.concatenate"(%1182, %1184, %1186) {dimension = 0 : i64, node_name = "Concat_478-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1188 = "dtu_hlir.dynamic_reshape"(%1174, %1187) {allowzero = 0 : i64, node_name = "Reshape_479-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %1189 = "dtu_hlir.dot_general"(%1188, %385) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_480-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1190 = "dtu_hlir.add"(%7, %1189) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_481-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1191 = "dtu_hlir.add"(%1190, %986) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_482-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1192 = dtu_hlir.constant  {node_name = "ReduceMean_483-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1193 = "dtu_hlir.reshape"(%1192) {node_name = "ReduceMean_483-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1194 = "dtu_hlir.reduce"(%1191, %1193) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_483-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_483-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1195 = dtu_hlir.constant  {node_name = "ReduceMean_483-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1196 = "dtu_hlir.unsqueeze"(%1194, %1195) {node_name = "ReduceMean_483-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1197 = dtu_hlir.constant  {node_name = "ReduceMean_483-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1198 = "dtu_hlir.broadcast_in_dim"(%1197) {node_name = "ReduceMean_483-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1199 = "dtu_hlir.div"(%1196, %1198) {node_name = "ReduceMean_483-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1200 = "dtu_hlir.sub"(%1191, %1199) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_484-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1201 = dtu_hlir.constant  {node_name = "Constant_485-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %1202 = "dtu_hlir.broadcast_in_dim"(%1201) {node_name = "Pow_486-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %1203 = "dtu_hlir.pow"(%1200, %1202) {node_name = "Pow_486-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1204 = dtu_hlir.constant  {node_name = "ReduceMean_487-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1205 = "dtu_hlir.reshape"(%1204) {node_name = "ReduceMean_487-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1206 = "dtu_hlir.reduce"(%1203, %1205) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_487-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_487-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1207 = dtu_hlir.constant  {node_name = "ReduceMean_487-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1208 = "dtu_hlir.unsqueeze"(%1206, %1207) {node_name = "ReduceMean_487-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1209 = dtu_hlir.constant  {node_name = "ReduceMean_487-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1210 = "dtu_hlir.broadcast_in_dim"(%1209) {node_name = "ReduceMean_487-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1211 = "dtu_hlir.div"(%1208, %1210) {node_name = "ReduceMean_487-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1212 = dtu_hlir.constant  {node_name = "Constant_488-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %1213 = "dtu_hlir.broadcast_in_dim"(%1212) {node_name = "Add_489-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %1214 = "dtu_hlir.add"(%1211, %1213) {node_name = "Add_489-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1215 = "dtu_hlir.sqrt"(%1214) {node_name = "Sqrt_490-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1216 = "dtu_hlir.div"(%1200, %1215) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_491-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1217 = "dtu_hlir.mul"(%1216, %13) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_492-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1218 = "dtu_hlir.add"(%1217, %14) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_493-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1219 = "dtu_hlir.dot_general"(%1218, %386) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_494-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1220 = "dtu_hlir.dot_general"(%arg2, %387) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_495-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %1221 = "dtu_hlir.dot_general"(%arg2, %388) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_496-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %1222 = "dtu_hlir.shape"(%1219) {end = 2147483647 : i64, node_name = "Shape_497-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1223 = dtu_hlir.constant  {node_name = "Constant_498-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1224 = "dtu_hlir.gather"(%1222, %1223) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_499-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1225 = "dtu_hlir.shape"(%1219) {end = 2147483647 : i64, node_name = "Shape_500-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1226 = dtu_hlir.constant  {node_name = "Constant_501-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1227 = "dtu_hlir.gather"(%1225, %1226) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_502-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1228 = "dtu_hlir.shape"(%1219) {end = 2147483647 : i64, node_name = "Shape_503-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1229 = dtu_hlir.constant  {node_name = "Constant_504-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1230 = "dtu_hlir.gather"(%1228, %1229) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_505-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1231 = dtu_hlir.constant  {node_name = "Constant_506-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1232 = "dtu_hlir.div"(%1230, %1231) {node_name = "Div_507-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1233 = "dtu_hlir.convert"(%1232) {node_name = "Cast_508-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1234 = "dtu_hlir.convert"(%1233) {node_name = "Cast_509-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1235 = dtu_hlir.constant  {node_name = "Constant_510-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1236 = "dtu_hlir.unsqueeze"(%1224, %1235) {node_name = "Unsqueeze_511-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1237 = dtu_hlir.constant  {node_name = "Constant_512-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1238 = "dtu_hlir.unsqueeze"(%1227, %1237) {node_name = "Unsqueeze_513-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1239 = dtu_hlir.constant  {node_name = "Constant_514-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1240 = "dtu_hlir.unsqueeze"(%1234, %1239) {node_name = "Unsqueeze_515-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1241 = "dtu_hlir.concatenate"(%1236, %1238, %877, %1240) {dimension = 0 : i64, node_name = "Concat_516-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1242 = "dtu_hlir.dynamic_reshape"(%1219, %1241) {allowzero = 0 : i64, node_name = "Reshape_517-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1243 = "dtu_hlir.transpose"(%1242) {node_name = "Transpose_518-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1244 = dtu_hlir.constant  {node_name = "Constant_519-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1245 = "dtu_hlir.mul"(%1224, %1244) {node_name = "Mul_520-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1246 = dtu_hlir.constant  {node_name = "Constant_521-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1247 = "dtu_hlir.div"(%1230, %1246) {node_name = "Div_522-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1248 = "dtu_hlir.convert"(%1247) {node_name = "Cast_523-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1249 = "dtu_hlir.convert"(%1248) {node_name = "Cast_524-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1250 = dtu_hlir.constant  {node_name = "Constant_525-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1251 = "dtu_hlir.unsqueeze"(%1245, %1250) {node_name = "Unsqueeze_526-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1252 = dtu_hlir.constant  {node_name = "Constant_527-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1253 = "dtu_hlir.unsqueeze"(%1227, %1252) {node_name = "Unsqueeze_528-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1254 = dtu_hlir.constant  {node_name = "Constant_529-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1255 = "dtu_hlir.unsqueeze"(%1249, %1254) {node_name = "Unsqueeze_530-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1256 = "dtu_hlir.concatenate"(%1251, %1253, %1255) {dimension = 0 : i64, node_name = "Concat_531-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1257 = "dtu_hlir.dynamic_reshape"(%1243, %1256) {allowzero = 0 : i64, node_name = "Reshape_532-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1258 = "dtu_hlir.shape"(%1220) {end = 2147483647 : i64, node_name = "Shape_533-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1259 = dtu_hlir.constant  {node_name = "Constant_534-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1260 = "dtu_hlir.gather"(%1258, %1259) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_535-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1261 = "dtu_hlir.shape"(%1220) {end = 2147483647 : i64, node_name = "Shape_536-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1262 = dtu_hlir.constant  {node_name = "Constant_537-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1263 = "dtu_hlir.gather"(%1261, %1262) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_538-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1264 = "dtu_hlir.shape"(%1220) {end = 2147483647 : i64, node_name = "Shape_539-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1265 = dtu_hlir.constant  {node_name = "Constant_540-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1266 = "dtu_hlir.gather"(%1264, %1265) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_541-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1267 = dtu_hlir.constant  {node_name = "Constant_542-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1268 = "dtu_hlir.div"(%1266, %1267) {node_name = "Div_543-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1269 = "dtu_hlir.convert"(%1268) {node_name = "Cast_544-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1270 = "dtu_hlir.convert"(%1269) {node_name = "Cast_545-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1271 = dtu_hlir.constant  {node_name = "Constant_546-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1272 = "dtu_hlir.unsqueeze"(%1260, %1271) {node_name = "Unsqueeze_547-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1273 = dtu_hlir.constant  {node_name = "Constant_548-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1274 = "dtu_hlir.unsqueeze"(%1263, %1273) {node_name = "Unsqueeze_549-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1275 = dtu_hlir.constant  {node_name = "Constant_550-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1276 = "dtu_hlir.unsqueeze"(%1270, %1275) {node_name = "Unsqueeze_551-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1277 = "dtu_hlir.concatenate"(%1272, %1274, %876, %1276) {dimension = 0 : i64, node_name = "Concat_552-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1278 = "dtu_hlir.dynamic_reshape"(%1220, %1277) {allowzero = 0 : i64, node_name = "Reshape_553-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %1279 = "dtu_hlir.transpose"(%1278) {node_name = "Transpose_554-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %1280 = dtu_hlir.constant  {node_name = "Constant_555-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1281 = "dtu_hlir.mul"(%1260, %1280) {node_name = "Mul_556-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1282 = dtu_hlir.constant  {node_name = "Constant_557-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1283 = "dtu_hlir.div"(%1266, %1282) {node_name = "Div_558-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1284 = "dtu_hlir.convert"(%1283) {node_name = "Cast_559-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1285 = "dtu_hlir.convert"(%1284) {node_name = "Cast_560-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1286 = dtu_hlir.constant  {node_name = "Constant_561-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1287 = "dtu_hlir.unsqueeze"(%1281, %1286) {node_name = "Unsqueeze_562-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1288 = dtu_hlir.constant  {node_name = "Constant_563-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1289 = "dtu_hlir.unsqueeze"(%1263, %1288) {node_name = "Unsqueeze_564-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1290 = dtu_hlir.constant  {node_name = "Constant_565-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1291 = "dtu_hlir.unsqueeze"(%1285, %1290) {node_name = "Unsqueeze_566-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1292 = "dtu_hlir.concatenate"(%1287, %1289, %1291) {dimension = 0 : i64, node_name = "Concat_567-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1293 = "dtu_hlir.dynamic_reshape"(%1279, %1292) {allowzero = 0 : i64, node_name = "Reshape_568-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %1294 = "dtu_hlir.shape"(%1221) {end = 2147483647 : i64, node_name = "Shape_569-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1295 = dtu_hlir.constant  {node_name = "Constant_570-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1296 = "dtu_hlir.gather"(%1294, %1295) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_571-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1297 = "dtu_hlir.shape"(%1221) {end = 2147483647 : i64, node_name = "Shape_572-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1298 = dtu_hlir.constant  {node_name = "Constant_573-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1299 = "dtu_hlir.gather"(%1297, %1298) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_574-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1300 = "dtu_hlir.shape"(%1221) {end = 2147483647 : i64, node_name = "Shape_575-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1301 = dtu_hlir.constant  {node_name = "Constant_576-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1302 = "dtu_hlir.gather"(%1300, %1301) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_577-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1303 = dtu_hlir.constant  {node_name = "Constant_578-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1304 = "dtu_hlir.div"(%1302, %1303) {node_name = "Div_579-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1305 = "dtu_hlir.convert"(%1304) {node_name = "Cast_580-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1306 = "dtu_hlir.convert"(%1305) {node_name = "Cast_581-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1307 = dtu_hlir.constant  {node_name = "Constant_582-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1308 = "dtu_hlir.unsqueeze"(%1296, %1307) {node_name = "Unsqueeze_583-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1309 = dtu_hlir.constant  {node_name = "Constant_584-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1310 = "dtu_hlir.unsqueeze"(%1299, %1309) {node_name = "Unsqueeze_585-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1311 = dtu_hlir.constant  {node_name = "Constant_586-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1312 = "dtu_hlir.unsqueeze"(%1306, %1311) {node_name = "Unsqueeze_587-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1313 = "dtu_hlir.concatenate"(%1308, %1310, %875, %1312) {dimension = 0 : i64, node_name = "Concat_588-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1314 = "dtu_hlir.dynamic_reshape"(%1221, %1313) {allowzero = 0 : i64, node_name = "Reshape_589-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %1315 = "dtu_hlir.transpose"(%1314) {node_name = "Transpose_590-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %1316 = dtu_hlir.constant  {node_name = "Constant_591-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1317 = "dtu_hlir.mul"(%1296, %1316) {node_name = "Mul_592-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1318 = dtu_hlir.constant  {node_name = "Constant_593-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1319 = "dtu_hlir.div"(%1302, %1318) {node_name = "Div_594-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1320 = "dtu_hlir.convert"(%1319) {node_name = "Cast_595-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1321 = "dtu_hlir.convert"(%1320) {node_name = "Cast_596-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1322 = dtu_hlir.constant  {node_name = "Constant_597-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1323 = "dtu_hlir.unsqueeze"(%1317, %1322) {node_name = "Unsqueeze_598-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1324 = dtu_hlir.constant  {node_name = "Constant_599-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1325 = "dtu_hlir.unsqueeze"(%1299, %1324) {node_name = "Unsqueeze_600-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1326 = dtu_hlir.constant  {node_name = "Constant_601-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1327 = "dtu_hlir.unsqueeze"(%1321, %1326) {node_name = "Unsqueeze_602-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1328 = "dtu_hlir.concatenate"(%1323, %1325, %1327) {dimension = 0 : i64, node_name = "Concat_603-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1329 = "dtu_hlir.dynamic_reshape"(%1315, %1328) {allowzero = 0 : i64, node_name = "Reshape_604-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %1330 = "dtu_hlir.shape"(%1257) {end = 2147483647 : i64, node_name = "Shape_605-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1331 = dtu_hlir.constant  {node_name = "Constant_606-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1332 = "dtu_hlir.gather"(%1330, %1331) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_607-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1333 = "dtu_hlir.shape"(%1257) {end = 2147483647 : i64, node_name = "Shape_608-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1334 = dtu_hlir.constant  {node_name = "Constant_609-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1335 = "dtu_hlir.gather"(%1333, %1334) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_610-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1336 = "dtu_hlir.shape"(%1293) {end = 2147483647 : i64, node_name = "Shape_611-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<3xi64>
    %1337 = dtu_hlir.constant  {node_name = "Constant_612-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1338 = "dtu_hlir.gather"(%1336, %1337) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_613-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1339 = dtu_hlir.constant  {node_name = "Constant_614-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1340 = "dtu_hlir.unsqueeze"(%1332, %1339) {node_name = "Unsqueeze_615-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1341 = dtu_hlir.constant  {node_name = "Constant_616-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1342 = "dtu_hlir.unsqueeze"(%1335, %1341) {node_name = "Unsqueeze_617-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1343 = dtu_hlir.constant  {node_name = "Constant_618-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1344 = "dtu_hlir.unsqueeze"(%1338, %1343) {node_name = "Unsqueeze_619-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1345 = "dtu_hlir.concatenate"(%1340, %1342, %1344) {dimension = 0 : i64, node_name = "Concat_620-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1346 = dtu_hlir.constant  {node_name = "ConstantOfShape_621-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %1347 = "dtu_hlir.dynamic_broadcast_in_dim"(%1346, %1345) {node_name = "ConstantOfShape_621-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x77xf32>
    %1348 = "dtu_hlir.transpose"(%1293) {node_name = "Transpose_622-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<10x64x77xf32>
    %1349 = "dtu_hlir.dot_general"(%1257, %1348) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_623-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x77xf32>) -> tensor<10x4096x77xf32>
    %1350 = "dtu_hlir.broadcast_in_dim"(%874) {node_name = "Mul_624-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %1351 = "dtu_hlir.mul"(%1349, %1350) {node_name = "Mul_624-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1352 = "dtu_hlir.broadcast_in_dim"(%873) {node_name = "Mul_625-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %1353 = "dtu_hlir.mul"(%1347, %1352) {node_name = "Mul_625-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1354 = "dtu_hlir.add"(%1351, %g) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_626-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1355 = "dtu_hlir.softmax"(%1354) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_627-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1356 = "dtu_hlir.convert"(%1355) {node_name = "Cast_628-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1357 = "dtu_hlir.dot_general"(%1356, %1329) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_629-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x77x64xf32>) -> tensor<10x4096x64xf32>
    %1358 = "dtu_hlir.shape"(%1357) {end = 2147483647 : i64, node_name = "Shape_630-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1359 = dtu_hlir.constant  {node_name = "Constant_631-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1360 = "dtu_hlir.gather"(%1358, %1359) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_632-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1361 = "dtu_hlir.shape"(%1357) {end = 2147483647 : i64, node_name = "Shape_633-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1362 = dtu_hlir.constant  {node_name = "Constant_634-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1363 = "dtu_hlir.gather"(%1361, %1362) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_635-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1364 = "dtu_hlir.shape"(%1357) {end = 2147483647 : i64, node_name = "Shape_636-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1365 = dtu_hlir.constant  {node_name = "Constant_637-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1366 = "dtu_hlir.gather"(%1364, %1365) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_638-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1367 = dtu_hlir.constant  {node_name = "Constant_639-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1368 = "dtu_hlir.div"(%1360, %1367) {node_name = "Div_640-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1369 = "dtu_hlir.convert"(%1368) {node_name = "Cast_641-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1370 = "dtu_hlir.convert"(%1369) {node_name = "Cast_642-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1371 = dtu_hlir.constant  {node_name = "Constant_643-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1372 = "dtu_hlir.unsqueeze"(%1370, %1371) {node_name = "Unsqueeze_644-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1373 = dtu_hlir.constant  {node_name = "Constant_645-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1374 = "dtu_hlir.unsqueeze"(%1363, %1373) {node_name = "Unsqueeze_646-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1375 = dtu_hlir.constant  {node_name = "Constant_647-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1376 = "dtu_hlir.unsqueeze"(%1366, %1375) {node_name = "Unsqueeze_648-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1377 = "dtu_hlir.concatenate"(%1372, %872, %1374, %1376) {dimension = 0 : i64, node_name = "Concat_649-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1378 = "dtu_hlir.dynamic_reshape"(%1357, %1377) {allowzero = 0 : i64, node_name = "Reshape_650-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %1379 = "dtu_hlir.transpose"(%1378) {node_name = "Transpose_651-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %1380 = dtu_hlir.constant  {node_name = "Constant_652-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1381 = "dtu_hlir.div"(%1360, %1380) {node_name = "Div_653-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1382 = "dtu_hlir.convert"(%1381) {node_name = "Cast_654-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1383 = "dtu_hlir.convert"(%1382) {node_name = "Cast_655-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1384 = dtu_hlir.constant  {node_name = "Constant_656-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1385 = "dtu_hlir.mul"(%1366, %1384) {node_name = "Mul_657-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1386 = dtu_hlir.constant  {node_name = "Constant_658-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1387 = "dtu_hlir.unsqueeze"(%1383, %1386) {node_name = "Unsqueeze_659-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1388 = dtu_hlir.constant  {node_name = "Constant_660-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1389 = "dtu_hlir.unsqueeze"(%1363, %1388) {node_name = "Unsqueeze_661-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1390 = dtu_hlir.constant  {node_name = "Constant_662-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1391 = "dtu_hlir.unsqueeze"(%1385, %1390) {node_name = "Unsqueeze_663-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1392 = "dtu_hlir.concatenate"(%1387, %1389, %1391) {dimension = 0 : i64, node_name = "Concat_664-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1393 = "dtu_hlir.dynamic_reshape"(%1379, %1392) {allowzero = 0 : i64, node_name = "Reshape_665-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %1394 = "dtu_hlir.dot_general"(%1393, %389) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_666-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1395 = "dtu_hlir.add"(%10, %1394) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_667-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1396 = "dtu_hlir.add"(%1395, %1191) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_668-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1397 = dtu_hlir.constant  {node_name = "ReduceMean_669-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1398 = "dtu_hlir.reshape"(%1397) {node_name = "ReduceMean_669-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1399 = "dtu_hlir.reduce"(%1396, %1398) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_669-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_669-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1400 = dtu_hlir.constant  {node_name = "ReduceMean_669-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1401 = "dtu_hlir.unsqueeze"(%1399, %1400) {node_name = "ReduceMean_669-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1402 = dtu_hlir.constant  {node_name = "ReduceMean_669-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1403 = "dtu_hlir.broadcast_in_dim"(%1402) {node_name = "ReduceMean_669-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1404 = "dtu_hlir.div"(%1401, %1403) {node_name = "ReduceMean_669-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1405 = "dtu_hlir.sub"(%1396, %1404) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_670-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1406 = dtu_hlir.constant  {node_name = "Constant_671-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %1407 = "dtu_hlir.broadcast_in_dim"(%1406) {node_name = "Pow_672-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %1408 = "dtu_hlir.pow"(%1405, %1407) {node_name = "Pow_672-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1409 = dtu_hlir.constant  {node_name = "ReduceMean_673-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1410 = "dtu_hlir.reshape"(%1409) {node_name = "ReduceMean_673-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1411 = "dtu_hlir.reduce"(%1408, %1410) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_673-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_673-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1412 = dtu_hlir.constant  {node_name = "ReduceMean_673-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1413 = "dtu_hlir.unsqueeze"(%1411, %1412) {node_name = "ReduceMean_673-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1414 = dtu_hlir.constant  {node_name = "ReduceMean_673-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1415 = "dtu_hlir.broadcast_in_dim"(%1414) {node_name = "ReduceMean_673-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1416 = "dtu_hlir.div"(%1413, %1415) {node_name = "ReduceMean_673-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1417 = dtu_hlir.constant  {node_name = "Constant_674-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %1418 = "dtu_hlir.broadcast_in_dim"(%1417) {node_name = "Add_675-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %1419 = "dtu_hlir.add"(%1416, %1418) {node_name = "Add_675-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1420 = "dtu_hlir.sqrt"(%1419) {node_name = "Sqrt_676-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1421 = "dtu_hlir.div"(%1405, %1420) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_677-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1422 = "dtu_hlir.mul"(%1421, %15) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_678-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1423 = "dtu_hlir.add"(%1422, %16) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_679-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1424 = "dtu_hlir.dot_general"(%1423, %390) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_680-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x2560xf32>) -> tensor<2x4096x2560xf32>
    %1425 = "dtu_hlir.add"(%8, %1424) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_681-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2560xf32>, tensor<2x4096x2560xf32>) -> tensor<2x4096x2560xf32>
    %1426 = "dtu_hlir.shape"(%1425) {end = 2147483647 : i64, node_name = "Shape_682-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>) -> tensor<3xi64>
    %1427 = dtu_hlir.constant  {node_name = "Constant_683-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %1428 = "dtu_hlir.gather"(%1426, %1427) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_684-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1429 = dtu_hlir.constant  {node_name = "Constant_685-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1430 = dtu_hlir.constant  {node_name = "Constant_686-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %1431 = "dtu_hlir.add"(%1428, %1430) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_687-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1432 = dtu_hlir.constant  {node_name = "Constant_688-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %1433 = "dtu_hlir.div"(%1431, %1432) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_689-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1434 = dtu_hlir.constant  {node_name = "Constant_690-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %1435 = "dtu_hlir.mul"(%1433, %1434) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_691-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1436 = dtu_hlir.constant  {node_name = "Slice_692-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %1437 = "dtu_hlir.real_dynamic_slice"(%1425, %1429, %1435, %1436, %1427) {node_name = "Slice_692-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %1438 = dtu_hlir.constant  {node_name = "Constant_693-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %1439 = "dtu_hlir.mul"(%1433, %1438) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_694-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1440 = "dtu_hlir.shape"(%1435) {end = 2147483647 : i64, node_name = "Slice_695-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %1441 = dtu_hlir.constant  {node_name = "Slice_695-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %1442 = "dtu_hlir.dynamic_broadcast_in_dim"(%1441, %1440) {node_name = "Slice_695-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1443 = "dtu_hlir.real_dynamic_slice"(%1425, %1435, %1439, %1442, %1427) {node_name = "Slice_695-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %1444 = dtu_hlir.constant  {node_name = "Constant_696-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %1445 = "dtu_hlir.broadcast_in_dim"(%1444) {node_name = "Div_697-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %1446 = "dtu_hlir.div"(%1443, %1445) {node_name = "Div_697-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %1447 = "dtu_hlir.erf"(%1446) {node_name = "Erf_698-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %1448 = dtu_hlir.constant  {node_name = "Constant_699-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %1449 = "dtu_hlir.broadcast_in_dim"(%1448) {node_name = "Add_700-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %1450 = "dtu_hlir.add"(%1447, %1449) {node_name = "Add_700-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %1451 = "dtu_hlir.mul"(%1443, %1450) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_701-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %1452 = dtu_hlir.constant  {node_name = "Constant_702-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %1453 = "dtu_hlir.broadcast_in_dim"(%1452) {node_name = "Mul_703-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %1454 = "dtu_hlir.mul"(%1451, %1453) {node_name = "Mul_703-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %1455 = "dtu_hlir.mul"(%1437, %1454) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_704-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %1456 = "dtu_hlir.dot_general"(%1455, %391) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_705-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<1280x320xf32>) -> tensor<2x4096x320xf32>
    %1457 = "dtu_hlir.add"(%9, %1456) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_706-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1458 = "dtu_hlir.add"(%1457, %1396) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_707-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1459 = "dtu_hlir.dot_general"(%1458, %392) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_708-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1460 = "dtu_hlir.add"(%17, %1459) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_709-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1461 = dtu_hlir.constant  {node_name = "Constant_710-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1462 = "dtu_hlir.unsqueeze"(%956, %1461) {node_name = "Unsqueeze_711-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1463 = dtu_hlir.constant  {node_name = "Constant_712-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1464 = "dtu_hlir.unsqueeze"(%959, %1463) {node_name = "Unsqueeze_713-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1465 = dtu_hlir.constant  {node_name = "Constant_714-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1466 = "dtu_hlir.unsqueeze"(%962, %1465) {node_name = "Unsqueeze_715-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1467 = dtu_hlir.constant  {node_name = "Constant_716-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1468 = "dtu_hlir.unsqueeze"(%974, %1467) {node_name = "Unsqueeze_717-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1469 = "dtu_hlir.concatenate"(%1462, %1464, %1466, %1468) {dimension = 0 : i64, node_name = "Concat_718-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1470 = "dtu_hlir.dynamic_reshape"(%1460, %1469) {allowzero = 0 : i64, node_name = "Reshape_719-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x64x64x320xf32>
    %1471 = "dtu_hlir.transpose"(%1470) {node_name = "Transpose_720-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>) -> tensor<2x320x64x64xf32>
    %1472 = "dtu_hlir.add"(%1471, %953) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_721-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %1473 = dtu_hlir.constant  {node_name = "Constant_722-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %1474 = "dtu_hlir.dynamic_reshape"(%1472, %1473) {allowzero = 0 : i64, node_name = "Reshape_723-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %1475 = dtu_hlir.constant  {node_name = "Constant_724-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %1476 = dtu_hlir.constant  {node_name = "Constant_725-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %1477 = "dtu_hlir.instance_norm"(%1474, %1475, %1476) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_726-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %1478 = "dtu_hlir.shape"(%1472) {end = 2147483647 : i64, node_name = "Shape_727-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %1479 = "dtu_hlir.dynamic_reshape"(%1477, %1478) {allowzero = 0 : i64, node_name = "Reshape_728-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %1480 = "dtu_hlir.mul"(%1479, %393) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_729-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %1481 = "dtu_hlir.add"(%1480, %394) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_730-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %1482 = "dtu_hlir.sigmoid"(%1481) {node_name = "Sigmoid_731-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %1483 = "dtu_hlir.mul"(%1481, %1482) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_732-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %1484 = "dtu_hlir.conv_bias"(%1483, %36, %37) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_733-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %1485 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_734-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %1486 = "dtu_hlir.mul"(%915, %1485) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_735-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %1487 = "dtu_hlir.transpose"(%38) {node_name = "Gemm_736-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<320x1280xf32>) -> tensor<1280x320xf32>
    %1488 = "dtu_hlir.gemm"(%1486, %1487, %39) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_736-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x320xf32>, tensor<320xf32>) -> tensor<2x320xf32>
    %1489 = dtu_hlir.constant  {node_name = "Constant_737-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %1490 = "dtu_hlir.unsqueeze"(%1488, %1489) {node_name = "Unsqueeze_738-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320xf32>, tensor<1xi64>) -> tensor<2x320x1xf32>
    %1491 = dtu_hlir.constant  {node_name = "Constant_739-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %1492 = "dtu_hlir.unsqueeze"(%1490, %1491) {node_name = "Unsqueeze_740-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320x1xf32>, tensor<1xi64>) -> tensor<2x320x1x1xf32>
    %1493 = "dtu_hlir.add"(%1484, %1492) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_741-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %1494 = dtu_hlir.constant  {node_name = "Constant_742-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %1495 = "dtu_hlir.dynamic_reshape"(%1493, %1494) {allowzero = 0 : i64, node_name = "Reshape_743-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %1496 = dtu_hlir.constant  {node_name = "Constant_744-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %1497 = dtu_hlir.constant  {node_name = "Constant_745-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %1498 = "dtu_hlir.instance_norm"(%1495, %1496, %1497) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_746-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %1499 = "dtu_hlir.shape"(%1493) {end = 2147483647 : i64, node_name = "Shape_747-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %1500 = "dtu_hlir.dynamic_reshape"(%1498, %1499) {allowzero = 0 : i64, node_name = "Reshape_748-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %1501 = "dtu_hlir.mul"(%1500, %395) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_749-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %1502 = "dtu_hlir.add"(%1501, %396) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_750-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %1503 = "dtu_hlir.sigmoid"(%1502) {node_name = "Sigmoid_751-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %1504 = "dtu_hlir.mul"(%1502, %1503) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_752-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %1505 = "dtu_hlir.conv_bias"(%1504, %40, %41) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_753-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %1506 = "dtu_hlir.add"(%1472, %1505) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_754-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %1507 = dtu_hlir.constant  {node_name = "Constant_755-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %1508 = "dtu_hlir.broadcast_in_dim"(%1507) {node_name = "Div_756-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x320x64x64xf32>
    %1509 = "dtu_hlir.div"(%1506, %1508) {node_name = "Div_756-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %1510 = "dtu_hlir.shape"(%1509) {end = 2147483647 : i64, node_name = "Shape_757-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %1511 = dtu_hlir.constant  {node_name = "Constant_758-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1512 = "dtu_hlir.gather"(%1510, %1511) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_759-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %1513 = "dtu_hlir.shape"(%1509) {end = 2147483647 : i64, node_name = "Shape_760-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %1514 = dtu_hlir.constant  {node_name = "Constant_761-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1515 = "dtu_hlir.gather"(%1513, %1514) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_762-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %1516 = "dtu_hlir.shape"(%1509) {end = 2147483647 : i64, node_name = "Shape_763-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %1517 = dtu_hlir.constant  {node_name = "Constant_764-0", node_type = "Constant"} dense<3> : tensor<i64>
    %1518 = "dtu_hlir.gather"(%1516, %1517) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_765-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %1519 = dtu_hlir.constant  {node_name = "Constant_766-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %1520 = "dtu_hlir.dynamic_reshape"(%1509, %1519) {allowzero = 0 : i64, node_name = "Reshape_767-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %1521 = dtu_hlir.constant  {node_name = "Constant_768-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %1522 = dtu_hlir.constant  {node_name = "Constant_769-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %1523 = "dtu_hlir.instance_norm"(%1520, %1521, %1522) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_770-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %1524 = "dtu_hlir.shape"(%1509) {end = 2147483647 : i64, node_name = "Shape_771-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %1525 = "dtu_hlir.dynamic_reshape"(%1523, %1524) {allowzero = 0 : i64, node_name = "Reshape_772-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %1526 = "dtu_hlir.mul"(%1525, %397) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_773-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %1527 = "dtu_hlir.add"(%1526, %398) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_774-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %1528 = "dtu_hlir.shape"(%1527) {end = 2147483647 : i64, node_name = "Shape_775-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %1529 = dtu_hlir.constant  {node_name = "Constant_776-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1530 = "dtu_hlir.gather"(%1528, %1529) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_777-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %1531 = "dtu_hlir.transpose"(%1527) {node_name = "Transpose_778-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x64x64x320xf32>
    %1532 = "dtu_hlir.mul"(%1515, %1518) {node_name = "Mul_779-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1533 = dtu_hlir.constant  {node_name = "Constant_780-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1534 = "dtu_hlir.unsqueeze"(%1512, %1533) {node_name = "Unsqueeze_781-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1535 = dtu_hlir.constant  {node_name = "Constant_782-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1536 = "dtu_hlir.unsqueeze"(%1532, %1535) {node_name = "Unsqueeze_783-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1537 = dtu_hlir.constant  {node_name = "Constant_784-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1538 = "dtu_hlir.unsqueeze"(%1530, %1537) {node_name = "Unsqueeze_785-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1539 = "dtu_hlir.concatenate"(%1534, %1536, %1538) {dimension = 0 : i64, node_name = "Concat_786-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1540 = "dtu_hlir.dynamic_reshape"(%1531, %1539) {allowzero = 0 : i64, node_name = "Reshape_787-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %1541 = "dtu_hlir.dot_general"(%1540, %399) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_788-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1542 = "dtu_hlir.add"(%18, %1541) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_789-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1543 = dtu_hlir.constant  {node_name = "ReduceMean_790-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1544 = "dtu_hlir.reshape"(%1543) {node_name = "ReduceMean_790-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1545 = "dtu_hlir.reduce"(%1542, %1544) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_790-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_790-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1546 = dtu_hlir.constant  {node_name = "ReduceMean_790-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1547 = "dtu_hlir.unsqueeze"(%1545, %1546) {node_name = "ReduceMean_790-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1548 = dtu_hlir.constant  {node_name = "ReduceMean_790-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1549 = "dtu_hlir.broadcast_in_dim"(%1548) {node_name = "ReduceMean_790-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1550 = "dtu_hlir.div"(%1547, %1549) {node_name = "ReduceMean_790-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1551 = "dtu_hlir.sub"(%1542, %1550) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_791-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1552 = dtu_hlir.constant  {node_name = "Constant_792-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %1553 = "dtu_hlir.broadcast_in_dim"(%1552) {node_name = "Pow_793-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %1554 = "dtu_hlir.pow"(%1551, %1553) {node_name = "Pow_793-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1555 = dtu_hlir.constant  {node_name = "ReduceMean_794-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1556 = "dtu_hlir.reshape"(%1555) {node_name = "ReduceMean_794-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1557 = "dtu_hlir.reduce"(%1554, %1556) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_794-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_794-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1558 = dtu_hlir.constant  {node_name = "ReduceMean_794-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1559 = "dtu_hlir.unsqueeze"(%1557, %1558) {node_name = "ReduceMean_794-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1560 = dtu_hlir.constant  {node_name = "ReduceMean_794-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1561 = "dtu_hlir.broadcast_in_dim"(%1560) {node_name = "ReduceMean_794-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1562 = "dtu_hlir.div"(%1559, %1561) {node_name = "ReduceMean_794-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1563 = dtu_hlir.constant  {node_name = "Constant_795-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %1564 = "dtu_hlir.broadcast_in_dim"(%1563) {node_name = "Add_796-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %1565 = "dtu_hlir.add"(%1562, %1564) {node_name = "Add_796-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1566 = "dtu_hlir.sqrt"(%1565) {node_name = "Sqrt_797-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1567 = "dtu_hlir.div"(%1551, %1566) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_798-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1568 = "dtu_hlir.mul"(%1567, %23) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_799-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1569 = "dtu_hlir.add"(%1568, %24) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_800-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1570 = "dtu_hlir.dot_general"(%1569, %400) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_801-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1571 = "dtu_hlir.dot_general"(%1569, %401) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_802-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1572 = "dtu_hlir.dot_general"(%1569, %402) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_803-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1573 = "dtu_hlir.shape"(%1570) {end = 2147483647 : i64, node_name = "Shape_804-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1574 = dtu_hlir.constant  {node_name = "Constant_805-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1575 = "dtu_hlir.gather"(%1573, %1574) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_806-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1576 = "dtu_hlir.shape"(%1570) {end = 2147483647 : i64, node_name = "Shape_807-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1577 = dtu_hlir.constant  {node_name = "Constant_808-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1578 = "dtu_hlir.gather"(%1576, %1577) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_809-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1579 = "dtu_hlir.shape"(%1570) {end = 2147483647 : i64, node_name = "Shape_810-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1580 = dtu_hlir.constant  {node_name = "Constant_811-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1581 = "dtu_hlir.gather"(%1579, %1580) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_812-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1582 = dtu_hlir.constant  {node_name = "Constant_813-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1583 = "dtu_hlir.div"(%1581, %1582) {node_name = "Div_814-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1584 = "dtu_hlir.convert"(%1583) {node_name = "Cast_815-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1585 = "dtu_hlir.convert"(%1584) {node_name = "Cast_816-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1586 = dtu_hlir.constant  {node_name = "Constant_817-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1587 = "dtu_hlir.unsqueeze"(%1575, %1586) {node_name = "Unsqueeze_818-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1588 = dtu_hlir.constant  {node_name = "Constant_819-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1589 = "dtu_hlir.unsqueeze"(%1578, %1588) {node_name = "Unsqueeze_820-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1590 = dtu_hlir.constant  {node_name = "Constant_821-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1591 = "dtu_hlir.unsqueeze"(%1585, %1590) {node_name = "Unsqueeze_822-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1592 = "dtu_hlir.concatenate"(%1587, %1589, %871, %1591) {dimension = 0 : i64, node_name = "Concat_823-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1593 = "dtu_hlir.dynamic_reshape"(%1570, %1592) {allowzero = 0 : i64, node_name = "Reshape_824-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1594 = "dtu_hlir.transpose"(%1593) {node_name = "Transpose_825-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1595 = dtu_hlir.constant  {node_name = "Constant_826-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1596 = "dtu_hlir.mul"(%1575, %1595) {node_name = "Mul_827-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1597 = dtu_hlir.constant  {node_name = "Constant_828-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1598 = "dtu_hlir.div"(%1581, %1597) {node_name = "Div_829-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1599 = "dtu_hlir.convert"(%1598) {node_name = "Cast_830-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1600 = "dtu_hlir.convert"(%1599) {node_name = "Cast_831-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1601 = dtu_hlir.constant  {node_name = "Constant_832-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1602 = "dtu_hlir.unsqueeze"(%1596, %1601) {node_name = "Unsqueeze_833-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1603 = dtu_hlir.constant  {node_name = "Constant_834-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1604 = "dtu_hlir.unsqueeze"(%1578, %1603) {node_name = "Unsqueeze_835-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1605 = dtu_hlir.constant  {node_name = "Constant_836-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1606 = "dtu_hlir.unsqueeze"(%1600, %1605) {node_name = "Unsqueeze_837-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1607 = "dtu_hlir.concatenate"(%1602, %1604, %1606) {dimension = 0 : i64, node_name = "Concat_838-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1608 = "dtu_hlir.dynamic_reshape"(%1594, %1607) {allowzero = 0 : i64, node_name = "Reshape_839-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1609 = "dtu_hlir.shape"(%1571) {end = 2147483647 : i64, node_name = "Shape_840-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1610 = dtu_hlir.constant  {node_name = "Constant_841-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1611 = "dtu_hlir.gather"(%1609, %1610) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_842-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1612 = "dtu_hlir.shape"(%1571) {end = 2147483647 : i64, node_name = "Shape_843-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1613 = dtu_hlir.constant  {node_name = "Constant_844-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1614 = "dtu_hlir.gather"(%1612, %1613) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_845-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1615 = "dtu_hlir.shape"(%1571) {end = 2147483647 : i64, node_name = "Shape_846-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1616 = dtu_hlir.constant  {node_name = "Constant_847-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1617 = "dtu_hlir.gather"(%1615, %1616) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_848-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1618 = dtu_hlir.constant  {node_name = "Constant_849-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1619 = "dtu_hlir.div"(%1617, %1618) {node_name = "Div_850-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1620 = "dtu_hlir.convert"(%1619) {node_name = "Cast_851-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1621 = "dtu_hlir.convert"(%1620) {node_name = "Cast_852-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1622 = dtu_hlir.constant  {node_name = "Constant_853-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1623 = "dtu_hlir.unsqueeze"(%1611, %1622) {node_name = "Unsqueeze_854-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1624 = dtu_hlir.constant  {node_name = "Constant_855-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1625 = "dtu_hlir.unsqueeze"(%1614, %1624) {node_name = "Unsqueeze_856-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1626 = dtu_hlir.constant  {node_name = "Constant_857-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1627 = "dtu_hlir.unsqueeze"(%1621, %1626) {node_name = "Unsqueeze_858-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1628 = "dtu_hlir.concatenate"(%1623, %1625, %870, %1627) {dimension = 0 : i64, node_name = "Concat_859-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1629 = "dtu_hlir.dynamic_reshape"(%1571, %1628) {allowzero = 0 : i64, node_name = "Reshape_860-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1630 = "dtu_hlir.transpose"(%1629) {node_name = "Transpose_861-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1631 = dtu_hlir.constant  {node_name = "Constant_862-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1632 = "dtu_hlir.mul"(%1611, %1631) {node_name = "Mul_863-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1633 = dtu_hlir.constant  {node_name = "Constant_864-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1634 = "dtu_hlir.div"(%1617, %1633) {node_name = "Div_865-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1635 = "dtu_hlir.convert"(%1634) {node_name = "Cast_866-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1636 = "dtu_hlir.convert"(%1635) {node_name = "Cast_867-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1637 = dtu_hlir.constant  {node_name = "Constant_868-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1638 = "dtu_hlir.unsqueeze"(%1632, %1637) {node_name = "Unsqueeze_869-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1639 = dtu_hlir.constant  {node_name = "Constant_870-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1640 = "dtu_hlir.unsqueeze"(%1614, %1639) {node_name = "Unsqueeze_871-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1641 = dtu_hlir.constant  {node_name = "Constant_872-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1642 = "dtu_hlir.unsqueeze"(%1636, %1641) {node_name = "Unsqueeze_873-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1643 = "dtu_hlir.concatenate"(%1638, %1640, %1642) {dimension = 0 : i64, node_name = "Concat_874-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1644 = "dtu_hlir.dynamic_reshape"(%1630, %1643) {allowzero = 0 : i64, node_name = "Reshape_875-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1645 = "dtu_hlir.shape"(%1572) {end = 2147483647 : i64, node_name = "Shape_876-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1646 = dtu_hlir.constant  {node_name = "Constant_877-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1647 = "dtu_hlir.gather"(%1645, %1646) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_878-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1648 = "dtu_hlir.shape"(%1572) {end = 2147483647 : i64, node_name = "Shape_879-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1649 = dtu_hlir.constant  {node_name = "Constant_880-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1650 = "dtu_hlir.gather"(%1648, %1649) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_881-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1651 = "dtu_hlir.shape"(%1572) {end = 2147483647 : i64, node_name = "Shape_882-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1652 = dtu_hlir.constant  {node_name = "Constant_883-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1653 = "dtu_hlir.gather"(%1651, %1652) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_884-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1654 = dtu_hlir.constant  {node_name = "Constant_885-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1655 = "dtu_hlir.div"(%1653, %1654) {node_name = "Div_886-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1656 = "dtu_hlir.convert"(%1655) {node_name = "Cast_887-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1657 = "dtu_hlir.convert"(%1656) {node_name = "Cast_888-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1658 = dtu_hlir.constant  {node_name = "Constant_889-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1659 = "dtu_hlir.unsqueeze"(%1647, %1658) {node_name = "Unsqueeze_890-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1660 = dtu_hlir.constant  {node_name = "Constant_891-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1661 = "dtu_hlir.unsqueeze"(%1650, %1660) {node_name = "Unsqueeze_892-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1662 = dtu_hlir.constant  {node_name = "Constant_893-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1663 = "dtu_hlir.unsqueeze"(%1657, %1662) {node_name = "Unsqueeze_894-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1664 = "dtu_hlir.concatenate"(%1659, %1661, %869, %1663) {dimension = 0 : i64, node_name = "Concat_895-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1665 = "dtu_hlir.dynamic_reshape"(%1572, %1664) {allowzero = 0 : i64, node_name = "Reshape_896-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1666 = "dtu_hlir.transpose"(%1665) {node_name = "Transpose_897-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1667 = dtu_hlir.constant  {node_name = "Constant_898-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1668 = "dtu_hlir.mul"(%1647, %1667) {node_name = "Mul_899-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1669 = dtu_hlir.constant  {node_name = "Constant_900-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1670 = "dtu_hlir.div"(%1653, %1669) {node_name = "Div_901-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1671 = "dtu_hlir.convert"(%1670) {node_name = "Cast_902-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1672 = "dtu_hlir.convert"(%1671) {node_name = "Cast_903-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1673 = dtu_hlir.constant  {node_name = "Constant_904-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1674 = "dtu_hlir.unsqueeze"(%1668, %1673) {node_name = "Unsqueeze_905-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1675 = dtu_hlir.constant  {node_name = "Constant_906-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1676 = "dtu_hlir.unsqueeze"(%1650, %1675) {node_name = "Unsqueeze_907-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1677 = dtu_hlir.constant  {node_name = "Constant_908-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1678 = "dtu_hlir.unsqueeze"(%1672, %1677) {node_name = "Unsqueeze_909-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1679 = "dtu_hlir.concatenate"(%1674, %1676, %1678) {dimension = 0 : i64, node_name = "Concat_910-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1680 = "dtu_hlir.dynamic_reshape"(%1666, %1679) {allowzero = 0 : i64, node_name = "Reshape_911-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1681 = "dtu_hlir.shape"(%1608) {end = 2147483647 : i64, node_name = "Shape_912-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1682 = dtu_hlir.constant  {node_name = "Constant_913-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1683 = "dtu_hlir.gather"(%1681, %1682) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_914-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1684 = "dtu_hlir.shape"(%1608) {end = 2147483647 : i64, node_name = "Shape_915-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1685 = dtu_hlir.constant  {node_name = "Constant_916-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1686 = "dtu_hlir.gather"(%1684, %1685) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_917-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1687 = "dtu_hlir.shape"(%1644) {end = 2147483647 : i64, node_name = "Shape_918-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1688 = dtu_hlir.constant  {node_name = "Constant_919-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1689 = "dtu_hlir.gather"(%1687, %1688) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_920-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1690 = dtu_hlir.constant  {node_name = "Constant_921-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1691 = "dtu_hlir.unsqueeze"(%1683, %1690) {node_name = "Unsqueeze_922-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1692 = dtu_hlir.constant  {node_name = "Constant_923-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1693 = "dtu_hlir.unsqueeze"(%1686, %1692) {node_name = "Unsqueeze_924-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1694 = dtu_hlir.constant  {node_name = "Constant_925-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1695 = "dtu_hlir.unsqueeze"(%1689, %1694) {node_name = "Unsqueeze_926-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1696 = "dtu_hlir.concatenate"(%1691, %1693, %1695) {dimension = 0 : i64, node_name = "Concat_927-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1697 = dtu_hlir.constant  {node_name = "ConstantOfShape_928-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %1698 = "dtu_hlir.dynamic_broadcast_in_dim"(%1697, %1696) {node_name = "ConstantOfShape_928-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x4096xf32>
    %1699 = "dtu_hlir.transpose"(%1644) {node_name = "Transpose_929-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<10x64x4096xf32>
    %1700 = "dtu_hlir.dot_general"(%1608, %1699) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_930-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x4096xf32>) -> tensor<10x4096x4096xf32>
    %1701 = "dtu_hlir.broadcast_in_dim"(%868) {node_name = "Mul_931-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %1702 = "dtu_hlir.mul"(%1700, %1701) {node_name = "Mul_931-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1703 = "dtu_hlir.broadcast_in_dim"(%867) {node_name = "Mul_932-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %1704 = "dtu_hlir.mul"(%1698, %1703) {node_name = "Mul_932-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1705 = "dtu_hlir.add"(%1702, %1704) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_933-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1706 = "dtu_hlir.softmax"(%1705) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_934-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1707 = "dtu_hlir.convert"(%1706) {node_name = "Cast_935-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %1708 = "dtu_hlir.dot_general"(%1707, %1680) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_936-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x64xf32>) -> tensor<10x4096x64xf32>
    %1709 = "dtu_hlir.shape"(%1708) {end = 2147483647 : i64, node_name = "Shape_937-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1710 = dtu_hlir.constant  {node_name = "Constant_938-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1711 = "dtu_hlir.gather"(%1709, %1710) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_939-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1712 = "dtu_hlir.shape"(%1708) {end = 2147483647 : i64, node_name = "Shape_940-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1713 = dtu_hlir.constant  {node_name = "Constant_941-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1714 = "dtu_hlir.gather"(%1712, %1713) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_942-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1715 = "dtu_hlir.shape"(%1708) {end = 2147483647 : i64, node_name = "Shape_943-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1716 = dtu_hlir.constant  {node_name = "Constant_944-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1717 = "dtu_hlir.gather"(%1715, %1716) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_945-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1718 = dtu_hlir.constant  {node_name = "Constant_946-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1719 = "dtu_hlir.div"(%1711, %1718) {node_name = "Div_947-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1720 = "dtu_hlir.convert"(%1719) {node_name = "Cast_948-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1721 = "dtu_hlir.convert"(%1720) {node_name = "Cast_949-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1722 = dtu_hlir.constant  {node_name = "Constant_950-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1723 = "dtu_hlir.unsqueeze"(%1721, %1722) {node_name = "Unsqueeze_951-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1724 = dtu_hlir.constant  {node_name = "Constant_952-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1725 = "dtu_hlir.unsqueeze"(%1714, %1724) {node_name = "Unsqueeze_953-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1726 = dtu_hlir.constant  {node_name = "Constant_954-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1727 = "dtu_hlir.unsqueeze"(%1717, %1726) {node_name = "Unsqueeze_955-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1728 = "dtu_hlir.concatenate"(%1723, %866, %1725, %1727) {dimension = 0 : i64, node_name = "Concat_956-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1729 = "dtu_hlir.dynamic_reshape"(%1708, %1728) {allowzero = 0 : i64, node_name = "Reshape_957-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %1730 = "dtu_hlir.transpose"(%1729) {node_name = "Transpose_958-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %1731 = dtu_hlir.constant  {node_name = "Constant_959-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1732 = "dtu_hlir.div"(%1711, %1731) {node_name = "Div_960-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1733 = "dtu_hlir.convert"(%1732) {node_name = "Cast_961-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1734 = "dtu_hlir.convert"(%1733) {node_name = "Cast_962-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1735 = dtu_hlir.constant  {node_name = "Constant_963-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1736 = "dtu_hlir.mul"(%1717, %1735) {node_name = "Mul_964-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1737 = dtu_hlir.constant  {node_name = "Constant_965-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1738 = "dtu_hlir.unsqueeze"(%1734, %1737) {node_name = "Unsqueeze_966-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1739 = dtu_hlir.constant  {node_name = "Constant_967-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1740 = "dtu_hlir.unsqueeze"(%1714, %1739) {node_name = "Unsqueeze_968-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1741 = dtu_hlir.constant  {node_name = "Constant_969-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1742 = "dtu_hlir.unsqueeze"(%1736, %1741) {node_name = "Unsqueeze_970-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1743 = "dtu_hlir.concatenate"(%1738, %1740, %1742) {dimension = 0 : i64, node_name = "Concat_971-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1744 = "dtu_hlir.dynamic_reshape"(%1730, %1743) {allowzero = 0 : i64, node_name = "Reshape_972-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %1745 = "dtu_hlir.dot_general"(%1744, %403) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_973-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1746 = "dtu_hlir.add"(%19, %1745) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_974-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1747 = "dtu_hlir.add"(%1746, %1542) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_975-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1748 = dtu_hlir.constant  {node_name = "ReduceMean_976-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1749 = "dtu_hlir.reshape"(%1748) {node_name = "ReduceMean_976-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1750 = "dtu_hlir.reduce"(%1747, %1749) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_976-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_976-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1751 = dtu_hlir.constant  {node_name = "ReduceMean_976-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1752 = "dtu_hlir.unsqueeze"(%1750, %1751) {node_name = "ReduceMean_976-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1753 = dtu_hlir.constant  {node_name = "ReduceMean_976-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1754 = "dtu_hlir.broadcast_in_dim"(%1753) {node_name = "ReduceMean_976-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1755 = "dtu_hlir.div"(%1752, %1754) {node_name = "ReduceMean_976-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1756 = "dtu_hlir.sub"(%1747, %1755) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_977-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1757 = dtu_hlir.constant  {node_name = "Constant_978-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %1758 = "dtu_hlir.broadcast_in_dim"(%1757) {node_name = "Pow_979-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %1759 = "dtu_hlir.pow"(%1756, %1758) {node_name = "Pow_979-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1760 = dtu_hlir.constant  {node_name = "ReduceMean_980-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1761 = "dtu_hlir.reshape"(%1760) {node_name = "ReduceMean_980-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1762 = "dtu_hlir.reduce"(%1759, %1761) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_980-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_980-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1763 = dtu_hlir.constant  {node_name = "ReduceMean_980-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1764 = "dtu_hlir.unsqueeze"(%1762, %1763) {node_name = "ReduceMean_980-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1765 = dtu_hlir.constant  {node_name = "ReduceMean_980-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1766 = "dtu_hlir.broadcast_in_dim"(%1765) {node_name = "ReduceMean_980-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1767 = "dtu_hlir.div"(%1764, %1766) {node_name = "ReduceMean_980-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1768 = dtu_hlir.constant  {node_name = "Constant_981-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %1769 = "dtu_hlir.broadcast_in_dim"(%1768) {node_name = "Add_982-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %1770 = "dtu_hlir.add"(%1767, %1769) {node_name = "Add_982-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1771 = "dtu_hlir.sqrt"(%1770) {node_name = "Sqrt_983-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1772 = "dtu_hlir.div"(%1756, %1771) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_984-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1773 = "dtu_hlir.mul"(%1772, %25) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_985-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1774 = "dtu_hlir.add"(%1773, %26) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_986-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1775 = "dtu_hlir.dot_general"(%1774, %404) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_987-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1776 = "dtu_hlir.dot_general"(%arg2, %405) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_988-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %1777 = "dtu_hlir.dot_general"(%arg2, %406) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_989-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %1778 = "dtu_hlir.shape"(%1775) {end = 2147483647 : i64, node_name = "Shape_990-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1779 = dtu_hlir.constant  {node_name = "Constant_991-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1780 = "dtu_hlir.gather"(%1778, %1779) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_992-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1781 = "dtu_hlir.shape"(%1775) {end = 2147483647 : i64, node_name = "Shape_993-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1782 = dtu_hlir.constant  {node_name = "Constant_994-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1783 = "dtu_hlir.gather"(%1781, %1782) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_995-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1784 = "dtu_hlir.shape"(%1775) {end = 2147483647 : i64, node_name = "Shape_996-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %1785 = dtu_hlir.constant  {node_name = "Constant_997-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1786 = "dtu_hlir.gather"(%1784, %1785) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_998-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1787 = dtu_hlir.constant  {node_name = "Constant_999-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1788 = "dtu_hlir.div"(%1786, %1787) {node_name = "Div_1000-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1789 = "dtu_hlir.convert"(%1788) {node_name = "Cast_1001-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1790 = "dtu_hlir.convert"(%1789) {node_name = "Cast_1002-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1791 = dtu_hlir.constant  {node_name = "Constant_1003-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1792 = "dtu_hlir.unsqueeze"(%1780, %1791) {node_name = "Unsqueeze_1004-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1793 = dtu_hlir.constant  {node_name = "Constant_1005-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1794 = "dtu_hlir.unsqueeze"(%1783, %1793) {node_name = "Unsqueeze_1006-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1795 = dtu_hlir.constant  {node_name = "Constant_1007-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1796 = "dtu_hlir.unsqueeze"(%1790, %1795) {node_name = "Unsqueeze_1008-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1797 = "dtu_hlir.concatenate"(%1792, %1794, %865, %1796) {dimension = 0 : i64, node_name = "Concat_1009-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1798 = "dtu_hlir.dynamic_reshape"(%1775, %1797) {allowzero = 0 : i64, node_name = "Reshape_1010-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %1799 = "dtu_hlir.transpose"(%1798) {node_name = "Transpose_1011-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %1800 = dtu_hlir.constant  {node_name = "Constant_1012-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1801 = "dtu_hlir.mul"(%1780, %1800) {node_name = "Mul_1013-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1802 = dtu_hlir.constant  {node_name = "Constant_1014-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1803 = "dtu_hlir.div"(%1786, %1802) {node_name = "Div_1015-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1804 = "dtu_hlir.convert"(%1803) {node_name = "Cast_1016-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1805 = "dtu_hlir.convert"(%1804) {node_name = "Cast_1017-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1806 = dtu_hlir.constant  {node_name = "Constant_1018-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1807 = "dtu_hlir.unsqueeze"(%1801, %1806) {node_name = "Unsqueeze_1019-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1808 = dtu_hlir.constant  {node_name = "Constant_1020-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1809 = "dtu_hlir.unsqueeze"(%1783, %1808) {node_name = "Unsqueeze_1021-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1810 = dtu_hlir.constant  {node_name = "Constant_1022-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1811 = "dtu_hlir.unsqueeze"(%1805, %1810) {node_name = "Unsqueeze_1023-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1812 = "dtu_hlir.concatenate"(%1807, %1809, %1811) {dimension = 0 : i64, node_name = "Concat_1024-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1813 = "dtu_hlir.dynamic_reshape"(%1799, %1812) {allowzero = 0 : i64, node_name = "Reshape_1025-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %1814 = "dtu_hlir.shape"(%1776) {end = 2147483647 : i64, node_name = "Shape_1026-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1815 = dtu_hlir.constant  {node_name = "Constant_1027-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1816 = "dtu_hlir.gather"(%1814, %1815) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1028-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1817 = "dtu_hlir.shape"(%1776) {end = 2147483647 : i64, node_name = "Shape_1029-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1818 = dtu_hlir.constant  {node_name = "Constant_1030-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1819 = "dtu_hlir.gather"(%1817, %1818) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1031-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1820 = "dtu_hlir.shape"(%1776) {end = 2147483647 : i64, node_name = "Shape_1032-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1821 = dtu_hlir.constant  {node_name = "Constant_1033-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1822 = "dtu_hlir.gather"(%1820, %1821) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1034-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1823 = dtu_hlir.constant  {node_name = "Constant_1035-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1824 = "dtu_hlir.div"(%1822, %1823) {node_name = "Div_1036-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1825 = "dtu_hlir.convert"(%1824) {node_name = "Cast_1037-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1826 = "dtu_hlir.convert"(%1825) {node_name = "Cast_1038-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1827 = dtu_hlir.constant  {node_name = "Constant_1039-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1828 = "dtu_hlir.unsqueeze"(%1816, %1827) {node_name = "Unsqueeze_1040-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1829 = dtu_hlir.constant  {node_name = "Constant_1041-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1830 = "dtu_hlir.unsqueeze"(%1819, %1829) {node_name = "Unsqueeze_1042-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1831 = dtu_hlir.constant  {node_name = "Constant_1043-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1832 = "dtu_hlir.unsqueeze"(%1826, %1831) {node_name = "Unsqueeze_1044-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1833 = "dtu_hlir.concatenate"(%1828, %1830, %864, %1832) {dimension = 0 : i64, node_name = "Concat_1045-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1834 = "dtu_hlir.dynamic_reshape"(%1776, %1833) {allowzero = 0 : i64, node_name = "Reshape_1046-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %1835 = "dtu_hlir.transpose"(%1834) {node_name = "Transpose_1047-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %1836 = dtu_hlir.constant  {node_name = "Constant_1048-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1837 = "dtu_hlir.mul"(%1816, %1836) {node_name = "Mul_1049-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1838 = dtu_hlir.constant  {node_name = "Constant_1050-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1839 = "dtu_hlir.div"(%1822, %1838) {node_name = "Div_1051-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1840 = "dtu_hlir.convert"(%1839) {node_name = "Cast_1052-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1841 = "dtu_hlir.convert"(%1840) {node_name = "Cast_1053-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1842 = dtu_hlir.constant  {node_name = "Constant_1054-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1843 = "dtu_hlir.unsqueeze"(%1837, %1842) {node_name = "Unsqueeze_1055-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1844 = dtu_hlir.constant  {node_name = "Constant_1056-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1845 = "dtu_hlir.unsqueeze"(%1819, %1844) {node_name = "Unsqueeze_1057-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1846 = dtu_hlir.constant  {node_name = "Constant_1058-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1847 = "dtu_hlir.unsqueeze"(%1841, %1846) {node_name = "Unsqueeze_1059-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1848 = "dtu_hlir.concatenate"(%1843, %1845, %1847) {dimension = 0 : i64, node_name = "Concat_1060-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1849 = "dtu_hlir.dynamic_reshape"(%1835, %1848) {allowzero = 0 : i64, node_name = "Reshape_1061-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %1850 = "dtu_hlir.shape"(%1777) {end = 2147483647 : i64, node_name = "Shape_1062-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1851 = dtu_hlir.constant  {node_name = "Constant_1063-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1852 = "dtu_hlir.gather"(%1850, %1851) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1064-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1853 = "dtu_hlir.shape"(%1777) {end = 2147483647 : i64, node_name = "Shape_1065-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1854 = dtu_hlir.constant  {node_name = "Constant_1066-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1855 = "dtu_hlir.gather"(%1853, %1854) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1067-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1856 = "dtu_hlir.shape"(%1777) {end = 2147483647 : i64, node_name = "Shape_1068-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %1857 = dtu_hlir.constant  {node_name = "Constant_1069-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1858 = "dtu_hlir.gather"(%1856, %1857) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1070-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1859 = dtu_hlir.constant  {node_name = "Constant_1071-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1860 = "dtu_hlir.div"(%1858, %1859) {node_name = "Div_1072-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1861 = "dtu_hlir.convert"(%1860) {node_name = "Cast_1073-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1862 = "dtu_hlir.convert"(%1861) {node_name = "Cast_1074-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1863 = dtu_hlir.constant  {node_name = "Constant_1075-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1864 = "dtu_hlir.unsqueeze"(%1852, %1863) {node_name = "Unsqueeze_1076-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1865 = dtu_hlir.constant  {node_name = "Constant_1077-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1866 = "dtu_hlir.unsqueeze"(%1855, %1865) {node_name = "Unsqueeze_1078-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1867 = dtu_hlir.constant  {node_name = "Constant_1079-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1868 = "dtu_hlir.unsqueeze"(%1862, %1867) {node_name = "Unsqueeze_1080-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1869 = "dtu_hlir.concatenate"(%1864, %1866, %863, %1868) {dimension = 0 : i64, node_name = "Concat_1081-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1870 = "dtu_hlir.dynamic_reshape"(%1777, %1869) {allowzero = 0 : i64, node_name = "Reshape_1082-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %1871 = "dtu_hlir.transpose"(%1870) {node_name = "Transpose_1083-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %1872 = dtu_hlir.constant  {node_name = "Constant_1084-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1873 = "dtu_hlir.mul"(%1852, %1872) {node_name = "Mul_1085-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1874 = dtu_hlir.constant  {node_name = "Constant_1086-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1875 = "dtu_hlir.div"(%1858, %1874) {node_name = "Div_1087-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1876 = "dtu_hlir.convert"(%1875) {node_name = "Cast_1088-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1877 = "dtu_hlir.convert"(%1876) {node_name = "Cast_1089-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1878 = dtu_hlir.constant  {node_name = "Constant_1090-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1879 = "dtu_hlir.unsqueeze"(%1873, %1878) {node_name = "Unsqueeze_1091-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1880 = dtu_hlir.constant  {node_name = "Constant_1092-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1881 = "dtu_hlir.unsqueeze"(%1855, %1880) {node_name = "Unsqueeze_1093-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1882 = dtu_hlir.constant  {node_name = "Constant_1094-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1883 = "dtu_hlir.unsqueeze"(%1877, %1882) {node_name = "Unsqueeze_1095-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1884 = "dtu_hlir.concatenate"(%1879, %1881, %1883) {dimension = 0 : i64, node_name = "Concat_1096-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1885 = "dtu_hlir.dynamic_reshape"(%1871, %1884) {allowzero = 0 : i64, node_name = "Reshape_1097-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %1886 = "dtu_hlir.shape"(%1813) {end = 2147483647 : i64, node_name = "Shape_1098-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1887 = dtu_hlir.constant  {node_name = "Constant_1099-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1888 = "dtu_hlir.gather"(%1886, %1887) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1100-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1889 = "dtu_hlir.shape"(%1813) {end = 2147483647 : i64, node_name = "Shape_1101-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1890 = dtu_hlir.constant  {node_name = "Constant_1102-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1891 = "dtu_hlir.gather"(%1889, %1890) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1103-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1892 = "dtu_hlir.shape"(%1849) {end = 2147483647 : i64, node_name = "Shape_1104-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<3xi64>
    %1893 = dtu_hlir.constant  {node_name = "Constant_1105-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1894 = "dtu_hlir.gather"(%1892, %1893) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1106-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1895 = dtu_hlir.constant  {node_name = "Constant_1107-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1896 = "dtu_hlir.unsqueeze"(%1888, %1895) {node_name = "Unsqueeze_1108-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1897 = dtu_hlir.constant  {node_name = "Constant_1109-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1898 = "dtu_hlir.unsqueeze"(%1891, %1897) {node_name = "Unsqueeze_1110-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1899 = dtu_hlir.constant  {node_name = "Constant_1111-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1900 = "dtu_hlir.unsqueeze"(%1894, %1899) {node_name = "Unsqueeze_1112-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1901 = "dtu_hlir.concatenate"(%1896, %1898, %1900) {dimension = 0 : i64, node_name = "Concat_1113-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1902 = dtu_hlir.constant  {node_name = "ConstantOfShape_1114-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %1903 = "dtu_hlir.dynamic_broadcast_in_dim"(%1902, %1901) {node_name = "ConstantOfShape_1114-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x77xf32>
    %1904 = "dtu_hlir.transpose"(%1849) {node_name = "Transpose_1115-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<10x64x77xf32>
    %1905 = "dtu_hlir.dot_general"(%1813, %1904) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1116-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x77xf32>) -> tensor<10x4096x77xf32>
    %1906 = "dtu_hlir.broadcast_in_dim"(%862) {node_name = "Mul_1117-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %1907 = "dtu_hlir.mul"(%1905, %1906) {node_name = "Mul_1117-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1908 = "dtu_hlir.broadcast_in_dim"(%861) {node_name = "Mul_1118-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %1909 = "dtu_hlir.mul"(%1903, %1908) {node_name = "Mul_1118-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1910 = "dtu_hlir.add"(%1907, %1909) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1119-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1911 = "dtu_hlir.softmax"(%1910) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1120-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1912 = "dtu_hlir.convert"(%1911) {node_name = "Cast_1121-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %1913 = "dtu_hlir.dot_general"(%1912, %1885) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1122-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x77x64xf32>) -> tensor<10x4096x64xf32>
    %1914 = "dtu_hlir.shape"(%1913) {end = 2147483647 : i64, node_name = "Shape_1123-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1915 = dtu_hlir.constant  {node_name = "Constant_1124-0", node_type = "Constant"} dense<0> : tensor<i64>
    %1916 = "dtu_hlir.gather"(%1914, %1915) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1125-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1917 = "dtu_hlir.shape"(%1913) {end = 2147483647 : i64, node_name = "Shape_1126-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1918 = dtu_hlir.constant  {node_name = "Constant_1127-0", node_type = "Constant"} dense<1> : tensor<i64>
    %1919 = "dtu_hlir.gather"(%1917, %1918) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1128-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1920 = "dtu_hlir.shape"(%1913) {end = 2147483647 : i64, node_name = "Shape_1129-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %1921 = dtu_hlir.constant  {node_name = "Constant_1130-0", node_type = "Constant"} dense<2> : tensor<i64>
    %1922 = "dtu_hlir.gather"(%1920, %1921) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1131-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %1923 = dtu_hlir.constant  {node_name = "Constant_1132-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1924 = "dtu_hlir.div"(%1916, %1923) {node_name = "Div_1133-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1925 = "dtu_hlir.convert"(%1924) {node_name = "Cast_1134-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1926 = "dtu_hlir.convert"(%1925) {node_name = "Cast_1135-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1927 = dtu_hlir.constant  {node_name = "Constant_1136-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1928 = "dtu_hlir.unsqueeze"(%1926, %1927) {node_name = "Unsqueeze_1137-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1929 = dtu_hlir.constant  {node_name = "Constant_1138-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1930 = "dtu_hlir.unsqueeze"(%1919, %1929) {node_name = "Unsqueeze_1139-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1931 = dtu_hlir.constant  {node_name = "Constant_1140-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1932 = "dtu_hlir.unsqueeze"(%1922, %1931) {node_name = "Unsqueeze_1141-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1933 = "dtu_hlir.concatenate"(%1928, %860, %1930, %1932) {dimension = 0 : i64, node_name = "Concat_1142-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %1934 = "dtu_hlir.dynamic_reshape"(%1913, %1933) {allowzero = 0 : i64, node_name = "Reshape_1143-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %1935 = "dtu_hlir.transpose"(%1934) {node_name = "Transpose_1144-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %1936 = dtu_hlir.constant  {node_name = "Constant_1145-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1937 = "dtu_hlir.div"(%1916, %1936) {node_name = "Div_1146-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1938 = "dtu_hlir.convert"(%1937) {node_name = "Cast_1147-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1939 = "dtu_hlir.convert"(%1938) {node_name = "Cast_1148-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %1940 = dtu_hlir.constant  {node_name = "Constant_1149-0", node_type = "Constant"} dense<5> : tensor<i64>
    %1941 = "dtu_hlir.mul"(%1922, %1940) {node_name = "Mul_1150-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %1942 = dtu_hlir.constant  {node_name = "Constant_1151-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1943 = "dtu_hlir.unsqueeze"(%1939, %1942) {node_name = "Unsqueeze_1152-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1944 = dtu_hlir.constant  {node_name = "Constant_1153-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1945 = "dtu_hlir.unsqueeze"(%1919, %1944) {node_name = "Unsqueeze_1154-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1946 = dtu_hlir.constant  {node_name = "Constant_1155-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1947 = "dtu_hlir.unsqueeze"(%1941, %1946) {node_name = "Unsqueeze_1156-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1948 = "dtu_hlir.concatenate"(%1943, %1945, %1947) {dimension = 0 : i64, node_name = "Concat_1157-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %1949 = "dtu_hlir.dynamic_reshape"(%1935, %1948) {allowzero = 0 : i64, node_name = "Reshape_1158-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %1950 = "dtu_hlir.dot_general"(%1949, %407) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1159-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %1951 = "dtu_hlir.add"(%22, %1950) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1160-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1952 = "dtu_hlir.add"(%1951, %1747) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1161-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1953 = dtu_hlir.constant  {node_name = "ReduceMean_1162-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1954 = "dtu_hlir.reshape"(%1953) {node_name = "ReduceMean_1162-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1955 = "dtu_hlir.reduce"(%1952, %1954) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1162-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1162-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1956 = dtu_hlir.constant  {node_name = "ReduceMean_1162-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1957 = "dtu_hlir.unsqueeze"(%1955, %1956) {node_name = "ReduceMean_1162-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1958 = dtu_hlir.constant  {node_name = "ReduceMean_1162-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1959 = "dtu_hlir.broadcast_in_dim"(%1958) {node_name = "ReduceMean_1162-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1960 = "dtu_hlir.div"(%1957, %1959) {node_name = "ReduceMean_1162-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1961 = "dtu_hlir.sub"(%1952, %1960) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_1163-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1962 = dtu_hlir.constant  {node_name = "Constant_1164-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %1963 = "dtu_hlir.broadcast_in_dim"(%1962) {node_name = "Pow_1165-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %1964 = "dtu_hlir.pow"(%1961, %1963) {node_name = "Pow_1165-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %1965 = dtu_hlir.constant  {node_name = "ReduceMean_1166-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %1966 = "dtu_hlir.reshape"(%1965) {node_name = "ReduceMean_1166-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %1967 = "dtu_hlir.reduce"(%1964, %1966) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1166-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1166-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %1968 = dtu_hlir.constant  {node_name = "ReduceMean_1166-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %1969 = "dtu_hlir.unsqueeze"(%1967, %1968) {node_name = "ReduceMean_1166-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %1970 = dtu_hlir.constant  {node_name = "ReduceMean_1166-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %1971 = "dtu_hlir.broadcast_in_dim"(%1970) {node_name = "ReduceMean_1166-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %1972 = "dtu_hlir.div"(%1969, %1971) {node_name = "ReduceMean_1166-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1973 = dtu_hlir.constant  {node_name = "Constant_1167-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %1974 = "dtu_hlir.broadcast_in_dim"(%1973) {node_name = "Add_1168-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %1975 = "dtu_hlir.add"(%1972, %1974) {node_name = "Add_1168-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1976 = "dtu_hlir.sqrt"(%1975) {node_name = "Sqrt_1169-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %1977 = "dtu_hlir.div"(%1961, %1976) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_1170-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %1978 = "dtu_hlir.mul"(%1977, %27) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_1171-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1979 = "dtu_hlir.add"(%1978, %28) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1172-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %1980 = "dtu_hlir.dot_general"(%1979, %408) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1173-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x2560xf32>) -> tensor<2x4096x2560xf32>
    %1981 = "dtu_hlir.add"(%20, %1980) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1174-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2560xf32>, tensor<2x4096x2560xf32>) -> tensor<2x4096x2560xf32>
    %1982 = "dtu_hlir.shape"(%1981) {end = 2147483647 : i64, node_name = "Shape_1175-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>) -> tensor<3xi64>
    %1983 = dtu_hlir.constant  {node_name = "Constant_1176-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %1984 = "dtu_hlir.gather"(%1982, %1983) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1177-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1985 = dtu_hlir.constant  {node_name = "Constant_1178-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %1986 = dtu_hlir.constant  {node_name = "Constant_1179-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %1987 = "dtu_hlir.add"(%1984, %1986) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_1180-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1988 = dtu_hlir.constant  {node_name = "Constant_1181-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %1989 = "dtu_hlir.div"(%1987, %1988) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_1182-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1990 = dtu_hlir.constant  {node_name = "Constant_1183-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %1991 = "dtu_hlir.mul"(%1989, %1990) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_1184-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1992 = dtu_hlir.constant  {node_name = "Slice_1185-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %1993 = "dtu_hlir.real_dynamic_slice"(%1981, %1985, %1991, %1992, %1983) {node_name = "Slice_1185-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %1994 = dtu_hlir.constant  {node_name = "Constant_1186-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %1995 = "dtu_hlir.mul"(%1989, %1994) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_1187-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %1996 = "dtu_hlir.shape"(%1991) {end = 2147483647 : i64, node_name = "Slice_1188-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %1997 = dtu_hlir.constant  {node_name = "Slice_1188-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %1998 = "dtu_hlir.dynamic_broadcast_in_dim"(%1997, %1996) {node_name = "Slice_1188-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %1999 = "dtu_hlir.real_dynamic_slice"(%1981, %1991, %1995, %1998, %1983) {node_name = "Slice_1188-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %2000 = dtu_hlir.constant  {node_name = "Constant_1189-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %2001 = "dtu_hlir.broadcast_in_dim"(%2000) {node_name = "Div_1190-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %2002 = "dtu_hlir.div"(%1999, %2001) {node_name = "Div_1190-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %2003 = "dtu_hlir.erf"(%2002) {node_name = "Erf_1191-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %2004 = dtu_hlir.constant  {node_name = "Constant_1192-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %2005 = "dtu_hlir.broadcast_in_dim"(%2004) {node_name = "Add_1193-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %2006 = "dtu_hlir.add"(%2003, %2005) {node_name = "Add_1193-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %2007 = "dtu_hlir.mul"(%1999, %2006) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_1194-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %2008 = dtu_hlir.constant  {node_name = "Constant_1195-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %2009 = "dtu_hlir.broadcast_in_dim"(%2008) {node_name = "Mul_1196-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %2010 = "dtu_hlir.mul"(%2007, %2009) {node_name = "Mul_1196-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %2011 = "dtu_hlir.mul"(%1993, %2010) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_1197-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %2012 = "dtu_hlir.dot_general"(%2011, %409) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1198-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<1280x320xf32>) -> tensor<2x4096x320xf32>
    %2013 = "dtu_hlir.add"(%21, %2012) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1199-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %2014 = "dtu_hlir.add"(%2013, %1952) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1200-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %2015 = "dtu_hlir.dot_general"(%2014, %410) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1201-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %2016 = "dtu_hlir.add"(%29, %2015) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1202-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %2017 = dtu_hlir.constant  {node_name = "Constant_1203-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2018 = "dtu_hlir.unsqueeze"(%1512, %2017) {node_name = "Unsqueeze_1204-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2019 = dtu_hlir.constant  {node_name = "Constant_1205-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2020 = "dtu_hlir.unsqueeze"(%1515, %2019) {node_name = "Unsqueeze_1206-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2021 = dtu_hlir.constant  {node_name = "Constant_1207-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2022 = "dtu_hlir.unsqueeze"(%1518, %2021) {node_name = "Unsqueeze_1208-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2023 = dtu_hlir.constant  {node_name = "Constant_1209-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2024 = "dtu_hlir.unsqueeze"(%1530, %2023) {node_name = "Unsqueeze_1210-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2025 = "dtu_hlir.concatenate"(%2018, %2020, %2022, %2024) {dimension = 0 : i64, node_name = "Concat_1211-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2026 = "dtu_hlir.dynamic_reshape"(%2016, %2025) {allowzero = 0 : i64, node_name = "Reshape_1212-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x64x64x320xf32>
    %2027 = "dtu_hlir.transpose"(%2026) {node_name = "Transpose_1213-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>) -> tensor<2x320x64x64xf32>
    %2028 = "dtu_hlir.add"(%2027, %1509) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_1214-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %2029 = "dtu_hlir.conv_bias"(%2028, %42, %43) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1215-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<2> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x32x32xf32>
    %2030 = dtu_hlir.constant  {node_name = "Constant_1216-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %2031 = "dtu_hlir.dynamic_reshape"(%2029, %2030) {allowzero = 0 : i64, node_name = "Reshape_1217-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x32x32xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %2032 = dtu_hlir.constant  {node_name = "Constant_1218-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %2033 = dtu_hlir.constant  {node_name = "Constant_1219-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %2034 = "dtu_hlir.instance_norm"(%2031, %2032, %2033) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_1220-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %2035 = "dtu_hlir.shape"(%2029) {end = 2147483647 : i64, node_name = "Shape_1221-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x32x32xf32>) -> tensor<4xi64>
    %2036 = "dtu_hlir.dynamic_reshape"(%2034, %2035) {allowzero = 0 : i64, node_name = "Reshape_1222-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x320x32x32xf32>
    %2037 = "dtu_hlir.mul"(%2036, %411) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_1223-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x32x32xf32>, tensor<320x1x1xf32>) -> tensor<2x320x32x32xf32>
    %2038 = "dtu_hlir.add"(%2037, %412) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_1224-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x32x32xf32>, tensor<320x1x1xf32>) -> tensor<2x320x32x32xf32>
    %2039 = "dtu_hlir.sigmoid"(%2038) {node_name = "Sigmoid_1225-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x32x32xf32>) -> tensor<2x320x32x32xf32>
    %2040 = "dtu_hlir.mul"(%2038, %2039) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_1226-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x32x32xf32>, tensor<2x320x32x32xf32>) -> tensor<2x320x32x32xf32>
    %2041 = "dtu_hlir.conv_bias"(%2040, %68, %69) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1227-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x32x32xf32>, tensor<640x320x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %2042 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_1228-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %2043 = "dtu_hlir.mul"(%915, %2042) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1229-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %2044 = "dtu_hlir.transpose"(%70) {node_name = "Gemm_1230-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<640x1280xf32>) -> tensor<1280x640xf32>
    %2045 = "dtu_hlir.gemm"(%2043, %2044, %71) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_1230-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x640xf32>, tensor<640xf32>) -> tensor<2x640xf32>
    %2046 = dtu_hlir.constant  {node_name = "Constant_1231-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %2047 = "dtu_hlir.unsqueeze"(%2045, %2046) {node_name = "Unsqueeze_1232-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640xf32>, tensor<1xi64>) -> tensor<2x640x1xf32>
    %2048 = dtu_hlir.constant  {node_name = "Constant_1233-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %2049 = "dtu_hlir.unsqueeze"(%2047, %2048) {node_name = "Unsqueeze_1234-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640x1xf32>, tensor<1xi64>) -> tensor<2x640x1x1xf32>
    %2050 = "dtu_hlir.add"(%2041, %2049) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_1235-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2051 = dtu_hlir.constant  {node_name = "Constant_1236-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %2052 = "dtu_hlir.dynamic_reshape"(%2050, %2051) {allowzero = 0 : i64, node_name = "Reshape_1237-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %2053 = dtu_hlir.constant  {node_name = "Constant_1238-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %2054 = dtu_hlir.constant  {node_name = "Constant_1239-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %2055 = "dtu_hlir.instance_norm"(%2052, %2053, %2054) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_1240-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %2056 = "dtu_hlir.shape"(%2050) {end = 2147483647 : i64, node_name = "Shape_1241-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2057 = "dtu_hlir.dynamic_reshape"(%2055, %2056) {allowzero = 0 : i64, node_name = "Reshape_1242-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %2058 = "dtu_hlir.mul"(%2057, %413) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_1243-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2059 = "dtu_hlir.add"(%2058, %414) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_1244-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2060 = "dtu_hlir.sigmoid"(%2059) {node_name = "Sigmoid_1245-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2061 = "dtu_hlir.mul"(%2059, %2060) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_1246-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2062 = "dtu_hlir.conv_bias"(%2061, %72, %73) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1247-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x32x32xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %2063 = "dtu_hlir.conv_bias"(%2029, %74, %75) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1248-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x32x32xf32>, tensor<640x320x1x1xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %2064 = "dtu_hlir.add"(%2063, %2062) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_1249-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2065 = dtu_hlir.constant  {node_name = "Constant_1250-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %2066 = "dtu_hlir.broadcast_in_dim"(%2065) {node_name = "Div_1251-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x640x32x32xf32>
    %2067 = "dtu_hlir.div"(%2064, %2066) {node_name = "Div_1251-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2068 = "dtu_hlir.shape"(%2067) {end = 2147483647 : i64, node_name = "Shape_1252-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2069 = dtu_hlir.constant  {node_name = "Constant_1253-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2070 = "dtu_hlir.gather"(%2068, %2069) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1254-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2071 = "dtu_hlir.shape"(%2067) {end = 2147483647 : i64, node_name = "Shape_1255-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2072 = dtu_hlir.constant  {node_name = "Constant_1256-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2073 = "dtu_hlir.gather"(%2071, %2072) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1257-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2074 = "dtu_hlir.shape"(%2067) {end = 2147483647 : i64, node_name = "Shape_1258-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2075 = dtu_hlir.constant  {node_name = "Constant_1259-0", node_type = "Constant"} dense<3> : tensor<i64>
    %2076 = "dtu_hlir.gather"(%2074, %2075) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1260-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2077 = dtu_hlir.constant  {node_name = "Constant_1261-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %2078 = "dtu_hlir.dynamic_reshape"(%2067, %2077) {allowzero = 0 : i64, node_name = "Reshape_1262-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %2079 = dtu_hlir.constant  {node_name = "Constant_1263-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %2080 = dtu_hlir.constant  {node_name = "Constant_1264-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %2081 = "dtu_hlir.instance_norm"(%2078, %2079, %2080) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_1265-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %2082 = "dtu_hlir.shape"(%2067) {end = 2147483647 : i64, node_name = "Shape_1266-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2083 = "dtu_hlir.dynamic_reshape"(%2081, %2082) {allowzero = 0 : i64, node_name = "Reshape_1267-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %2084 = "dtu_hlir.mul"(%2083, %415) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_1268-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2085 = "dtu_hlir.add"(%2084, %416) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_1269-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2086 = "dtu_hlir.shape"(%2085) {end = 2147483647 : i64, node_name = "Shape_1270-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2087 = dtu_hlir.constant  {node_name = "Constant_1271-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2088 = "dtu_hlir.gather"(%2086, %2087) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1272-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2089 = "dtu_hlir.transpose"(%2085) {node_name = "Transpose_1273-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x32x32x640xf32>
    %2090 = "dtu_hlir.mul"(%2073, %2076) {node_name = "Mul_1274-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2091 = dtu_hlir.constant  {node_name = "Constant_1275-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2092 = "dtu_hlir.unsqueeze"(%2070, %2091) {node_name = "Unsqueeze_1276-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2093 = dtu_hlir.constant  {node_name = "Constant_1277-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2094 = "dtu_hlir.unsqueeze"(%2090, %2093) {node_name = "Unsqueeze_1278-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2095 = dtu_hlir.constant  {node_name = "Constant_1279-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2096 = "dtu_hlir.unsqueeze"(%2088, %2095) {node_name = "Unsqueeze_1280-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2097 = "dtu_hlir.concatenate"(%2092, %2094, %2096) {dimension = 0 : i64, node_name = "Concat_1281-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2098 = "dtu_hlir.dynamic_reshape"(%2089, %2097) {allowzero = 0 : i64, node_name = "Reshape_1282-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %2099 = "dtu_hlir.dot_general"(%2098, %417) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1283-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2100 = "dtu_hlir.add"(%44, %2099) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1284-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2101 = dtu_hlir.constant  {node_name = "ReduceMean_1285-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2102 = "dtu_hlir.reshape"(%2101) {node_name = "ReduceMean_1285-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2103 = "dtu_hlir.reduce"(%2100, %2102) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1285-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1285-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2104 = dtu_hlir.constant  {node_name = "ReduceMean_1285-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2105 = "dtu_hlir.unsqueeze"(%2103, %2104) {node_name = "ReduceMean_1285-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2106 = dtu_hlir.constant  {node_name = "ReduceMean_1285-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2107 = "dtu_hlir.broadcast_in_dim"(%2106) {node_name = "ReduceMean_1285-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2108 = "dtu_hlir.div"(%2105, %2107) {node_name = "ReduceMean_1285-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2109 = "dtu_hlir.sub"(%2100, %2108) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_1286-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2110 = dtu_hlir.constant  {node_name = "Constant_1287-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %2111 = "dtu_hlir.broadcast_in_dim"(%2110) {node_name = "Pow_1288-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %2112 = "dtu_hlir.pow"(%2109, %2111) {node_name = "Pow_1288-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2113 = dtu_hlir.constant  {node_name = "ReduceMean_1289-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2114 = "dtu_hlir.reshape"(%2113) {node_name = "ReduceMean_1289-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2115 = "dtu_hlir.reduce"(%2112, %2114) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1289-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1289-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2116 = dtu_hlir.constant  {node_name = "ReduceMean_1289-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2117 = "dtu_hlir.unsqueeze"(%2115, %2116) {node_name = "ReduceMean_1289-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2118 = dtu_hlir.constant  {node_name = "ReduceMean_1289-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2119 = "dtu_hlir.broadcast_in_dim"(%2118) {node_name = "ReduceMean_1289-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2120 = "dtu_hlir.div"(%2117, %2119) {node_name = "ReduceMean_1289-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2121 = dtu_hlir.constant  {node_name = "Constant_1290-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %2122 = "dtu_hlir.broadcast_in_dim"(%2121) {node_name = "Add_1291-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %2123 = "dtu_hlir.add"(%2120, %2122) {node_name = "Add_1291-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2124 = "dtu_hlir.sqrt"(%2123) {node_name = "Sqrt_1292-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2125 = "dtu_hlir.div"(%2109, %2124) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_1293-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2126 = "dtu_hlir.mul"(%2125, %49) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_1294-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2127 = "dtu_hlir.add"(%2126, %50) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1295-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2128 = "dtu_hlir.dot_general"(%2127, %418) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1296-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2129 = "dtu_hlir.dot_general"(%2127, %419) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1297-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2130 = "dtu_hlir.dot_general"(%2127, %420) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1298-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2131 = "dtu_hlir.shape"(%2128) {end = 2147483647 : i64, node_name = "Shape_1299-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2132 = dtu_hlir.constant  {node_name = "Constant_1300-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2133 = "dtu_hlir.gather"(%2131, %2132) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1301-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2134 = "dtu_hlir.shape"(%2128) {end = 2147483647 : i64, node_name = "Shape_1302-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2135 = dtu_hlir.constant  {node_name = "Constant_1303-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2136 = "dtu_hlir.gather"(%2134, %2135) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1304-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2137 = "dtu_hlir.shape"(%2128) {end = 2147483647 : i64, node_name = "Shape_1305-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2138 = dtu_hlir.constant  {node_name = "Constant_1306-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2139 = "dtu_hlir.gather"(%2137, %2138) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1307-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2140 = dtu_hlir.constant  {node_name = "Constant_1308-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2141 = "dtu_hlir.div"(%2139, %2140) {node_name = "Div_1309-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2142 = "dtu_hlir.convert"(%2141) {node_name = "Cast_1310-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2143 = "dtu_hlir.convert"(%2142) {node_name = "Cast_1311-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2144 = dtu_hlir.constant  {node_name = "Constant_1312-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2145 = "dtu_hlir.unsqueeze"(%2133, %2144) {node_name = "Unsqueeze_1313-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2146 = dtu_hlir.constant  {node_name = "Constant_1314-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2147 = "dtu_hlir.unsqueeze"(%2136, %2146) {node_name = "Unsqueeze_1315-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2148 = dtu_hlir.constant  {node_name = "Constant_1316-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2149 = "dtu_hlir.unsqueeze"(%2143, %2148) {node_name = "Unsqueeze_1317-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2150 = "dtu_hlir.concatenate"(%2145, %2147, %421, %2149) {dimension = 0 : i64, node_name = "Concat_1318-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2151 = "dtu_hlir.dynamic_reshape"(%2128, %2150) {allowzero = 0 : i64, node_name = "Reshape_1319-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2152 = "dtu_hlir.transpose"(%2151) {node_name = "Transpose_1320-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2153 = dtu_hlir.constant  {node_name = "Constant_1321-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2154 = "dtu_hlir.mul"(%2133, %2153) {node_name = "Mul_1322-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2155 = dtu_hlir.constant  {node_name = "Constant_1323-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2156 = "dtu_hlir.div"(%2139, %2155) {node_name = "Div_1324-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2157 = "dtu_hlir.convert"(%2156) {node_name = "Cast_1325-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2158 = "dtu_hlir.convert"(%2157) {node_name = "Cast_1326-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2159 = dtu_hlir.constant  {node_name = "Constant_1327-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2160 = "dtu_hlir.unsqueeze"(%2154, %2159) {node_name = "Unsqueeze_1328-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2161 = dtu_hlir.constant  {node_name = "Constant_1329-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2162 = "dtu_hlir.unsqueeze"(%2136, %2161) {node_name = "Unsqueeze_1330-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2163 = dtu_hlir.constant  {node_name = "Constant_1331-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2164 = "dtu_hlir.unsqueeze"(%2158, %2163) {node_name = "Unsqueeze_1332-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2165 = "dtu_hlir.concatenate"(%2160, %2162, %2164) {dimension = 0 : i64, node_name = "Concat_1333-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2166 = "dtu_hlir.dynamic_reshape"(%2152, %2165) {allowzero = 0 : i64, node_name = "Reshape_1334-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2167 = "dtu_hlir.shape"(%2129) {end = 2147483647 : i64, node_name = "Shape_1335-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2168 = dtu_hlir.constant  {node_name = "Constant_1336-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2169 = "dtu_hlir.gather"(%2167, %2168) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1337-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2170 = "dtu_hlir.shape"(%2129) {end = 2147483647 : i64, node_name = "Shape_1338-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2171 = dtu_hlir.constant  {node_name = "Constant_1339-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2172 = "dtu_hlir.gather"(%2170, %2171) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1340-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2173 = "dtu_hlir.shape"(%2129) {end = 2147483647 : i64, node_name = "Shape_1341-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2174 = dtu_hlir.constant  {node_name = "Constant_1342-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2175 = "dtu_hlir.gather"(%2173, %2174) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1343-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2176 = dtu_hlir.constant  {node_name = "Constant_1344-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2177 = "dtu_hlir.div"(%2175, %2176) {node_name = "Div_1345-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2178 = "dtu_hlir.convert"(%2177) {node_name = "Cast_1346-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2179 = "dtu_hlir.convert"(%2178) {node_name = "Cast_1347-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2180 = dtu_hlir.constant  {node_name = "Constant_1348-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2181 = "dtu_hlir.unsqueeze"(%2169, %2180) {node_name = "Unsqueeze_1349-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2182 = dtu_hlir.constant  {node_name = "Constant_1350-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2183 = "dtu_hlir.unsqueeze"(%2172, %2182) {node_name = "Unsqueeze_1351-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2184 = dtu_hlir.constant  {node_name = "Constant_1352-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2185 = "dtu_hlir.unsqueeze"(%2179, %2184) {node_name = "Unsqueeze_1353-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2186 = "dtu_hlir.concatenate"(%2181, %2183, %859, %2185) {dimension = 0 : i64, node_name = "Concat_1354-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2187 = "dtu_hlir.dynamic_reshape"(%2129, %2186) {allowzero = 0 : i64, node_name = "Reshape_1355-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2188 = "dtu_hlir.transpose"(%2187) {node_name = "Transpose_1356-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2189 = dtu_hlir.constant  {node_name = "Constant_1357-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2190 = "dtu_hlir.mul"(%2169, %2189) {node_name = "Mul_1358-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2191 = dtu_hlir.constant  {node_name = "Constant_1359-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2192 = "dtu_hlir.div"(%2175, %2191) {node_name = "Div_1360-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2193 = "dtu_hlir.convert"(%2192) {node_name = "Cast_1361-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2194 = "dtu_hlir.convert"(%2193) {node_name = "Cast_1362-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2195 = dtu_hlir.constant  {node_name = "Constant_1363-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2196 = "dtu_hlir.unsqueeze"(%2190, %2195) {node_name = "Unsqueeze_1364-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2197 = dtu_hlir.constant  {node_name = "Constant_1365-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2198 = "dtu_hlir.unsqueeze"(%2172, %2197) {node_name = "Unsqueeze_1366-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2199 = dtu_hlir.constant  {node_name = "Constant_1367-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2200 = "dtu_hlir.unsqueeze"(%2194, %2199) {node_name = "Unsqueeze_1368-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2201 = "dtu_hlir.concatenate"(%2196, %2198, %2200) {dimension = 0 : i64, node_name = "Concat_1369-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2202 = "dtu_hlir.dynamic_reshape"(%2188, %2201) {allowzero = 0 : i64, node_name = "Reshape_1370-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2203 = "dtu_hlir.shape"(%2130) {end = 2147483647 : i64, node_name = "Shape_1371-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2204 = dtu_hlir.constant  {node_name = "Constant_1372-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2205 = "dtu_hlir.gather"(%2203, %2204) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1373-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2206 = "dtu_hlir.shape"(%2130) {end = 2147483647 : i64, node_name = "Shape_1374-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2207 = dtu_hlir.constant  {node_name = "Constant_1375-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2208 = "dtu_hlir.gather"(%2206, %2207) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1376-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2209 = "dtu_hlir.shape"(%2130) {end = 2147483647 : i64, node_name = "Shape_1377-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2210 = dtu_hlir.constant  {node_name = "Constant_1378-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2211 = "dtu_hlir.gather"(%2209, %2210) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1379-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2212 = dtu_hlir.constant  {node_name = "Constant_1380-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2213 = "dtu_hlir.div"(%2211, %2212) {node_name = "Div_1381-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2214 = "dtu_hlir.convert"(%2213) {node_name = "Cast_1382-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2215 = "dtu_hlir.convert"(%2214) {node_name = "Cast_1383-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2216 = dtu_hlir.constant  {node_name = "Constant_1384-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2217 = "dtu_hlir.unsqueeze"(%2205, %2216) {node_name = "Unsqueeze_1385-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2218 = dtu_hlir.constant  {node_name = "Constant_1386-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2219 = "dtu_hlir.unsqueeze"(%2208, %2218) {node_name = "Unsqueeze_1387-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2220 = dtu_hlir.constant  {node_name = "Constant_1388-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2221 = "dtu_hlir.unsqueeze"(%2215, %2220) {node_name = "Unsqueeze_1389-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2222 = "dtu_hlir.concatenate"(%2217, %2219, %858, %2221) {dimension = 0 : i64, node_name = "Concat_1390-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2223 = "dtu_hlir.dynamic_reshape"(%2130, %2222) {allowzero = 0 : i64, node_name = "Reshape_1391-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2224 = "dtu_hlir.transpose"(%2223) {node_name = "Transpose_1392-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2225 = dtu_hlir.constant  {node_name = "Constant_1393-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2226 = "dtu_hlir.mul"(%2205, %2225) {node_name = "Mul_1394-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2227 = dtu_hlir.constant  {node_name = "Constant_1395-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2228 = "dtu_hlir.div"(%2211, %2227) {node_name = "Div_1396-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2229 = "dtu_hlir.convert"(%2228) {node_name = "Cast_1397-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2230 = "dtu_hlir.convert"(%2229) {node_name = "Cast_1398-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2231 = dtu_hlir.constant  {node_name = "Constant_1399-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2232 = "dtu_hlir.unsqueeze"(%2226, %2231) {node_name = "Unsqueeze_1400-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2233 = dtu_hlir.constant  {node_name = "Constant_1401-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2234 = "dtu_hlir.unsqueeze"(%2208, %2233) {node_name = "Unsqueeze_1402-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2235 = dtu_hlir.constant  {node_name = "Constant_1403-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2236 = "dtu_hlir.unsqueeze"(%2230, %2235) {node_name = "Unsqueeze_1404-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2237 = "dtu_hlir.concatenate"(%2232, %2234, %2236) {dimension = 0 : i64, node_name = "Concat_1405-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2238 = "dtu_hlir.dynamic_reshape"(%2224, %2237) {allowzero = 0 : i64, node_name = "Reshape_1406-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2239 = "dtu_hlir.shape"(%2166) {end = 2147483647 : i64, node_name = "Shape_1407-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2240 = dtu_hlir.constant  {node_name = "Constant_1408-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2241 = "dtu_hlir.gather"(%2239, %2240) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1409-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2242 = "dtu_hlir.shape"(%2166) {end = 2147483647 : i64, node_name = "Shape_1410-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2243 = dtu_hlir.constant  {node_name = "Constant_1411-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2244 = "dtu_hlir.gather"(%2242, %2243) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1412-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2245 = "dtu_hlir.shape"(%2202) {end = 2147483647 : i64, node_name = "Shape_1413-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2246 = dtu_hlir.constant  {node_name = "Constant_1414-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2247 = "dtu_hlir.gather"(%2245, %2246) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1415-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2248 = dtu_hlir.constant  {node_name = "Constant_1416-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2249 = "dtu_hlir.unsqueeze"(%2241, %2248) {node_name = "Unsqueeze_1417-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2250 = dtu_hlir.constant  {node_name = "Constant_1418-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2251 = "dtu_hlir.unsqueeze"(%2244, %2250) {node_name = "Unsqueeze_1419-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2252 = dtu_hlir.constant  {node_name = "Constant_1420-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2253 = "dtu_hlir.unsqueeze"(%2247, %2252) {node_name = "Unsqueeze_1421-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2254 = "dtu_hlir.concatenate"(%2249, %2251, %2253) {dimension = 0 : i64, node_name = "Concat_1422-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2255 = dtu_hlir.constant  {node_name = "ConstantOfShape_1423-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %2256 = "dtu_hlir.dynamic_broadcast_in_dim"(%2255, %2254) {node_name = "ConstantOfShape_1423-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x1024xf32>
    %2257 = "dtu_hlir.transpose"(%2202) {node_name = "Transpose_1424-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<20x64x1024xf32>
    %2258 = "dtu_hlir.dot_general"(%2166, %2257) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1425-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x1024xf32>) -> tensor<20x1024x1024xf32>
    %2259 = "dtu_hlir.broadcast_in_dim"(%857) {node_name = "Mul_1426-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %2260 = "dtu_hlir.mul"(%2258, %2259) {node_name = "Mul_1426-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2261 = "dtu_hlir.broadcast_in_dim"(%856) {node_name = "Mul_1427-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %2262 = "dtu_hlir.mul"(%2256, %2261) {node_name = "Mul_1427-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2263 = "dtu_hlir.add"(%2260, %2262) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1428-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2264 = "dtu_hlir.softmax"(%2263) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1429-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2265 = "dtu_hlir.convert"(%2264) {node_name = "Cast_1430-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2266 = "dtu_hlir.dot_general"(%2265, %2238) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1431-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x64xf32>) -> tensor<20x1024x64xf32>
    %2267 = "dtu_hlir.shape"(%2266) {end = 2147483647 : i64, node_name = "Shape_1432-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2268 = dtu_hlir.constant  {node_name = "Constant_1433-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2269 = "dtu_hlir.gather"(%2267, %2268) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1434-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2270 = "dtu_hlir.shape"(%2266) {end = 2147483647 : i64, node_name = "Shape_1435-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2271 = dtu_hlir.constant  {node_name = "Constant_1436-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2272 = "dtu_hlir.gather"(%2270, %2271) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1437-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2273 = "dtu_hlir.shape"(%2266) {end = 2147483647 : i64, node_name = "Shape_1438-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2274 = dtu_hlir.constant  {node_name = "Constant_1439-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2275 = "dtu_hlir.gather"(%2273, %2274) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1440-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2276 = dtu_hlir.constant  {node_name = "Constant_1441-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2277 = "dtu_hlir.div"(%2269, %2276) {node_name = "Div_1442-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2278 = "dtu_hlir.convert"(%2277) {node_name = "Cast_1443-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2279 = "dtu_hlir.convert"(%2278) {node_name = "Cast_1444-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2280 = dtu_hlir.constant  {node_name = "Constant_1445-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2281 = "dtu_hlir.unsqueeze"(%2279, %2280) {node_name = "Unsqueeze_1446-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2282 = dtu_hlir.constant  {node_name = "Constant_1447-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2283 = "dtu_hlir.unsqueeze"(%2272, %2282) {node_name = "Unsqueeze_1448-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2284 = dtu_hlir.constant  {node_name = "Constant_1449-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2285 = "dtu_hlir.unsqueeze"(%2275, %2284) {node_name = "Unsqueeze_1450-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2286 = "dtu_hlir.concatenate"(%2281, %855, %2283, %2285) {dimension = 0 : i64, node_name = "Concat_1451-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2287 = "dtu_hlir.dynamic_reshape"(%2266, %2286) {allowzero = 0 : i64, node_name = "Reshape_1452-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %2288 = "dtu_hlir.transpose"(%2287) {node_name = "Transpose_1453-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %2289 = dtu_hlir.constant  {node_name = "Constant_1454-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2290 = "dtu_hlir.div"(%2269, %2289) {node_name = "Div_1455-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2291 = "dtu_hlir.convert"(%2290) {node_name = "Cast_1456-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2292 = "dtu_hlir.convert"(%2291) {node_name = "Cast_1457-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2293 = dtu_hlir.constant  {node_name = "Constant_1458-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2294 = "dtu_hlir.mul"(%2275, %2293) {node_name = "Mul_1459-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2295 = dtu_hlir.constant  {node_name = "Constant_1460-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2296 = "dtu_hlir.unsqueeze"(%2292, %2295) {node_name = "Unsqueeze_1461-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2297 = dtu_hlir.constant  {node_name = "Constant_1462-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2298 = "dtu_hlir.unsqueeze"(%2272, %2297) {node_name = "Unsqueeze_1463-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2299 = dtu_hlir.constant  {node_name = "Constant_1464-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2300 = "dtu_hlir.unsqueeze"(%2294, %2299) {node_name = "Unsqueeze_1465-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2301 = "dtu_hlir.concatenate"(%2296, %2298, %2300) {dimension = 0 : i64, node_name = "Concat_1466-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2302 = "dtu_hlir.dynamic_reshape"(%2288, %2301) {allowzero = 0 : i64, node_name = "Reshape_1467-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %2303 = "dtu_hlir.dot_general"(%2302, %422) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1468-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2304 = "dtu_hlir.add"(%45, %2303) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1469-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2305 = "dtu_hlir.add"(%2304, %2100) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1470-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2306 = dtu_hlir.constant  {node_name = "ReduceMean_1471-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2307 = "dtu_hlir.reshape"(%2306) {node_name = "ReduceMean_1471-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2308 = "dtu_hlir.reduce"(%2305, %2307) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1471-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1471-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2309 = dtu_hlir.constant  {node_name = "ReduceMean_1471-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2310 = "dtu_hlir.unsqueeze"(%2308, %2309) {node_name = "ReduceMean_1471-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2311 = dtu_hlir.constant  {node_name = "ReduceMean_1471-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2312 = "dtu_hlir.broadcast_in_dim"(%2311) {node_name = "ReduceMean_1471-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2313 = "dtu_hlir.div"(%2310, %2312) {node_name = "ReduceMean_1471-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2314 = "dtu_hlir.sub"(%2305, %2313) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_1472-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2315 = dtu_hlir.constant  {node_name = "Constant_1473-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %2316 = "dtu_hlir.broadcast_in_dim"(%2315) {node_name = "Pow_1474-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %2317 = "dtu_hlir.pow"(%2314, %2316) {node_name = "Pow_1474-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2318 = dtu_hlir.constant  {node_name = "ReduceMean_1475-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2319 = "dtu_hlir.reshape"(%2318) {node_name = "ReduceMean_1475-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2320 = "dtu_hlir.reduce"(%2317, %2319) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1475-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1475-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2321 = dtu_hlir.constant  {node_name = "ReduceMean_1475-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2322 = "dtu_hlir.unsqueeze"(%2320, %2321) {node_name = "ReduceMean_1475-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2323 = dtu_hlir.constant  {node_name = "ReduceMean_1475-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2324 = "dtu_hlir.broadcast_in_dim"(%2323) {node_name = "ReduceMean_1475-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2325 = "dtu_hlir.div"(%2322, %2324) {node_name = "ReduceMean_1475-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2326 = dtu_hlir.constant  {node_name = "Constant_1476-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %2327 = "dtu_hlir.broadcast_in_dim"(%2326) {node_name = "Add_1477-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %2328 = "dtu_hlir.add"(%2325, %2327) {node_name = "Add_1477-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2329 = "dtu_hlir.sqrt"(%2328) {node_name = "Sqrt_1478-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2330 = "dtu_hlir.div"(%2314, %2329) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_1479-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2331 = "dtu_hlir.mul"(%2330, %51) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_1480-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2332 = "dtu_hlir.add"(%2331, %52) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1481-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2333 = "dtu_hlir.dot_general"(%2332, %423) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1482-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2334 = "dtu_hlir.dot_general"(%arg2, %424) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1483-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %2335 = "dtu_hlir.dot_general"(%arg2, %425) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1484-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %2336 = "dtu_hlir.shape"(%2333) {end = 2147483647 : i64, node_name = "Shape_1485-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2337 = dtu_hlir.constant  {node_name = "Constant_1486-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2338 = "dtu_hlir.gather"(%2336, %2337) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1487-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2339 = "dtu_hlir.shape"(%2333) {end = 2147483647 : i64, node_name = "Shape_1488-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2340 = dtu_hlir.constant  {node_name = "Constant_1489-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2341 = "dtu_hlir.gather"(%2339, %2340) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1490-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2342 = "dtu_hlir.shape"(%2333) {end = 2147483647 : i64, node_name = "Shape_1491-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2343 = dtu_hlir.constant  {node_name = "Constant_1492-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2344 = "dtu_hlir.gather"(%2342, %2343) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1493-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2345 = dtu_hlir.constant  {node_name = "Constant_1494-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2346 = "dtu_hlir.div"(%2344, %2345) {node_name = "Div_1495-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2347 = "dtu_hlir.convert"(%2346) {node_name = "Cast_1496-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2348 = "dtu_hlir.convert"(%2347) {node_name = "Cast_1497-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2349 = dtu_hlir.constant  {node_name = "Constant_1498-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2350 = "dtu_hlir.unsqueeze"(%2338, %2349) {node_name = "Unsqueeze_1499-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2351 = dtu_hlir.constant  {node_name = "Constant_1500-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2352 = "dtu_hlir.unsqueeze"(%2341, %2351) {node_name = "Unsqueeze_1501-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2353 = dtu_hlir.constant  {node_name = "Constant_1502-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2354 = "dtu_hlir.unsqueeze"(%2348, %2353) {node_name = "Unsqueeze_1503-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2355 = "dtu_hlir.concatenate"(%2350, %2352, %854, %2354) {dimension = 0 : i64, node_name = "Concat_1504-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2356 = "dtu_hlir.dynamic_reshape"(%2333, %2355) {allowzero = 0 : i64, node_name = "Reshape_1505-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2357 = "dtu_hlir.transpose"(%2356) {node_name = "Transpose_1506-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2358 = dtu_hlir.constant  {node_name = "Constant_1507-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2359 = "dtu_hlir.mul"(%2338, %2358) {node_name = "Mul_1508-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2360 = dtu_hlir.constant  {node_name = "Constant_1509-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2361 = "dtu_hlir.div"(%2344, %2360) {node_name = "Div_1510-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2362 = "dtu_hlir.convert"(%2361) {node_name = "Cast_1511-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2363 = "dtu_hlir.convert"(%2362) {node_name = "Cast_1512-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2364 = dtu_hlir.constant  {node_name = "Constant_1513-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2365 = "dtu_hlir.unsqueeze"(%2359, %2364) {node_name = "Unsqueeze_1514-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2366 = dtu_hlir.constant  {node_name = "Constant_1515-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2367 = "dtu_hlir.unsqueeze"(%2341, %2366) {node_name = "Unsqueeze_1516-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2368 = dtu_hlir.constant  {node_name = "Constant_1517-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2369 = "dtu_hlir.unsqueeze"(%2363, %2368) {node_name = "Unsqueeze_1518-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2370 = "dtu_hlir.concatenate"(%2365, %2367, %2369) {dimension = 0 : i64, node_name = "Concat_1519-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2371 = "dtu_hlir.dynamic_reshape"(%2357, %2370) {allowzero = 0 : i64, node_name = "Reshape_1520-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2372 = "dtu_hlir.shape"(%2334) {end = 2147483647 : i64, node_name = "Shape_1521-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2373 = dtu_hlir.constant  {node_name = "Constant_1522-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2374 = "dtu_hlir.gather"(%2372, %2373) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1523-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2375 = "dtu_hlir.shape"(%2334) {end = 2147483647 : i64, node_name = "Shape_1524-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2376 = dtu_hlir.constant  {node_name = "Constant_1525-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2377 = "dtu_hlir.gather"(%2375, %2376) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1526-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2378 = "dtu_hlir.shape"(%2334) {end = 2147483647 : i64, node_name = "Shape_1527-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2379 = dtu_hlir.constant  {node_name = "Constant_1528-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2380 = "dtu_hlir.gather"(%2378, %2379) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1529-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2381 = dtu_hlir.constant  {node_name = "Constant_1530-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2382 = "dtu_hlir.div"(%2380, %2381) {node_name = "Div_1531-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2383 = "dtu_hlir.convert"(%2382) {node_name = "Cast_1532-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2384 = "dtu_hlir.convert"(%2383) {node_name = "Cast_1533-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2385 = dtu_hlir.constant  {node_name = "Constant_1534-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2386 = "dtu_hlir.unsqueeze"(%2374, %2385) {node_name = "Unsqueeze_1535-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2387 = dtu_hlir.constant  {node_name = "Constant_1536-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2388 = "dtu_hlir.unsqueeze"(%2377, %2387) {node_name = "Unsqueeze_1537-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2389 = dtu_hlir.constant  {node_name = "Constant_1538-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2390 = "dtu_hlir.unsqueeze"(%2384, %2389) {node_name = "Unsqueeze_1539-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2391 = "dtu_hlir.concatenate"(%2386, %2388, %853, %2390) {dimension = 0 : i64, node_name = "Concat_1540-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2392 = "dtu_hlir.dynamic_reshape"(%2334, %2391) {allowzero = 0 : i64, node_name = "Reshape_1541-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %2393 = "dtu_hlir.transpose"(%2392) {node_name = "Transpose_1542-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %2394 = dtu_hlir.constant  {node_name = "Constant_1543-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2395 = "dtu_hlir.mul"(%2374, %2394) {node_name = "Mul_1544-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2396 = dtu_hlir.constant  {node_name = "Constant_1545-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2397 = "dtu_hlir.div"(%2380, %2396) {node_name = "Div_1546-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2398 = "dtu_hlir.convert"(%2397) {node_name = "Cast_1547-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2399 = "dtu_hlir.convert"(%2398) {node_name = "Cast_1548-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2400 = dtu_hlir.constant  {node_name = "Constant_1549-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2401 = "dtu_hlir.unsqueeze"(%2395, %2400) {node_name = "Unsqueeze_1550-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2402 = dtu_hlir.constant  {node_name = "Constant_1551-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2403 = "dtu_hlir.unsqueeze"(%2377, %2402) {node_name = "Unsqueeze_1552-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2404 = dtu_hlir.constant  {node_name = "Constant_1553-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2405 = "dtu_hlir.unsqueeze"(%2399, %2404) {node_name = "Unsqueeze_1554-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2406 = "dtu_hlir.concatenate"(%2401, %2403, %2405) {dimension = 0 : i64, node_name = "Concat_1555-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2407 = "dtu_hlir.dynamic_reshape"(%2393, %2406) {allowzero = 0 : i64, node_name = "Reshape_1556-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %2408 = "dtu_hlir.shape"(%2335) {end = 2147483647 : i64, node_name = "Shape_1557-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2409 = dtu_hlir.constant  {node_name = "Constant_1558-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2410 = "dtu_hlir.gather"(%2408, %2409) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1559-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2411 = "dtu_hlir.shape"(%2335) {end = 2147483647 : i64, node_name = "Shape_1560-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2412 = dtu_hlir.constant  {node_name = "Constant_1561-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2413 = "dtu_hlir.gather"(%2411, %2412) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1562-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2414 = "dtu_hlir.shape"(%2335) {end = 2147483647 : i64, node_name = "Shape_1563-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2415 = dtu_hlir.constant  {node_name = "Constant_1564-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2416 = "dtu_hlir.gather"(%2414, %2415) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1565-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2417 = dtu_hlir.constant  {node_name = "Constant_1566-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2418 = "dtu_hlir.div"(%2416, %2417) {node_name = "Div_1567-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2419 = "dtu_hlir.convert"(%2418) {node_name = "Cast_1568-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2420 = "dtu_hlir.convert"(%2419) {node_name = "Cast_1569-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2421 = dtu_hlir.constant  {node_name = "Constant_1570-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2422 = "dtu_hlir.unsqueeze"(%2410, %2421) {node_name = "Unsqueeze_1571-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2423 = dtu_hlir.constant  {node_name = "Constant_1572-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2424 = "dtu_hlir.unsqueeze"(%2413, %2423) {node_name = "Unsqueeze_1573-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2425 = dtu_hlir.constant  {node_name = "Constant_1574-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2426 = "dtu_hlir.unsqueeze"(%2420, %2425) {node_name = "Unsqueeze_1575-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2427 = "dtu_hlir.concatenate"(%2422, %2424, %852, %2426) {dimension = 0 : i64, node_name = "Concat_1576-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2428 = "dtu_hlir.dynamic_reshape"(%2335, %2427) {allowzero = 0 : i64, node_name = "Reshape_1577-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %2429 = "dtu_hlir.transpose"(%2428) {node_name = "Transpose_1578-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %2430 = dtu_hlir.constant  {node_name = "Constant_1579-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2431 = "dtu_hlir.mul"(%2410, %2430) {node_name = "Mul_1580-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2432 = dtu_hlir.constant  {node_name = "Constant_1581-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2433 = "dtu_hlir.div"(%2416, %2432) {node_name = "Div_1582-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2434 = "dtu_hlir.convert"(%2433) {node_name = "Cast_1583-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2435 = "dtu_hlir.convert"(%2434) {node_name = "Cast_1584-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2436 = dtu_hlir.constant  {node_name = "Constant_1585-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2437 = "dtu_hlir.unsqueeze"(%2431, %2436) {node_name = "Unsqueeze_1586-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2438 = dtu_hlir.constant  {node_name = "Constant_1587-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2439 = "dtu_hlir.unsqueeze"(%2413, %2438) {node_name = "Unsqueeze_1588-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2440 = dtu_hlir.constant  {node_name = "Constant_1589-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2441 = "dtu_hlir.unsqueeze"(%2435, %2440) {node_name = "Unsqueeze_1590-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2442 = "dtu_hlir.concatenate"(%2437, %2439, %2441) {dimension = 0 : i64, node_name = "Concat_1591-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2443 = "dtu_hlir.dynamic_reshape"(%2429, %2442) {allowzero = 0 : i64, node_name = "Reshape_1592-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %2444 = "dtu_hlir.shape"(%2371) {end = 2147483647 : i64, node_name = "Shape_1593-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2445 = dtu_hlir.constant  {node_name = "Constant_1594-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2446 = "dtu_hlir.gather"(%2444, %2445) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1595-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2447 = "dtu_hlir.shape"(%2371) {end = 2147483647 : i64, node_name = "Shape_1596-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2448 = dtu_hlir.constant  {node_name = "Constant_1597-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2449 = "dtu_hlir.gather"(%2447, %2448) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1598-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2450 = "dtu_hlir.shape"(%2407) {end = 2147483647 : i64, node_name = "Shape_1599-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<3xi64>
    %2451 = dtu_hlir.constant  {node_name = "Constant_1600-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2452 = "dtu_hlir.gather"(%2450, %2451) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1601-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2453 = dtu_hlir.constant  {node_name = "Constant_1602-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2454 = "dtu_hlir.unsqueeze"(%2446, %2453) {node_name = "Unsqueeze_1603-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2455 = dtu_hlir.constant  {node_name = "Constant_1604-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2456 = "dtu_hlir.unsqueeze"(%2449, %2455) {node_name = "Unsqueeze_1605-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2457 = dtu_hlir.constant  {node_name = "Constant_1606-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2458 = "dtu_hlir.unsqueeze"(%2452, %2457) {node_name = "Unsqueeze_1607-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2459 = "dtu_hlir.concatenate"(%2454, %2456, %2458) {dimension = 0 : i64, node_name = "Concat_1608-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2460 = dtu_hlir.constant  {node_name = "ConstantOfShape_1609-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %2461 = "dtu_hlir.dynamic_broadcast_in_dim"(%2460, %2459) {node_name = "ConstantOfShape_1609-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x77xf32>
    %2462 = "dtu_hlir.transpose"(%2407) {node_name = "Transpose_1610-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<20x64x77xf32>
    %2463 = "dtu_hlir.dot_general"(%2371, %2462) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1611-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x77xf32>) -> tensor<20x1024x77xf32>
    %2464 = "dtu_hlir.broadcast_in_dim"(%851) {node_name = "Mul_1612-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %2465 = "dtu_hlir.mul"(%2463, %2464) {node_name = "Mul_1612-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %2466 = "dtu_hlir.broadcast_in_dim"(%850) {node_name = "Mul_1613-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %2467 = "dtu_hlir.mul"(%2461, %2466) {node_name = "Mul_1613-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %2468 = "dtu_hlir.add"(%2465, %2467) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1614-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %2469 = "dtu_hlir.softmax"(%2468) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1615-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %2470 = "dtu_hlir.convert"(%2469) {node_name = "Cast_1616-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %2471 = "dtu_hlir.dot_general"(%2470, %2443) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1617-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x77x64xf32>) -> tensor<20x1024x64xf32>
    %2472 = "dtu_hlir.shape"(%2471) {end = 2147483647 : i64, node_name = "Shape_1618-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2473 = dtu_hlir.constant  {node_name = "Constant_1619-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2474 = "dtu_hlir.gather"(%2472, %2473) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1620-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2475 = "dtu_hlir.shape"(%2471) {end = 2147483647 : i64, node_name = "Shape_1621-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2476 = dtu_hlir.constant  {node_name = "Constant_1622-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2477 = "dtu_hlir.gather"(%2475, %2476) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1623-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2478 = "dtu_hlir.shape"(%2471) {end = 2147483647 : i64, node_name = "Shape_1624-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2479 = dtu_hlir.constant  {node_name = "Constant_1625-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2480 = "dtu_hlir.gather"(%2478, %2479) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1626-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2481 = dtu_hlir.constant  {node_name = "Constant_1627-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2482 = "dtu_hlir.div"(%2474, %2481) {node_name = "Div_1628-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2483 = "dtu_hlir.convert"(%2482) {node_name = "Cast_1629-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2484 = "dtu_hlir.convert"(%2483) {node_name = "Cast_1630-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2485 = dtu_hlir.constant  {node_name = "Constant_1631-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2486 = "dtu_hlir.unsqueeze"(%2484, %2485) {node_name = "Unsqueeze_1632-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2487 = dtu_hlir.constant  {node_name = "Constant_1633-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2488 = "dtu_hlir.unsqueeze"(%2477, %2487) {node_name = "Unsqueeze_1634-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2489 = dtu_hlir.constant  {node_name = "Constant_1635-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2490 = "dtu_hlir.unsqueeze"(%2480, %2489) {node_name = "Unsqueeze_1636-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2491 = "dtu_hlir.concatenate"(%2486, %849, %2488, %2490) {dimension = 0 : i64, node_name = "Concat_1637-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2492 = "dtu_hlir.dynamic_reshape"(%2471, %2491) {allowzero = 0 : i64, node_name = "Reshape_1638-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %2493 = "dtu_hlir.transpose"(%2492) {node_name = "Transpose_1639-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %2494 = dtu_hlir.constant  {node_name = "Constant_1640-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2495 = "dtu_hlir.div"(%2474, %2494) {node_name = "Div_1641-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2496 = "dtu_hlir.convert"(%2495) {node_name = "Cast_1642-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2497 = "dtu_hlir.convert"(%2496) {node_name = "Cast_1643-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2498 = dtu_hlir.constant  {node_name = "Constant_1644-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2499 = "dtu_hlir.mul"(%2480, %2498) {node_name = "Mul_1645-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2500 = dtu_hlir.constant  {node_name = "Constant_1646-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2501 = "dtu_hlir.unsqueeze"(%2497, %2500) {node_name = "Unsqueeze_1647-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2502 = dtu_hlir.constant  {node_name = "Constant_1648-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2503 = "dtu_hlir.unsqueeze"(%2477, %2502) {node_name = "Unsqueeze_1649-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2504 = dtu_hlir.constant  {node_name = "Constant_1650-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2505 = "dtu_hlir.unsqueeze"(%2499, %2504) {node_name = "Unsqueeze_1651-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2506 = "dtu_hlir.concatenate"(%2501, %2503, %2505) {dimension = 0 : i64, node_name = "Concat_1652-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2507 = "dtu_hlir.dynamic_reshape"(%2493, %2506) {allowzero = 0 : i64, node_name = "Reshape_1653-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %2508 = "dtu_hlir.dot_general"(%2507, %426) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1654-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2509 = "dtu_hlir.add"(%48, %2508) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1655-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2510 = "dtu_hlir.add"(%2509, %2305) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1656-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2511 = dtu_hlir.constant  {node_name = "ReduceMean_1657-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2512 = "dtu_hlir.reshape"(%2511) {node_name = "ReduceMean_1657-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2513 = "dtu_hlir.reduce"(%2510, %2512) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1657-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1657-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2514 = dtu_hlir.constant  {node_name = "ReduceMean_1657-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2515 = "dtu_hlir.unsqueeze"(%2513, %2514) {node_name = "ReduceMean_1657-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2516 = dtu_hlir.constant  {node_name = "ReduceMean_1657-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2517 = "dtu_hlir.broadcast_in_dim"(%2516) {node_name = "ReduceMean_1657-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2518 = "dtu_hlir.div"(%2515, %2517) {node_name = "ReduceMean_1657-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2519 = "dtu_hlir.sub"(%2510, %2518) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_1658-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2520 = dtu_hlir.constant  {node_name = "Constant_1659-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %2521 = "dtu_hlir.broadcast_in_dim"(%2520) {node_name = "Pow_1660-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %2522 = "dtu_hlir.pow"(%2519, %2521) {node_name = "Pow_1660-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2523 = dtu_hlir.constant  {node_name = "ReduceMean_1661-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2524 = "dtu_hlir.reshape"(%2523) {node_name = "ReduceMean_1661-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2525 = "dtu_hlir.reduce"(%2522, %2524) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1661-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1661-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2526 = dtu_hlir.constant  {node_name = "ReduceMean_1661-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2527 = "dtu_hlir.unsqueeze"(%2525, %2526) {node_name = "ReduceMean_1661-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2528 = dtu_hlir.constant  {node_name = "ReduceMean_1661-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2529 = "dtu_hlir.broadcast_in_dim"(%2528) {node_name = "ReduceMean_1661-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2530 = "dtu_hlir.div"(%2527, %2529) {node_name = "ReduceMean_1661-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2531 = dtu_hlir.constant  {node_name = "Constant_1662-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %2532 = "dtu_hlir.broadcast_in_dim"(%2531) {node_name = "Add_1663-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %2533 = "dtu_hlir.add"(%2530, %2532) {node_name = "Add_1663-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2534 = "dtu_hlir.sqrt"(%2533) {node_name = "Sqrt_1664-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2535 = "dtu_hlir.div"(%2519, %2534) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_1665-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2536 = "dtu_hlir.mul"(%2535, %53) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_1666-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2537 = "dtu_hlir.add"(%2536, %54) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1667-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2538 = "dtu_hlir.dot_general"(%2537, %427) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1668-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x5120xf32>) -> tensor<2x1024x5120xf32>
    %2539 = "dtu_hlir.add"(%46, %2538) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1669-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<5120xf32>, tensor<2x1024x5120xf32>) -> tensor<2x1024x5120xf32>
    %2540 = "dtu_hlir.shape"(%2539) {end = 2147483647 : i64, node_name = "Shape_1670-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>) -> tensor<3xi64>
    %2541 = dtu_hlir.constant  {node_name = "Constant_1671-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %2542 = "dtu_hlir.gather"(%2540, %2541) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1672-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2543 = dtu_hlir.constant  {node_name = "Constant_1673-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2544 = dtu_hlir.constant  {node_name = "Constant_1674-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %2545 = "dtu_hlir.add"(%2542, %2544) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_1675-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2546 = dtu_hlir.constant  {node_name = "Constant_1676-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %2547 = "dtu_hlir.div"(%2545, %2546) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_1677-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2548 = dtu_hlir.constant  {node_name = "Constant_1678-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %2549 = "dtu_hlir.mul"(%2547, %2548) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_1679-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2550 = dtu_hlir.constant  {node_name = "Slice_1680-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %2551 = "dtu_hlir.real_dynamic_slice"(%2539, %2543, %2549, %2550, %2541) {node_name = "Slice_1680-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %2552 = dtu_hlir.constant  {node_name = "Constant_1681-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %2553 = "dtu_hlir.mul"(%2547, %2552) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_1682-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2554 = "dtu_hlir.shape"(%2549) {end = 2147483647 : i64, node_name = "Slice_1683-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %2555 = dtu_hlir.constant  {node_name = "Slice_1683-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %2556 = "dtu_hlir.dynamic_broadcast_in_dim"(%2555, %2554) {node_name = "Slice_1683-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2557 = "dtu_hlir.real_dynamic_slice"(%2539, %2549, %2553, %2556, %2541) {node_name = "Slice_1683-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %2558 = dtu_hlir.constant  {node_name = "Constant_1684-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %2559 = "dtu_hlir.broadcast_in_dim"(%2558) {node_name = "Div_1685-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %2560 = "dtu_hlir.div"(%2557, %2559) {node_name = "Div_1685-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %2561 = "dtu_hlir.erf"(%2560) {node_name = "Erf_1686-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %2562 = dtu_hlir.constant  {node_name = "Constant_1687-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %2563 = "dtu_hlir.broadcast_in_dim"(%2562) {node_name = "Add_1688-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %2564 = "dtu_hlir.add"(%2561, %2563) {node_name = "Add_1688-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %2565 = "dtu_hlir.mul"(%2557, %2564) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_1689-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %2566 = dtu_hlir.constant  {node_name = "Constant_1690-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %2567 = "dtu_hlir.broadcast_in_dim"(%2566) {node_name = "Mul_1691-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %2568 = "dtu_hlir.mul"(%2565, %2567) {node_name = "Mul_1691-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %2569 = "dtu_hlir.mul"(%2551, %2568) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_1692-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %2570 = "dtu_hlir.dot_general"(%2569, %428) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1693-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2560x640xf32>) -> tensor<2x1024x640xf32>
    %2571 = "dtu_hlir.add"(%47, %2570) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1694-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2572 = "dtu_hlir.add"(%2571, %2510) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1695-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2573 = "dtu_hlir.dot_general"(%2572, %429) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1696-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2574 = "dtu_hlir.add"(%55, %2573) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1697-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2575 = dtu_hlir.constant  {node_name = "Constant_1698-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2576 = "dtu_hlir.unsqueeze"(%2070, %2575) {node_name = "Unsqueeze_1699-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2577 = dtu_hlir.constant  {node_name = "Constant_1700-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2578 = "dtu_hlir.unsqueeze"(%2073, %2577) {node_name = "Unsqueeze_1701-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2579 = dtu_hlir.constant  {node_name = "Constant_1702-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2580 = "dtu_hlir.unsqueeze"(%2076, %2579) {node_name = "Unsqueeze_1703-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2581 = dtu_hlir.constant  {node_name = "Constant_1704-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2582 = "dtu_hlir.unsqueeze"(%2088, %2581) {node_name = "Unsqueeze_1705-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2583 = "dtu_hlir.concatenate"(%2576, %2578, %2580, %2582) {dimension = 0 : i64, node_name = "Concat_1706-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2584 = "dtu_hlir.dynamic_reshape"(%2574, %2583) {allowzero = 0 : i64, node_name = "Reshape_1707-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x32x32x640xf32>
    %2585 = "dtu_hlir.transpose"(%2584) {node_name = "Transpose_1708-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>) -> tensor<2x640x32x32xf32>
    %2586 = "dtu_hlir.add"(%2585, %2067) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_1709-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2587 = dtu_hlir.constant  {node_name = "Constant_1710-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %2588 = "dtu_hlir.dynamic_reshape"(%2586, %2587) {allowzero = 0 : i64, node_name = "Reshape_1711-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %2589 = dtu_hlir.constant  {node_name = "Constant_1712-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %2590 = dtu_hlir.constant  {node_name = "Constant_1713-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %2591 = "dtu_hlir.instance_norm"(%2588, %2589, %2590) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_1714-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %2592 = "dtu_hlir.shape"(%2586) {end = 2147483647 : i64, node_name = "Shape_1715-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2593 = "dtu_hlir.dynamic_reshape"(%2591, %2592) {allowzero = 0 : i64, node_name = "Reshape_1716-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %2594 = "dtu_hlir.mul"(%2593, %430) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_1717-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2595 = "dtu_hlir.add"(%2594, %431) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_1718-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2596 = "dtu_hlir.sigmoid"(%2595) {node_name = "Sigmoid_1719-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2597 = "dtu_hlir.mul"(%2595, %2596) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_1720-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2598 = "dtu_hlir.conv_bias"(%2597, %76, %77) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1721-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x32x32xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %2599 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_1722-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %2600 = "dtu_hlir.mul"(%915, %2599) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1723-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %2601 = "dtu_hlir.transpose"(%78) {node_name = "Gemm_1724-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<640x1280xf32>) -> tensor<1280x640xf32>
    %2602 = "dtu_hlir.gemm"(%2600, %2601, %79) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_1724-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x640xf32>, tensor<640xf32>) -> tensor<2x640xf32>
    %2603 = dtu_hlir.constant  {node_name = "Constant_1725-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %2604 = "dtu_hlir.unsqueeze"(%2602, %2603) {node_name = "Unsqueeze_1726-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640xf32>, tensor<1xi64>) -> tensor<2x640x1xf32>
    %2605 = dtu_hlir.constant  {node_name = "Constant_1727-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %2606 = "dtu_hlir.unsqueeze"(%2604, %2605) {node_name = "Unsqueeze_1728-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640x1xf32>, tensor<1xi64>) -> tensor<2x640x1x1xf32>
    %2607 = "dtu_hlir.add"(%2598, %2606) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_1729-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2608 = dtu_hlir.constant  {node_name = "Constant_1730-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %2609 = "dtu_hlir.dynamic_reshape"(%2607, %2608) {allowzero = 0 : i64, node_name = "Reshape_1731-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %2610 = dtu_hlir.constant  {node_name = "Constant_1732-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %2611 = dtu_hlir.constant  {node_name = "Constant_1733-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %2612 = "dtu_hlir.instance_norm"(%2609, %2610, %2611) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_1734-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %2613 = "dtu_hlir.shape"(%2607) {end = 2147483647 : i64, node_name = "Shape_1735-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2614 = "dtu_hlir.dynamic_reshape"(%2612, %2613) {allowzero = 0 : i64, node_name = "Reshape_1736-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %2615 = "dtu_hlir.mul"(%2614, %432) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_1737-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2616 = "dtu_hlir.add"(%2615, %433) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_1738-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2617 = "dtu_hlir.sigmoid"(%2616) {node_name = "Sigmoid_1739-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2618 = "dtu_hlir.mul"(%2616, %2617) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_1740-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2619 = "dtu_hlir.conv_bias"(%2618, %80, %81) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1741-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x32x32xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %2620 = "dtu_hlir.add"(%2586, %2619) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_1742-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2621 = dtu_hlir.constant  {node_name = "Constant_1743-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %2622 = "dtu_hlir.broadcast_in_dim"(%2621) {node_name = "Div_1744-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x640x32x32xf32>
    %2623 = "dtu_hlir.div"(%2620, %2622) {node_name = "Div_1744-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %2624 = "dtu_hlir.shape"(%2623) {end = 2147483647 : i64, node_name = "Shape_1745-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2625 = dtu_hlir.constant  {node_name = "Constant_1746-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2626 = "dtu_hlir.gather"(%2624, %2625) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1747-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2627 = "dtu_hlir.shape"(%2623) {end = 2147483647 : i64, node_name = "Shape_1748-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2628 = dtu_hlir.constant  {node_name = "Constant_1749-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2629 = "dtu_hlir.gather"(%2627, %2628) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1750-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2630 = "dtu_hlir.shape"(%2623) {end = 2147483647 : i64, node_name = "Shape_1751-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2631 = dtu_hlir.constant  {node_name = "Constant_1752-0", node_type = "Constant"} dense<3> : tensor<i64>
    %2632 = "dtu_hlir.gather"(%2630, %2631) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1753-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2633 = dtu_hlir.constant  {node_name = "Constant_1754-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %2634 = "dtu_hlir.dynamic_reshape"(%2623, %2633) {allowzero = 0 : i64, node_name = "Reshape_1755-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %2635 = dtu_hlir.constant  {node_name = "Constant_1756-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %2636 = dtu_hlir.constant  {node_name = "Constant_1757-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %2637 = "dtu_hlir.instance_norm"(%2634, %2635, %2636) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_1758-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %2638 = "dtu_hlir.shape"(%2623) {end = 2147483647 : i64, node_name = "Shape_1759-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2639 = "dtu_hlir.dynamic_reshape"(%2637, %2638) {allowzero = 0 : i64, node_name = "Reshape_1760-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %2640 = "dtu_hlir.mul"(%2639, %434) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_1761-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2641 = "dtu_hlir.add"(%2640, %435) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_1762-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %2642 = "dtu_hlir.shape"(%2641) {end = 2147483647 : i64, node_name = "Shape_1763-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %2643 = dtu_hlir.constant  {node_name = "Constant_1764-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2644 = "dtu_hlir.gather"(%2642, %2643) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1765-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %2645 = "dtu_hlir.transpose"(%2641) {node_name = "Transpose_1766-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x32x32x640xf32>
    %2646 = "dtu_hlir.mul"(%2629, %2632) {node_name = "Mul_1767-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2647 = dtu_hlir.constant  {node_name = "Constant_1768-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2648 = "dtu_hlir.unsqueeze"(%2626, %2647) {node_name = "Unsqueeze_1769-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2649 = dtu_hlir.constant  {node_name = "Constant_1770-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2650 = "dtu_hlir.unsqueeze"(%2646, %2649) {node_name = "Unsqueeze_1771-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2651 = dtu_hlir.constant  {node_name = "Constant_1772-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2652 = "dtu_hlir.unsqueeze"(%2644, %2651) {node_name = "Unsqueeze_1773-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2653 = "dtu_hlir.concatenate"(%2648, %2650, %2652) {dimension = 0 : i64, node_name = "Concat_1774-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2654 = "dtu_hlir.dynamic_reshape"(%2645, %2653) {allowzero = 0 : i64, node_name = "Reshape_1775-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %2655 = "dtu_hlir.dot_general"(%2654, %436) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1776-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2656 = "dtu_hlir.add"(%56, %2655) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1777-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2657 = dtu_hlir.constant  {node_name = "ReduceMean_1778-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2658 = "dtu_hlir.reshape"(%2657) {node_name = "ReduceMean_1778-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2659 = "dtu_hlir.reduce"(%2656, %2658) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1778-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1778-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2660 = dtu_hlir.constant  {node_name = "ReduceMean_1778-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2661 = "dtu_hlir.unsqueeze"(%2659, %2660) {node_name = "ReduceMean_1778-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2662 = dtu_hlir.constant  {node_name = "ReduceMean_1778-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2663 = "dtu_hlir.broadcast_in_dim"(%2662) {node_name = "ReduceMean_1778-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2664 = "dtu_hlir.div"(%2661, %2663) {node_name = "ReduceMean_1778-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2665 = "dtu_hlir.sub"(%2656, %2664) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_1779-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2666 = dtu_hlir.constant  {node_name = "Constant_1780-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %2667 = "dtu_hlir.broadcast_in_dim"(%2666) {node_name = "Pow_1781-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %2668 = "dtu_hlir.pow"(%2665, %2667) {node_name = "Pow_1781-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2669 = dtu_hlir.constant  {node_name = "ReduceMean_1782-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2670 = "dtu_hlir.reshape"(%2669) {node_name = "ReduceMean_1782-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2671 = "dtu_hlir.reduce"(%2668, %2670) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1782-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1782-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2672 = dtu_hlir.constant  {node_name = "ReduceMean_1782-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2673 = "dtu_hlir.unsqueeze"(%2671, %2672) {node_name = "ReduceMean_1782-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2674 = dtu_hlir.constant  {node_name = "ReduceMean_1782-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2675 = "dtu_hlir.broadcast_in_dim"(%2674) {node_name = "ReduceMean_1782-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2676 = "dtu_hlir.div"(%2673, %2675) {node_name = "ReduceMean_1782-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2677 = dtu_hlir.constant  {node_name = "Constant_1783-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %2678 = "dtu_hlir.broadcast_in_dim"(%2677) {node_name = "Add_1784-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %2679 = "dtu_hlir.add"(%2676, %2678) {node_name = "Add_1784-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2680 = "dtu_hlir.sqrt"(%2679) {node_name = "Sqrt_1785-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2681 = "dtu_hlir.div"(%2665, %2680) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_1786-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2682 = "dtu_hlir.mul"(%2681, %61) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_1787-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2683 = "dtu_hlir.add"(%2682, %62) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1788-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2684 = "dtu_hlir.dot_general"(%2683, %437) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1789-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2685 = "dtu_hlir.dot_general"(%2683, %438) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1790-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2686 = "dtu_hlir.dot_general"(%2683, %439) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1791-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2687 = "dtu_hlir.shape"(%2684) {end = 2147483647 : i64, node_name = "Shape_1792-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2688 = dtu_hlir.constant  {node_name = "Constant_1793-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2689 = "dtu_hlir.gather"(%2687, %2688) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1794-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2690 = "dtu_hlir.shape"(%2684) {end = 2147483647 : i64, node_name = "Shape_1795-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2691 = dtu_hlir.constant  {node_name = "Constant_1796-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2692 = "dtu_hlir.gather"(%2690, %2691) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1797-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2693 = "dtu_hlir.shape"(%2684) {end = 2147483647 : i64, node_name = "Shape_1798-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2694 = dtu_hlir.constant  {node_name = "Constant_1799-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2695 = "dtu_hlir.gather"(%2693, %2694) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1800-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2696 = dtu_hlir.constant  {node_name = "Constant_1801-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2697 = "dtu_hlir.div"(%2695, %2696) {node_name = "Div_1802-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2698 = "dtu_hlir.convert"(%2697) {node_name = "Cast_1803-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2699 = "dtu_hlir.convert"(%2698) {node_name = "Cast_1804-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2700 = dtu_hlir.constant  {node_name = "Constant_1805-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2701 = "dtu_hlir.unsqueeze"(%2689, %2700) {node_name = "Unsqueeze_1806-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2702 = dtu_hlir.constant  {node_name = "Constant_1807-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2703 = "dtu_hlir.unsqueeze"(%2692, %2702) {node_name = "Unsqueeze_1808-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2704 = dtu_hlir.constant  {node_name = "Constant_1809-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2705 = "dtu_hlir.unsqueeze"(%2699, %2704) {node_name = "Unsqueeze_1810-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2706 = "dtu_hlir.concatenate"(%2701, %2703, %848, %2705) {dimension = 0 : i64, node_name = "Concat_1811-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2707 = "dtu_hlir.dynamic_reshape"(%2684, %2706) {allowzero = 0 : i64, node_name = "Reshape_1812-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2708 = "dtu_hlir.transpose"(%2707) {node_name = "Transpose_1813-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2709 = dtu_hlir.constant  {node_name = "Constant_1814-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2710 = "dtu_hlir.mul"(%2689, %2709) {node_name = "Mul_1815-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2711 = dtu_hlir.constant  {node_name = "Constant_1816-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2712 = "dtu_hlir.div"(%2695, %2711) {node_name = "Div_1817-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2713 = "dtu_hlir.convert"(%2712) {node_name = "Cast_1818-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2714 = "dtu_hlir.convert"(%2713) {node_name = "Cast_1819-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2715 = dtu_hlir.constant  {node_name = "Constant_1820-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2716 = "dtu_hlir.unsqueeze"(%2710, %2715) {node_name = "Unsqueeze_1821-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2717 = dtu_hlir.constant  {node_name = "Constant_1822-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2718 = "dtu_hlir.unsqueeze"(%2692, %2717) {node_name = "Unsqueeze_1823-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2719 = dtu_hlir.constant  {node_name = "Constant_1824-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2720 = "dtu_hlir.unsqueeze"(%2714, %2719) {node_name = "Unsqueeze_1825-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2721 = "dtu_hlir.concatenate"(%2716, %2718, %2720) {dimension = 0 : i64, node_name = "Concat_1826-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2722 = "dtu_hlir.dynamic_reshape"(%2708, %2721) {allowzero = 0 : i64, node_name = "Reshape_1827-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2723 = "dtu_hlir.shape"(%2685) {end = 2147483647 : i64, node_name = "Shape_1828-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2724 = dtu_hlir.constant  {node_name = "Constant_1829-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2725 = "dtu_hlir.gather"(%2723, %2724) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1830-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2726 = "dtu_hlir.shape"(%2685) {end = 2147483647 : i64, node_name = "Shape_1831-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2727 = dtu_hlir.constant  {node_name = "Constant_1832-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2728 = "dtu_hlir.gather"(%2726, %2727) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1833-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2729 = "dtu_hlir.shape"(%2685) {end = 2147483647 : i64, node_name = "Shape_1834-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2730 = dtu_hlir.constant  {node_name = "Constant_1835-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2731 = "dtu_hlir.gather"(%2729, %2730) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1836-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2732 = dtu_hlir.constant  {node_name = "Constant_1837-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2733 = "dtu_hlir.div"(%2731, %2732) {node_name = "Div_1838-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2734 = "dtu_hlir.convert"(%2733) {node_name = "Cast_1839-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2735 = "dtu_hlir.convert"(%2734) {node_name = "Cast_1840-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2736 = dtu_hlir.constant  {node_name = "Constant_1841-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2737 = "dtu_hlir.unsqueeze"(%2725, %2736) {node_name = "Unsqueeze_1842-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2738 = dtu_hlir.constant  {node_name = "Constant_1843-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2739 = "dtu_hlir.unsqueeze"(%2728, %2738) {node_name = "Unsqueeze_1844-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2740 = dtu_hlir.constant  {node_name = "Constant_1845-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2741 = "dtu_hlir.unsqueeze"(%2735, %2740) {node_name = "Unsqueeze_1846-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2742 = "dtu_hlir.concatenate"(%2737, %2739, %847, %2741) {dimension = 0 : i64, node_name = "Concat_1847-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2743 = "dtu_hlir.dynamic_reshape"(%2685, %2742) {allowzero = 0 : i64, node_name = "Reshape_1848-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2744 = "dtu_hlir.transpose"(%2743) {node_name = "Transpose_1849-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2745 = dtu_hlir.constant  {node_name = "Constant_1850-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2746 = "dtu_hlir.mul"(%2725, %2745) {node_name = "Mul_1851-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2747 = dtu_hlir.constant  {node_name = "Constant_1852-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2748 = "dtu_hlir.div"(%2731, %2747) {node_name = "Div_1853-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2749 = "dtu_hlir.convert"(%2748) {node_name = "Cast_1854-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2750 = "dtu_hlir.convert"(%2749) {node_name = "Cast_1855-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2751 = dtu_hlir.constant  {node_name = "Constant_1856-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2752 = "dtu_hlir.unsqueeze"(%2746, %2751) {node_name = "Unsqueeze_1857-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2753 = dtu_hlir.constant  {node_name = "Constant_1858-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2754 = "dtu_hlir.unsqueeze"(%2728, %2753) {node_name = "Unsqueeze_1859-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2755 = dtu_hlir.constant  {node_name = "Constant_1860-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2756 = "dtu_hlir.unsqueeze"(%2750, %2755) {node_name = "Unsqueeze_1861-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2757 = "dtu_hlir.concatenate"(%2752, %2754, %2756) {dimension = 0 : i64, node_name = "Concat_1862-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2758 = "dtu_hlir.dynamic_reshape"(%2744, %2757) {allowzero = 0 : i64, node_name = "Reshape_1863-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2759 = "dtu_hlir.shape"(%2686) {end = 2147483647 : i64, node_name = "Shape_1864-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2760 = dtu_hlir.constant  {node_name = "Constant_1865-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2761 = "dtu_hlir.gather"(%2759, %2760) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1866-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2762 = "dtu_hlir.shape"(%2686) {end = 2147483647 : i64, node_name = "Shape_1867-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2763 = dtu_hlir.constant  {node_name = "Constant_1868-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2764 = "dtu_hlir.gather"(%2762, %2763) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1869-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2765 = "dtu_hlir.shape"(%2686) {end = 2147483647 : i64, node_name = "Shape_1870-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2766 = dtu_hlir.constant  {node_name = "Constant_1871-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2767 = "dtu_hlir.gather"(%2765, %2766) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1872-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2768 = dtu_hlir.constant  {node_name = "Constant_1873-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2769 = "dtu_hlir.div"(%2767, %2768) {node_name = "Div_1874-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2770 = "dtu_hlir.convert"(%2769) {node_name = "Cast_1875-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2771 = "dtu_hlir.convert"(%2770) {node_name = "Cast_1876-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2772 = dtu_hlir.constant  {node_name = "Constant_1877-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2773 = "dtu_hlir.unsqueeze"(%2761, %2772) {node_name = "Unsqueeze_1878-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2774 = dtu_hlir.constant  {node_name = "Constant_1879-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2775 = "dtu_hlir.unsqueeze"(%2764, %2774) {node_name = "Unsqueeze_1880-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2776 = dtu_hlir.constant  {node_name = "Constant_1881-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2777 = "dtu_hlir.unsqueeze"(%2771, %2776) {node_name = "Unsqueeze_1882-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2778 = "dtu_hlir.concatenate"(%2773, %2775, %846, %2777) {dimension = 0 : i64, node_name = "Concat_1883-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2779 = "dtu_hlir.dynamic_reshape"(%2686, %2778) {allowzero = 0 : i64, node_name = "Reshape_1884-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2780 = "dtu_hlir.transpose"(%2779) {node_name = "Transpose_1885-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2781 = dtu_hlir.constant  {node_name = "Constant_1886-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2782 = "dtu_hlir.mul"(%2761, %2781) {node_name = "Mul_1887-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2783 = dtu_hlir.constant  {node_name = "Constant_1888-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2784 = "dtu_hlir.div"(%2767, %2783) {node_name = "Div_1889-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2785 = "dtu_hlir.convert"(%2784) {node_name = "Cast_1890-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2786 = "dtu_hlir.convert"(%2785) {node_name = "Cast_1891-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2787 = dtu_hlir.constant  {node_name = "Constant_1892-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2788 = "dtu_hlir.unsqueeze"(%2782, %2787) {node_name = "Unsqueeze_1893-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2789 = dtu_hlir.constant  {node_name = "Constant_1894-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2790 = "dtu_hlir.unsqueeze"(%2764, %2789) {node_name = "Unsqueeze_1895-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2791 = dtu_hlir.constant  {node_name = "Constant_1896-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2792 = "dtu_hlir.unsqueeze"(%2786, %2791) {node_name = "Unsqueeze_1897-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2793 = "dtu_hlir.concatenate"(%2788, %2790, %2792) {dimension = 0 : i64, node_name = "Concat_1898-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2794 = "dtu_hlir.dynamic_reshape"(%2780, %2793) {allowzero = 0 : i64, node_name = "Reshape_1899-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2795 = "dtu_hlir.shape"(%2722) {end = 2147483647 : i64, node_name = "Shape_1900-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2796 = dtu_hlir.constant  {node_name = "Constant_1901-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2797 = "dtu_hlir.gather"(%2795, %2796) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1902-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2798 = "dtu_hlir.shape"(%2722) {end = 2147483647 : i64, node_name = "Shape_1903-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2799 = dtu_hlir.constant  {node_name = "Constant_1904-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2800 = "dtu_hlir.gather"(%2798, %2799) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1905-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2801 = "dtu_hlir.shape"(%2758) {end = 2147483647 : i64, node_name = "Shape_1906-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2802 = dtu_hlir.constant  {node_name = "Constant_1907-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2803 = "dtu_hlir.gather"(%2801, %2802) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1908-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2804 = dtu_hlir.constant  {node_name = "Constant_1909-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2805 = "dtu_hlir.unsqueeze"(%2797, %2804) {node_name = "Unsqueeze_1910-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2806 = dtu_hlir.constant  {node_name = "Constant_1911-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2807 = "dtu_hlir.unsqueeze"(%2800, %2806) {node_name = "Unsqueeze_1912-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2808 = dtu_hlir.constant  {node_name = "Constant_1913-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2809 = "dtu_hlir.unsqueeze"(%2803, %2808) {node_name = "Unsqueeze_1914-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2810 = "dtu_hlir.concatenate"(%2805, %2807, %2809) {dimension = 0 : i64, node_name = "Concat_1915-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2811 = dtu_hlir.constant  {node_name = "ConstantOfShape_1916-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %2812 = "dtu_hlir.dynamic_broadcast_in_dim"(%2811, %2810) {node_name = "ConstantOfShape_1916-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x1024xf32>
    %2813 = "dtu_hlir.transpose"(%2758) {node_name = "Transpose_1917-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<20x64x1024xf32>
    %2814 = "dtu_hlir.dot_general"(%2722, %2813) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1918-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x1024xf32>) -> tensor<20x1024x1024xf32>
    %2815 = "dtu_hlir.broadcast_in_dim"(%845) {node_name = "Mul_1919-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %2816 = "dtu_hlir.mul"(%2814, %2815) {node_name = "Mul_1919-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2817 = "dtu_hlir.broadcast_in_dim"(%844) {node_name = "Mul_1920-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %2818 = "dtu_hlir.mul"(%2812, %2817) {node_name = "Mul_1920-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2819 = "dtu_hlir.add"(%2816, %2818) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1921-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2820 = "dtu_hlir.softmax"(%2819) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1922-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2821 = "dtu_hlir.convert"(%2820) {node_name = "Cast_1923-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %2822 = "dtu_hlir.dot_general"(%2821, %2794) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1924-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x64xf32>) -> tensor<20x1024x64xf32>
    %2823 = "dtu_hlir.shape"(%2822) {end = 2147483647 : i64, node_name = "Shape_1925-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2824 = dtu_hlir.constant  {node_name = "Constant_1926-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2825 = "dtu_hlir.gather"(%2823, %2824) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1927-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2826 = "dtu_hlir.shape"(%2822) {end = 2147483647 : i64, node_name = "Shape_1928-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2827 = dtu_hlir.constant  {node_name = "Constant_1929-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2828 = "dtu_hlir.gather"(%2826, %2827) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1930-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2829 = "dtu_hlir.shape"(%2822) {end = 2147483647 : i64, node_name = "Shape_1931-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %2830 = dtu_hlir.constant  {node_name = "Constant_1932-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2831 = "dtu_hlir.gather"(%2829, %2830) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1933-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2832 = dtu_hlir.constant  {node_name = "Constant_1934-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2833 = "dtu_hlir.div"(%2825, %2832) {node_name = "Div_1935-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2834 = "dtu_hlir.convert"(%2833) {node_name = "Cast_1936-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2835 = "dtu_hlir.convert"(%2834) {node_name = "Cast_1937-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2836 = dtu_hlir.constant  {node_name = "Constant_1938-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2837 = "dtu_hlir.unsqueeze"(%2835, %2836) {node_name = "Unsqueeze_1939-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2838 = dtu_hlir.constant  {node_name = "Constant_1940-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2839 = "dtu_hlir.unsqueeze"(%2828, %2838) {node_name = "Unsqueeze_1941-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2840 = dtu_hlir.constant  {node_name = "Constant_1942-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2841 = "dtu_hlir.unsqueeze"(%2831, %2840) {node_name = "Unsqueeze_1943-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2842 = "dtu_hlir.concatenate"(%2837, %843, %2839, %2841) {dimension = 0 : i64, node_name = "Concat_1944-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2843 = "dtu_hlir.dynamic_reshape"(%2822, %2842) {allowzero = 0 : i64, node_name = "Reshape_1945-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %2844 = "dtu_hlir.transpose"(%2843) {node_name = "Transpose_1946-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %2845 = dtu_hlir.constant  {node_name = "Constant_1947-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2846 = "dtu_hlir.div"(%2825, %2845) {node_name = "Div_1948-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2847 = "dtu_hlir.convert"(%2846) {node_name = "Cast_1949-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2848 = "dtu_hlir.convert"(%2847) {node_name = "Cast_1950-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2849 = dtu_hlir.constant  {node_name = "Constant_1951-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2850 = "dtu_hlir.mul"(%2831, %2849) {node_name = "Mul_1952-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2851 = dtu_hlir.constant  {node_name = "Constant_1953-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2852 = "dtu_hlir.unsqueeze"(%2848, %2851) {node_name = "Unsqueeze_1954-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2853 = dtu_hlir.constant  {node_name = "Constant_1955-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2854 = "dtu_hlir.unsqueeze"(%2828, %2853) {node_name = "Unsqueeze_1956-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2855 = dtu_hlir.constant  {node_name = "Constant_1957-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2856 = "dtu_hlir.unsqueeze"(%2850, %2855) {node_name = "Unsqueeze_1958-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2857 = "dtu_hlir.concatenate"(%2852, %2854, %2856) {dimension = 0 : i64, node_name = "Concat_1959-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2858 = "dtu_hlir.dynamic_reshape"(%2844, %2857) {allowzero = 0 : i64, node_name = "Reshape_1960-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %2859 = "dtu_hlir.dot_general"(%2858, %440) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1961-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2860 = "dtu_hlir.add"(%57, %2859) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1962-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2861 = "dtu_hlir.add"(%2860, %2656) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_1963-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2862 = dtu_hlir.constant  {node_name = "ReduceMean_1964-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2863 = "dtu_hlir.reshape"(%2862) {node_name = "ReduceMean_1964-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2864 = "dtu_hlir.reduce"(%2861, %2863) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1964-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1964-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2865 = dtu_hlir.constant  {node_name = "ReduceMean_1964-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2866 = "dtu_hlir.unsqueeze"(%2864, %2865) {node_name = "ReduceMean_1964-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2867 = dtu_hlir.constant  {node_name = "ReduceMean_1964-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2868 = "dtu_hlir.broadcast_in_dim"(%2867) {node_name = "ReduceMean_1964-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2869 = "dtu_hlir.div"(%2866, %2868) {node_name = "ReduceMean_1964-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2870 = "dtu_hlir.sub"(%2861, %2869) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_1965-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2871 = dtu_hlir.constant  {node_name = "Constant_1966-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %2872 = "dtu_hlir.broadcast_in_dim"(%2871) {node_name = "Pow_1967-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %2873 = "dtu_hlir.pow"(%2870, %2872) {node_name = "Pow_1967-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %2874 = dtu_hlir.constant  {node_name = "ReduceMean_1968-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %2875 = "dtu_hlir.reshape"(%2874) {node_name = "ReduceMean_1968-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %2876 = "dtu_hlir.reduce"(%2873, %2875) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_1968-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_1968-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %2877 = dtu_hlir.constant  {node_name = "ReduceMean_1968-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %2878 = "dtu_hlir.unsqueeze"(%2876, %2877) {node_name = "ReduceMean_1968-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %2879 = dtu_hlir.constant  {node_name = "ReduceMean_1968-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %2880 = "dtu_hlir.broadcast_in_dim"(%2879) {node_name = "ReduceMean_1968-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %2881 = "dtu_hlir.div"(%2878, %2880) {node_name = "ReduceMean_1968-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2882 = dtu_hlir.constant  {node_name = "Constant_1969-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %2883 = "dtu_hlir.broadcast_in_dim"(%2882) {node_name = "Add_1970-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %2884 = "dtu_hlir.add"(%2881, %2883) {node_name = "Add_1970-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2885 = "dtu_hlir.sqrt"(%2884) {node_name = "Sqrt_1971-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %2886 = "dtu_hlir.div"(%2870, %2885) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_1972-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %2887 = "dtu_hlir.mul"(%2886, %63) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_1973-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2888 = "dtu_hlir.add"(%2887, %64) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_1974-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %2889 = "dtu_hlir.dot_general"(%2888, %441) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1975-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %2890 = "dtu_hlir.dot_general"(%arg2, %442) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1976-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %2891 = "dtu_hlir.dot_general"(%arg2, %443) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_1977-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %2892 = "dtu_hlir.shape"(%2889) {end = 2147483647 : i64, node_name = "Shape_1978-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2893 = dtu_hlir.constant  {node_name = "Constant_1979-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2894 = "dtu_hlir.gather"(%2892, %2893) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1980-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2895 = "dtu_hlir.shape"(%2889) {end = 2147483647 : i64, node_name = "Shape_1981-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2896 = dtu_hlir.constant  {node_name = "Constant_1982-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2897 = "dtu_hlir.gather"(%2895, %2896) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1983-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2898 = "dtu_hlir.shape"(%2889) {end = 2147483647 : i64, node_name = "Shape_1984-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %2899 = dtu_hlir.constant  {node_name = "Constant_1985-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2900 = "dtu_hlir.gather"(%2898, %2899) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_1986-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2901 = dtu_hlir.constant  {node_name = "Constant_1987-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2902 = "dtu_hlir.div"(%2900, %2901) {node_name = "Div_1988-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2903 = "dtu_hlir.convert"(%2902) {node_name = "Cast_1989-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2904 = "dtu_hlir.convert"(%2903) {node_name = "Cast_1990-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2905 = dtu_hlir.constant  {node_name = "Constant_1991-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2906 = "dtu_hlir.unsqueeze"(%2894, %2905) {node_name = "Unsqueeze_1992-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2907 = dtu_hlir.constant  {node_name = "Constant_1993-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2908 = "dtu_hlir.unsqueeze"(%2897, %2907) {node_name = "Unsqueeze_1994-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2909 = dtu_hlir.constant  {node_name = "Constant_1995-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2910 = "dtu_hlir.unsqueeze"(%2904, %2909) {node_name = "Unsqueeze_1996-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2911 = "dtu_hlir.concatenate"(%2906, %2908, %842, %2910) {dimension = 0 : i64, node_name = "Concat_1997-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2912 = "dtu_hlir.dynamic_reshape"(%2889, %2911) {allowzero = 0 : i64, node_name = "Reshape_1998-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %2913 = "dtu_hlir.transpose"(%2912) {node_name = "Transpose_1999-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %2914 = dtu_hlir.constant  {node_name = "Constant_2000-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2915 = "dtu_hlir.mul"(%2894, %2914) {node_name = "Mul_2001-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2916 = dtu_hlir.constant  {node_name = "Constant_2002-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2917 = "dtu_hlir.div"(%2900, %2916) {node_name = "Div_2003-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2918 = "dtu_hlir.convert"(%2917) {node_name = "Cast_2004-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2919 = "dtu_hlir.convert"(%2918) {node_name = "Cast_2005-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2920 = dtu_hlir.constant  {node_name = "Constant_2006-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2921 = "dtu_hlir.unsqueeze"(%2915, %2920) {node_name = "Unsqueeze_2007-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2922 = dtu_hlir.constant  {node_name = "Constant_2008-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2923 = "dtu_hlir.unsqueeze"(%2897, %2922) {node_name = "Unsqueeze_2009-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2924 = dtu_hlir.constant  {node_name = "Constant_2010-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2925 = "dtu_hlir.unsqueeze"(%2919, %2924) {node_name = "Unsqueeze_2011-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2926 = "dtu_hlir.concatenate"(%2921, %2923, %2925) {dimension = 0 : i64, node_name = "Concat_2012-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2927 = "dtu_hlir.dynamic_reshape"(%2913, %2926) {allowzero = 0 : i64, node_name = "Reshape_2013-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %2928 = "dtu_hlir.shape"(%2890) {end = 2147483647 : i64, node_name = "Shape_2014-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2929 = dtu_hlir.constant  {node_name = "Constant_2015-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2930 = "dtu_hlir.gather"(%2928, %2929) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2016-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2931 = "dtu_hlir.shape"(%2890) {end = 2147483647 : i64, node_name = "Shape_2017-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2932 = dtu_hlir.constant  {node_name = "Constant_2018-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2933 = "dtu_hlir.gather"(%2931, %2932) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2019-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2934 = "dtu_hlir.shape"(%2890) {end = 2147483647 : i64, node_name = "Shape_2020-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2935 = dtu_hlir.constant  {node_name = "Constant_2021-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2936 = "dtu_hlir.gather"(%2934, %2935) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2022-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2937 = dtu_hlir.constant  {node_name = "Constant_2023-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2938 = "dtu_hlir.div"(%2936, %2937) {node_name = "Div_2024-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2939 = "dtu_hlir.convert"(%2938) {node_name = "Cast_2025-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2940 = "dtu_hlir.convert"(%2939) {node_name = "Cast_2026-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2941 = dtu_hlir.constant  {node_name = "Constant_2027-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2942 = "dtu_hlir.unsqueeze"(%2930, %2941) {node_name = "Unsqueeze_2028-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2943 = dtu_hlir.constant  {node_name = "Constant_2029-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2944 = "dtu_hlir.unsqueeze"(%2933, %2943) {node_name = "Unsqueeze_2030-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2945 = dtu_hlir.constant  {node_name = "Constant_2031-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2946 = "dtu_hlir.unsqueeze"(%2940, %2945) {node_name = "Unsqueeze_2032-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2947 = "dtu_hlir.concatenate"(%2942, %2944, %841, %2946) {dimension = 0 : i64, node_name = "Concat_2033-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2948 = "dtu_hlir.dynamic_reshape"(%2890, %2947) {allowzero = 0 : i64, node_name = "Reshape_2034-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %2949 = "dtu_hlir.transpose"(%2948) {node_name = "Transpose_2035-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %2950 = dtu_hlir.constant  {node_name = "Constant_2036-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2951 = "dtu_hlir.mul"(%2930, %2950) {node_name = "Mul_2037-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2952 = dtu_hlir.constant  {node_name = "Constant_2038-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2953 = "dtu_hlir.div"(%2936, %2952) {node_name = "Div_2039-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2954 = "dtu_hlir.convert"(%2953) {node_name = "Cast_2040-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2955 = "dtu_hlir.convert"(%2954) {node_name = "Cast_2041-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2956 = dtu_hlir.constant  {node_name = "Constant_2042-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2957 = "dtu_hlir.unsqueeze"(%2951, %2956) {node_name = "Unsqueeze_2043-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2958 = dtu_hlir.constant  {node_name = "Constant_2044-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2959 = "dtu_hlir.unsqueeze"(%2933, %2958) {node_name = "Unsqueeze_2045-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2960 = dtu_hlir.constant  {node_name = "Constant_2046-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2961 = "dtu_hlir.unsqueeze"(%2955, %2960) {node_name = "Unsqueeze_2047-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2962 = "dtu_hlir.concatenate"(%2957, %2959, %2961) {dimension = 0 : i64, node_name = "Concat_2048-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2963 = "dtu_hlir.dynamic_reshape"(%2949, %2962) {allowzero = 0 : i64, node_name = "Reshape_2049-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %2964 = "dtu_hlir.shape"(%2891) {end = 2147483647 : i64, node_name = "Shape_2050-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2965 = dtu_hlir.constant  {node_name = "Constant_2051-0", node_type = "Constant"} dense<0> : tensor<i64>
    %2966 = "dtu_hlir.gather"(%2964, %2965) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2052-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2967 = "dtu_hlir.shape"(%2891) {end = 2147483647 : i64, node_name = "Shape_2053-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2968 = dtu_hlir.constant  {node_name = "Constant_2054-0", node_type = "Constant"} dense<1> : tensor<i64>
    %2969 = "dtu_hlir.gather"(%2967, %2968) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2055-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2970 = "dtu_hlir.shape"(%2891) {end = 2147483647 : i64, node_name = "Shape_2056-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %2971 = dtu_hlir.constant  {node_name = "Constant_2057-0", node_type = "Constant"} dense<2> : tensor<i64>
    %2972 = "dtu_hlir.gather"(%2970, %2971) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2058-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %2973 = dtu_hlir.constant  {node_name = "Constant_2059-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2974 = "dtu_hlir.div"(%2972, %2973) {node_name = "Div_2060-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2975 = "dtu_hlir.convert"(%2974) {node_name = "Cast_2061-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2976 = "dtu_hlir.convert"(%2975) {node_name = "Cast_2062-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2977 = dtu_hlir.constant  {node_name = "Constant_2063-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2978 = "dtu_hlir.unsqueeze"(%2966, %2977) {node_name = "Unsqueeze_2064-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2979 = dtu_hlir.constant  {node_name = "Constant_2065-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2980 = "dtu_hlir.unsqueeze"(%2969, %2979) {node_name = "Unsqueeze_2066-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2981 = dtu_hlir.constant  {node_name = "Constant_2067-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2982 = "dtu_hlir.unsqueeze"(%2976, %2981) {node_name = "Unsqueeze_2068-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2983 = "dtu_hlir.concatenate"(%2978, %2980, %840, %2982) {dimension = 0 : i64, node_name = "Concat_2069-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %2984 = "dtu_hlir.dynamic_reshape"(%2891, %2983) {allowzero = 0 : i64, node_name = "Reshape_2070-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %2985 = "dtu_hlir.transpose"(%2984) {node_name = "Transpose_2071-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %2986 = dtu_hlir.constant  {node_name = "Constant_2072-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2987 = "dtu_hlir.mul"(%2966, %2986) {node_name = "Mul_2073-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2988 = dtu_hlir.constant  {node_name = "Constant_2074-0", node_type = "Constant"} dense<10> : tensor<i64>
    %2989 = "dtu_hlir.div"(%2972, %2988) {node_name = "Div_2075-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %2990 = "dtu_hlir.convert"(%2989) {node_name = "Cast_2076-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2991 = "dtu_hlir.convert"(%2990) {node_name = "Cast_2077-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %2992 = dtu_hlir.constant  {node_name = "Constant_2078-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2993 = "dtu_hlir.unsqueeze"(%2987, %2992) {node_name = "Unsqueeze_2079-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2994 = dtu_hlir.constant  {node_name = "Constant_2080-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2995 = "dtu_hlir.unsqueeze"(%2969, %2994) {node_name = "Unsqueeze_2081-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2996 = dtu_hlir.constant  {node_name = "Constant_2082-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %2997 = "dtu_hlir.unsqueeze"(%2991, %2996) {node_name = "Unsqueeze_2083-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %2998 = "dtu_hlir.concatenate"(%2993, %2995, %2997) {dimension = 0 : i64, node_name = "Concat_2084-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %2999 = "dtu_hlir.dynamic_reshape"(%2985, %2998) {allowzero = 0 : i64, node_name = "Reshape_2085-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %3000 = "dtu_hlir.shape"(%2927) {end = 2147483647 : i64, node_name = "Shape_2086-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %3001 = dtu_hlir.constant  {node_name = "Constant_2087-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3002 = "dtu_hlir.gather"(%3000, %3001) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2088-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3003 = "dtu_hlir.shape"(%2927) {end = 2147483647 : i64, node_name = "Shape_2089-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %3004 = dtu_hlir.constant  {node_name = "Constant_2090-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3005 = "dtu_hlir.gather"(%3003, %3004) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2091-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3006 = "dtu_hlir.shape"(%2963) {end = 2147483647 : i64, node_name = "Shape_2092-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<3xi64>
    %3007 = dtu_hlir.constant  {node_name = "Constant_2093-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3008 = "dtu_hlir.gather"(%3006, %3007) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2094-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3009 = dtu_hlir.constant  {node_name = "Constant_2095-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3010 = "dtu_hlir.unsqueeze"(%3002, %3009) {node_name = "Unsqueeze_2096-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3011 = dtu_hlir.constant  {node_name = "Constant_2097-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3012 = "dtu_hlir.unsqueeze"(%3005, %3011) {node_name = "Unsqueeze_2098-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3013 = dtu_hlir.constant  {node_name = "Constant_2099-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3014 = "dtu_hlir.unsqueeze"(%3008, %3013) {node_name = "Unsqueeze_2100-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3015 = "dtu_hlir.concatenate"(%3010, %3012, %3014) {dimension = 0 : i64, node_name = "Concat_2101-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3016 = dtu_hlir.constant  {node_name = "ConstantOfShape_2102-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %3017 = "dtu_hlir.dynamic_broadcast_in_dim"(%3016, %3015) {node_name = "ConstantOfShape_2102-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x77xf32>
    %3018 = "dtu_hlir.transpose"(%2963) {node_name = "Transpose_2103-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<20x64x77xf32>
    %3019 = "dtu_hlir.dot_general"(%2927, %3018) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2104-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x77xf32>) -> tensor<20x1024x77xf32>
    %3020 = "dtu_hlir.broadcast_in_dim"(%839) {node_name = "Mul_2105-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %3021 = "dtu_hlir.mul"(%3019, %3020) {node_name = "Mul_2105-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %3022 = "dtu_hlir.broadcast_in_dim"(%838) {node_name = "Mul_2106-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %3023 = "dtu_hlir.mul"(%3017, %3022) {node_name = "Mul_2106-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %3024 = "dtu_hlir.add"(%3021, %3023) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2107-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %3025 = "dtu_hlir.softmax"(%3024) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2108-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %3026 = "dtu_hlir.convert"(%3025) {node_name = "Cast_2109-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %3027 = "dtu_hlir.dot_general"(%3026, %2999) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2110-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x77x64xf32>) -> tensor<20x1024x64xf32>
    %3028 = "dtu_hlir.shape"(%3027) {end = 2147483647 : i64, node_name = "Shape_2111-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %3029 = dtu_hlir.constant  {node_name = "Constant_2112-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3030 = "dtu_hlir.gather"(%3028, %3029) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2113-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3031 = "dtu_hlir.shape"(%3027) {end = 2147483647 : i64, node_name = "Shape_2114-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %3032 = dtu_hlir.constant  {node_name = "Constant_2115-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3033 = "dtu_hlir.gather"(%3031, %3032) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2116-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3034 = "dtu_hlir.shape"(%3027) {end = 2147483647 : i64, node_name = "Shape_2117-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %3035 = dtu_hlir.constant  {node_name = "Constant_2118-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3036 = "dtu_hlir.gather"(%3034, %3035) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2119-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3037 = dtu_hlir.constant  {node_name = "Constant_2120-0", node_type = "Constant"} dense<10> : tensor<i64>
    %3038 = "dtu_hlir.div"(%3030, %3037) {node_name = "Div_2121-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3039 = "dtu_hlir.convert"(%3038) {node_name = "Cast_2122-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3040 = "dtu_hlir.convert"(%3039) {node_name = "Cast_2123-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3041 = dtu_hlir.constant  {node_name = "Constant_2124-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3042 = "dtu_hlir.unsqueeze"(%3040, %3041) {node_name = "Unsqueeze_2125-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3043 = dtu_hlir.constant  {node_name = "Constant_2126-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3044 = "dtu_hlir.unsqueeze"(%3033, %3043) {node_name = "Unsqueeze_2127-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3045 = dtu_hlir.constant  {node_name = "Constant_2128-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3046 = "dtu_hlir.unsqueeze"(%3036, %3045) {node_name = "Unsqueeze_2129-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3047 = "dtu_hlir.concatenate"(%3042, %837, %3044, %3046) {dimension = 0 : i64, node_name = "Concat_2130-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3048 = "dtu_hlir.dynamic_reshape"(%3027, %3047) {allowzero = 0 : i64, node_name = "Reshape_2131-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %3049 = "dtu_hlir.transpose"(%3048) {node_name = "Transpose_2132-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %3050 = dtu_hlir.constant  {node_name = "Constant_2133-0", node_type = "Constant"} dense<10> : tensor<i64>
    %3051 = "dtu_hlir.div"(%3030, %3050) {node_name = "Div_2134-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3052 = "dtu_hlir.convert"(%3051) {node_name = "Cast_2135-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3053 = "dtu_hlir.convert"(%3052) {node_name = "Cast_2136-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3054 = dtu_hlir.constant  {node_name = "Constant_2137-0", node_type = "Constant"} dense<10> : tensor<i64>
    %3055 = "dtu_hlir.mul"(%3036, %3054) {node_name = "Mul_2138-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3056 = dtu_hlir.constant  {node_name = "Constant_2139-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3057 = "dtu_hlir.unsqueeze"(%3053, %3056) {node_name = "Unsqueeze_2140-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3058 = dtu_hlir.constant  {node_name = "Constant_2141-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3059 = "dtu_hlir.unsqueeze"(%3033, %3058) {node_name = "Unsqueeze_2142-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3060 = dtu_hlir.constant  {node_name = "Constant_2143-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3061 = "dtu_hlir.unsqueeze"(%3055, %3060) {node_name = "Unsqueeze_2144-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3062 = "dtu_hlir.concatenate"(%3057, %3059, %3061) {dimension = 0 : i64, node_name = "Concat_2145-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3063 = "dtu_hlir.dynamic_reshape"(%3049, %3062) {allowzero = 0 : i64, node_name = "Reshape_2146-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %3064 = "dtu_hlir.dot_general"(%3063, %444) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2147-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %3065 = "dtu_hlir.add"(%60, %3064) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2148-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %3066 = "dtu_hlir.add"(%3065, %2861) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2149-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %3067 = dtu_hlir.constant  {node_name = "ReduceMean_2150-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3068 = "dtu_hlir.reshape"(%3067) {node_name = "ReduceMean_2150-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3069 = "dtu_hlir.reduce"(%3066, %3068) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2150-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2150-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %3070 = dtu_hlir.constant  {node_name = "ReduceMean_2150-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3071 = "dtu_hlir.unsqueeze"(%3069, %3070) {node_name = "ReduceMean_2150-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %3072 = dtu_hlir.constant  {node_name = "ReduceMean_2150-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %3073 = "dtu_hlir.broadcast_in_dim"(%3072) {node_name = "ReduceMean_2150-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %3074 = "dtu_hlir.div"(%3071, %3073) {node_name = "ReduceMean_2150-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %3075 = "dtu_hlir.sub"(%3066, %3074) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_2151-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %3076 = dtu_hlir.constant  {node_name = "Constant_2152-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %3077 = "dtu_hlir.broadcast_in_dim"(%3076) {node_name = "Pow_2153-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %3078 = "dtu_hlir.pow"(%3075, %3077) {node_name = "Pow_2153-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %3079 = dtu_hlir.constant  {node_name = "ReduceMean_2154-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3080 = "dtu_hlir.reshape"(%3079) {node_name = "ReduceMean_2154-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3081 = "dtu_hlir.reduce"(%3078, %3080) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2154-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2154-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %3082 = dtu_hlir.constant  {node_name = "ReduceMean_2154-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3083 = "dtu_hlir.unsqueeze"(%3081, %3082) {node_name = "ReduceMean_2154-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %3084 = dtu_hlir.constant  {node_name = "ReduceMean_2154-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %3085 = "dtu_hlir.broadcast_in_dim"(%3084) {node_name = "ReduceMean_2154-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %3086 = "dtu_hlir.div"(%3083, %3085) {node_name = "ReduceMean_2154-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %3087 = dtu_hlir.constant  {node_name = "Constant_2155-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %3088 = "dtu_hlir.broadcast_in_dim"(%3087) {node_name = "Add_2156-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %3089 = "dtu_hlir.add"(%3086, %3088) {node_name = "Add_2156-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %3090 = "dtu_hlir.sqrt"(%3089) {node_name = "Sqrt_2157-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %3091 = "dtu_hlir.div"(%3075, %3090) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_2158-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %3092 = "dtu_hlir.mul"(%3091, %65) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_2159-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %3093 = "dtu_hlir.add"(%3092, %66) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2160-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %3094 = "dtu_hlir.dot_general"(%3093, %445) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2161-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x5120xf32>) -> tensor<2x1024x5120xf32>
    %3095 = "dtu_hlir.add"(%58, %3094) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2162-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<5120xf32>, tensor<2x1024x5120xf32>) -> tensor<2x1024x5120xf32>
    %3096 = "dtu_hlir.shape"(%3095) {end = 2147483647 : i64, node_name = "Shape_2163-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>) -> tensor<3xi64>
    %3097 = dtu_hlir.constant  {node_name = "Constant_2164-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %3098 = "dtu_hlir.gather"(%3096, %3097) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2165-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3099 = dtu_hlir.constant  {node_name = "Constant_2166-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3100 = dtu_hlir.constant  {node_name = "Constant_2167-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %3101 = "dtu_hlir.add"(%3098, %3100) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_2168-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3102 = dtu_hlir.constant  {node_name = "Constant_2169-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %3103 = "dtu_hlir.div"(%3101, %3102) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_2170-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3104 = dtu_hlir.constant  {node_name = "Constant_2171-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %3105 = "dtu_hlir.mul"(%3103, %3104) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_2172-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3106 = dtu_hlir.constant  {node_name = "Slice_2173-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %3107 = "dtu_hlir.real_dynamic_slice"(%3095, %3099, %3105, %3106, %3097) {node_name = "Slice_2173-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %3108 = dtu_hlir.constant  {node_name = "Constant_2174-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %3109 = "dtu_hlir.mul"(%3103, %3108) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_2175-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3110 = "dtu_hlir.shape"(%3105) {end = 2147483647 : i64, node_name = "Slice_2176-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %3111 = dtu_hlir.constant  {node_name = "Slice_2176-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %3112 = "dtu_hlir.dynamic_broadcast_in_dim"(%3111, %3110) {node_name = "Slice_2176-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3113 = "dtu_hlir.real_dynamic_slice"(%3095, %3105, %3109, %3112, %3097) {node_name = "Slice_2176-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %3114 = dtu_hlir.constant  {node_name = "Constant_2177-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %3115 = "dtu_hlir.broadcast_in_dim"(%3114) {node_name = "Div_2178-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %3116 = "dtu_hlir.div"(%3113, %3115) {node_name = "Div_2178-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %3117 = "dtu_hlir.erf"(%3116) {node_name = "Erf_2179-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %3118 = dtu_hlir.constant  {node_name = "Constant_2180-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %3119 = "dtu_hlir.broadcast_in_dim"(%3118) {node_name = "Add_2181-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %3120 = "dtu_hlir.add"(%3117, %3119) {node_name = "Add_2181-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %3121 = "dtu_hlir.mul"(%3113, %3120) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_2182-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %3122 = dtu_hlir.constant  {node_name = "Constant_2183-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %3123 = "dtu_hlir.broadcast_in_dim"(%3122) {node_name = "Mul_2184-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %3124 = "dtu_hlir.mul"(%3121, %3123) {node_name = "Mul_2184-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %3125 = "dtu_hlir.mul"(%3107, %3124) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_2185-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %3126 = "dtu_hlir.dot_general"(%3125, %446) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2186-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2560x640xf32>) -> tensor<2x1024x640xf32>
    %3127 = "dtu_hlir.add"(%59, %3126) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2187-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %3128 = "dtu_hlir.add"(%3127, %3066) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2188-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %3129 = "dtu_hlir.dot_general"(%3128, %447) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2189-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %3130 = "dtu_hlir.add"(%67, %3129) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2190-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %3131 = dtu_hlir.constant  {node_name = "Constant_2191-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3132 = "dtu_hlir.unsqueeze"(%2626, %3131) {node_name = "Unsqueeze_2192-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3133 = dtu_hlir.constant  {node_name = "Constant_2193-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3134 = "dtu_hlir.unsqueeze"(%2629, %3133) {node_name = "Unsqueeze_2194-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3135 = dtu_hlir.constant  {node_name = "Constant_2195-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3136 = "dtu_hlir.unsqueeze"(%2632, %3135) {node_name = "Unsqueeze_2196-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3137 = dtu_hlir.constant  {node_name = "Constant_2197-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3138 = "dtu_hlir.unsqueeze"(%2644, %3137) {node_name = "Unsqueeze_2198-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3139 = "dtu_hlir.concatenate"(%3132, %3134, %3136, %3138) {dimension = 0 : i64, node_name = "Concat_2199-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3140 = "dtu_hlir.dynamic_reshape"(%3130, %3139) {allowzero = 0 : i64, node_name = "Reshape_2200-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x32x32x640xf32>
    %3141 = "dtu_hlir.transpose"(%3140) {node_name = "Transpose_2201-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>) -> tensor<2x640x32x32xf32>
    %3142 = "dtu_hlir.add"(%3141, %2623) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_2202-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %3143 = "dtu_hlir.conv_bias"(%3142, %82, %83) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2203-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<2> : tensor<2xi64>} : (tensor<2x640x32x32xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x16x16xf32>
    %3144 = dtu_hlir.constant  {node_name = "Constant_2204-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %3145 = "dtu_hlir.dynamic_reshape"(%3143, %3144) {allowzero = 0 : i64, node_name = "Reshape_2205-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x16x16xf32>, tensor<3xi64>) -> tensor<2x32x5120xf32>
    %3146 = dtu_hlir.constant  {node_name = "Constant_2206-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %3147 = dtu_hlir.constant  {node_name = "Constant_2207-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %3148 = "dtu_hlir.instance_norm"(%3145, %3146, %3147) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_2208-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x5120xf32>
    %3149 = "dtu_hlir.shape"(%3143) {end = 2147483647 : i64, node_name = "Shape_2209-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x16x16xf32>) -> tensor<4xi64>
    %3150 = "dtu_hlir.dynamic_reshape"(%3148, %3149) {allowzero = 0 : i64, node_name = "Reshape_2210-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<4xi64>) -> tensor<2x640x16x16xf32>
    %3151 = "dtu_hlir.mul"(%3150, %448) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_2211-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x16x16xf32>, tensor<640x1x1xf32>) -> tensor<2x640x16x16xf32>
    %3152 = "dtu_hlir.add"(%3151, %449) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_2212-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x16x16xf32>, tensor<640x1x1xf32>) -> tensor<2x640x16x16xf32>
    %3153 = "dtu_hlir.sigmoid"(%3152) {node_name = "Sigmoid_2213-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x16x16xf32>) -> tensor<2x640x16x16xf32>
    %3154 = "dtu_hlir.mul"(%3152, %3153) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_2214-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x16x16xf32>, tensor<2x640x16x16xf32>) -> tensor<2x640x16x16xf32>
    %3155 = "dtu_hlir.conv_bias"(%3154, %108, %109) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2215-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x16x16xf32>, tensor<1280x640x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %3156 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_2216-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %3157 = "dtu_hlir.mul"(%915, %3156) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2217-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %3158 = "dtu_hlir.transpose"(%110) {node_name = "Gemm_2218-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %3159 = "dtu_hlir.gemm"(%3157, %3158, %111) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_2218-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %3160 = dtu_hlir.constant  {node_name = "Constant_2219-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %3161 = "dtu_hlir.unsqueeze"(%3159, %3160) {node_name = "Unsqueeze_2220-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %3162 = dtu_hlir.constant  {node_name = "Constant_2221-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %3163 = "dtu_hlir.unsqueeze"(%3161, %3162) {node_name = "Unsqueeze_2222-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %3164 = "dtu_hlir.add"(%3155, %3163) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_2223-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3165 = dtu_hlir.constant  {node_name = "Constant_2224-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %3166 = "dtu_hlir.dynamic_reshape"(%3164, %3165) {allowzero = 0 : i64, node_name = "Reshape_2225-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %3167 = dtu_hlir.constant  {node_name = "Constant_2226-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %3168 = dtu_hlir.constant  {node_name = "Constant_2227-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %3169 = "dtu_hlir.instance_norm"(%3166, %3167, %3168) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_2228-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %3170 = "dtu_hlir.shape"(%3164) {end = 2147483647 : i64, node_name = "Shape_2229-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3171 = "dtu_hlir.dynamic_reshape"(%3169, %3170) {allowzero = 0 : i64, node_name = "Reshape_2230-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %3172 = "dtu_hlir.mul"(%3171, %450) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_2231-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3173 = "dtu_hlir.add"(%3172, %451) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_2232-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3174 = "dtu_hlir.sigmoid"(%3173) {node_name = "Sigmoid_2233-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3175 = "dtu_hlir.mul"(%3173, %3174) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_2234-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3176 = "dtu_hlir.conv_bias"(%3175, %112, %113) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2235-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %3177 = "dtu_hlir.conv_bias"(%3143, %114, %115) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2236-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x16x16xf32>, tensor<1280x640x1x1xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %3178 = "dtu_hlir.add"(%3177, %3176) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_2237-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3179 = dtu_hlir.constant  {node_name = "Constant_2238-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %3180 = "dtu_hlir.broadcast_in_dim"(%3179) {node_name = "Div_2239-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x16x16xf32>
    %3181 = "dtu_hlir.div"(%3178, %3180) {node_name = "Div_2239-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3182 = "dtu_hlir.shape"(%3181) {end = 2147483647 : i64, node_name = "Shape_2240-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3183 = dtu_hlir.constant  {node_name = "Constant_2241-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3184 = "dtu_hlir.gather"(%3182, %3183) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2242-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3185 = "dtu_hlir.shape"(%3181) {end = 2147483647 : i64, node_name = "Shape_2243-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3186 = dtu_hlir.constant  {node_name = "Constant_2244-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3187 = "dtu_hlir.gather"(%3185, %3186) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2245-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3188 = "dtu_hlir.shape"(%3181) {end = 2147483647 : i64, node_name = "Shape_2246-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3189 = dtu_hlir.constant  {node_name = "Constant_2247-0", node_type = "Constant"} dense<3> : tensor<i64>
    %3190 = "dtu_hlir.gather"(%3188, %3189) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2248-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3191 = dtu_hlir.constant  {node_name = "Constant_2249-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %3192 = "dtu_hlir.dynamic_reshape"(%3181, %3191) {allowzero = 0 : i64, node_name = "Reshape_2250-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %3193 = dtu_hlir.constant  {node_name = "Constant_2251-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %3194 = dtu_hlir.constant  {node_name = "Constant_2252-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %3195 = "dtu_hlir.instance_norm"(%3192, %3193, %3194) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_2253-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %3196 = "dtu_hlir.shape"(%3181) {end = 2147483647 : i64, node_name = "Shape_2254-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3197 = "dtu_hlir.dynamic_reshape"(%3195, %3196) {allowzero = 0 : i64, node_name = "Reshape_2255-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %3198 = "dtu_hlir.mul"(%3197, %452) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_2256-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3199 = "dtu_hlir.add"(%3198, %453) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_2257-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3200 = "dtu_hlir.shape"(%3199) {end = 2147483647 : i64, node_name = "Shape_2258-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3201 = dtu_hlir.constant  {node_name = "Constant_2259-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3202 = "dtu_hlir.gather"(%3200, %3201) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2260-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3203 = "dtu_hlir.transpose"(%3199) {node_name = "Transpose_2261-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x16x16x1280xf32>
    %3204 = "dtu_hlir.mul"(%3187, %3190) {node_name = "Mul_2262-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3205 = dtu_hlir.constant  {node_name = "Constant_2263-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3206 = "dtu_hlir.unsqueeze"(%3184, %3205) {node_name = "Unsqueeze_2264-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3207 = dtu_hlir.constant  {node_name = "Constant_2265-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3208 = "dtu_hlir.unsqueeze"(%3204, %3207) {node_name = "Unsqueeze_2266-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3209 = dtu_hlir.constant  {node_name = "Constant_2267-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3210 = "dtu_hlir.unsqueeze"(%3202, %3209) {node_name = "Unsqueeze_2268-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3211 = "dtu_hlir.concatenate"(%3206, %3208, %3210) {dimension = 0 : i64, node_name = "Concat_2269-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3212 = "dtu_hlir.dynamic_reshape"(%3203, %3211) {allowzero = 0 : i64, node_name = "Reshape_2270-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %3213 = "dtu_hlir.dot_general"(%3212, %454) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2271-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3214 = "dtu_hlir.add"(%84, %3213) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2272-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3215 = dtu_hlir.constant  {node_name = "ReduceMean_2273-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3216 = "dtu_hlir.reshape"(%3215) {node_name = "ReduceMean_2273-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3217 = "dtu_hlir.reduce"(%3214, %3216) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2273-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2273-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3218 = dtu_hlir.constant  {node_name = "ReduceMean_2273-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3219 = "dtu_hlir.unsqueeze"(%3217, %3218) {node_name = "ReduceMean_2273-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3220 = dtu_hlir.constant  {node_name = "ReduceMean_2273-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3221 = "dtu_hlir.broadcast_in_dim"(%3220) {node_name = "ReduceMean_2273-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3222 = "dtu_hlir.div"(%3219, %3221) {node_name = "ReduceMean_2273-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3223 = "dtu_hlir.sub"(%3214, %3222) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_2274-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3224 = dtu_hlir.constant  {node_name = "Constant_2275-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %3225 = "dtu_hlir.broadcast_in_dim"(%3224) {node_name = "Pow_2276-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %3226 = "dtu_hlir.pow"(%3223, %3225) {node_name = "Pow_2276-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3227 = dtu_hlir.constant  {node_name = "ReduceMean_2277-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3228 = "dtu_hlir.reshape"(%3227) {node_name = "ReduceMean_2277-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3229 = "dtu_hlir.reduce"(%3226, %3228) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2277-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2277-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3230 = dtu_hlir.constant  {node_name = "ReduceMean_2277-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3231 = "dtu_hlir.unsqueeze"(%3229, %3230) {node_name = "ReduceMean_2277-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3232 = dtu_hlir.constant  {node_name = "ReduceMean_2277-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3233 = "dtu_hlir.broadcast_in_dim"(%3232) {node_name = "ReduceMean_2277-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3234 = "dtu_hlir.div"(%3231, %3233) {node_name = "ReduceMean_2277-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3235 = dtu_hlir.constant  {node_name = "Constant_2278-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %3236 = "dtu_hlir.broadcast_in_dim"(%3235) {node_name = "Add_2279-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %3237 = "dtu_hlir.add"(%3234, %3236) {node_name = "Add_2279-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3238 = "dtu_hlir.sqrt"(%3237) {node_name = "Sqrt_2280-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3239 = "dtu_hlir.div"(%3223, %3238) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_2281-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3240 = "dtu_hlir.mul"(%3239, %89) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_2282-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3241 = "dtu_hlir.add"(%3240, %90) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2283-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3242 = "dtu_hlir.dot_general"(%3241, %455) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2284-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3243 = "dtu_hlir.dot_general"(%3241, %456) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2285-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3244 = "dtu_hlir.dot_general"(%3241, %457) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2286-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3245 = "dtu_hlir.shape"(%3242) {end = 2147483647 : i64, node_name = "Shape_2287-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3246 = dtu_hlir.constant  {node_name = "Constant_2288-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3247 = "dtu_hlir.gather"(%3245, %3246) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2289-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3248 = "dtu_hlir.shape"(%3242) {end = 2147483647 : i64, node_name = "Shape_2290-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3249 = dtu_hlir.constant  {node_name = "Constant_2291-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3250 = "dtu_hlir.gather"(%3248, %3249) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2292-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3251 = "dtu_hlir.shape"(%3242) {end = 2147483647 : i64, node_name = "Shape_2293-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3252 = dtu_hlir.constant  {node_name = "Constant_2294-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3253 = "dtu_hlir.gather"(%3251, %3252) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2295-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3254 = dtu_hlir.constant  {node_name = "Constant_2296-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3255 = "dtu_hlir.div"(%3253, %3254) {node_name = "Div_2297-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3256 = "dtu_hlir.convert"(%3255) {node_name = "Cast_2298-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3257 = "dtu_hlir.convert"(%3256) {node_name = "Cast_2299-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3258 = dtu_hlir.constant  {node_name = "Constant_2300-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3259 = "dtu_hlir.unsqueeze"(%3247, %3258) {node_name = "Unsqueeze_2301-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3260 = dtu_hlir.constant  {node_name = "Constant_2302-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3261 = "dtu_hlir.unsqueeze"(%3250, %3260) {node_name = "Unsqueeze_2303-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3262 = dtu_hlir.constant  {node_name = "Constant_2304-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3263 = "dtu_hlir.unsqueeze"(%3257, %3262) {node_name = "Unsqueeze_2305-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3264 = "dtu_hlir.concatenate"(%3259, %3261, %458, %3263) {dimension = 0 : i64, node_name = "Concat_2306-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3265 = "dtu_hlir.dynamic_reshape"(%3242, %3264) {allowzero = 0 : i64, node_name = "Reshape_2307-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %3266 = "dtu_hlir.transpose"(%3265) {node_name = "Transpose_2308-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %3267 = dtu_hlir.constant  {node_name = "Constant_2309-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3268 = "dtu_hlir.mul"(%3247, %3267) {node_name = "Mul_2310-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3269 = dtu_hlir.constant  {node_name = "Constant_2311-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3270 = "dtu_hlir.div"(%3253, %3269) {node_name = "Div_2312-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3271 = "dtu_hlir.convert"(%3270) {node_name = "Cast_2313-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3272 = "dtu_hlir.convert"(%3271) {node_name = "Cast_2314-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3273 = dtu_hlir.constant  {node_name = "Constant_2315-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3274 = "dtu_hlir.unsqueeze"(%3268, %3273) {node_name = "Unsqueeze_2316-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3275 = dtu_hlir.constant  {node_name = "Constant_2317-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3276 = "dtu_hlir.unsqueeze"(%3250, %3275) {node_name = "Unsqueeze_2318-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3277 = dtu_hlir.constant  {node_name = "Constant_2319-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3278 = "dtu_hlir.unsqueeze"(%3272, %3277) {node_name = "Unsqueeze_2320-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3279 = "dtu_hlir.concatenate"(%3274, %3276, %3278) {dimension = 0 : i64, node_name = "Concat_2321-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3280 = "dtu_hlir.dynamic_reshape"(%3266, %3279) {allowzero = 0 : i64, node_name = "Reshape_2322-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %3281 = "dtu_hlir.shape"(%3243) {end = 2147483647 : i64, node_name = "Shape_2323-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3282 = dtu_hlir.constant  {node_name = "Constant_2324-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3283 = "dtu_hlir.gather"(%3281, %3282) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2325-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3284 = "dtu_hlir.shape"(%3243) {end = 2147483647 : i64, node_name = "Shape_2326-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3285 = dtu_hlir.constant  {node_name = "Constant_2327-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3286 = "dtu_hlir.gather"(%3284, %3285) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2328-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3287 = "dtu_hlir.shape"(%3243) {end = 2147483647 : i64, node_name = "Shape_2329-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3288 = dtu_hlir.constant  {node_name = "Constant_2330-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3289 = "dtu_hlir.gather"(%3287, %3288) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2331-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3290 = dtu_hlir.constant  {node_name = "Constant_2332-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3291 = "dtu_hlir.div"(%3289, %3290) {node_name = "Div_2333-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3292 = "dtu_hlir.convert"(%3291) {node_name = "Cast_2334-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3293 = "dtu_hlir.convert"(%3292) {node_name = "Cast_2335-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3294 = dtu_hlir.constant  {node_name = "Constant_2336-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3295 = "dtu_hlir.unsqueeze"(%3283, %3294) {node_name = "Unsqueeze_2337-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3296 = dtu_hlir.constant  {node_name = "Constant_2338-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3297 = "dtu_hlir.unsqueeze"(%3286, %3296) {node_name = "Unsqueeze_2339-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3298 = dtu_hlir.constant  {node_name = "Constant_2340-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3299 = "dtu_hlir.unsqueeze"(%3293, %3298) {node_name = "Unsqueeze_2341-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3300 = "dtu_hlir.concatenate"(%3295, %3297, %836, %3299) {dimension = 0 : i64, node_name = "Concat_2342-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3301 = "dtu_hlir.dynamic_reshape"(%3243, %3300) {allowzero = 0 : i64, node_name = "Reshape_2343-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %3302 = "dtu_hlir.transpose"(%3301) {node_name = "Transpose_2344-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %3303 = dtu_hlir.constant  {node_name = "Constant_2345-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3304 = "dtu_hlir.mul"(%3283, %3303) {node_name = "Mul_2346-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3305 = dtu_hlir.constant  {node_name = "Constant_2347-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3306 = "dtu_hlir.div"(%3289, %3305) {node_name = "Div_2348-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3307 = "dtu_hlir.convert"(%3306) {node_name = "Cast_2349-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3308 = "dtu_hlir.convert"(%3307) {node_name = "Cast_2350-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3309 = dtu_hlir.constant  {node_name = "Constant_2351-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3310 = "dtu_hlir.unsqueeze"(%3304, %3309) {node_name = "Unsqueeze_2352-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3311 = dtu_hlir.constant  {node_name = "Constant_2353-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3312 = "dtu_hlir.unsqueeze"(%3286, %3311) {node_name = "Unsqueeze_2354-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3313 = dtu_hlir.constant  {node_name = "Constant_2355-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3314 = "dtu_hlir.unsqueeze"(%3308, %3313) {node_name = "Unsqueeze_2356-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3315 = "dtu_hlir.concatenate"(%3310, %3312, %3314) {dimension = 0 : i64, node_name = "Concat_2357-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3316 = "dtu_hlir.dynamic_reshape"(%3302, %3315) {allowzero = 0 : i64, node_name = "Reshape_2358-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %3317 = "dtu_hlir.shape"(%3244) {end = 2147483647 : i64, node_name = "Shape_2359-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3318 = dtu_hlir.constant  {node_name = "Constant_2360-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3319 = "dtu_hlir.gather"(%3317, %3318) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2361-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3320 = "dtu_hlir.shape"(%3244) {end = 2147483647 : i64, node_name = "Shape_2362-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3321 = dtu_hlir.constant  {node_name = "Constant_2363-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3322 = "dtu_hlir.gather"(%3320, %3321) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2364-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3323 = "dtu_hlir.shape"(%3244) {end = 2147483647 : i64, node_name = "Shape_2365-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3324 = dtu_hlir.constant  {node_name = "Constant_2366-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3325 = "dtu_hlir.gather"(%3323, %3324) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2367-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3326 = dtu_hlir.constant  {node_name = "Constant_2368-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3327 = "dtu_hlir.div"(%3325, %3326) {node_name = "Div_2369-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3328 = "dtu_hlir.convert"(%3327) {node_name = "Cast_2370-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3329 = "dtu_hlir.convert"(%3328) {node_name = "Cast_2371-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3330 = dtu_hlir.constant  {node_name = "Constant_2372-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3331 = "dtu_hlir.unsqueeze"(%3319, %3330) {node_name = "Unsqueeze_2373-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3332 = dtu_hlir.constant  {node_name = "Constant_2374-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3333 = "dtu_hlir.unsqueeze"(%3322, %3332) {node_name = "Unsqueeze_2375-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3334 = dtu_hlir.constant  {node_name = "Constant_2376-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3335 = "dtu_hlir.unsqueeze"(%3329, %3334) {node_name = "Unsqueeze_2377-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3336 = "dtu_hlir.concatenate"(%3331, %3333, %835, %3335) {dimension = 0 : i64, node_name = "Concat_2378-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3337 = "dtu_hlir.dynamic_reshape"(%3244, %3336) {allowzero = 0 : i64, node_name = "Reshape_2379-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %3338 = "dtu_hlir.transpose"(%3337) {node_name = "Transpose_2380-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %3339 = dtu_hlir.constant  {node_name = "Constant_2381-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3340 = "dtu_hlir.mul"(%3319, %3339) {node_name = "Mul_2382-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3341 = dtu_hlir.constant  {node_name = "Constant_2383-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3342 = "dtu_hlir.div"(%3325, %3341) {node_name = "Div_2384-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3343 = "dtu_hlir.convert"(%3342) {node_name = "Cast_2385-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3344 = "dtu_hlir.convert"(%3343) {node_name = "Cast_2386-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3345 = dtu_hlir.constant  {node_name = "Constant_2387-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3346 = "dtu_hlir.unsqueeze"(%3340, %3345) {node_name = "Unsqueeze_2388-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3347 = dtu_hlir.constant  {node_name = "Constant_2389-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3348 = "dtu_hlir.unsqueeze"(%3322, %3347) {node_name = "Unsqueeze_2390-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3349 = dtu_hlir.constant  {node_name = "Constant_2391-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3350 = "dtu_hlir.unsqueeze"(%3344, %3349) {node_name = "Unsqueeze_2392-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3351 = "dtu_hlir.concatenate"(%3346, %3348, %3350) {dimension = 0 : i64, node_name = "Concat_2393-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3352 = "dtu_hlir.dynamic_reshape"(%3338, %3351) {allowzero = 0 : i64, node_name = "Reshape_2394-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %3353 = "dtu_hlir.shape"(%3280) {end = 2147483647 : i64, node_name = "Shape_2395-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3354 = dtu_hlir.constant  {node_name = "Constant_2396-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3355 = "dtu_hlir.gather"(%3353, %3354) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2397-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3356 = "dtu_hlir.shape"(%3280) {end = 2147483647 : i64, node_name = "Shape_2398-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3357 = dtu_hlir.constant  {node_name = "Constant_2399-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3358 = "dtu_hlir.gather"(%3356, %3357) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2400-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3359 = "dtu_hlir.shape"(%3316) {end = 2147483647 : i64, node_name = "Shape_2401-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3360 = dtu_hlir.constant  {node_name = "Constant_2402-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3361 = "dtu_hlir.gather"(%3359, %3360) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2403-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3362 = dtu_hlir.constant  {node_name = "Constant_2404-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3363 = "dtu_hlir.unsqueeze"(%3355, %3362) {node_name = "Unsqueeze_2405-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3364 = dtu_hlir.constant  {node_name = "Constant_2406-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3365 = "dtu_hlir.unsqueeze"(%3358, %3364) {node_name = "Unsqueeze_2407-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3366 = dtu_hlir.constant  {node_name = "Constant_2408-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3367 = "dtu_hlir.unsqueeze"(%3361, %3366) {node_name = "Unsqueeze_2409-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3368 = "dtu_hlir.concatenate"(%3363, %3365, %3367) {dimension = 0 : i64, node_name = "Concat_2410-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3369 = dtu_hlir.constant  {node_name = "ConstantOfShape_2411-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %3370 = "dtu_hlir.dynamic_broadcast_in_dim"(%3369, %3368) {node_name = "ConstantOfShape_2411-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x256xf32>
    %3371 = "dtu_hlir.transpose"(%3316) {node_name = "Transpose_2412-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<40x64x256xf32>
    %3372 = "dtu_hlir.dot_general"(%3280, %3371) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2413-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x256xf32>) -> tensor<40x256x256xf32>
    %3373 = "dtu_hlir.broadcast_in_dim"(%834) {node_name = "Mul_2414-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %3374 = "dtu_hlir.mul"(%3372, %3373) {node_name = "Mul_2414-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3375 = "dtu_hlir.broadcast_in_dim"(%833) {node_name = "Mul_2415-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %3376 = "dtu_hlir.mul"(%3370, %3375) {node_name = "Mul_2415-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3377 = "dtu_hlir.add"(%3374, %3376) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2416-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3378 = "dtu_hlir.softmax"(%3377) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2417-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3379 = "dtu_hlir.convert"(%3378) {node_name = "Cast_2418-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3380 = "dtu_hlir.dot_general"(%3379, %3352) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2419-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x64xf32>) -> tensor<40x256x64xf32>
    %3381 = "dtu_hlir.shape"(%3380) {end = 2147483647 : i64, node_name = "Shape_2420-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3382 = dtu_hlir.constant  {node_name = "Constant_2421-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3383 = "dtu_hlir.gather"(%3381, %3382) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2422-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3384 = "dtu_hlir.shape"(%3380) {end = 2147483647 : i64, node_name = "Shape_2423-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3385 = dtu_hlir.constant  {node_name = "Constant_2424-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3386 = "dtu_hlir.gather"(%3384, %3385) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2425-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3387 = "dtu_hlir.shape"(%3380) {end = 2147483647 : i64, node_name = "Shape_2426-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3388 = dtu_hlir.constant  {node_name = "Constant_2427-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3389 = "dtu_hlir.gather"(%3387, %3388) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2428-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3390 = dtu_hlir.constant  {node_name = "Constant_2429-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3391 = "dtu_hlir.div"(%3383, %3390) {node_name = "Div_2430-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3392 = "dtu_hlir.convert"(%3391) {node_name = "Cast_2431-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3393 = "dtu_hlir.convert"(%3392) {node_name = "Cast_2432-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3394 = dtu_hlir.constant  {node_name = "Constant_2433-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3395 = "dtu_hlir.unsqueeze"(%3393, %3394) {node_name = "Unsqueeze_2434-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3396 = dtu_hlir.constant  {node_name = "Constant_2435-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3397 = "dtu_hlir.unsqueeze"(%3386, %3396) {node_name = "Unsqueeze_2436-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3398 = dtu_hlir.constant  {node_name = "Constant_2437-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3399 = "dtu_hlir.unsqueeze"(%3389, %3398) {node_name = "Unsqueeze_2438-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3400 = "dtu_hlir.concatenate"(%3395, %832, %3397, %3399) {dimension = 0 : i64, node_name = "Concat_2439-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3401 = "dtu_hlir.dynamic_reshape"(%3380, %3400) {allowzero = 0 : i64, node_name = "Reshape_2440-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %3402 = "dtu_hlir.transpose"(%3401) {node_name = "Transpose_2441-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %3403 = dtu_hlir.constant  {node_name = "Constant_2442-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3404 = "dtu_hlir.div"(%3383, %3403) {node_name = "Div_2443-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3405 = "dtu_hlir.convert"(%3404) {node_name = "Cast_2444-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3406 = "dtu_hlir.convert"(%3405) {node_name = "Cast_2445-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3407 = dtu_hlir.constant  {node_name = "Constant_2446-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3408 = "dtu_hlir.mul"(%3389, %3407) {node_name = "Mul_2447-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3409 = dtu_hlir.constant  {node_name = "Constant_2448-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3410 = "dtu_hlir.unsqueeze"(%3406, %3409) {node_name = "Unsqueeze_2449-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3411 = dtu_hlir.constant  {node_name = "Constant_2450-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3412 = "dtu_hlir.unsqueeze"(%3386, %3411) {node_name = "Unsqueeze_2451-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3413 = dtu_hlir.constant  {node_name = "Constant_2452-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3414 = "dtu_hlir.unsqueeze"(%3408, %3413) {node_name = "Unsqueeze_2453-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3415 = "dtu_hlir.concatenate"(%3410, %3412, %3414) {dimension = 0 : i64, node_name = "Concat_2454-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3416 = "dtu_hlir.dynamic_reshape"(%3402, %3415) {allowzero = 0 : i64, node_name = "Reshape_2455-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %3417 = "dtu_hlir.dot_general"(%3416, %459) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2456-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3418 = "dtu_hlir.add"(%85, %3417) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2457-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3419 = "dtu_hlir.add"(%3418, %3214) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2458-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3420 = dtu_hlir.constant  {node_name = "ReduceMean_2459-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3421 = "dtu_hlir.reshape"(%3420) {node_name = "ReduceMean_2459-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3422 = "dtu_hlir.reduce"(%3419, %3421) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2459-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2459-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3423 = dtu_hlir.constant  {node_name = "ReduceMean_2459-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3424 = "dtu_hlir.unsqueeze"(%3422, %3423) {node_name = "ReduceMean_2459-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3425 = dtu_hlir.constant  {node_name = "ReduceMean_2459-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3426 = "dtu_hlir.broadcast_in_dim"(%3425) {node_name = "ReduceMean_2459-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3427 = "dtu_hlir.div"(%3424, %3426) {node_name = "ReduceMean_2459-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3428 = "dtu_hlir.sub"(%3419, %3427) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_2460-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3429 = dtu_hlir.constant  {node_name = "Constant_2461-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %3430 = "dtu_hlir.broadcast_in_dim"(%3429) {node_name = "Pow_2462-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %3431 = "dtu_hlir.pow"(%3428, %3430) {node_name = "Pow_2462-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3432 = dtu_hlir.constant  {node_name = "ReduceMean_2463-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3433 = "dtu_hlir.reshape"(%3432) {node_name = "ReduceMean_2463-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3434 = "dtu_hlir.reduce"(%3431, %3433) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2463-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2463-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3435 = dtu_hlir.constant  {node_name = "ReduceMean_2463-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3436 = "dtu_hlir.unsqueeze"(%3434, %3435) {node_name = "ReduceMean_2463-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3437 = dtu_hlir.constant  {node_name = "ReduceMean_2463-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3438 = "dtu_hlir.broadcast_in_dim"(%3437) {node_name = "ReduceMean_2463-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3439 = "dtu_hlir.div"(%3436, %3438) {node_name = "ReduceMean_2463-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3440 = dtu_hlir.constant  {node_name = "Constant_2464-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %3441 = "dtu_hlir.broadcast_in_dim"(%3440) {node_name = "Add_2465-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %3442 = "dtu_hlir.add"(%3439, %3441) {node_name = "Add_2465-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3443 = "dtu_hlir.sqrt"(%3442) {node_name = "Sqrt_2466-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3444 = "dtu_hlir.div"(%3428, %3443) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_2467-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3445 = "dtu_hlir.mul"(%3444, %91) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_2468-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3446 = "dtu_hlir.add"(%3445, %92) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2469-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3447 = "dtu_hlir.dot_general"(%3446, %460) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2470-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3448 = "dtu_hlir.dot_general"(%arg2, %461) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2471-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %3449 = "dtu_hlir.dot_general"(%arg2, %462) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2472-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %3450 = "dtu_hlir.shape"(%3447) {end = 2147483647 : i64, node_name = "Shape_2473-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3451 = dtu_hlir.constant  {node_name = "Constant_2474-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3452 = "dtu_hlir.gather"(%3450, %3451) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2475-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3453 = "dtu_hlir.shape"(%3447) {end = 2147483647 : i64, node_name = "Shape_2476-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3454 = dtu_hlir.constant  {node_name = "Constant_2477-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3455 = "dtu_hlir.gather"(%3453, %3454) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2478-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3456 = "dtu_hlir.shape"(%3447) {end = 2147483647 : i64, node_name = "Shape_2479-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3457 = dtu_hlir.constant  {node_name = "Constant_2480-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3458 = "dtu_hlir.gather"(%3456, %3457) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2481-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3459 = dtu_hlir.constant  {node_name = "Constant_2482-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3460 = "dtu_hlir.div"(%3458, %3459) {node_name = "Div_2483-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3461 = "dtu_hlir.convert"(%3460) {node_name = "Cast_2484-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3462 = "dtu_hlir.convert"(%3461) {node_name = "Cast_2485-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3463 = dtu_hlir.constant  {node_name = "Constant_2486-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3464 = "dtu_hlir.unsqueeze"(%3452, %3463) {node_name = "Unsqueeze_2487-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3465 = dtu_hlir.constant  {node_name = "Constant_2488-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3466 = "dtu_hlir.unsqueeze"(%3455, %3465) {node_name = "Unsqueeze_2489-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3467 = dtu_hlir.constant  {node_name = "Constant_2490-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3468 = "dtu_hlir.unsqueeze"(%3462, %3467) {node_name = "Unsqueeze_2491-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3469 = "dtu_hlir.concatenate"(%3464, %3466, %831, %3468) {dimension = 0 : i64, node_name = "Concat_2492-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3470 = "dtu_hlir.dynamic_reshape"(%3447, %3469) {allowzero = 0 : i64, node_name = "Reshape_2493-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %3471 = "dtu_hlir.transpose"(%3470) {node_name = "Transpose_2494-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %3472 = dtu_hlir.constant  {node_name = "Constant_2495-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3473 = "dtu_hlir.mul"(%3452, %3472) {node_name = "Mul_2496-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3474 = dtu_hlir.constant  {node_name = "Constant_2497-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3475 = "dtu_hlir.div"(%3458, %3474) {node_name = "Div_2498-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3476 = "dtu_hlir.convert"(%3475) {node_name = "Cast_2499-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3477 = "dtu_hlir.convert"(%3476) {node_name = "Cast_2500-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3478 = dtu_hlir.constant  {node_name = "Constant_2501-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3479 = "dtu_hlir.unsqueeze"(%3473, %3478) {node_name = "Unsqueeze_2502-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3480 = dtu_hlir.constant  {node_name = "Constant_2503-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3481 = "dtu_hlir.unsqueeze"(%3455, %3480) {node_name = "Unsqueeze_2504-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3482 = dtu_hlir.constant  {node_name = "Constant_2505-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3483 = "dtu_hlir.unsqueeze"(%3477, %3482) {node_name = "Unsqueeze_2506-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3484 = "dtu_hlir.concatenate"(%3479, %3481, %3483) {dimension = 0 : i64, node_name = "Concat_2507-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3485 = "dtu_hlir.dynamic_reshape"(%3471, %3484) {allowzero = 0 : i64, node_name = "Reshape_2508-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %3486 = "dtu_hlir.shape"(%3448) {end = 2147483647 : i64, node_name = "Shape_2509-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %3487 = dtu_hlir.constant  {node_name = "Constant_2510-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3488 = "dtu_hlir.gather"(%3486, %3487) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2511-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3489 = "dtu_hlir.shape"(%3448) {end = 2147483647 : i64, node_name = "Shape_2512-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %3490 = dtu_hlir.constant  {node_name = "Constant_2513-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3491 = "dtu_hlir.gather"(%3489, %3490) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2514-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3492 = "dtu_hlir.shape"(%3448) {end = 2147483647 : i64, node_name = "Shape_2515-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %3493 = dtu_hlir.constant  {node_name = "Constant_2516-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3494 = "dtu_hlir.gather"(%3492, %3493) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2517-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3495 = dtu_hlir.constant  {node_name = "Constant_2518-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3496 = "dtu_hlir.div"(%3494, %3495) {node_name = "Div_2519-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3497 = "dtu_hlir.convert"(%3496) {node_name = "Cast_2520-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3498 = "dtu_hlir.convert"(%3497) {node_name = "Cast_2521-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3499 = dtu_hlir.constant  {node_name = "Constant_2522-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3500 = "dtu_hlir.unsqueeze"(%3488, %3499) {node_name = "Unsqueeze_2523-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3501 = dtu_hlir.constant  {node_name = "Constant_2524-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3502 = "dtu_hlir.unsqueeze"(%3491, %3501) {node_name = "Unsqueeze_2525-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3503 = dtu_hlir.constant  {node_name = "Constant_2526-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3504 = "dtu_hlir.unsqueeze"(%3498, %3503) {node_name = "Unsqueeze_2527-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3505 = "dtu_hlir.concatenate"(%3500, %3502, %830, %3504) {dimension = 0 : i64, node_name = "Concat_2528-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3506 = "dtu_hlir.dynamic_reshape"(%3448, %3505) {allowzero = 0 : i64, node_name = "Reshape_2529-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %3507 = "dtu_hlir.transpose"(%3506) {node_name = "Transpose_2530-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %3508 = dtu_hlir.constant  {node_name = "Constant_2531-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3509 = "dtu_hlir.mul"(%3488, %3508) {node_name = "Mul_2532-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3510 = dtu_hlir.constant  {node_name = "Constant_2533-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3511 = "dtu_hlir.div"(%3494, %3510) {node_name = "Div_2534-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3512 = "dtu_hlir.convert"(%3511) {node_name = "Cast_2535-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3513 = "dtu_hlir.convert"(%3512) {node_name = "Cast_2536-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3514 = dtu_hlir.constant  {node_name = "Constant_2537-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3515 = "dtu_hlir.unsqueeze"(%3509, %3514) {node_name = "Unsqueeze_2538-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3516 = dtu_hlir.constant  {node_name = "Constant_2539-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3517 = "dtu_hlir.unsqueeze"(%3491, %3516) {node_name = "Unsqueeze_2540-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3518 = dtu_hlir.constant  {node_name = "Constant_2541-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3519 = "dtu_hlir.unsqueeze"(%3513, %3518) {node_name = "Unsqueeze_2542-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3520 = "dtu_hlir.concatenate"(%3515, %3517, %3519) {dimension = 0 : i64, node_name = "Concat_2543-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3521 = "dtu_hlir.dynamic_reshape"(%3507, %3520) {allowzero = 0 : i64, node_name = "Reshape_2544-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %3522 = "dtu_hlir.shape"(%3449) {end = 2147483647 : i64, node_name = "Shape_2545-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %3523 = dtu_hlir.constant  {node_name = "Constant_2546-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3524 = "dtu_hlir.gather"(%3522, %3523) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2547-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3525 = "dtu_hlir.shape"(%3449) {end = 2147483647 : i64, node_name = "Shape_2548-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %3526 = dtu_hlir.constant  {node_name = "Constant_2549-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3527 = "dtu_hlir.gather"(%3525, %3526) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2550-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3528 = "dtu_hlir.shape"(%3449) {end = 2147483647 : i64, node_name = "Shape_2551-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %3529 = dtu_hlir.constant  {node_name = "Constant_2552-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3530 = "dtu_hlir.gather"(%3528, %3529) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2553-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3531 = dtu_hlir.constant  {node_name = "Constant_2554-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3532 = "dtu_hlir.div"(%3530, %3531) {node_name = "Div_2555-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3533 = "dtu_hlir.convert"(%3532) {node_name = "Cast_2556-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3534 = "dtu_hlir.convert"(%3533) {node_name = "Cast_2557-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3535 = dtu_hlir.constant  {node_name = "Constant_2558-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3536 = "dtu_hlir.unsqueeze"(%3524, %3535) {node_name = "Unsqueeze_2559-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3537 = dtu_hlir.constant  {node_name = "Constant_2560-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3538 = "dtu_hlir.unsqueeze"(%3527, %3537) {node_name = "Unsqueeze_2561-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3539 = dtu_hlir.constant  {node_name = "Constant_2562-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3540 = "dtu_hlir.unsqueeze"(%3534, %3539) {node_name = "Unsqueeze_2563-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3541 = "dtu_hlir.concatenate"(%3536, %3538, %829, %3540) {dimension = 0 : i64, node_name = "Concat_2564-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3542 = "dtu_hlir.dynamic_reshape"(%3449, %3541) {allowzero = 0 : i64, node_name = "Reshape_2565-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %3543 = "dtu_hlir.transpose"(%3542) {node_name = "Transpose_2566-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %3544 = dtu_hlir.constant  {node_name = "Constant_2567-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3545 = "dtu_hlir.mul"(%3524, %3544) {node_name = "Mul_2568-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3546 = dtu_hlir.constant  {node_name = "Constant_2569-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3547 = "dtu_hlir.div"(%3530, %3546) {node_name = "Div_2570-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3548 = "dtu_hlir.convert"(%3547) {node_name = "Cast_2571-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3549 = "dtu_hlir.convert"(%3548) {node_name = "Cast_2572-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3550 = dtu_hlir.constant  {node_name = "Constant_2573-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3551 = "dtu_hlir.unsqueeze"(%3545, %3550) {node_name = "Unsqueeze_2574-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3552 = dtu_hlir.constant  {node_name = "Constant_2575-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3553 = "dtu_hlir.unsqueeze"(%3527, %3552) {node_name = "Unsqueeze_2576-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3554 = dtu_hlir.constant  {node_name = "Constant_2577-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3555 = "dtu_hlir.unsqueeze"(%3549, %3554) {node_name = "Unsqueeze_2578-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3556 = "dtu_hlir.concatenate"(%3551, %3553, %3555) {dimension = 0 : i64, node_name = "Concat_2579-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3557 = "dtu_hlir.dynamic_reshape"(%3543, %3556) {allowzero = 0 : i64, node_name = "Reshape_2580-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %3558 = "dtu_hlir.shape"(%3485) {end = 2147483647 : i64, node_name = "Shape_2581-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3559 = dtu_hlir.constant  {node_name = "Constant_2582-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3560 = "dtu_hlir.gather"(%3558, %3559) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2583-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3561 = "dtu_hlir.shape"(%3485) {end = 2147483647 : i64, node_name = "Shape_2584-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3562 = dtu_hlir.constant  {node_name = "Constant_2585-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3563 = "dtu_hlir.gather"(%3561, %3562) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2586-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3564 = "dtu_hlir.shape"(%3521) {end = 2147483647 : i64, node_name = "Shape_2587-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<3xi64>
    %3565 = dtu_hlir.constant  {node_name = "Constant_2588-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3566 = "dtu_hlir.gather"(%3564, %3565) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2589-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3567 = dtu_hlir.constant  {node_name = "Constant_2590-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3568 = "dtu_hlir.unsqueeze"(%3560, %3567) {node_name = "Unsqueeze_2591-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3569 = dtu_hlir.constant  {node_name = "Constant_2592-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3570 = "dtu_hlir.unsqueeze"(%3563, %3569) {node_name = "Unsqueeze_2593-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3571 = dtu_hlir.constant  {node_name = "Constant_2594-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3572 = "dtu_hlir.unsqueeze"(%3566, %3571) {node_name = "Unsqueeze_2595-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3573 = "dtu_hlir.concatenate"(%3568, %3570, %3572) {dimension = 0 : i64, node_name = "Concat_2596-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3574 = dtu_hlir.constant  {node_name = "ConstantOfShape_2597-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %3575 = "dtu_hlir.dynamic_broadcast_in_dim"(%3574, %3573) {node_name = "ConstantOfShape_2597-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x77xf32>
    %3576 = "dtu_hlir.transpose"(%3521) {node_name = "Transpose_2598-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<40x64x77xf32>
    %3577 = "dtu_hlir.dot_general"(%3485, %3576) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2599-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x77xf32>) -> tensor<40x256x77xf32>
    %3578 = "dtu_hlir.broadcast_in_dim"(%828) {node_name = "Mul_2600-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %3579 = "dtu_hlir.mul"(%3577, %3578) {node_name = "Mul_2600-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %3580 = "dtu_hlir.broadcast_in_dim"(%827) {node_name = "Mul_2601-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %3581 = "dtu_hlir.mul"(%3575, %3580) {node_name = "Mul_2601-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %3582 = "dtu_hlir.add"(%3579, %3581) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2602-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %3583 = "dtu_hlir.softmax"(%3582) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2603-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %3584 = "dtu_hlir.convert"(%3583) {node_name = "Cast_2604-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %3585 = "dtu_hlir.dot_general"(%3584, %3557) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2605-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x77x64xf32>) -> tensor<40x256x64xf32>
    %3586 = "dtu_hlir.shape"(%3585) {end = 2147483647 : i64, node_name = "Shape_2606-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3587 = dtu_hlir.constant  {node_name = "Constant_2607-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3588 = "dtu_hlir.gather"(%3586, %3587) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2608-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3589 = "dtu_hlir.shape"(%3585) {end = 2147483647 : i64, node_name = "Shape_2609-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3590 = dtu_hlir.constant  {node_name = "Constant_2610-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3591 = "dtu_hlir.gather"(%3589, %3590) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2611-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3592 = "dtu_hlir.shape"(%3585) {end = 2147483647 : i64, node_name = "Shape_2612-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3593 = dtu_hlir.constant  {node_name = "Constant_2613-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3594 = "dtu_hlir.gather"(%3592, %3593) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2614-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3595 = dtu_hlir.constant  {node_name = "Constant_2615-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3596 = "dtu_hlir.div"(%3588, %3595) {node_name = "Div_2616-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3597 = "dtu_hlir.convert"(%3596) {node_name = "Cast_2617-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3598 = "dtu_hlir.convert"(%3597) {node_name = "Cast_2618-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3599 = dtu_hlir.constant  {node_name = "Constant_2619-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3600 = "dtu_hlir.unsqueeze"(%3598, %3599) {node_name = "Unsqueeze_2620-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3601 = dtu_hlir.constant  {node_name = "Constant_2621-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3602 = "dtu_hlir.unsqueeze"(%3591, %3601) {node_name = "Unsqueeze_2622-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3603 = dtu_hlir.constant  {node_name = "Constant_2623-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3604 = "dtu_hlir.unsqueeze"(%3594, %3603) {node_name = "Unsqueeze_2624-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3605 = "dtu_hlir.concatenate"(%3600, %826, %3602, %3604) {dimension = 0 : i64, node_name = "Concat_2625-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3606 = "dtu_hlir.dynamic_reshape"(%3585, %3605) {allowzero = 0 : i64, node_name = "Reshape_2626-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %3607 = "dtu_hlir.transpose"(%3606) {node_name = "Transpose_2627-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %3608 = dtu_hlir.constant  {node_name = "Constant_2628-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3609 = "dtu_hlir.div"(%3588, %3608) {node_name = "Div_2629-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3610 = "dtu_hlir.convert"(%3609) {node_name = "Cast_2630-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3611 = "dtu_hlir.convert"(%3610) {node_name = "Cast_2631-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3612 = dtu_hlir.constant  {node_name = "Constant_2632-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3613 = "dtu_hlir.mul"(%3594, %3612) {node_name = "Mul_2633-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3614 = dtu_hlir.constant  {node_name = "Constant_2634-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3615 = "dtu_hlir.unsqueeze"(%3611, %3614) {node_name = "Unsqueeze_2635-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3616 = dtu_hlir.constant  {node_name = "Constant_2636-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3617 = "dtu_hlir.unsqueeze"(%3591, %3616) {node_name = "Unsqueeze_2637-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3618 = dtu_hlir.constant  {node_name = "Constant_2638-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3619 = "dtu_hlir.unsqueeze"(%3613, %3618) {node_name = "Unsqueeze_2639-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3620 = "dtu_hlir.concatenate"(%3615, %3617, %3619) {dimension = 0 : i64, node_name = "Concat_2640-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3621 = "dtu_hlir.dynamic_reshape"(%3607, %3620) {allowzero = 0 : i64, node_name = "Reshape_2641-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %3622 = "dtu_hlir.dot_general"(%3621, %463) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2642-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3623 = "dtu_hlir.add"(%88, %3622) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2643-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3624 = "dtu_hlir.add"(%3623, %3419) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2644-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3625 = dtu_hlir.constant  {node_name = "ReduceMean_2645-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3626 = "dtu_hlir.reshape"(%3625) {node_name = "ReduceMean_2645-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3627 = "dtu_hlir.reduce"(%3624, %3626) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2645-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2645-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3628 = dtu_hlir.constant  {node_name = "ReduceMean_2645-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3629 = "dtu_hlir.unsqueeze"(%3627, %3628) {node_name = "ReduceMean_2645-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3630 = dtu_hlir.constant  {node_name = "ReduceMean_2645-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3631 = "dtu_hlir.broadcast_in_dim"(%3630) {node_name = "ReduceMean_2645-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3632 = "dtu_hlir.div"(%3629, %3631) {node_name = "ReduceMean_2645-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3633 = "dtu_hlir.sub"(%3624, %3632) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_2646-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3634 = dtu_hlir.constant  {node_name = "Constant_2647-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %3635 = "dtu_hlir.broadcast_in_dim"(%3634) {node_name = "Pow_2648-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %3636 = "dtu_hlir.pow"(%3633, %3635) {node_name = "Pow_2648-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3637 = dtu_hlir.constant  {node_name = "ReduceMean_2649-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3638 = "dtu_hlir.reshape"(%3637) {node_name = "ReduceMean_2649-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3639 = "dtu_hlir.reduce"(%3636, %3638) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2649-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2649-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3640 = dtu_hlir.constant  {node_name = "ReduceMean_2649-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3641 = "dtu_hlir.unsqueeze"(%3639, %3640) {node_name = "ReduceMean_2649-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3642 = dtu_hlir.constant  {node_name = "ReduceMean_2649-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3643 = "dtu_hlir.broadcast_in_dim"(%3642) {node_name = "ReduceMean_2649-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3644 = "dtu_hlir.div"(%3641, %3643) {node_name = "ReduceMean_2649-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3645 = dtu_hlir.constant  {node_name = "Constant_2650-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %3646 = "dtu_hlir.broadcast_in_dim"(%3645) {node_name = "Add_2651-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %3647 = "dtu_hlir.add"(%3644, %3646) {node_name = "Add_2651-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3648 = "dtu_hlir.sqrt"(%3647) {node_name = "Sqrt_2652-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3649 = "dtu_hlir.div"(%3633, %3648) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_2653-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3650 = "dtu_hlir.mul"(%3649, %93) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_2654-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3651 = "dtu_hlir.add"(%3650, %94) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2655-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3652 = "dtu_hlir.dot_general"(%3651, %464) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2656-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x10240xf32>) -> tensor<2x256x10240xf32>
    %3653 = "dtu_hlir.add"(%86, %3652) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2657-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10240xf32>, tensor<2x256x10240xf32>) -> tensor<2x256x10240xf32>
    %3654 = "dtu_hlir.shape"(%3653) {end = 2147483647 : i64, node_name = "Shape_2658-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>) -> tensor<3xi64>
    %3655 = dtu_hlir.constant  {node_name = "Constant_2659-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %3656 = "dtu_hlir.gather"(%3654, %3655) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2660-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3657 = dtu_hlir.constant  {node_name = "Constant_2661-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3658 = dtu_hlir.constant  {node_name = "Constant_2662-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %3659 = "dtu_hlir.add"(%3656, %3658) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_2663-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3660 = dtu_hlir.constant  {node_name = "Constant_2664-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %3661 = "dtu_hlir.div"(%3659, %3660) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_2665-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3662 = dtu_hlir.constant  {node_name = "Constant_2666-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %3663 = "dtu_hlir.mul"(%3661, %3662) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_2667-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3664 = dtu_hlir.constant  {node_name = "Slice_2668-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %3665 = "dtu_hlir.real_dynamic_slice"(%3653, %3657, %3663, %3664, %3655) {node_name = "Slice_2668-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %3666 = dtu_hlir.constant  {node_name = "Constant_2669-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %3667 = "dtu_hlir.mul"(%3661, %3666) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_2670-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %3668 = "dtu_hlir.shape"(%3663) {end = 2147483647 : i64, node_name = "Slice_2671-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %3669 = dtu_hlir.constant  {node_name = "Slice_2671-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %3670 = "dtu_hlir.dynamic_broadcast_in_dim"(%3669, %3668) {node_name = "Slice_2671-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3671 = "dtu_hlir.real_dynamic_slice"(%3653, %3663, %3667, %3670, %3655) {node_name = "Slice_2671-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %3672 = dtu_hlir.constant  {node_name = "Constant_2672-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %3673 = "dtu_hlir.broadcast_in_dim"(%3672) {node_name = "Div_2673-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %3674 = "dtu_hlir.div"(%3671, %3673) {node_name = "Div_2673-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %3675 = "dtu_hlir.erf"(%3674) {node_name = "Erf_2674-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %3676 = dtu_hlir.constant  {node_name = "Constant_2675-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %3677 = "dtu_hlir.broadcast_in_dim"(%3676) {node_name = "Add_2676-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %3678 = "dtu_hlir.add"(%3675, %3677) {node_name = "Add_2676-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %3679 = "dtu_hlir.mul"(%3671, %3678) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_2677-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %3680 = dtu_hlir.constant  {node_name = "Constant_2678-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %3681 = "dtu_hlir.broadcast_in_dim"(%3680) {node_name = "Mul_2679-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %3682 = "dtu_hlir.mul"(%3679, %3681) {node_name = "Mul_2679-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %3683 = "dtu_hlir.mul"(%3665, %3682) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_2680-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %3684 = "dtu_hlir.dot_general"(%3683, %465) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2681-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<5120x1280xf32>) -> tensor<2x256x1280xf32>
    %3685 = "dtu_hlir.add"(%87, %3684) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2682-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3686 = "dtu_hlir.add"(%3685, %3624) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2683-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3687 = "dtu_hlir.dot_general"(%3686, %466) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2684-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3688 = "dtu_hlir.add"(%95, %3687) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2685-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3689 = dtu_hlir.constant  {node_name = "Constant_2686-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3690 = "dtu_hlir.unsqueeze"(%3184, %3689) {node_name = "Unsqueeze_2687-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3691 = dtu_hlir.constant  {node_name = "Constant_2688-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3692 = "dtu_hlir.unsqueeze"(%3187, %3691) {node_name = "Unsqueeze_2689-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3693 = dtu_hlir.constant  {node_name = "Constant_2690-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3694 = "dtu_hlir.unsqueeze"(%3190, %3693) {node_name = "Unsqueeze_2691-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3695 = dtu_hlir.constant  {node_name = "Constant_2692-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3696 = "dtu_hlir.unsqueeze"(%3202, %3695) {node_name = "Unsqueeze_2693-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3697 = "dtu_hlir.concatenate"(%3690, %3692, %3694, %3696) {dimension = 0 : i64, node_name = "Concat_2694-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3698 = "dtu_hlir.dynamic_reshape"(%3688, %3697) {allowzero = 0 : i64, node_name = "Reshape_2695-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x16x16x1280xf32>
    %3699 = "dtu_hlir.transpose"(%3698) {node_name = "Transpose_2696-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>) -> tensor<2x1280x16x16xf32>
    %3700 = "dtu_hlir.add"(%3699, %3181) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_2697-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3701 = dtu_hlir.constant  {node_name = "Constant_2698-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %3702 = "dtu_hlir.dynamic_reshape"(%3700, %3701) {allowzero = 0 : i64, node_name = "Reshape_2699-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %3703 = dtu_hlir.constant  {node_name = "Constant_2700-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %3704 = dtu_hlir.constant  {node_name = "Constant_2701-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %3705 = "dtu_hlir.instance_norm"(%3702, %3703, %3704) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_2702-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %3706 = "dtu_hlir.shape"(%3700) {end = 2147483647 : i64, node_name = "Shape_2703-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3707 = "dtu_hlir.dynamic_reshape"(%3705, %3706) {allowzero = 0 : i64, node_name = "Reshape_2704-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %3708 = "dtu_hlir.mul"(%3707, %467) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_2705-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3709 = "dtu_hlir.add"(%3708, %468) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_2706-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3710 = "dtu_hlir.sigmoid"(%3709) {node_name = "Sigmoid_2707-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3711 = "dtu_hlir.mul"(%3709, %3710) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_2708-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3712 = "dtu_hlir.conv_bias"(%3711, %116, %117) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2709-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %3713 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_2710-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %3714 = "dtu_hlir.mul"(%915, %3713) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2711-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %3715 = "dtu_hlir.transpose"(%118) {node_name = "Gemm_2712-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %3716 = "dtu_hlir.gemm"(%3714, %3715, %119) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_2712-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %3717 = dtu_hlir.constant  {node_name = "Constant_2713-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %3718 = "dtu_hlir.unsqueeze"(%3716, %3717) {node_name = "Unsqueeze_2714-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %3719 = dtu_hlir.constant  {node_name = "Constant_2715-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %3720 = "dtu_hlir.unsqueeze"(%3718, %3719) {node_name = "Unsqueeze_2716-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %3721 = "dtu_hlir.add"(%3712, %3720) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_2717-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3722 = dtu_hlir.constant  {node_name = "Constant_2718-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %3723 = "dtu_hlir.dynamic_reshape"(%3721, %3722) {allowzero = 0 : i64, node_name = "Reshape_2719-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %3724 = dtu_hlir.constant  {node_name = "Constant_2720-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %3725 = dtu_hlir.constant  {node_name = "Constant_2721-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %3726 = "dtu_hlir.instance_norm"(%3723, %3724, %3725) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_2722-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %3727 = "dtu_hlir.shape"(%3721) {end = 2147483647 : i64, node_name = "Shape_2723-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3728 = "dtu_hlir.dynamic_reshape"(%3726, %3727) {allowzero = 0 : i64, node_name = "Reshape_2724-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %3729 = "dtu_hlir.mul"(%3728, %469) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_2725-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3730 = "dtu_hlir.add"(%3729, %470) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_2726-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3731 = "dtu_hlir.sigmoid"(%3730) {node_name = "Sigmoid_2727-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3732 = "dtu_hlir.mul"(%3730, %3731) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_2728-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3733 = "dtu_hlir.conv_bias"(%3732, %120, %121) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2729-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %3734 = "dtu_hlir.add"(%3700, %3733) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_2730-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3735 = dtu_hlir.constant  {node_name = "Constant_2731-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %3736 = "dtu_hlir.broadcast_in_dim"(%3735) {node_name = "Div_2732-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x16x16xf32>
    %3737 = "dtu_hlir.div"(%3734, %3736) {node_name = "Div_2732-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %3738 = "dtu_hlir.shape"(%3737) {end = 2147483647 : i64, node_name = "Shape_2733-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3739 = dtu_hlir.constant  {node_name = "Constant_2734-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3740 = "dtu_hlir.gather"(%3738, %3739) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2735-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3741 = "dtu_hlir.shape"(%3737) {end = 2147483647 : i64, node_name = "Shape_2736-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3742 = dtu_hlir.constant  {node_name = "Constant_2737-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3743 = "dtu_hlir.gather"(%3741, %3742) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2738-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3744 = "dtu_hlir.shape"(%3737) {end = 2147483647 : i64, node_name = "Shape_2739-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3745 = dtu_hlir.constant  {node_name = "Constant_2740-0", node_type = "Constant"} dense<3> : tensor<i64>
    %3746 = "dtu_hlir.gather"(%3744, %3745) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2741-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3747 = dtu_hlir.constant  {node_name = "Constant_2742-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %3748 = "dtu_hlir.dynamic_reshape"(%3737, %3747) {allowzero = 0 : i64, node_name = "Reshape_2743-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %3749 = dtu_hlir.constant  {node_name = "Constant_2744-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %3750 = dtu_hlir.constant  {node_name = "Constant_2745-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %3751 = "dtu_hlir.instance_norm"(%3748, %3749, %3750) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_2746-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %3752 = "dtu_hlir.shape"(%3737) {end = 2147483647 : i64, node_name = "Shape_2747-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3753 = "dtu_hlir.dynamic_reshape"(%3751, %3752) {allowzero = 0 : i64, node_name = "Reshape_2748-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %3754 = "dtu_hlir.mul"(%3753, %471) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_2749-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3755 = "dtu_hlir.add"(%3754, %472) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_2750-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %3756 = "dtu_hlir.shape"(%3755) {end = 2147483647 : i64, node_name = "Shape_2751-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %3757 = dtu_hlir.constant  {node_name = "Constant_2752-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3758 = "dtu_hlir.gather"(%3756, %3757) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2753-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %3759 = "dtu_hlir.transpose"(%3755) {node_name = "Transpose_2754-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x16x16x1280xf32>
    %3760 = "dtu_hlir.mul"(%3743, %3746) {node_name = "Mul_2755-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3761 = dtu_hlir.constant  {node_name = "Constant_2756-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3762 = "dtu_hlir.unsqueeze"(%3740, %3761) {node_name = "Unsqueeze_2757-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3763 = dtu_hlir.constant  {node_name = "Constant_2758-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3764 = "dtu_hlir.unsqueeze"(%3760, %3763) {node_name = "Unsqueeze_2759-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3765 = dtu_hlir.constant  {node_name = "Constant_2760-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3766 = "dtu_hlir.unsqueeze"(%3758, %3765) {node_name = "Unsqueeze_2761-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3767 = "dtu_hlir.concatenate"(%3762, %3764, %3766) {dimension = 0 : i64, node_name = "Concat_2762-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3768 = "dtu_hlir.dynamic_reshape"(%3759, %3767) {allowzero = 0 : i64, node_name = "Reshape_2763-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %3769 = "dtu_hlir.dot_general"(%3768, %473) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2764-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3770 = "dtu_hlir.add"(%96, %3769) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2765-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3771 = dtu_hlir.constant  {node_name = "ReduceMean_2766-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3772 = "dtu_hlir.reshape"(%3771) {node_name = "ReduceMean_2766-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3773 = "dtu_hlir.reduce"(%3770, %3772) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2766-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2766-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3774 = dtu_hlir.constant  {node_name = "ReduceMean_2766-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3775 = "dtu_hlir.unsqueeze"(%3773, %3774) {node_name = "ReduceMean_2766-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3776 = dtu_hlir.constant  {node_name = "ReduceMean_2766-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3777 = "dtu_hlir.broadcast_in_dim"(%3776) {node_name = "ReduceMean_2766-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3778 = "dtu_hlir.div"(%3775, %3777) {node_name = "ReduceMean_2766-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3779 = "dtu_hlir.sub"(%3770, %3778) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_2767-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3780 = dtu_hlir.constant  {node_name = "Constant_2768-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %3781 = "dtu_hlir.broadcast_in_dim"(%3780) {node_name = "Pow_2769-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %3782 = "dtu_hlir.pow"(%3779, %3781) {node_name = "Pow_2769-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3783 = dtu_hlir.constant  {node_name = "ReduceMean_2770-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3784 = "dtu_hlir.reshape"(%3783) {node_name = "ReduceMean_2770-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3785 = "dtu_hlir.reduce"(%3782, %3784) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2770-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2770-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3786 = dtu_hlir.constant  {node_name = "ReduceMean_2770-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3787 = "dtu_hlir.unsqueeze"(%3785, %3786) {node_name = "ReduceMean_2770-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3788 = dtu_hlir.constant  {node_name = "ReduceMean_2770-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3789 = "dtu_hlir.broadcast_in_dim"(%3788) {node_name = "ReduceMean_2770-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3790 = "dtu_hlir.div"(%3787, %3789) {node_name = "ReduceMean_2770-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3791 = dtu_hlir.constant  {node_name = "Constant_2771-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %3792 = "dtu_hlir.broadcast_in_dim"(%3791) {node_name = "Add_2772-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %3793 = "dtu_hlir.add"(%3790, %3792) {node_name = "Add_2772-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3794 = "dtu_hlir.sqrt"(%3793) {node_name = "Sqrt_2773-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3795 = "dtu_hlir.div"(%3779, %3794) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_2774-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3796 = "dtu_hlir.mul"(%3795, %101) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_2775-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3797 = "dtu_hlir.add"(%3796, %102) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2776-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %3798 = "dtu_hlir.dot_general"(%3797, %474) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2777-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3799 = "dtu_hlir.dot_general"(%3797, %475) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2778-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3800 = "dtu_hlir.dot_general"(%3797, %476) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2779-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3801 = "dtu_hlir.shape"(%3798) {end = 2147483647 : i64, node_name = "Shape_2780-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3802 = dtu_hlir.constant  {node_name = "Constant_2781-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3803 = "dtu_hlir.gather"(%3801, %3802) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2782-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3804 = "dtu_hlir.shape"(%3798) {end = 2147483647 : i64, node_name = "Shape_2783-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3805 = dtu_hlir.constant  {node_name = "Constant_2784-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3806 = "dtu_hlir.gather"(%3804, %3805) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2785-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3807 = "dtu_hlir.shape"(%3798) {end = 2147483647 : i64, node_name = "Shape_2786-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3808 = dtu_hlir.constant  {node_name = "Constant_2787-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3809 = "dtu_hlir.gather"(%3807, %3808) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2788-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3810 = dtu_hlir.constant  {node_name = "Constant_2789-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3811 = "dtu_hlir.div"(%3809, %3810) {node_name = "Div_2790-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3812 = "dtu_hlir.convert"(%3811) {node_name = "Cast_2791-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3813 = "dtu_hlir.convert"(%3812) {node_name = "Cast_2792-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3814 = dtu_hlir.constant  {node_name = "Constant_2793-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3815 = "dtu_hlir.unsqueeze"(%3803, %3814) {node_name = "Unsqueeze_2794-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3816 = dtu_hlir.constant  {node_name = "Constant_2795-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3817 = "dtu_hlir.unsqueeze"(%3806, %3816) {node_name = "Unsqueeze_2796-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3818 = dtu_hlir.constant  {node_name = "Constant_2797-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3819 = "dtu_hlir.unsqueeze"(%3813, %3818) {node_name = "Unsqueeze_2798-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3820 = "dtu_hlir.concatenate"(%3815, %3817, %825, %3819) {dimension = 0 : i64, node_name = "Concat_2799-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3821 = "dtu_hlir.dynamic_reshape"(%3798, %3820) {allowzero = 0 : i64, node_name = "Reshape_2800-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %3822 = "dtu_hlir.transpose"(%3821) {node_name = "Transpose_2801-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %3823 = dtu_hlir.constant  {node_name = "Constant_2802-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3824 = "dtu_hlir.mul"(%3803, %3823) {node_name = "Mul_2803-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3825 = dtu_hlir.constant  {node_name = "Constant_2804-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3826 = "dtu_hlir.div"(%3809, %3825) {node_name = "Div_2805-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3827 = "dtu_hlir.convert"(%3826) {node_name = "Cast_2806-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3828 = "dtu_hlir.convert"(%3827) {node_name = "Cast_2807-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3829 = dtu_hlir.constant  {node_name = "Constant_2808-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3830 = "dtu_hlir.unsqueeze"(%3824, %3829) {node_name = "Unsqueeze_2809-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3831 = dtu_hlir.constant  {node_name = "Constant_2810-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3832 = "dtu_hlir.unsqueeze"(%3806, %3831) {node_name = "Unsqueeze_2811-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3833 = dtu_hlir.constant  {node_name = "Constant_2812-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3834 = "dtu_hlir.unsqueeze"(%3828, %3833) {node_name = "Unsqueeze_2813-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3835 = "dtu_hlir.concatenate"(%3830, %3832, %3834) {dimension = 0 : i64, node_name = "Concat_2814-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3836 = "dtu_hlir.dynamic_reshape"(%3822, %3835) {allowzero = 0 : i64, node_name = "Reshape_2815-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %3837 = "dtu_hlir.shape"(%3799) {end = 2147483647 : i64, node_name = "Shape_2816-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3838 = dtu_hlir.constant  {node_name = "Constant_2817-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3839 = "dtu_hlir.gather"(%3837, %3838) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2818-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3840 = "dtu_hlir.shape"(%3799) {end = 2147483647 : i64, node_name = "Shape_2819-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3841 = dtu_hlir.constant  {node_name = "Constant_2820-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3842 = "dtu_hlir.gather"(%3840, %3841) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2821-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3843 = "dtu_hlir.shape"(%3799) {end = 2147483647 : i64, node_name = "Shape_2822-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3844 = dtu_hlir.constant  {node_name = "Constant_2823-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3845 = "dtu_hlir.gather"(%3843, %3844) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2824-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3846 = dtu_hlir.constant  {node_name = "Constant_2825-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3847 = "dtu_hlir.div"(%3845, %3846) {node_name = "Div_2826-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3848 = "dtu_hlir.convert"(%3847) {node_name = "Cast_2827-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3849 = "dtu_hlir.convert"(%3848) {node_name = "Cast_2828-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3850 = dtu_hlir.constant  {node_name = "Constant_2829-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3851 = "dtu_hlir.unsqueeze"(%3839, %3850) {node_name = "Unsqueeze_2830-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3852 = dtu_hlir.constant  {node_name = "Constant_2831-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3853 = "dtu_hlir.unsqueeze"(%3842, %3852) {node_name = "Unsqueeze_2832-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3854 = dtu_hlir.constant  {node_name = "Constant_2833-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3855 = "dtu_hlir.unsqueeze"(%3849, %3854) {node_name = "Unsqueeze_2834-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3856 = "dtu_hlir.concatenate"(%3851, %3853, %824, %3855) {dimension = 0 : i64, node_name = "Concat_2835-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3857 = "dtu_hlir.dynamic_reshape"(%3799, %3856) {allowzero = 0 : i64, node_name = "Reshape_2836-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %3858 = "dtu_hlir.transpose"(%3857) {node_name = "Transpose_2837-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %3859 = dtu_hlir.constant  {node_name = "Constant_2838-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3860 = "dtu_hlir.mul"(%3839, %3859) {node_name = "Mul_2839-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3861 = dtu_hlir.constant  {node_name = "Constant_2840-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3862 = "dtu_hlir.div"(%3845, %3861) {node_name = "Div_2841-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3863 = "dtu_hlir.convert"(%3862) {node_name = "Cast_2842-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3864 = "dtu_hlir.convert"(%3863) {node_name = "Cast_2843-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3865 = dtu_hlir.constant  {node_name = "Constant_2844-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3866 = "dtu_hlir.unsqueeze"(%3860, %3865) {node_name = "Unsqueeze_2845-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3867 = dtu_hlir.constant  {node_name = "Constant_2846-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3868 = "dtu_hlir.unsqueeze"(%3842, %3867) {node_name = "Unsqueeze_2847-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3869 = dtu_hlir.constant  {node_name = "Constant_2848-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3870 = "dtu_hlir.unsqueeze"(%3864, %3869) {node_name = "Unsqueeze_2849-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3871 = "dtu_hlir.concatenate"(%3866, %3868, %3870) {dimension = 0 : i64, node_name = "Concat_2850-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3872 = "dtu_hlir.dynamic_reshape"(%3858, %3871) {allowzero = 0 : i64, node_name = "Reshape_2851-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %3873 = "dtu_hlir.shape"(%3800) {end = 2147483647 : i64, node_name = "Shape_2852-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3874 = dtu_hlir.constant  {node_name = "Constant_2853-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3875 = "dtu_hlir.gather"(%3873, %3874) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2854-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3876 = "dtu_hlir.shape"(%3800) {end = 2147483647 : i64, node_name = "Shape_2855-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3877 = dtu_hlir.constant  {node_name = "Constant_2856-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3878 = "dtu_hlir.gather"(%3876, %3877) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2857-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3879 = "dtu_hlir.shape"(%3800) {end = 2147483647 : i64, node_name = "Shape_2858-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %3880 = dtu_hlir.constant  {node_name = "Constant_2859-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3881 = "dtu_hlir.gather"(%3879, %3880) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2860-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3882 = dtu_hlir.constant  {node_name = "Constant_2861-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3883 = "dtu_hlir.div"(%3881, %3882) {node_name = "Div_2862-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3884 = "dtu_hlir.convert"(%3883) {node_name = "Cast_2863-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3885 = "dtu_hlir.convert"(%3884) {node_name = "Cast_2864-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3886 = dtu_hlir.constant  {node_name = "Constant_2865-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3887 = "dtu_hlir.unsqueeze"(%3875, %3886) {node_name = "Unsqueeze_2866-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3888 = dtu_hlir.constant  {node_name = "Constant_2867-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3889 = "dtu_hlir.unsqueeze"(%3878, %3888) {node_name = "Unsqueeze_2868-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3890 = dtu_hlir.constant  {node_name = "Constant_2869-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3891 = "dtu_hlir.unsqueeze"(%3885, %3890) {node_name = "Unsqueeze_2870-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3892 = "dtu_hlir.concatenate"(%3887, %3889, %823, %3891) {dimension = 0 : i64, node_name = "Concat_2871-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3893 = "dtu_hlir.dynamic_reshape"(%3800, %3892) {allowzero = 0 : i64, node_name = "Reshape_2872-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %3894 = "dtu_hlir.transpose"(%3893) {node_name = "Transpose_2873-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %3895 = dtu_hlir.constant  {node_name = "Constant_2874-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3896 = "dtu_hlir.mul"(%3875, %3895) {node_name = "Mul_2875-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3897 = dtu_hlir.constant  {node_name = "Constant_2876-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3898 = "dtu_hlir.div"(%3881, %3897) {node_name = "Div_2877-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3899 = "dtu_hlir.convert"(%3898) {node_name = "Cast_2878-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3900 = "dtu_hlir.convert"(%3899) {node_name = "Cast_2879-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3901 = dtu_hlir.constant  {node_name = "Constant_2880-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3902 = "dtu_hlir.unsqueeze"(%3896, %3901) {node_name = "Unsqueeze_2881-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3903 = dtu_hlir.constant  {node_name = "Constant_2882-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3904 = "dtu_hlir.unsqueeze"(%3878, %3903) {node_name = "Unsqueeze_2883-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3905 = dtu_hlir.constant  {node_name = "Constant_2884-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3906 = "dtu_hlir.unsqueeze"(%3900, %3905) {node_name = "Unsqueeze_2885-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3907 = "dtu_hlir.concatenate"(%3902, %3904, %3906) {dimension = 0 : i64, node_name = "Concat_2886-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3908 = "dtu_hlir.dynamic_reshape"(%3894, %3907) {allowzero = 0 : i64, node_name = "Reshape_2887-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %3909 = "dtu_hlir.shape"(%3836) {end = 2147483647 : i64, node_name = "Shape_2888-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3910 = dtu_hlir.constant  {node_name = "Constant_2889-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3911 = "dtu_hlir.gather"(%3909, %3910) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2890-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3912 = "dtu_hlir.shape"(%3836) {end = 2147483647 : i64, node_name = "Shape_2891-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3913 = dtu_hlir.constant  {node_name = "Constant_2892-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3914 = "dtu_hlir.gather"(%3912, %3913) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2893-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3915 = "dtu_hlir.shape"(%3872) {end = 2147483647 : i64, node_name = "Shape_2894-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3916 = dtu_hlir.constant  {node_name = "Constant_2895-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3917 = "dtu_hlir.gather"(%3915, %3916) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2896-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3918 = dtu_hlir.constant  {node_name = "Constant_2897-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3919 = "dtu_hlir.unsqueeze"(%3911, %3918) {node_name = "Unsqueeze_2898-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3920 = dtu_hlir.constant  {node_name = "Constant_2899-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3921 = "dtu_hlir.unsqueeze"(%3914, %3920) {node_name = "Unsqueeze_2900-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3922 = dtu_hlir.constant  {node_name = "Constant_2901-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3923 = "dtu_hlir.unsqueeze"(%3917, %3922) {node_name = "Unsqueeze_2902-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3924 = "dtu_hlir.concatenate"(%3919, %3921, %3923) {dimension = 0 : i64, node_name = "Concat_2903-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3925 = dtu_hlir.constant  {node_name = "ConstantOfShape_2904-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %3926 = "dtu_hlir.dynamic_broadcast_in_dim"(%3925, %3924) {node_name = "ConstantOfShape_2904-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x256xf32>
    %3927 = "dtu_hlir.transpose"(%3872) {node_name = "Transpose_2905-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<40x64x256xf32>
    %3928 = "dtu_hlir.dot_general"(%3836, %3927) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2906-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x256xf32>) -> tensor<40x256x256xf32>
    %3929 = "dtu_hlir.broadcast_in_dim"(%822) {node_name = "Mul_2907-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %3930 = "dtu_hlir.mul"(%3928, %3929) {node_name = "Mul_2907-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3931 = "dtu_hlir.broadcast_in_dim"(%821) {node_name = "Mul_2908-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %3932 = "dtu_hlir.mul"(%3926, %3931) {node_name = "Mul_2908-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3933 = "dtu_hlir.add"(%3930, %3932) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2909-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3934 = "dtu_hlir.softmax"(%3933) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2910-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3935 = "dtu_hlir.convert"(%3934) {node_name = "Cast_2911-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %3936 = "dtu_hlir.dot_general"(%3935, %3908) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2912-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x64xf32>) -> tensor<40x256x64xf32>
    %3937 = "dtu_hlir.shape"(%3936) {end = 2147483647 : i64, node_name = "Shape_2913-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3938 = dtu_hlir.constant  {node_name = "Constant_2914-0", node_type = "Constant"} dense<0> : tensor<i64>
    %3939 = "dtu_hlir.gather"(%3937, %3938) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2915-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3940 = "dtu_hlir.shape"(%3936) {end = 2147483647 : i64, node_name = "Shape_2916-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3941 = dtu_hlir.constant  {node_name = "Constant_2917-0", node_type = "Constant"} dense<1> : tensor<i64>
    %3942 = "dtu_hlir.gather"(%3940, %3941) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2918-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3943 = "dtu_hlir.shape"(%3936) {end = 2147483647 : i64, node_name = "Shape_2919-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %3944 = dtu_hlir.constant  {node_name = "Constant_2920-0", node_type = "Constant"} dense<2> : tensor<i64>
    %3945 = "dtu_hlir.gather"(%3943, %3944) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2921-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %3946 = dtu_hlir.constant  {node_name = "Constant_2922-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3947 = "dtu_hlir.div"(%3939, %3946) {node_name = "Div_2923-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3948 = "dtu_hlir.convert"(%3947) {node_name = "Cast_2924-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3949 = "dtu_hlir.convert"(%3948) {node_name = "Cast_2925-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3950 = dtu_hlir.constant  {node_name = "Constant_2926-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3951 = "dtu_hlir.unsqueeze"(%3949, %3950) {node_name = "Unsqueeze_2927-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3952 = dtu_hlir.constant  {node_name = "Constant_2928-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3953 = "dtu_hlir.unsqueeze"(%3942, %3952) {node_name = "Unsqueeze_2929-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3954 = dtu_hlir.constant  {node_name = "Constant_2930-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3955 = "dtu_hlir.unsqueeze"(%3945, %3954) {node_name = "Unsqueeze_2931-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3956 = "dtu_hlir.concatenate"(%3951, %820, %3953, %3955) {dimension = 0 : i64, node_name = "Concat_2932-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %3957 = "dtu_hlir.dynamic_reshape"(%3936, %3956) {allowzero = 0 : i64, node_name = "Reshape_2933-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %3958 = "dtu_hlir.transpose"(%3957) {node_name = "Transpose_2934-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %3959 = dtu_hlir.constant  {node_name = "Constant_2935-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3960 = "dtu_hlir.div"(%3939, %3959) {node_name = "Div_2936-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3961 = "dtu_hlir.convert"(%3960) {node_name = "Cast_2937-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3962 = "dtu_hlir.convert"(%3961) {node_name = "Cast_2938-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %3963 = dtu_hlir.constant  {node_name = "Constant_2939-0", node_type = "Constant"} dense<20> : tensor<i64>
    %3964 = "dtu_hlir.mul"(%3945, %3963) {node_name = "Mul_2940-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %3965 = dtu_hlir.constant  {node_name = "Constant_2941-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3966 = "dtu_hlir.unsqueeze"(%3962, %3965) {node_name = "Unsqueeze_2942-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3967 = dtu_hlir.constant  {node_name = "Constant_2943-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3968 = "dtu_hlir.unsqueeze"(%3942, %3967) {node_name = "Unsqueeze_2944-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3969 = dtu_hlir.constant  {node_name = "Constant_2945-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %3970 = "dtu_hlir.unsqueeze"(%3964, %3969) {node_name = "Unsqueeze_2946-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %3971 = "dtu_hlir.concatenate"(%3966, %3968, %3970) {dimension = 0 : i64, node_name = "Concat_2947-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %3972 = "dtu_hlir.dynamic_reshape"(%3958, %3971) {allowzero = 0 : i64, node_name = "Reshape_2948-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %3973 = "dtu_hlir.dot_general"(%3972, %477) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2949-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %3974 = "dtu_hlir.add"(%97, %3973) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2950-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3975 = "dtu_hlir.add"(%3974, %3770) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_2951-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3976 = dtu_hlir.constant  {node_name = "ReduceMean_2952-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3977 = "dtu_hlir.reshape"(%3976) {node_name = "ReduceMean_2952-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3978 = "dtu_hlir.reduce"(%3975, %3977) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2952-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2952-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3979 = dtu_hlir.constant  {node_name = "ReduceMean_2952-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3980 = "dtu_hlir.unsqueeze"(%3978, %3979) {node_name = "ReduceMean_2952-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3981 = dtu_hlir.constant  {node_name = "ReduceMean_2952-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3982 = "dtu_hlir.broadcast_in_dim"(%3981) {node_name = "ReduceMean_2952-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3983 = "dtu_hlir.div"(%3980, %3982) {node_name = "ReduceMean_2952-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3984 = "dtu_hlir.sub"(%3975, %3983) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_2953-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %3985 = dtu_hlir.constant  {node_name = "Constant_2954-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %3986 = "dtu_hlir.broadcast_in_dim"(%3985) {node_name = "Pow_2955-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %3987 = "dtu_hlir.pow"(%3984, %3986) {node_name = "Pow_2955-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %3988 = dtu_hlir.constant  {node_name = "ReduceMean_2956-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %3989 = "dtu_hlir.reshape"(%3988) {node_name = "ReduceMean_2956-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %3990 = "dtu_hlir.reduce"(%3987, %3989) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_2956-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_2956-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %3991 = dtu_hlir.constant  {node_name = "ReduceMean_2956-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %3992 = "dtu_hlir.unsqueeze"(%3990, %3991) {node_name = "ReduceMean_2956-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %3993 = dtu_hlir.constant  {node_name = "ReduceMean_2956-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %3994 = "dtu_hlir.broadcast_in_dim"(%3993) {node_name = "ReduceMean_2956-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %3995 = "dtu_hlir.div"(%3992, %3994) {node_name = "ReduceMean_2956-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3996 = dtu_hlir.constant  {node_name = "Constant_2957-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %3997 = "dtu_hlir.broadcast_in_dim"(%3996) {node_name = "Add_2958-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %3998 = "dtu_hlir.add"(%3995, %3997) {node_name = "Add_2958-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %3999 = "dtu_hlir.sqrt"(%3998) {node_name = "Sqrt_2959-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %4000 = "dtu_hlir.div"(%3984, %3999) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_2960-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %4001 = "dtu_hlir.mul"(%4000, %103) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_2961-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %4002 = "dtu_hlir.add"(%4001, %104) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_2962-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %4003 = "dtu_hlir.dot_general"(%4002, %478) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2963-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %4004 = "dtu_hlir.dot_general"(%arg2, %479) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2964-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %4005 = "dtu_hlir.dot_general"(%arg2, %480) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_2965-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %4006 = "dtu_hlir.shape"(%4003) {end = 2147483647 : i64, node_name = "Shape_2966-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %4007 = dtu_hlir.constant  {node_name = "Constant_2967-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4008 = "dtu_hlir.gather"(%4006, %4007) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2968-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4009 = "dtu_hlir.shape"(%4003) {end = 2147483647 : i64, node_name = "Shape_2969-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %4010 = dtu_hlir.constant  {node_name = "Constant_2970-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4011 = "dtu_hlir.gather"(%4009, %4010) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2971-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4012 = "dtu_hlir.shape"(%4003) {end = 2147483647 : i64, node_name = "Shape_2972-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %4013 = dtu_hlir.constant  {node_name = "Constant_2973-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4014 = "dtu_hlir.gather"(%4012, %4013) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_2974-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4015 = dtu_hlir.constant  {node_name = "Constant_2975-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4016 = "dtu_hlir.div"(%4014, %4015) {node_name = "Div_2976-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4017 = "dtu_hlir.convert"(%4016) {node_name = "Cast_2977-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4018 = "dtu_hlir.convert"(%4017) {node_name = "Cast_2978-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4019 = dtu_hlir.constant  {node_name = "Constant_2979-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4020 = "dtu_hlir.unsqueeze"(%4008, %4019) {node_name = "Unsqueeze_2980-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4021 = dtu_hlir.constant  {node_name = "Constant_2981-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4022 = "dtu_hlir.unsqueeze"(%4011, %4021) {node_name = "Unsqueeze_2982-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4023 = dtu_hlir.constant  {node_name = "Constant_2983-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4024 = "dtu_hlir.unsqueeze"(%4018, %4023) {node_name = "Unsqueeze_2984-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4025 = "dtu_hlir.concatenate"(%4020, %4022, %819, %4024) {dimension = 0 : i64, node_name = "Concat_2985-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4026 = "dtu_hlir.dynamic_reshape"(%4003, %4025) {allowzero = 0 : i64, node_name = "Reshape_2986-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %4027 = "dtu_hlir.transpose"(%4026) {node_name = "Transpose_2987-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %4028 = dtu_hlir.constant  {node_name = "Constant_2988-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4029 = "dtu_hlir.mul"(%4008, %4028) {node_name = "Mul_2989-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4030 = dtu_hlir.constant  {node_name = "Constant_2990-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4031 = "dtu_hlir.div"(%4014, %4030) {node_name = "Div_2991-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4032 = "dtu_hlir.convert"(%4031) {node_name = "Cast_2992-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4033 = "dtu_hlir.convert"(%4032) {node_name = "Cast_2993-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4034 = dtu_hlir.constant  {node_name = "Constant_2994-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4035 = "dtu_hlir.unsqueeze"(%4029, %4034) {node_name = "Unsqueeze_2995-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4036 = dtu_hlir.constant  {node_name = "Constant_2996-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4037 = "dtu_hlir.unsqueeze"(%4011, %4036) {node_name = "Unsqueeze_2997-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4038 = dtu_hlir.constant  {node_name = "Constant_2998-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4039 = "dtu_hlir.unsqueeze"(%4033, %4038) {node_name = "Unsqueeze_2999-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4040 = "dtu_hlir.concatenate"(%4035, %4037, %4039) {dimension = 0 : i64, node_name = "Concat_3000-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4041 = "dtu_hlir.dynamic_reshape"(%4027, %4040) {allowzero = 0 : i64, node_name = "Reshape_3001-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %4042 = "dtu_hlir.shape"(%4004) {end = 2147483647 : i64, node_name = "Shape_3002-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4043 = dtu_hlir.constant  {node_name = "Constant_3003-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4044 = "dtu_hlir.gather"(%4042, %4043) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3004-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4045 = "dtu_hlir.shape"(%4004) {end = 2147483647 : i64, node_name = "Shape_3005-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4046 = dtu_hlir.constant  {node_name = "Constant_3006-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4047 = "dtu_hlir.gather"(%4045, %4046) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3007-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4048 = "dtu_hlir.shape"(%4004) {end = 2147483647 : i64, node_name = "Shape_3008-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4049 = dtu_hlir.constant  {node_name = "Constant_3009-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4050 = "dtu_hlir.gather"(%4048, %4049) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3010-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4051 = dtu_hlir.constant  {node_name = "Constant_3011-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4052 = "dtu_hlir.div"(%4050, %4051) {node_name = "Div_3012-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4053 = "dtu_hlir.convert"(%4052) {node_name = "Cast_3013-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4054 = "dtu_hlir.convert"(%4053) {node_name = "Cast_3014-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4055 = dtu_hlir.constant  {node_name = "Constant_3015-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4056 = "dtu_hlir.unsqueeze"(%4044, %4055) {node_name = "Unsqueeze_3016-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4057 = dtu_hlir.constant  {node_name = "Constant_3017-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4058 = "dtu_hlir.unsqueeze"(%4047, %4057) {node_name = "Unsqueeze_3018-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4059 = dtu_hlir.constant  {node_name = "Constant_3019-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4060 = "dtu_hlir.unsqueeze"(%4054, %4059) {node_name = "Unsqueeze_3020-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4061 = "dtu_hlir.concatenate"(%4056, %4058, %818, %4060) {dimension = 0 : i64, node_name = "Concat_3021-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4062 = "dtu_hlir.dynamic_reshape"(%4004, %4061) {allowzero = 0 : i64, node_name = "Reshape_3022-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %4063 = "dtu_hlir.transpose"(%4062) {node_name = "Transpose_3023-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %4064 = dtu_hlir.constant  {node_name = "Constant_3024-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4065 = "dtu_hlir.mul"(%4044, %4064) {node_name = "Mul_3025-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4066 = dtu_hlir.constant  {node_name = "Constant_3026-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4067 = "dtu_hlir.div"(%4050, %4066) {node_name = "Div_3027-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4068 = "dtu_hlir.convert"(%4067) {node_name = "Cast_3028-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4069 = "dtu_hlir.convert"(%4068) {node_name = "Cast_3029-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4070 = dtu_hlir.constant  {node_name = "Constant_3030-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4071 = "dtu_hlir.unsqueeze"(%4065, %4070) {node_name = "Unsqueeze_3031-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4072 = dtu_hlir.constant  {node_name = "Constant_3032-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4073 = "dtu_hlir.unsqueeze"(%4047, %4072) {node_name = "Unsqueeze_3033-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4074 = dtu_hlir.constant  {node_name = "Constant_3034-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4075 = "dtu_hlir.unsqueeze"(%4069, %4074) {node_name = "Unsqueeze_3035-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4076 = "dtu_hlir.concatenate"(%4071, %4073, %4075) {dimension = 0 : i64, node_name = "Concat_3036-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4077 = "dtu_hlir.dynamic_reshape"(%4063, %4076) {allowzero = 0 : i64, node_name = "Reshape_3037-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %4078 = "dtu_hlir.shape"(%4005) {end = 2147483647 : i64, node_name = "Shape_3038-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4079 = dtu_hlir.constant  {node_name = "Constant_3039-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4080 = "dtu_hlir.gather"(%4078, %4079) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3040-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4081 = "dtu_hlir.shape"(%4005) {end = 2147483647 : i64, node_name = "Shape_3041-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4082 = dtu_hlir.constant  {node_name = "Constant_3042-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4083 = "dtu_hlir.gather"(%4081, %4082) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3043-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4084 = "dtu_hlir.shape"(%4005) {end = 2147483647 : i64, node_name = "Shape_3044-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4085 = dtu_hlir.constant  {node_name = "Constant_3045-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4086 = "dtu_hlir.gather"(%4084, %4085) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3046-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4087 = dtu_hlir.constant  {node_name = "Constant_3047-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4088 = "dtu_hlir.div"(%4086, %4087) {node_name = "Div_3048-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4089 = "dtu_hlir.convert"(%4088) {node_name = "Cast_3049-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4090 = "dtu_hlir.convert"(%4089) {node_name = "Cast_3050-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4091 = dtu_hlir.constant  {node_name = "Constant_3051-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4092 = "dtu_hlir.unsqueeze"(%4080, %4091) {node_name = "Unsqueeze_3052-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4093 = dtu_hlir.constant  {node_name = "Constant_3053-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4094 = "dtu_hlir.unsqueeze"(%4083, %4093) {node_name = "Unsqueeze_3054-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4095 = dtu_hlir.constant  {node_name = "Constant_3055-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4096 = "dtu_hlir.unsqueeze"(%4090, %4095) {node_name = "Unsqueeze_3056-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4097 = "dtu_hlir.concatenate"(%4092, %4094, %817, %4096) {dimension = 0 : i64, node_name = "Concat_3057-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4098 = "dtu_hlir.dynamic_reshape"(%4005, %4097) {allowzero = 0 : i64, node_name = "Reshape_3058-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %4099 = "dtu_hlir.transpose"(%4098) {node_name = "Transpose_3059-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %4100 = dtu_hlir.constant  {node_name = "Constant_3060-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4101 = "dtu_hlir.mul"(%4080, %4100) {node_name = "Mul_3061-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4102 = dtu_hlir.constant  {node_name = "Constant_3062-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4103 = "dtu_hlir.div"(%4086, %4102) {node_name = "Div_3063-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4104 = "dtu_hlir.convert"(%4103) {node_name = "Cast_3064-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4105 = "dtu_hlir.convert"(%4104) {node_name = "Cast_3065-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4106 = dtu_hlir.constant  {node_name = "Constant_3066-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4107 = "dtu_hlir.unsqueeze"(%4101, %4106) {node_name = "Unsqueeze_3067-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4108 = dtu_hlir.constant  {node_name = "Constant_3068-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4109 = "dtu_hlir.unsqueeze"(%4083, %4108) {node_name = "Unsqueeze_3069-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4110 = dtu_hlir.constant  {node_name = "Constant_3070-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4111 = "dtu_hlir.unsqueeze"(%4105, %4110) {node_name = "Unsqueeze_3071-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4112 = "dtu_hlir.concatenate"(%4107, %4109, %4111) {dimension = 0 : i64, node_name = "Concat_3072-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4113 = "dtu_hlir.dynamic_reshape"(%4099, %4112) {allowzero = 0 : i64, node_name = "Reshape_3073-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %4114 = "dtu_hlir.shape"(%4041) {end = 2147483647 : i64, node_name = "Shape_3074-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %4115 = dtu_hlir.constant  {node_name = "Constant_3075-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4116 = "dtu_hlir.gather"(%4114, %4115) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3076-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4117 = "dtu_hlir.shape"(%4041) {end = 2147483647 : i64, node_name = "Shape_3077-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %4118 = dtu_hlir.constant  {node_name = "Constant_3078-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4119 = "dtu_hlir.gather"(%4117, %4118) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3079-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4120 = "dtu_hlir.shape"(%4077) {end = 2147483647 : i64, node_name = "Shape_3080-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<3xi64>
    %4121 = dtu_hlir.constant  {node_name = "Constant_3081-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4122 = "dtu_hlir.gather"(%4120, %4121) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3082-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4123 = dtu_hlir.constant  {node_name = "Constant_3083-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4124 = "dtu_hlir.unsqueeze"(%4116, %4123) {node_name = "Unsqueeze_3084-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4125 = dtu_hlir.constant  {node_name = "Constant_3085-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4126 = "dtu_hlir.unsqueeze"(%4119, %4125) {node_name = "Unsqueeze_3086-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4127 = dtu_hlir.constant  {node_name = "Constant_3087-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4128 = "dtu_hlir.unsqueeze"(%4122, %4127) {node_name = "Unsqueeze_3088-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4129 = "dtu_hlir.concatenate"(%4124, %4126, %4128) {dimension = 0 : i64, node_name = "Concat_3089-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4130 = dtu_hlir.constant  {node_name = "ConstantOfShape_3090-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %4131 = "dtu_hlir.dynamic_broadcast_in_dim"(%4130, %4129) {node_name = "ConstantOfShape_3090-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x77xf32>
    %4132 = "dtu_hlir.transpose"(%4077) {node_name = "Transpose_3091-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<40x64x77xf32>
    %4133 = "dtu_hlir.dot_general"(%4041, %4132) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3092-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x77xf32>) -> tensor<40x256x77xf32>
    %4134 = "dtu_hlir.broadcast_in_dim"(%816) {node_name = "Mul_3093-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %4135 = "dtu_hlir.mul"(%4133, %4134) {node_name = "Mul_3093-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %4136 = "dtu_hlir.broadcast_in_dim"(%815) {node_name = "Mul_3094-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %4137 = "dtu_hlir.mul"(%4131, %4136) {node_name = "Mul_3094-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %4138 = "dtu_hlir.add"(%4135, %4137) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3095-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %4139 = "dtu_hlir.softmax"(%4138) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_3096-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %4140 = "dtu_hlir.convert"(%4139) {node_name = "Cast_3097-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %4141 = "dtu_hlir.dot_general"(%4140, %4113) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3098-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x77x64xf32>) -> tensor<40x256x64xf32>
    %4142 = "dtu_hlir.shape"(%4141) {end = 2147483647 : i64, node_name = "Shape_3099-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %4143 = dtu_hlir.constant  {node_name = "Constant_3100-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4144 = "dtu_hlir.gather"(%4142, %4143) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3101-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4145 = "dtu_hlir.shape"(%4141) {end = 2147483647 : i64, node_name = "Shape_3102-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %4146 = dtu_hlir.constant  {node_name = "Constant_3103-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4147 = "dtu_hlir.gather"(%4145, %4146) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3104-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4148 = "dtu_hlir.shape"(%4141) {end = 2147483647 : i64, node_name = "Shape_3105-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %4149 = dtu_hlir.constant  {node_name = "Constant_3106-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4150 = "dtu_hlir.gather"(%4148, %4149) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3107-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4151 = dtu_hlir.constant  {node_name = "Constant_3108-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4152 = "dtu_hlir.div"(%4144, %4151) {node_name = "Div_3109-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4153 = "dtu_hlir.convert"(%4152) {node_name = "Cast_3110-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4154 = "dtu_hlir.convert"(%4153) {node_name = "Cast_3111-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4155 = dtu_hlir.constant  {node_name = "Constant_3112-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4156 = "dtu_hlir.unsqueeze"(%4154, %4155) {node_name = "Unsqueeze_3113-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4157 = dtu_hlir.constant  {node_name = "Constant_3114-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4158 = "dtu_hlir.unsqueeze"(%4147, %4157) {node_name = "Unsqueeze_3115-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4159 = dtu_hlir.constant  {node_name = "Constant_3116-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4160 = "dtu_hlir.unsqueeze"(%4150, %4159) {node_name = "Unsqueeze_3117-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4161 = "dtu_hlir.concatenate"(%4156, %814, %4158, %4160) {dimension = 0 : i64, node_name = "Concat_3118-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4162 = "dtu_hlir.dynamic_reshape"(%4141, %4161) {allowzero = 0 : i64, node_name = "Reshape_3119-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %4163 = "dtu_hlir.transpose"(%4162) {node_name = "Transpose_3120-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %4164 = dtu_hlir.constant  {node_name = "Constant_3121-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4165 = "dtu_hlir.div"(%4144, %4164) {node_name = "Div_3122-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4166 = "dtu_hlir.convert"(%4165) {node_name = "Cast_3123-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4167 = "dtu_hlir.convert"(%4166) {node_name = "Cast_3124-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4168 = dtu_hlir.constant  {node_name = "Constant_3125-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4169 = "dtu_hlir.mul"(%4150, %4168) {node_name = "Mul_3126-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4170 = dtu_hlir.constant  {node_name = "Constant_3127-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4171 = "dtu_hlir.unsqueeze"(%4167, %4170) {node_name = "Unsqueeze_3128-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4172 = dtu_hlir.constant  {node_name = "Constant_3129-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4173 = "dtu_hlir.unsqueeze"(%4147, %4172) {node_name = "Unsqueeze_3130-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4174 = dtu_hlir.constant  {node_name = "Constant_3131-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4175 = "dtu_hlir.unsqueeze"(%4169, %4174) {node_name = "Unsqueeze_3132-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4176 = "dtu_hlir.concatenate"(%4171, %4173, %4175) {dimension = 0 : i64, node_name = "Concat_3133-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4177 = "dtu_hlir.dynamic_reshape"(%4163, %4176) {allowzero = 0 : i64, node_name = "Reshape_3134-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %4178 = "dtu_hlir.dot_general"(%4177, %481) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3135-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %4179 = "dtu_hlir.add"(%100, %4178) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3136-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %4180 = "dtu_hlir.add"(%4179, %3975) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3137-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %4181 = dtu_hlir.constant  {node_name = "ReduceMean_3138-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4182 = "dtu_hlir.reshape"(%4181) {node_name = "ReduceMean_3138-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4183 = "dtu_hlir.reduce"(%4180, %4182) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3138-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3138-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %4184 = dtu_hlir.constant  {node_name = "ReduceMean_3138-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4185 = "dtu_hlir.unsqueeze"(%4183, %4184) {node_name = "ReduceMean_3138-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %4186 = dtu_hlir.constant  {node_name = "ReduceMean_3138-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4187 = "dtu_hlir.broadcast_in_dim"(%4186) {node_name = "ReduceMean_3138-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %4188 = "dtu_hlir.div"(%4185, %4187) {node_name = "ReduceMean_3138-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %4189 = "dtu_hlir.sub"(%4180, %4188) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_3139-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %4190 = dtu_hlir.constant  {node_name = "Constant_3140-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %4191 = "dtu_hlir.broadcast_in_dim"(%4190) {node_name = "Pow_3141-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %4192 = "dtu_hlir.pow"(%4189, %4191) {node_name = "Pow_3141-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %4193 = dtu_hlir.constant  {node_name = "ReduceMean_3142-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4194 = "dtu_hlir.reshape"(%4193) {node_name = "ReduceMean_3142-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4195 = "dtu_hlir.reduce"(%4192, %4194) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3142-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3142-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %4196 = dtu_hlir.constant  {node_name = "ReduceMean_3142-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4197 = "dtu_hlir.unsqueeze"(%4195, %4196) {node_name = "ReduceMean_3142-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %4198 = dtu_hlir.constant  {node_name = "ReduceMean_3142-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4199 = "dtu_hlir.broadcast_in_dim"(%4198) {node_name = "ReduceMean_3142-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %4200 = "dtu_hlir.div"(%4197, %4199) {node_name = "ReduceMean_3142-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %4201 = dtu_hlir.constant  {node_name = "Constant_3143-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %4202 = "dtu_hlir.broadcast_in_dim"(%4201) {node_name = "Add_3144-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %4203 = "dtu_hlir.add"(%4200, %4202) {node_name = "Add_3144-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %4204 = "dtu_hlir.sqrt"(%4203) {node_name = "Sqrt_3145-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %4205 = "dtu_hlir.div"(%4189, %4204) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_3146-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %4206 = "dtu_hlir.mul"(%4205, %105) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_3147-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %4207 = "dtu_hlir.add"(%4206, %106) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3148-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %4208 = "dtu_hlir.dot_general"(%4207, %482) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3149-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x10240xf32>) -> tensor<2x256x10240xf32>
    %4209 = "dtu_hlir.add"(%98, %4208) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3150-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10240xf32>, tensor<2x256x10240xf32>) -> tensor<2x256x10240xf32>
    %4210 = "dtu_hlir.shape"(%4209) {end = 2147483647 : i64, node_name = "Shape_3151-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>) -> tensor<3xi64>
    %4211 = dtu_hlir.constant  {node_name = "Constant_3152-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %4212 = "dtu_hlir.gather"(%4210, %4211) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3153-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4213 = dtu_hlir.constant  {node_name = "Constant_3154-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4214 = dtu_hlir.constant  {node_name = "Constant_3155-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %4215 = "dtu_hlir.add"(%4212, %4214) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_3156-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4216 = dtu_hlir.constant  {node_name = "Constant_3157-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4217 = "dtu_hlir.div"(%4215, %4216) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_3158-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4218 = dtu_hlir.constant  {node_name = "Constant_3159-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %4219 = "dtu_hlir.mul"(%4217, %4218) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_3160-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4220 = dtu_hlir.constant  {node_name = "Slice_3161-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %4221 = "dtu_hlir.real_dynamic_slice"(%4209, %4213, %4219, %4220, %4211) {node_name = "Slice_3161-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %4222 = dtu_hlir.constant  {node_name = "Constant_3162-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4223 = "dtu_hlir.mul"(%4217, %4222) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_3163-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4224 = "dtu_hlir.shape"(%4219) {end = 2147483647 : i64, node_name = "Slice_3164-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %4225 = dtu_hlir.constant  {node_name = "Slice_3164-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %4226 = "dtu_hlir.dynamic_broadcast_in_dim"(%4225, %4224) {node_name = "Slice_3164-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4227 = "dtu_hlir.real_dynamic_slice"(%4209, %4219, %4223, %4226, %4211) {node_name = "Slice_3164-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %4228 = dtu_hlir.constant  {node_name = "Constant_3165-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %4229 = "dtu_hlir.broadcast_in_dim"(%4228) {node_name = "Div_3166-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %4230 = "dtu_hlir.div"(%4227, %4229) {node_name = "Div_3166-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %4231 = "dtu_hlir.erf"(%4230) {node_name = "Erf_3167-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %4232 = dtu_hlir.constant  {node_name = "Constant_3168-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %4233 = "dtu_hlir.broadcast_in_dim"(%4232) {node_name = "Add_3169-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %4234 = "dtu_hlir.add"(%4231, %4233) {node_name = "Add_3169-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %4235 = "dtu_hlir.mul"(%4227, %4234) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_3170-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %4236 = dtu_hlir.constant  {node_name = "Constant_3171-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %4237 = "dtu_hlir.broadcast_in_dim"(%4236) {node_name = "Mul_3172-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %4238 = "dtu_hlir.mul"(%4235, %4237) {node_name = "Mul_3172-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %4239 = "dtu_hlir.mul"(%4221, %4238) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_3173-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %4240 = "dtu_hlir.dot_general"(%4239, %483) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3174-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<5120x1280xf32>) -> tensor<2x256x1280xf32>
    %4241 = "dtu_hlir.add"(%99, %4240) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3175-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %4242 = "dtu_hlir.add"(%4241, %4180) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3176-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %4243 = "dtu_hlir.dot_general"(%4242, %484) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3177-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %4244 = "dtu_hlir.add"(%107, %4243) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3178-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %4245 = dtu_hlir.constant  {node_name = "Constant_3179-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4246 = "dtu_hlir.unsqueeze"(%3740, %4245) {node_name = "Unsqueeze_3180-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4247 = dtu_hlir.constant  {node_name = "Constant_3181-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4248 = "dtu_hlir.unsqueeze"(%3743, %4247) {node_name = "Unsqueeze_3182-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4249 = dtu_hlir.constant  {node_name = "Constant_3183-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4250 = "dtu_hlir.unsqueeze"(%3746, %4249) {node_name = "Unsqueeze_3184-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4251 = dtu_hlir.constant  {node_name = "Constant_3185-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4252 = "dtu_hlir.unsqueeze"(%3758, %4251) {node_name = "Unsqueeze_3186-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4253 = "dtu_hlir.concatenate"(%4246, %4248, %4250, %4252) {dimension = 0 : i64, node_name = "Concat_3187-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4254 = "dtu_hlir.dynamic_reshape"(%4244, %4253) {allowzero = 0 : i64, node_name = "Reshape_3188-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x16x16x1280xf32>
    %4255 = "dtu_hlir.transpose"(%4254) {node_name = "Transpose_3189-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>) -> tensor<2x1280x16x16xf32>
    %4256 = "dtu_hlir.add"(%4255, %3737) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3190-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %4257 = "dtu_hlir.conv_bias"(%4256, %122, %123) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3191-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<2> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4258 = dtu_hlir.constant  {node_name = "Constant_3192-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4259 = "dtu_hlir.dynamic_reshape"(%4257, %4258) {allowzero = 0 : i64, node_name = "Reshape_3193-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4260 = dtu_hlir.constant  {node_name = "Constant_3194-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4261 = dtu_hlir.constant  {node_name = "Constant_3195-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4262 = "dtu_hlir.instance_norm"(%4259, %4260, %4261) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3196-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4263 = "dtu_hlir.shape"(%4257) {end = 2147483647 : i64, node_name = "Shape_3197-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4264 = "dtu_hlir.dynamic_reshape"(%4262, %4263) {allowzero = 0 : i64, node_name = "Reshape_3198-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4265 = "dtu_hlir.mul"(%4264, %485) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3199-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4266 = "dtu_hlir.add"(%4265, %486) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3200-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4267 = "dtu_hlir.sigmoid"(%4266) {node_name = "Sigmoid_3201-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4268 = "dtu_hlir.mul"(%4266, %4267) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3202-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4269 = "dtu_hlir.conv_bias"(%4268, %124, %125) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3203-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4270 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3204-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4271 = "dtu_hlir.mul"(%915, %4270) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3205-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4272 = "dtu_hlir.transpose"(%126) {node_name = "Gemm_3206-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %4273 = "dtu_hlir.gemm"(%4271, %4272, %127) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3206-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %4274 = dtu_hlir.constant  {node_name = "Constant_3207-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4275 = "dtu_hlir.unsqueeze"(%4273, %4274) {node_name = "Unsqueeze_3208-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %4276 = dtu_hlir.constant  {node_name = "Constant_3209-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %4277 = "dtu_hlir.unsqueeze"(%4275, %4276) {node_name = "Unsqueeze_3210-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %4278 = "dtu_hlir.add"(%4269, %4277) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3211-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4279 = dtu_hlir.constant  {node_name = "Constant_3212-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4280 = "dtu_hlir.dynamic_reshape"(%4278, %4279) {allowzero = 0 : i64, node_name = "Reshape_3213-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4281 = dtu_hlir.constant  {node_name = "Constant_3214-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4282 = dtu_hlir.constant  {node_name = "Constant_3215-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4283 = "dtu_hlir.instance_norm"(%4280, %4281, %4282) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3216-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4284 = "dtu_hlir.shape"(%4278) {end = 2147483647 : i64, node_name = "Shape_3217-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4285 = "dtu_hlir.dynamic_reshape"(%4283, %4284) {allowzero = 0 : i64, node_name = "Reshape_3218-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4286 = "dtu_hlir.mul"(%4285, %487) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3219-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4287 = "dtu_hlir.add"(%4286, %488) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3220-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4288 = "dtu_hlir.sigmoid"(%4287) {node_name = "Sigmoid_3221-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4289 = "dtu_hlir.mul"(%4287, %4288) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3222-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4290 = "dtu_hlir.conv_bias"(%4289, %128, %129) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3223-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4291 = "dtu_hlir.add"(%4257, %4290) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3224-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4292 = dtu_hlir.constant  {node_name = "Constant_3225-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %4293 = "dtu_hlir.broadcast_in_dim"(%4292) {node_name = "Div_3226-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x8x8xf32>
    %4294 = "dtu_hlir.div"(%4291, %4293) {node_name = "Div_3226-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4295 = dtu_hlir.constant  {node_name = "Constant_3227-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4296 = "dtu_hlir.dynamic_reshape"(%4294, %4295) {allowzero = 0 : i64, node_name = "Reshape_3228-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4297 = dtu_hlir.constant  {node_name = "Constant_3229-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4298 = dtu_hlir.constant  {node_name = "Constant_3230-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4299 = "dtu_hlir.instance_norm"(%4296, %4297, %4298) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3231-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4300 = "dtu_hlir.shape"(%4294) {end = 2147483647 : i64, node_name = "Shape_3232-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4301 = "dtu_hlir.dynamic_reshape"(%4299, %4300) {allowzero = 0 : i64, node_name = "Reshape_3233-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4302 = "dtu_hlir.mul"(%4301, %489) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3234-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4303 = "dtu_hlir.add"(%4302, %490) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3235-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4304 = "dtu_hlir.sigmoid"(%4303) {node_name = "Sigmoid_3236-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4305 = "dtu_hlir.mul"(%4303, %4304) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3237-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4306 = "dtu_hlir.conv_bias"(%4305, %130, %131) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3238-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4307 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3239-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4308 = "dtu_hlir.mul"(%915, %4307) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3240-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4309 = "dtu_hlir.transpose"(%132) {node_name = "Gemm_3241-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %4310 = "dtu_hlir.gemm"(%4308, %4309, %133) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3241-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %4311 = dtu_hlir.constant  {node_name = "Constant_3242-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4312 = "dtu_hlir.unsqueeze"(%4310, %4311) {node_name = "Unsqueeze_3243-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %4313 = dtu_hlir.constant  {node_name = "Constant_3244-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %4314 = "dtu_hlir.unsqueeze"(%4312, %4313) {node_name = "Unsqueeze_3245-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %4315 = "dtu_hlir.add"(%4306, %4314) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3246-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4316 = dtu_hlir.constant  {node_name = "Constant_3247-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4317 = "dtu_hlir.dynamic_reshape"(%4315, %4316) {allowzero = 0 : i64, node_name = "Reshape_3248-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4318 = dtu_hlir.constant  {node_name = "Constant_3249-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4319 = dtu_hlir.constant  {node_name = "Constant_3250-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4320 = "dtu_hlir.instance_norm"(%4317, %4318, %4319) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3251-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4321 = "dtu_hlir.shape"(%4315) {end = 2147483647 : i64, node_name = "Shape_3252-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4322 = "dtu_hlir.dynamic_reshape"(%4320, %4321) {allowzero = 0 : i64, node_name = "Reshape_3253-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4323 = "dtu_hlir.mul"(%4322, %491) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3254-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4324 = "dtu_hlir.add"(%4323, %492) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3255-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4325 = "dtu_hlir.sigmoid"(%4324) {node_name = "Sigmoid_3256-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4326 = "dtu_hlir.mul"(%4324, %4325) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3257-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4327 = "dtu_hlir.conv_bias"(%4326, %134, %135) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3258-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4328 = "dtu_hlir.add"(%4294, %4327) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3259-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4329 = dtu_hlir.constant  {node_name = "Constant_3260-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %4330 = "dtu_hlir.broadcast_in_dim"(%4329) {node_name = "Div_3261-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x8x8xf32>
    %4331 = "dtu_hlir.div"(%4328, %4330) {node_name = "Div_3261-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4332 = dtu_hlir.constant  {node_name = "Constant_3262-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4333 = "dtu_hlir.dynamic_reshape"(%4331, %4332) {allowzero = 0 : i64, node_name = "Reshape_3263-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4334 = dtu_hlir.constant  {node_name = "Constant_3264-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4335 = dtu_hlir.constant  {node_name = "Constant_3265-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4336 = "dtu_hlir.instance_norm"(%4333, %4334, %4335) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3266-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4337 = "dtu_hlir.shape"(%4331) {end = 2147483647 : i64, node_name = "Shape_3267-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4338 = "dtu_hlir.dynamic_reshape"(%4336, %4337) {allowzero = 0 : i64, node_name = "Reshape_3268-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4339 = "dtu_hlir.mul"(%4338, %493) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3269-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4340 = "dtu_hlir.add"(%4339, %494) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3270-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4341 = "dtu_hlir.sigmoid"(%4340) {node_name = "Sigmoid_3271-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4342 = "dtu_hlir.mul"(%4340, %4341) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3272-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4343 = "dtu_hlir.conv_bias"(%4342, %358, %359) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3273-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4344 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3274-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4345 = "dtu_hlir.mul"(%915, %4344) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3275-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4346 = "dtu_hlir.transpose"(%360) {node_name = "Gemm_3276-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %4347 = "dtu_hlir.gemm"(%4345, %4346, %361) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3276-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %4348 = dtu_hlir.constant  {node_name = "Constant_3277-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4349 = "dtu_hlir.unsqueeze"(%4347, %4348) {node_name = "Unsqueeze_3278-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %4350 = dtu_hlir.constant  {node_name = "Constant_3279-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %4351 = "dtu_hlir.unsqueeze"(%4349, %4350) {node_name = "Unsqueeze_3280-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %4352 = "dtu_hlir.add"(%4343, %4351) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3281-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4353 = dtu_hlir.constant  {node_name = "Constant_3282-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4354 = "dtu_hlir.dynamic_reshape"(%4352, %4353) {allowzero = 0 : i64, node_name = "Reshape_3283-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4355 = dtu_hlir.constant  {node_name = "Constant_3284-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4356 = dtu_hlir.constant  {node_name = "Constant_3285-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4357 = "dtu_hlir.instance_norm"(%4354, %4355, %4356) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3286-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4358 = "dtu_hlir.shape"(%4352) {end = 2147483647 : i64, node_name = "Shape_3287-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4359 = "dtu_hlir.dynamic_reshape"(%4357, %4358) {allowzero = 0 : i64, node_name = "Reshape_3288-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4360 = "dtu_hlir.mul"(%4359, %495) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3289-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4361 = "dtu_hlir.add"(%4360, %496) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3290-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4362 = "dtu_hlir.sigmoid"(%4361) {node_name = "Sigmoid_3291-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4363 = "dtu_hlir.mul"(%4361, %4362) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3292-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4364 = "dtu_hlir.conv_bias"(%4363, %362, %363) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3293-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4365 = "dtu_hlir.add"(%4331, %4364) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3294-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4366 = dtu_hlir.constant  {node_name = "Constant_3295-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %4367 = "dtu_hlir.broadcast_in_dim"(%4366) {node_name = "Div_3296-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x8x8xf32>
    %4368 = "dtu_hlir.div"(%4365, %4367) {node_name = "Div_3296-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4369 = "dtu_hlir.shape"(%4368) {end = 2147483647 : i64, node_name = "Shape_3297-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4370 = dtu_hlir.constant  {node_name = "Constant_3298-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4371 = "dtu_hlir.gather"(%4369, %4370) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3299-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %4372 = "dtu_hlir.shape"(%4368) {end = 2147483647 : i64, node_name = "Shape_3300-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4373 = dtu_hlir.constant  {node_name = "Constant_3301-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4374 = "dtu_hlir.gather"(%4372, %4373) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3302-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %4375 = "dtu_hlir.shape"(%4368) {end = 2147483647 : i64, node_name = "Shape_3303-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4376 = dtu_hlir.constant  {node_name = "Constant_3304-0", node_type = "Constant"} dense<3> : tensor<i64>
    %4377 = "dtu_hlir.gather"(%4375, %4376) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3305-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %4378 = dtu_hlir.constant  {node_name = "Constant_3306-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4379 = "dtu_hlir.dynamic_reshape"(%4368, %4378) {allowzero = 0 : i64, node_name = "Reshape_3307-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4380 = dtu_hlir.constant  {node_name = "Constant_3308-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4381 = dtu_hlir.constant  {node_name = "Constant_3309-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4382 = "dtu_hlir.instance_norm"(%4379, %4380, %4381) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3310-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4383 = "dtu_hlir.shape"(%4368) {end = 2147483647 : i64, node_name = "Shape_3311-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4384 = "dtu_hlir.dynamic_reshape"(%4382, %4383) {allowzero = 0 : i64, node_name = "Reshape_3312-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4385 = "dtu_hlir.mul"(%4384, %497) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3313-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4386 = "dtu_hlir.add"(%4385, %498) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3314-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4387 = "dtu_hlir.shape"(%4386) {end = 2147483647 : i64, node_name = "Shape_3315-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4388 = dtu_hlir.constant  {node_name = "Constant_3316-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4389 = "dtu_hlir.gather"(%4387, %4388) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3317-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %4390 = "dtu_hlir.transpose"(%4386) {node_name = "Transpose_3318-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x8x8x1280xf32>
    %4391 = "dtu_hlir.mul"(%4374, %4377) {node_name = "Mul_3319-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4392 = dtu_hlir.constant  {node_name = "Constant_3320-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4393 = "dtu_hlir.unsqueeze"(%4371, %4392) {node_name = "Unsqueeze_3321-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4394 = dtu_hlir.constant  {node_name = "Constant_3322-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4395 = "dtu_hlir.unsqueeze"(%4391, %4394) {node_name = "Unsqueeze_3323-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4396 = dtu_hlir.constant  {node_name = "Constant_3324-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4397 = "dtu_hlir.unsqueeze"(%4389, %4396) {node_name = "Unsqueeze_3325-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4398 = "dtu_hlir.concatenate"(%4393, %4395, %4397) {dimension = 0 : i64, node_name = "Concat_3326-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4399 = "dtu_hlir.dynamic_reshape"(%4390, %4398) {allowzero = 0 : i64, node_name = "Reshape_3327-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x8x8x1280xf32>, tensor<3xi64>) -> tensor<2x64x1280xf32>
    %4400 = "dtu_hlir.dot_general"(%4399, %499) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3328-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4401 = "dtu_hlir.add"(%346, %4400) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3329-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4402 = dtu_hlir.constant  {node_name = "ReduceMean_3330-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4403 = "dtu_hlir.reshape"(%4402) {node_name = "ReduceMean_3330-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4404 = "dtu_hlir.reduce"(%4401, %4403) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3330-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3330-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<f32>) -> tensor<2x64xf32>
    %4405 = dtu_hlir.constant  {node_name = "ReduceMean_3330-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4406 = "dtu_hlir.unsqueeze"(%4404, %4405) {node_name = "ReduceMean_3330-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64xf32>, tensor<1xi64>) -> tensor<2x64x1xf32>
    %4407 = dtu_hlir.constant  {node_name = "ReduceMean_3330-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4408 = "dtu_hlir.broadcast_in_dim"(%4407) {node_name = "ReduceMean_3330-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x64x1xf32>
    %4409 = "dtu_hlir.div"(%4406, %4408) {node_name = "ReduceMean_3330-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4410 = "dtu_hlir.sub"(%4401, %4409) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_3331-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1280xf32>
    %4411 = dtu_hlir.constant  {node_name = "Constant_3332-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %4412 = "dtu_hlir.broadcast_in_dim"(%4411) {node_name = "Pow_3333-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x1280xf32>
    %4413 = "dtu_hlir.pow"(%4410, %4412) {node_name = "Pow_3333-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4414 = dtu_hlir.constant  {node_name = "ReduceMean_3334-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4415 = "dtu_hlir.reshape"(%4414) {node_name = "ReduceMean_3334-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4416 = "dtu_hlir.reduce"(%4413, %4415) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3334-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3334-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<f32>) -> tensor<2x64xf32>
    %4417 = dtu_hlir.constant  {node_name = "ReduceMean_3334-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4418 = "dtu_hlir.unsqueeze"(%4416, %4417) {node_name = "ReduceMean_3334-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64xf32>, tensor<1xi64>) -> tensor<2x64x1xf32>
    %4419 = dtu_hlir.constant  {node_name = "ReduceMean_3334-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4420 = "dtu_hlir.broadcast_in_dim"(%4419) {node_name = "ReduceMean_3334-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x64x1xf32>
    %4421 = "dtu_hlir.div"(%4418, %4420) {node_name = "ReduceMean_3334-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4422 = dtu_hlir.constant  {node_name = "Constant_3335-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %4423 = "dtu_hlir.broadcast_in_dim"(%4422) {node_name = "Add_3336-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x1xf32>
    %4424 = "dtu_hlir.add"(%4421, %4423) {node_name = "Add_3336-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4425 = "dtu_hlir.sqrt"(%4424) {node_name = "Sqrt_3337-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4426 = "dtu_hlir.div"(%4410, %4425) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_3338-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1280xf32>
    %4427 = "dtu_hlir.mul"(%4426, %351) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_3339-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf32>
    %4428 = "dtu_hlir.add"(%4427, %352) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3340-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf32>
    %4429 = "dtu_hlir.dot_general"(%4428, %500) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3341-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4430 = "dtu_hlir.dot_general"(%4428, %501) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3342-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4431 = "dtu_hlir.dot_general"(%4428, %502) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3343-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4432 = "dtu_hlir.shape"(%4429) {end = 2147483647 : i64, node_name = "Shape_3344-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4433 = dtu_hlir.constant  {node_name = "Constant_3345-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4434 = "dtu_hlir.gather"(%4432, %4433) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3346-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4435 = "dtu_hlir.shape"(%4429) {end = 2147483647 : i64, node_name = "Shape_3347-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4436 = dtu_hlir.constant  {node_name = "Constant_3348-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4437 = "dtu_hlir.gather"(%4435, %4436) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3349-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4438 = "dtu_hlir.shape"(%4429) {end = 2147483647 : i64, node_name = "Shape_3350-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4439 = dtu_hlir.constant  {node_name = "Constant_3351-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4440 = "dtu_hlir.gather"(%4438, %4439) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3352-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4441 = dtu_hlir.constant  {node_name = "Constant_3353-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4442 = "dtu_hlir.div"(%4440, %4441) {node_name = "Div_3354-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4443 = "dtu_hlir.convert"(%4442) {node_name = "Cast_3355-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4444 = "dtu_hlir.convert"(%4443) {node_name = "Cast_3356-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4445 = dtu_hlir.constant  {node_name = "Constant_3357-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4446 = "dtu_hlir.unsqueeze"(%4434, %4445) {node_name = "Unsqueeze_3358-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4447 = dtu_hlir.constant  {node_name = "Constant_3359-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4448 = "dtu_hlir.unsqueeze"(%4437, %4447) {node_name = "Unsqueeze_3360-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4449 = dtu_hlir.constant  {node_name = "Constant_3361-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4450 = "dtu_hlir.unsqueeze"(%4444, %4449) {node_name = "Unsqueeze_3362-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4451 = "dtu_hlir.concatenate"(%4446, %4448, %813, %4450) {dimension = 0 : i64, node_name = "Concat_3363-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4452 = "dtu_hlir.dynamic_reshape"(%4429, %4451) {allowzero = 0 : i64, node_name = "Reshape_3364-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<4xi64>) -> tensor<2x64x20x64xf32>
    %4453 = "dtu_hlir.transpose"(%4452) {node_name = "Transpose_3365-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x20x64xf32>) -> tensor<2x20x64x64xf32>
    %4454 = dtu_hlir.constant  {node_name = "Constant_3366-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4455 = "dtu_hlir.mul"(%4434, %4454) {node_name = "Mul_3367-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4456 = dtu_hlir.constant  {node_name = "Constant_3368-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4457 = "dtu_hlir.div"(%4440, %4456) {node_name = "Div_3369-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4458 = "dtu_hlir.convert"(%4457) {node_name = "Cast_3370-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4459 = "dtu_hlir.convert"(%4458) {node_name = "Cast_3371-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4460 = dtu_hlir.constant  {node_name = "Constant_3372-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4461 = "dtu_hlir.unsqueeze"(%4455, %4460) {node_name = "Unsqueeze_3373-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4462 = dtu_hlir.constant  {node_name = "Constant_3374-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4463 = "dtu_hlir.unsqueeze"(%4437, %4462) {node_name = "Unsqueeze_3375-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4464 = dtu_hlir.constant  {node_name = "Constant_3376-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4465 = "dtu_hlir.unsqueeze"(%4459, %4464) {node_name = "Unsqueeze_3377-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4466 = "dtu_hlir.concatenate"(%4461, %4463, %4465) {dimension = 0 : i64, node_name = "Concat_3378-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4467 = "dtu_hlir.dynamic_reshape"(%4453, %4466) {allowzero = 0 : i64, node_name = "Reshape_3379-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x64x64xf32>, tensor<3xi64>) -> tensor<40x64x64xf32>
    %4468 = "dtu_hlir.shape"(%4430) {end = 2147483647 : i64, node_name = "Shape_3380-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4469 = dtu_hlir.constant  {node_name = "Constant_3381-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4470 = "dtu_hlir.gather"(%4468, %4469) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3382-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4471 = "dtu_hlir.shape"(%4430) {end = 2147483647 : i64, node_name = "Shape_3383-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4472 = dtu_hlir.constant  {node_name = "Constant_3384-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4473 = "dtu_hlir.gather"(%4471, %4472) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3385-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4474 = "dtu_hlir.shape"(%4430) {end = 2147483647 : i64, node_name = "Shape_3386-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4475 = dtu_hlir.constant  {node_name = "Constant_3387-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4476 = "dtu_hlir.gather"(%4474, %4475) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3388-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4477 = dtu_hlir.constant  {node_name = "Constant_3389-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4478 = "dtu_hlir.div"(%4476, %4477) {node_name = "Div_3390-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4479 = "dtu_hlir.convert"(%4478) {node_name = "Cast_3391-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4480 = "dtu_hlir.convert"(%4479) {node_name = "Cast_3392-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4481 = dtu_hlir.constant  {node_name = "Constant_3393-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4482 = "dtu_hlir.unsqueeze"(%4470, %4481) {node_name = "Unsqueeze_3394-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4483 = dtu_hlir.constant  {node_name = "Constant_3395-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4484 = "dtu_hlir.unsqueeze"(%4473, %4483) {node_name = "Unsqueeze_3396-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4485 = dtu_hlir.constant  {node_name = "Constant_3397-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4486 = "dtu_hlir.unsqueeze"(%4480, %4485) {node_name = "Unsqueeze_3398-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4487 = "dtu_hlir.concatenate"(%4482, %4484, %812, %4486) {dimension = 0 : i64, node_name = "Concat_3399-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4488 = "dtu_hlir.dynamic_reshape"(%4430, %4487) {allowzero = 0 : i64, node_name = "Reshape_3400-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<4xi64>) -> tensor<2x64x20x64xf32>
    %4489 = "dtu_hlir.transpose"(%4488) {node_name = "Transpose_3401-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x20x64xf32>) -> tensor<2x20x64x64xf32>
    %4490 = dtu_hlir.constant  {node_name = "Constant_3402-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4491 = "dtu_hlir.mul"(%4470, %4490) {node_name = "Mul_3403-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4492 = dtu_hlir.constant  {node_name = "Constant_3404-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4493 = "dtu_hlir.div"(%4476, %4492) {node_name = "Div_3405-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4494 = "dtu_hlir.convert"(%4493) {node_name = "Cast_3406-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4495 = "dtu_hlir.convert"(%4494) {node_name = "Cast_3407-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4496 = dtu_hlir.constant  {node_name = "Constant_3408-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4497 = "dtu_hlir.unsqueeze"(%4491, %4496) {node_name = "Unsqueeze_3409-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4498 = dtu_hlir.constant  {node_name = "Constant_3410-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4499 = "dtu_hlir.unsqueeze"(%4473, %4498) {node_name = "Unsqueeze_3411-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4500 = dtu_hlir.constant  {node_name = "Constant_3412-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4501 = "dtu_hlir.unsqueeze"(%4495, %4500) {node_name = "Unsqueeze_3413-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4502 = "dtu_hlir.concatenate"(%4497, %4499, %4501) {dimension = 0 : i64, node_name = "Concat_3414-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4503 = "dtu_hlir.dynamic_reshape"(%4489, %4502) {allowzero = 0 : i64, node_name = "Reshape_3415-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x64x64xf32>, tensor<3xi64>) -> tensor<40x64x64xf32>
    %4504 = "dtu_hlir.shape"(%4431) {end = 2147483647 : i64, node_name = "Shape_3416-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4505 = dtu_hlir.constant  {node_name = "Constant_3417-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4506 = "dtu_hlir.gather"(%4504, %4505) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3418-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4507 = "dtu_hlir.shape"(%4431) {end = 2147483647 : i64, node_name = "Shape_3419-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4508 = dtu_hlir.constant  {node_name = "Constant_3420-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4509 = "dtu_hlir.gather"(%4507, %4508) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3421-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4510 = "dtu_hlir.shape"(%4431) {end = 2147483647 : i64, node_name = "Shape_3422-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4511 = dtu_hlir.constant  {node_name = "Constant_3423-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4512 = "dtu_hlir.gather"(%4510, %4511) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3424-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4513 = dtu_hlir.constant  {node_name = "Constant_3425-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4514 = "dtu_hlir.div"(%4512, %4513) {node_name = "Div_3426-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4515 = "dtu_hlir.convert"(%4514) {node_name = "Cast_3427-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4516 = "dtu_hlir.convert"(%4515) {node_name = "Cast_3428-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4517 = dtu_hlir.constant  {node_name = "Constant_3429-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4518 = "dtu_hlir.unsqueeze"(%4506, %4517) {node_name = "Unsqueeze_3430-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4519 = dtu_hlir.constant  {node_name = "Constant_3431-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4520 = "dtu_hlir.unsqueeze"(%4509, %4519) {node_name = "Unsqueeze_3432-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4521 = dtu_hlir.constant  {node_name = "Constant_3433-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4522 = "dtu_hlir.unsqueeze"(%4516, %4521) {node_name = "Unsqueeze_3434-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4523 = "dtu_hlir.concatenate"(%4518, %4520, %811, %4522) {dimension = 0 : i64, node_name = "Concat_3435-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4524 = "dtu_hlir.dynamic_reshape"(%4431, %4523) {allowzero = 0 : i64, node_name = "Reshape_3436-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<4xi64>) -> tensor<2x64x20x64xf32>
    %4525 = "dtu_hlir.transpose"(%4524) {node_name = "Transpose_3437-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x20x64xf32>) -> tensor<2x20x64x64xf32>
    %4526 = dtu_hlir.constant  {node_name = "Constant_3438-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4527 = "dtu_hlir.mul"(%4506, %4526) {node_name = "Mul_3439-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4528 = dtu_hlir.constant  {node_name = "Constant_3440-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4529 = "dtu_hlir.div"(%4512, %4528) {node_name = "Div_3441-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4530 = "dtu_hlir.convert"(%4529) {node_name = "Cast_3442-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4531 = "dtu_hlir.convert"(%4530) {node_name = "Cast_3443-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4532 = dtu_hlir.constant  {node_name = "Constant_3444-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4533 = "dtu_hlir.unsqueeze"(%4527, %4532) {node_name = "Unsqueeze_3445-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4534 = dtu_hlir.constant  {node_name = "Constant_3446-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4535 = "dtu_hlir.unsqueeze"(%4509, %4534) {node_name = "Unsqueeze_3447-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4536 = dtu_hlir.constant  {node_name = "Constant_3448-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4537 = "dtu_hlir.unsqueeze"(%4531, %4536) {node_name = "Unsqueeze_3449-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4538 = "dtu_hlir.concatenate"(%4533, %4535, %4537) {dimension = 0 : i64, node_name = "Concat_3450-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4539 = "dtu_hlir.dynamic_reshape"(%4525, %4538) {allowzero = 0 : i64, node_name = "Reshape_3451-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x64x64xf32>, tensor<3xi64>) -> tensor<40x64x64xf32>
    %4540 = "dtu_hlir.shape"(%4467) {end = 2147483647 : i64, node_name = "Shape_3452-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4541 = dtu_hlir.constant  {node_name = "Constant_3453-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4542 = "dtu_hlir.gather"(%4540, %4541) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3454-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4543 = "dtu_hlir.shape"(%4467) {end = 2147483647 : i64, node_name = "Shape_3455-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4544 = dtu_hlir.constant  {node_name = "Constant_3456-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4545 = "dtu_hlir.gather"(%4543, %4544) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3457-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4546 = "dtu_hlir.shape"(%4503) {end = 2147483647 : i64, node_name = "Shape_3458-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4547 = dtu_hlir.constant  {node_name = "Constant_3459-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4548 = "dtu_hlir.gather"(%4546, %4547) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3460-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4549 = dtu_hlir.constant  {node_name = "Constant_3461-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4550 = "dtu_hlir.unsqueeze"(%4542, %4549) {node_name = "Unsqueeze_3462-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4551 = dtu_hlir.constant  {node_name = "Constant_3463-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4552 = "dtu_hlir.unsqueeze"(%4545, %4551) {node_name = "Unsqueeze_3464-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4553 = dtu_hlir.constant  {node_name = "Constant_3465-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4554 = "dtu_hlir.unsqueeze"(%4548, %4553) {node_name = "Unsqueeze_3466-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4555 = "dtu_hlir.concatenate"(%4550, %4552, %4554) {dimension = 0 : i64, node_name = "Concat_3467-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4556 = dtu_hlir.constant  {node_name = "ConstantOfShape_3468-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %4557 = "dtu_hlir.dynamic_broadcast_in_dim"(%4556, %4555) {node_name = "ConstantOfShape_3468-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x64x64xf32>
    %4558 = "dtu_hlir.transpose"(%4503) {node_name = "Transpose_3469-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4559 = "dtu_hlir.dot_general"(%4467, %4558) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3470-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4560 = "dtu_hlir.broadcast_in_dim"(%810) {node_name = "Mul_3471-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x64x64xf32>
    %4561 = "dtu_hlir.mul"(%4559, %4560) {node_name = "Mul_3471-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4562 = "dtu_hlir.broadcast_in_dim"(%809) {node_name = "Mul_3472-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x64x64xf32>
    %4563 = "dtu_hlir.mul"(%4557, %4562) {node_name = "Mul_3472-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4564 = "dtu_hlir.add"(%4561, %4563) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3473-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4565 = "dtu_hlir.softmax"(%4564) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_3474-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4566 = "dtu_hlir.convert"(%4565) {node_name = "Cast_3475-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4567 = "dtu_hlir.dot_general"(%4566, %4539) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3476-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<40x64x64xf32>) -> tensor<40x64x64xf32>
    %4568 = "dtu_hlir.shape"(%4567) {end = 2147483647 : i64, node_name = "Shape_3477-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4569 = dtu_hlir.constant  {node_name = "Constant_3478-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4570 = "dtu_hlir.gather"(%4568, %4569) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3479-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4571 = "dtu_hlir.shape"(%4567) {end = 2147483647 : i64, node_name = "Shape_3480-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4572 = dtu_hlir.constant  {node_name = "Constant_3481-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4573 = "dtu_hlir.gather"(%4571, %4572) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3482-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4574 = "dtu_hlir.shape"(%4567) {end = 2147483647 : i64, node_name = "Shape_3483-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4575 = dtu_hlir.constant  {node_name = "Constant_3484-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4576 = "dtu_hlir.gather"(%4574, %4575) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3485-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4577 = dtu_hlir.constant  {node_name = "Constant_3486-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4578 = "dtu_hlir.div"(%4570, %4577) {node_name = "Div_3487-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4579 = "dtu_hlir.convert"(%4578) {node_name = "Cast_3488-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4580 = "dtu_hlir.convert"(%4579) {node_name = "Cast_3489-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4581 = dtu_hlir.constant  {node_name = "Constant_3490-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4582 = "dtu_hlir.unsqueeze"(%4580, %4581) {node_name = "Unsqueeze_3491-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4583 = dtu_hlir.constant  {node_name = "Constant_3492-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4584 = "dtu_hlir.unsqueeze"(%4573, %4583) {node_name = "Unsqueeze_3493-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4585 = dtu_hlir.constant  {node_name = "Constant_3494-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4586 = "dtu_hlir.unsqueeze"(%4576, %4585) {node_name = "Unsqueeze_3495-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4587 = "dtu_hlir.concatenate"(%4582, %808, %4584, %4586) {dimension = 0 : i64, node_name = "Concat_3496-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4588 = "dtu_hlir.dynamic_reshape"(%4567, %4587) {allowzero = 0 : i64, node_name = "Reshape_3497-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<4xi64>) -> tensor<2x20x64x64xf32>
    %4589 = "dtu_hlir.transpose"(%4588) {node_name = "Transpose_3498-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x64x64xf32>) -> tensor<2x64x20x64xf32>
    %4590 = dtu_hlir.constant  {node_name = "Constant_3499-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4591 = "dtu_hlir.div"(%4570, %4590) {node_name = "Div_3500-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4592 = "dtu_hlir.convert"(%4591) {node_name = "Cast_3501-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4593 = "dtu_hlir.convert"(%4592) {node_name = "Cast_3502-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4594 = dtu_hlir.constant  {node_name = "Constant_3503-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4595 = "dtu_hlir.mul"(%4576, %4594) {node_name = "Mul_3504-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4596 = dtu_hlir.constant  {node_name = "Constant_3505-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4597 = "dtu_hlir.unsqueeze"(%4593, %4596) {node_name = "Unsqueeze_3506-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4598 = dtu_hlir.constant  {node_name = "Constant_3507-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4599 = "dtu_hlir.unsqueeze"(%4573, %4598) {node_name = "Unsqueeze_3508-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4600 = dtu_hlir.constant  {node_name = "Constant_3509-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4601 = "dtu_hlir.unsqueeze"(%4595, %4600) {node_name = "Unsqueeze_3510-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4602 = "dtu_hlir.concatenate"(%4597, %4599, %4601) {dimension = 0 : i64, node_name = "Concat_3511-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4603 = "dtu_hlir.dynamic_reshape"(%4589, %4602) {allowzero = 0 : i64, node_name = "Reshape_3512-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x20x64xf32>, tensor<3xi64>) -> tensor<2x64x1280xf32>
    %4604 = "dtu_hlir.dot_general"(%4603, %503) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3513-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4605 = "dtu_hlir.add"(%347, %4604) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3514-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4606 = "dtu_hlir.add"(%4605, %4401) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3515-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4607 = dtu_hlir.constant  {node_name = "ReduceMean_3516-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4608 = "dtu_hlir.reshape"(%4607) {node_name = "ReduceMean_3516-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4609 = "dtu_hlir.reduce"(%4606, %4608) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3516-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3516-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<f32>) -> tensor<2x64xf32>
    %4610 = dtu_hlir.constant  {node_name = "ReduceMean_3516-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4611 = "dtu_hlir.unsqueeze"(%4609, %4610) {node_name = "ReduceMean_3516-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64xf32>, tensor<1xi64>) -> tensor<2x64x1xf32>
    %4612 = dtu_hlir.constant  {node_name = "ReduceMean_3516-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4613 = "dtu_hlir.broadcast_in_dim"(%4612) {node_name = "ReduceMean_3516-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x64x1xf32>
    %4614 = "dtu_hlir.div"(%4611, %4613) {node_name = "ReduceMean_3516-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4615 = "dtu_hlir.sub"(%4606, %4614) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_3517-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1280xf32>
    %4616 = dtu_hlir.constant  {node_name = "Constant_3518-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %4617 = "dtu_hlir.broadcast_in_dim"(%4616) {node_name = "Pow_3519-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x1280xf32>
    %4618 = "dtu_hlir.pow"(%4615, %4617) {node_name = "Pow_3519-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4619 = dtu_hlir.constant  {node_name = "ReduceMean_3520-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4620 = "dtu_hlir.reshape"(%4619) {node_name = "ReduceMean_3520-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4621 = "dtu_hlir.reduce"(%4618, %4620) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3520-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3520-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<f32>) -> tensor<2x64xf32>
    %4622 = dtu_hlir.constant  {node_name = "ReduceMean_3520-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4623 = "dtu_hlir.unsqueeze"(%4621, %4622) {node_name = "ReduceMean_3520-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64xf32>, tensor<1xi64>) -> tensor<2x64x1xf32>
    %4624 = dtu_hlir.constant  {node_name = "ReduceMean_3520-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4625 = "dtu_hlir.broadcast_in_dim"(%4624) {node_name = "ReduceMean_3520-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x64x1xf32>
    %4626 = "dtu_hlir.div"(%4623, %4625) {node_name = "ReduceMean_3520-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4627 = dtu_hlir.constant  {node_name = "Constant_3521-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %4628 = "dtu_hlir.broadcast_in_dim"(%4627) {node_name = "Add_3522-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x1xf32>
    %4629 = "dtu_hlir.add"(%4626, %4628) {node_name = "Add_3522-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4630 = "dtu_hlir.sqrt"(%4629) {node_name = "Sqrt_3523-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4631 = "dtu_hlir.div"(%4615, %4630) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_3524-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1280xf32>
    %4632 = "dtu_hlir.mul"(%4631, %353) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_3525-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf32>
    %4633 = "dtu_hlir.add"(%4632, %354) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3526-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf32>
    %4634 = "dtu_hlir.dot_general"(%4633, %504) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3527-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4635 = "dtu_hlir.dot_general"(%arg2, %505) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3528-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %4636 = "dtu_hlir.dot_general"(%arg2, %506) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3529-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %4637 = "dtu_hlir.shape"(%4634) {end = 2147483647 : i64, node_name = "Shape_3530-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4638 = dtu_hlir.constant  {node_name = "Constant_3531-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4639 = "dtu_hlir.gather"(%4637, %4638) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3532-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4640 = "dtu_hlir.shape"(%4634) {end = 2147483647 : i64, node_name = "Shape_3533-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4641 = dtu_hlir.constant  {node_name = "Constant_3534-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4642 = "dtu_hlir.gather"(%4640, %4641) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3535-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4643 = "dtu_hlir.shape"(%4634) {end = 2147483647 : i64, node_name = "Shape_3536-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>) -> tensor<3xi64>
    %4644 = dtu_hlir.constant  {node_name = "Constant_3537-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4645 = "dtu_hlir.gather"(%4643, %4644) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3538-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4646 = dtu_hlir.constant  {node_name = "Constant_3539-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4647 = "dtu_hlir.div"(%4645, %4646) {node_name = "Div_3540-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4648 = "dtu_hlir.convert"(%4647) {node_name = "Cast_3541-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4649 = "dtu_hlir.convert"(%4648) {node_name = "Cast_3542-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4650 = dtu_hlir.constant  {node_name = "Constant_3543-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4651 = "dtu_hlir.unsqueeze"(%4639, %4650) {node_name = "Unsqueeze_3544-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4652 = dtu_hlir.constant  {node_name = "Constant_3545-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4653 = "dtu_hlir.unsqueeze"(%4642, %4652) {node_name = "Unsqueeze_3546-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4654 = dtu_hlir.constant  {node_name = "Constant_3547-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4655 = "dtu_hlir.unsqueeze"(%4649, %4654) {node_name = "Unsqueeze_3548-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4656 = "dtu_hlir.concatenate"(%4651, %4653, %807, %4655) {dimension = 0 : i64, node_name = "Concat_3549-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4657 = "dtu_hlir.dynamic_reshape"(%4634, %4656) {allowzero = 0 : i64, node_name = "Reshape_3550-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<4xi64>) -> tensor<2x64x20x64xf32>
    %4658 = "dtu_hlir.transpose"(%4657) {node_name = "Transpose_3551-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x20x64xf32>) -> tensor<2x20x64x64xf32>
    %4659 = dtu_hlir.constant  {node_name = "Constant_3552-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4660 = "dtu_hlir.mul"(%4639, %4659) {node_name = "Mul_3553-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4661 = dtu_hlir.constant  {node_name = "Constant_3554-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4662 = "dtu_hlir.div"(%4645, %4661) {node_name = "Div_3555-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4663 = "dtu_hlir.convert"(%4662) {node_name = "Cast_3556-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4664 = "dtu_hlir.convert"(%4663) {node_name = "Cast_3557-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4665 = dtu_hlir.constant  {node_name = "Constant_3558-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4666 = "dtu_hlir.unsqueeze"(%4660, %4665) {node_name = "Unsqueeze_3559-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4667 = dtu_hlir.constant  {node_name = "Constant_3560-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4668 = "dtu_hlir.unsqueeze"(%4642, %4667) {node_name = "Unsqueeze_3561-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4669 = dtu_hlir.constant  {node_name = "Constant_3562-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4670 = "dtu_hlir.unsqueeze"(%4664, %4669) {node_name = "Unsqueeze_3563-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4671 = "dtu_hlir.concatenate"(%4666, %4668, %4670) {dimension = 0 : i64, node_name = "Concat_3564-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4672 = "dtu_hlir.dynamic_reshape"(%4658, %4671) {allowzero = 0 : i64, node_name = "Reshape_3565-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x64x64xf32>, tensor<3xi64>) -> tensor<40x64x64xf32>
    %4673 = "dtu_hlir.shape"(%4635) {end = 2147483647 : i64, node_name = "Shape_3566-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4674 = dtu_hlir.constant  {node_name = "Constant_3567-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4675 = "dtu_hlir.gather"(%4673, %4674) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3568-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4676 = "dtu_hlir.shape"(%4635) {end = 2147483647 : i64, node_name = "Shape_3569-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4677 = dtu_hlir.constant  {node_name = "Constant_3570-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4678 = "dtu_hlir.gather"(%4676, %4677) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3571-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4679 = "dtu_hlir.shape"(%4635) {end = 2147483647 : i64, node_name = "Shape_3572-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4680 = dtu_hlir.constant  {node_name = "Constant_3573-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4681 = "dtu_hlir.gather"(%4679, %4680) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3574-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4682 = dtu_hlir.constant  {node_name = "Constant_3575-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4683 = "dtu_hlir.div"(%4681, %4682) {node_name = "Div_3576-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4684 = "dtu_hlir.convert"(%4683) {node_name = "Cast_3577-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4685 = "dtu_hlir.convert"(%4684) {node_name = "Cast_3578-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4686 = dtu_hlir.constant  {node_name = "Constant_3579-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4687 = "dtu_hlir.unsqueeze"(%4675, %4686) {node_name = "Unsqueeze_3580-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4688 = dtu_hlir.constant  {node_name = "Constant_3581-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4689 = "dtu_hlir.unsqueeze"(%4678, %4688) {node_name = "Unsqueeze_3582-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4690 = dtu_hlir.constant  {node_name = "Constant_3583-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4691 = "dtu_hlir.unsqueeze"(%4685, %4690) {node_name = "Unsqueeze_3584-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4692 = "dtu_hlir.concatenate"(%4687, %4689, %806, %4691) {dimension = 0 : i64, node_name = "Concat_3585-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4693 = "dtu_hlir.dynamic_reshape"(%4635, %4692) {allowzero = 0 : i64, node_name = "Reshape_3586-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %4694 = "dtu_hlir.transpose"(%4693) {node_name = "Transpose_3587-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %4695 = dtu_hlir.constant  {node_name = "Constant_3588-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4696 = "dtu_hlir.mul"(%4675, %4695) {node_name = "Mul_3589-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4697 = dtu_hlir.constant  {node_name = "Constant_3590-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4698 = "dtu_hlir.div"(%4681, %4697) {node_name = "Div_3591-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4699 = "dtu_hlir.convert"(%4698) {node_name = "Cast_3592-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4700 = "dtu_hlir.convert"(%4699) {node_name = "Cast_3593-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4701 = dtu_hlir.constant  {node_name = "Constant_3594-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4702 = "dtu_hlir.unsqueeze"(%4696, %4701) {node_name = "Unsqueeze_3595-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4703 = dtu_hlir.constant  {node_name = "Constant_3596-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4704 = "dtu_hlir.unsqueeze"(%4678, %4703) {node_name = "Unsqueeze_3597-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4705 = dtu_hlir.constant  {node_name = "Constant_3598-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4706 = "dtu_hlir.unsqueeze"(%4700, %4705) {node_name = "Unsqueeze_3599-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4707 = "dtu_hlir.concatenate"(%4702, %4704, %4706) {dimension = 0 : i64, node_name = "Concat_3600-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4708 = "dtu_hlir.dynamic_reshape"(%4694, %4707) {allowzero = 0 : i64, node_name = "Reshape_3601-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %4709 = "dtu_hlir.shape"(%4636) {end = 2147483647 : i64, node_name = "Shape_3602-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4710 = dtu_hlir.constant  {node_name = "Constant_3603-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4711 = "dtu_hlir.gather"(%4709, %4710) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3604-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4712 = "dtu_hlir.shape"(%4636) {end = 2147483647 : i64, node_name = "Shape_3605-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4713 = dtu_hlir.constant  {node_name = "Constant_3606-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4714 = "dtu_hlir.gather"(%4712, %4713) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3607-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4715 = "dtu_hlir.shape"(%4636) {end = 2147483647 : i64, node_name = "Shape_3608-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %4716 = dtu_hlir.constant  {node_name = "Constant_3609-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4717 = "dtu_hlir.gather"(%4715, %4716) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3610-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4718 = dtu_hlir.constant  {node_name = "Constant_3611-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4719 = "dtu_hlir.div"(%4717, %4718) {node_name = "Div_3612-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4720 = "dtu_hlir.convert"(%4719) {node_name = "Cast_3613-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4721 = "dtu_hlir.convert"(%4720) {node_name = "Cast_3614-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4722 = dtu_hlir.constant  {node_name = "Constant_3615-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4723 = "dtu_hlir.unsqueeze"(%4711, %4722) {node_name = "Unsqueeze_3616-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4724 = dtu_hlir.constant  {node_name = "Constant_3617-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4725 = "dtu_hlir.unsqueeze"(%4714, %4724) {node_name = "Unsqueeze_3618-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4726 = dtu_hlir.constant  {node_name = "Constant_3619-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4727 = "dtu_hlir.unsqueeze"(%4721, %4726) {node_name = "Unsqueeze_3620-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4728 = "dtu_hlir.concatenate"(%4723, %4725, %805, %4727) {dimension = 0 : i64, node_name = "Concat_3621-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4729 = "dtu_hlir.dynamic_reshape"(%4636, %4728) {allowzero = 0 : i64, node_name = "Reshape_3622-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %4730 = "dtu_hlir.transpose"(%4729) {node_name = "Transpose_3623-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %4731 = dtu_hlir.constant  {node_name = "Constant_3624-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4732 = "dtu_hlir.mul"(%4711, %4731) {node_name = "Mul_3625-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4733 = dtu_hlir.constant  {node_name = "Constant_3626-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4734 = "dtu_hlir.div"(%4717, %4733) {node_name = "Div_3627-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4735 = "dtu_hlir.convert"(%4734) {node_name = "Cast_3628-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4736 = "dtu_hlir.convert"(%4735) {node_name = "Cast_3629-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4737 = dtu_hlir.constant  {node_name = "Constant_3630-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4738 = "dtu_hlir.unsqueeze"(%4732, %4737) {node_name = "Unsqueeze_3631-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4739 = dtu_hlir.constant  {node_name = "Constant_3632-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4740 = "dtu_hlir.unsqueeze"(%4714, %4739) {node_name = "Unsqueeze_3633-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4741 = dtu_hlir.constant  {node_name = "Constant_3634-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4742 = "dtu_hlir.unsqueeze"(%4736, %4741) {node_name = "Unsqueeze_3635-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4743 = "dtu_hlir.concatenate"(%4738, %4740, %4742) {dimension = 0 : i64, node_name = "Concat_3636-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4744 = "dtu_hlir.dynamic_reshape"(%4730, %4743) {allowzero = 0 : i64, node_name = "Reshape_3637-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %4745 = "dtu_hlir.shape"(%4672) {end = 2147483647 : i64, node_name = "Shape_3638-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4746 = dtu_hlir.constant  {node_name = "Constant_3639-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4747 = "dtu_hlir.gather"(%4745, %4746) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3640-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4748 = "dtu_hlir.shape"(%4672) {end = 2147483647 : i64, node_name = "Shape_3641-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4749 = dtu_hlir.constant  {node_name = "Constant_3642-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4750 = "dtu_hlir.gather"(%4748, %4749) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3643-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4751 = "dtu_hlir.shape"(%4708) {end = 2147483647 : i64, node_name = "Shape_3644-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<3xi64>
    %4752 = dtu_hlir.constant  {node_name = "Constant_3645-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4753 = "dtu_hlir.gather"(%4751, %4752) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3646-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4754 = dtu_hlir.constant  {node_name = "Constant_3647-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4755 = "dtu_hlir.unsqueeze"(%4747, %4754) {node_name = "Unsqueeze_3648-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4756 = dtu_hlir.constant  {node_name = "Constant_3649-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4757 = "dtu_hlir.unsqueeze"(%4750, %4756) {node_name = "Unsqueeze_3650-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4758 = dtu_hlir.constant  {node_name = "Constant_3651-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4759 = "dtu_hlir.unsqueeze"(%4753, %4758) {node_name = "Unsqueeze_3652-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4760 = "dtu_hlir.concatenate"(%4755, %4757, %4759) {dimension = 0 : i64, node_name = "Concat_3653-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4761 = dtu_hlir.constant  {node_name = "ConstantOfShape_3654-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %4762 = "dtu_hlir.dynamic_broadcast_in_dim"(%4761, %4760) {node_name = "ConstantOfShape_3654-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x64x77xf32>
    %4763 = "dtu_hlir.transpose"(%4708) {node_name = "Transpose_3655-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<40x64x77xf32>
    %4764 = "dtu_hlir.dot_general"(%4672, %4763) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3656-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<40x64x77xf32>) -> tensor<40x64x77xf32>
    %4765 = "dtu_hlir.broadcast_in_dim"(%804) {node_name = "Mul_3657-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x64x77xf32>
    %4766 = "dtu_hlir.mul"(%4764, %4765) {node_name = "Mul_3657-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x64x77xf32>, tensor<40x64x77xf32>) -> tensor<40x64x77xf32>
    %4767 = "dtu_hlir.broadcast_in_dim"(%803) {node_name = "Mul_3658-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x64x77xf32>
    %4768 = "dtu_hlir.mul"(%4762, %4767) {node_name = "Mul_3658-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x64x77xf32>, tensor<40x64x77xf32>) -> tensor<40x64x77xf32>
    %4769 = "dtu_hlir.add"(%4766, %4768) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3659-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x64x77xf32>, tensor<40x64x77xf32>) -> tensor<40x64x77xf32>
    %4770 = "dtu_hlir.softmax"(%4769) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_3660-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x64x77xf32>) -> tensor<40x64x77xf32>
    %4771 = "dtu_hlir.convert"(%4770) {node_name = "Cast_3661-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x64x77xf32>) -> tensor<40x64x77xf32>
    %4772 = "dtu_hlir.dot_general"(%4771, %4744) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3662-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x64x77xf32>, tensor<40x77x64xf32>) -> tensor<40x64x64xf32>
    %4773 = "dtu_hlir.shape"(%4772) {end = 2147483647 : i64, node_name = "Shape_3663-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4774 = dtu_hlir.constant  {node_name = "Constant_3664-0", node_type = "Constant"} dense<0> : tensor<i64>
    %4775 = "dtu_hlir.gather"(%4773, %4774) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3665-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4776 = "dtu_hlir.shape"(%4772) {end = 2147483647 : i64, node_name = "Shape_3666-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4777 = dtu_hlir.constant  {node_name = "Constant_3667-0", node_type = "Constant"} dense<1> : tensor<i64>
    %4778 = "dtu_hlir.gather"(%4776, %4777) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3668-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4779 = "dtu_hlir.shape"(%4772) {end = 2147483647 : i64, node_name = "Shape_3669-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x64x64xf32>) -> tensor<3xi64>
    %4780 = dtu_hlir.constant  {node_name = "Constant_3670-0", node_type = "Constant"} dense<2> : tensor<i64>
    %4781 = "dtu_hlir.gather"(%4779, %4780) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3671-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %4782 = dtu_hlir.constant  {node_name = "Constant_3672-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4783 = "dtu_hlir.div"(%4775, %4782) {node_name = "Div_3673-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4784 = "dtu_hlir.convert"(%4783) {node_name = "Cast_3674-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4785 = "dtu_hlir.convert"(%4784) {node_name = "Cast_3675-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4786 = dtu_hlir.constant  {node_name = "Constant_3676-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4787 = "dtu_hlir.unsqueeze"(%4785, %4786) {node_name = "Unsqueeze_3677-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4788 = dtu_hlir.constant  {node_name = "Constant_3678-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4789 = "dtu_hlir.unsqueeze"(%4778, %4788) {node_name = "Unsqueeze_3679-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4790 = dtu_hlir.constant  {node_name = "Constant_3680-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4791 = "dtu_hlir.unsqueeze"(%4781, %4790) {node_name = "Unsqueeze_3681-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4792 = "dtu_hlir.concatenate"(%4787, %802, %4789, %4791) {dimension = 0 : i64, node_name = "Concat_3682-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4793 = "dtu_hlir.dynamic_reshape"(%4772, %4792) {allowzero = 0 : i64, node_name = "Reshape_3683-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x64x64xf32>, tensor<4xi64>) -> tensor<2x20x64x64xf32>
    %4794 = "dtu_hlir.transpose"(%4793) {node_name = "Transpose_3684-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x64x64xf32>) -> tensor<2x64x20x64xf32>
    %4795 = dtu_hlir.constant  {node_name = "Constant_3685-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4796 = "dtu_hlir.div"(%4775, %4795) {node_name = "Div_3686-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4797 = "dtu_hlir.convert"(%4796) {node_name = "Cast_3687-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4798 = "dtu_hlir.convert"(%4797) {node_name = "Cast_3688-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %4799 = dtu_hlir.constant  {node_name = "Constant_3689-0", node_type = "Constant"} dense<20> : tensor<i64>
    %4800 = "dtu_hlir.mul"(%4781, %4799) {node_name = "Mul_3690-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %4801 = dtu_hlir.constant  {node_name = "Constant_3691-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4802 = "dtu_hlir.unsqueeze"(%4798, %4801) {node_name = "Unsqueeze_3692-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4803 = dtu_hlir.constant  {node_name = "Constant_3693-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4804 = "dtu_hlir.unsqueeze"(%4778, %4803) {node_name = "Unsqueeze_3694-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4805 = dtu_hlir.constant  {node_name = "Constant_3695-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4806 = "dtu_hlir.unsqueeze"(%4800, %4805) {node_name = "Unsqueeze_3696-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4807 = "dtu_hlir.concatenate"(%4802, %4804, %4806) {dimension = 0 : i64, node_name = "Concat_3697-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %4808 = "dtu_hlir.dynamic_reshape"(%4794, %4807) {allowzero = 0 : i64, node_name = "Reshape_3698-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x20x64xf32>, tensor<3xi64>) -> tensor<2x64x1280xf32>
    %4809 = "dtu_hlir.dot_general"(%4808, %507) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3699-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4810 = "dtu_hlir.add"(%350, %4809) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3700-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4811 = "dtu_hlir.add"(%4810, %4606) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3701-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4812 = dtu_hlir.constant  {node_name = "ReduceMean_3702-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4813 = "dtu_hlir.reshape"(%4812) {node_name = "ReduceMean_3702-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4814 = "dtu_hlir.reduce"(%4811, %4813) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3702-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3702-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<f32>) -> tensor<2x64xf32>
    %4815 = dtu_hlir.constant  {node_name = "ReduceMean_3702-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4816 = "dtu_hlir.unsqueeze"(%4814, %4815) {node_name = "ReduceMean_3702-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64xf32>, tensor<1xi64>) -> tensor<2x64x1xf32>
    %4817 = dtu_hlir.constant  {node_name = "ReduceMean_3702-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4818 = "dtu_hlir.broadcast_in_dim"(%4817) {node_name = "ReduceMean_3702-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x64x1xf32>
    %4819 = "dtu_hlir.div"(%4816, %4818) {node_name = "ReduceMean_3702-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4820 = "dtu_hlir.sub"(%4811, %4819) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_3703-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1280xf32>
    %4821 = dtu_hlir.constant  {node_name = "Constant_3704-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %4822 = "dtu_hlir.broadcast_in_dim"(%4821) {node_name = "Pow_3705-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x1280xf32>
    %4823 = "dtu_hlir.pow"(%4820, %4822) {node_name = "Pow_3705-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4824 = dtu_hlir.constant  {node_name = "ReduceMean_3706-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %4825 = "dtu_hlir.reshape"(%4824) {node_name = "ReduceMean_3706-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %4826 = "dtu_hlir.reduce"(%4823, %4825) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3706-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3706-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<f32>) -> tensor<2x64xf32>
    %4827 = dtu_hlir.constant  {node_name = "ReduceMean_3706-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %4828 = "dtu_hlir.unsqueeze"(%4826, %4827) {node_name = "ReduceMean_3706-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64xf32>, tensor<1xi64>) -> tensor<2x64x1xf32>
    %4829 = dtu_hlir.constant  {node_name = "ReduceMean_3706-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %4830 = "dtu_hlir.broadcast_in_dim"(%4829) {node_name = "ReduceMean_3706-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x64x1xf32>
    %4831 = "dtu_hlir.div"(%4828, %4830) {node_name = "ReduceMean_3706-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4832 = dtu_hlir.constant  {node_name = "Constant_3707-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %4833 = "dtu_hlir.broadcast_in_dim"(%4832) {node_name = "Add_3708-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x1xf32>
    %4834 = "dtu_hlir.add"(%4831, %4833) {node_name = "Add_3708-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4835 = "dtu_hlir.sqrt"(%4834) {node_name = "Sqrt_3709-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x64x1xf32>) -> tensor<2x64x1xf32>
    %4836 = "dtu_hlir.div"(%4820, %4835) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_3710-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1xf32>) -> tensor<2x64x1280xf32>
    %4837 = "dtu_hlir.mul"(%4836, %355) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_3711-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf32>
    %4838 = "dtu_hlir.add"(%4837, %356) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3712-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf32>
    %4839 = "dtu_hlir.dot_general"(%4838, %508) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3713-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x10240xf32>) -> tensor<2x64x10240xf32>
    %4840 = "dtu_hlir.add"(%348, %4839) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3714-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10240xf32>, tensor<2x64x10240xf32>) -> tensor<2x64x10240xf32>
    %4841 = "dtu_hlir.shape"(%4840) {end = 2147483647 : i64, node_name = "Shape_3715-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x64x10240xf32>) -> tensor<3xi64>
    %4842 = dtu_hlir.constant  {node_name = "Constant_3716-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %4843 = "dtu_hlir.gather"(%4841, %4842) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3717-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4844 = dtu_hlir.constant  {node_name = "Constant_3718-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4845 = dtu_hlir.constant  {node_name = "Constant_3719-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %4846 = "dtu_hlir.add"(%4843, %4845) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_3720-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4847 = dtu_hlir.constant  {node_name = "Constant_3721-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4848 = "dtu_hlir.div"(%4846, %4847) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_3722-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4849 = dtu_hlir.constant  {node_name = "Constant_3723-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %4850 = "dtu_hlir.mul"(%4848, %4849) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_3724-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4851 = dtu_hlir.constant  {node_name = "Slice_3725-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %4852 = "dtu_hlir.real_dynamic_slice"(%4840, %4844, %4850, %4851, %4842) {node_name = "Slice_3725-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x64x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x64x5120xf32>
    %4853 = dtu_hlir.constant  {node_name = "Constant_3726-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4854 = "dtu_hlir.mul"(%4848, %4853) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_3727-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %4855 = "dtu_hlir.shape"(%4850) {end = 2147483647 : i64, node_name = "Slice_3728-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %4856 = dtu_hlir.constant  {node_name = "Slice_3728-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %4857 = "dtu_hlir.dynamic_broadcast_in_dim"(%4856, %4855) {node_name = "Slice_3728-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4858 = "dtu_hlir.real_dynamic_slice"(%4840, %4850, %4854, %4857, %4842) {node_name = "Slice_3728-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x64x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x64x5120xf32>
    %4859 = dtu_hlir.constant  {node_name = "Constant_3729-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %4860 = "dtu_hlir.broadcast_in_dim"(%4859) {node_name = "Div_3730-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x5120xf32>
    %4861 = "dtu_hlir.div"(%4858, %4860) {node_name = "Div_3730-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x64x5120xf32>, tensor<2x64x5120xf32>) -> tensor<2x64x5120xf32>
    %4862 = "dtu_hlir.erf"(%4861) {node_name = "Erf_3731-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x64x5120xf32>) -> tensor<2x64x5120xf32>
    %4863 = dtu_hlir.constant  {node_name = "Constant_3732-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %4864 = "dtu_hlir.broadcast_in_dim"(%4863) {node_name = "Add_3733-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x5120xf32>
    %4865 = "dtu_hlir.add"(%4862, %4864) {node_name = "Add_3733-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x5120xf32>, tensor<2x64x5120xf32>) -> tensor<2x64x5120xf32>
    %4866 = "dtu_hlir.mul"(%4858, %4865) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_3734-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x64x5120xf32>, tensor<2x64x5120xf32>) -> tensor<2x64x5120xf32>
    %4867 = dtu_hlir.constant  {node_name = "Constant_3735-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %4868 = "dtu_hlir.broadcast_in_dim"(%4867) {node_name = "Mul_3736-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x64x5120xf32>
    %4869 = "dtu_hlir.mul"(%4866, %4868) {node_name = "Mul_3736-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x64x5120xf32>, tensor<2x64x5120xf32>) -> tensor<2x64x5120xf32>
    %4870 = "dtu_hlir.mul"(%4852, %4869) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_3737-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x64x5120xf32>, tensor<2x64x5120xf32>) -> tensor<2x64x5120xf32>
    %4871 = "dtu_hlir.dot_general"(%4870, %509) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3738-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x5120xf32>, tensor<5120x1280xf32>) -> tensor<2x64x1280xf32>
    %4872 = "dtu_hlir.add"(%349, %4871) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3739-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4873 = "dtu_hlir.add"(%4872, %4811) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_3740-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4874 = "dtu_hlir.dot_general"(%4873, %510) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3741-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x64x1280xf32>
    %4875 = "dtu_hlir.add"(%357, %4874) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3742-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x64x1280xf32>) -> tensor<2x64x1280xf32>
    %4876 = dtu_hlir.constant  {node_name = "Constant_3743-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4877 = "dtu_hlir.unsqueeze"(%4371, %4876) {node_name = "Unsqueeze_3744-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4878 = dtu_hlir.constant  {node_name = "Constant_3745-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4879 = "dtu_hlir.unsqueeze"(%4374, %4878) {node_name = "Unsqueeze_3746-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4880 = dtu_hlir.constant  {node_name = "Constant_3747-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4881 = "dtu_hlir.unsqueeze"(%4377, %4880) {node_name = "Unsqueeze_3748-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4882 = dtu_hlir.constant  {node_name = "Constant_3749-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %4883 = "dtu_hlir.unsqueeze"(%4389, %4882) {node_name = "Unsqueeze_3750-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %4884 = "dtu_hlir.concatenate"(%4877, %4879, %4881, %4883) {dimension = 0 : i64, node_name = "Concat_3751-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %4885 = "dtu_hlir.dynamic_reshape"(%4875, %4884) {allowzero = 0 : i64, node_name = "Reshape_3752-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x1280xf32>, tensor<4xi64>) -> tensor<2x8x8x1280xf32>
    %4886 = "dtu_hlir.transpose"(%4885) {node_name = "Transpose_3753-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x8x8x1280xf32>) -> tensor<2x1280x8x8xf32>
    %4887 = "dtu_hlir.add"(%4886, %4368) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3754-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4888 = dtu_hlir.constant  {node_name = "Constant_3755-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4889 = "dtu_hlir.dynamic_reshape"(%4887, %4888) {allowzero = 0 : i64, node_name = "Reshape_3756-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4890 = dtu_hlir.constant  {node_name = "Constant_3757-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4891 = dtu_hlir.constant  {node_name = "Constant_3758-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4892 = "dtu_hlir.instance_norm"(%4889, %4890, %4891) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3759-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4893 = "dtu_hlir.shape"(%4887) {end = 2147483647 : i64, node_name = "Shape_3760-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4894 = "dtu_hlir.dynamic_reshape"(%4892, %4893) {allowzero = 0 : i64, node_name = "Reshape_3761-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4895 = "dtu_hlir.mul"(%4894, %511) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3762-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4896 = "dtu_hlir.add"(%4895, %512) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3763-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4897 = "dtu_hlir.sigmoid"(%4896) {node_name = "Sigmoid_3764-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4898 = "dtu_hlir.mul"(%4896, %4897) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3765-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4899 = "dtu_hlir.conv_bias"(%4898, %364, %365) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3766-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4900 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3767-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4901 = "dtu_hlir.mul"(%915, %4900) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3768-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4902 = "dtu_hlir.transpose"(%366) {node_name = "Gemm_3769-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %4903 = "dtu_hlir.gemm"(%4901, %4902, %367) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3769-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %4904 = dtu_hlir.constant  {node_name = "Constant_3770-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4905 = "dtu_hlir.unsqueeze"(%4903, %4904) {node_name = "Unsqueeze_3771-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %4906 = dtu_hlir.constant  {node_name = "Constant_3772-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %4907 = "dtu_hlir.unsqueeze"(%4905, %4906) {node_name = "Unsqueeze_3773-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %4908 = "dtu_hlir.add"(%4899, %4907) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3774-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4909 = dtu_hlir.constant  {node_name = "Constant_3775-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4910 = "dtu_hlir.dynamic_reshape"(%4908, %4909) {allowzero = 0 : i64, node_name = "Reshape_3776-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4911 = dtu_hlir.constant  {node_name = "Constant_3777-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4912 = dtu_hlir.constant  {node_name = "Constant_3778-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4913 = "dtu_hlir.instance_norm"(%4910, %4911, %4912) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3779-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4914 = "dtu_hlir.shape"(%4908) {end = 2147483647 : i64, node_name = "Shape_3780-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4915 = "dtu_hlir.dynamic_reshape"(%4913, %4914) {allowzero = 0 : i64, node_name = "Reshape_3781-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4916 = "dtu_hlir.mul"(%4915, %513) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3782-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4917 = "dtu_hlir.add"(%4916, %514) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3783-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4918 = "dtu_hlir.sigmoid"(%4917) {node_name = "Sigmoid_3784-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4919 = "dtu_hlir.mul"(%4917, %4918) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3785-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4920 = "dtu_hlir.conv_bias"(%4919, %368, %369) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3786-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4921 = "dtu_hlir.add"(%4887, %4920) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3787-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4922 = dtu_hlir.constant  {node_name = "Constant_3788-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %4923 = "dtu_hlir.broadcast_in_dim"(%4922) {node_name = "Div_3789-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x8x8xf32>
    %4924 = "dtu_hlir.div"(%4921, %4923) {node_name = "Div_3789-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4925 = "dtu_hlir.concatenate"(%4924, %4331) {dimension = 1 : i64, node_name = "Concat_3790-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %4926 = dtu_hlir.constant  {node_name = "Constant_3791-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4927 = "dtu_hlir.dynamic_reshape"(%4925, %4926) {allowzero = 0 : i64, node_name = "Reshape_3792-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<3xi64>) -> tensor<2x32x5120xf32>
    %4928 = dtu_hlir.constant  {node_name = "Constant_3793-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4929 = dtu_hlir.constant  {node_name = "Constant_3794-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4930 = "dtu_hlir.instance_norm"(%4927, %4928, %4929) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3795-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x5120xf32>
    %4931 = "dtu_hlir.shape"(%4925) {end = 2147483647 : i64, node_name = "Shape_3796-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>) -> tensor<4xi64>
    %4932 = "dtu_hlir.dynamic_reshape"(%4930, %4931) {allowzero = 0 : i64, node_name = "Reshape_3797-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<4xi64>) -> tensor<2x2560x8x8xf32>
    %4933 = "dtu_hlir.mul"(%4932, %515) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3798-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x8x8xf32>
    %4934 = "dtu_hlir.add"(%4933, %516) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3799-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x8x8xf32>
    %4935 = "dtu_hlir.sigmoid"(%4934) {node_name = "Sigmoid_3800-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %4936 = "dtu_hlir.mul"(%4934, %4935) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3801-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2x2560x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %4937 = "dtu_hlir.conv_bias"(%4936, %136, %137) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3802-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x8x8xf32>, tensor<1280x2560x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4938 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3803-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4939 = "dtu_hlir.mul"(%915, %4938) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3804-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4940 = "dtu_hlir.transpose"(%138) {node_name = "Gemm_3805-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %4941 = "dtu_hlir.gemm"(%4939, %4940, %139) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3805-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %4942 = dtu_hlir.constant  {node_name = "Constant_3806-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4943 = "dtu_hlir.unsqueeze"(%4941, %4942) {node_name = "Unsqueeze_3807-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %4944 = dtu_hlir.constant  {node_name = "Constant_3808-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %4945 = "dtu_hlir.unsqueeze"(%4943, %4944) {node_name = "Unsqueeze_3809-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %4946 = "dtu_hlir.add"(%4937, %4945) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3810-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4947 = dtu_hlir.constant  {node_name = "Constant_3811-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4948 = "dtu_hlir.dynamic_reshape"(%4946, %4947) {allowzero = 0 : i64, node_name = "Reshape_3812-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4949 = dtu_hlir.constant  {node_name = "Constant_3813-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4950 = dtu_hlir.constant  {node_name = "Constant_3814-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4951 = "dtu_hlir.instance_norm"(%4948, %4949, %4950) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3815-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4952 = "dtu_hlir.shape"(%4946) {end = 2147483647 : i64, node_name = "Shape_3816-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4953 = "dtu_hlir.dynamic_reshape"(%4951, %4952) {allowzero = 0 : i64, node_name = "Reshape_3817-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4954 = "dtu_hlir.mul"(%4953, %517) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3818-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4955 = "dtu_hlir.add"(%4954, %518) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3819-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4956 = "dtu_hlir.sigmoid"(%4955) {node_name = "Sigmoid_3820-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4957 = "dtu_hlir.mul"(%4955, %4956) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3821-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4958 = "dtu_hlir.conv_bias"(%4957, %140, %141) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3822-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4959 = "dtu_hlir.conv_bias"(%4925, %142, %143) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3823-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x8x8xf32>, tensor<1280x2560x1x1xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4960 = "dtu_hlir.add"(%4959, %4958) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3824-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4961 = dtu_hlir.constant  {node_name = "Constant_3825-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %4962 = "dtu_hlir.broadcast_in_dim"(%4961) {node_name = "Div_3826-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x8x8xf32>
    %4963 = "dtu_hlir.div"(%4960, %4962) {node_name = "Div_3826-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4964 = "dtu_hlir.concatenate"(%4963, %4294) {dimension = 1 : i64, node_name = "Concat_3827-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %4965 = dtu_hlir.constant  {node_name = "Constant_3828-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4966 = "dtu_hlir.dynamic_reshape"(%4964, %4965) {allowzero = 0 : i64, node_name = "Reshape_3829-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<3xi64>) -> tensor<2x32x5120xf32>
    %4967 = dtu_hlir.constant  {node_name = "Constant_3830-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4968 = dtu_hlir.constant  {node_name = "Constant_3831-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4969 = "dtu_hlir.instance_norm"(%4966, %4967, %4968) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3832-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x5120xf32>
    %4970 = "dtu_hlir.shape"(%4964) {end = 2147483647 : i64, node_name = "Shape_3833-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>) -> tensor<4xi64>
    %4971 = "dtu_hlir.dynamic_reshape"(%4969, %4970) {allowzero = 0 : i64, node_name = "Reshape_3834-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<4xi64>) -> tensor<2x2560x8x8xf32>
    %4972 = "dtu_hlir.mul"(%4971, %519) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3835-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x8x8xf32>
    %4973 = "dtu_hlir.add"(%4972, %520) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3836-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x8x8xf32>
    %4974 = "dtu_hlir.sigmoid"(%4973) {node_name = "Sigmoid_3837-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %4975 = "dtu_hlir.mul"(%4973, %4974) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3838-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2x2560x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %4976 = "dtu_hlir.conv_bias"(%4975, %144, %145) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3839-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x8x8xf32>, tensor<1280x2560x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4977 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3840-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4978 = "dtu_hlir.mul"(%915, %4977) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3841-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %4979 = "dtu_hlir.transpose"(%146) {node_name = "Gemm_3842-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %4980 = "dtu_hlir.gemm"(%4978, %4979, %147) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3842-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %4981 = dtu_hlir.constant  {node_name = "Constant_3843-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %4982 = "dtu_hlir.unsqueeze"(%4980, %4981) {node_name = "Unsqueeze_3844-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %4983 = dtu_hlir.constant  {node_name = "Constant_3845-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %4984 = "dtu_hlir.unsqueeze"(%4982, %4983) {node_name = "Unsqueeze_3846-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %4985 = "dtu_hlir.add"(%4976, %4984) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3847-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4986 = dtu_hlir.constant  {node_name = "Constant_3848-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %4987 = "dtu_hlir.dynamic_reshape"(%4985, %4986) {allowzero = 0 : i64, node_name = "Reshape_3849-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %4988 = dtu_hlir.constant  {node_name = "Constant_3850-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %4989 = dtu_hlir.constant  {node_name = "Constant_3851-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %4990 = "dtu_hlir.instance_norm"(%4987, %4988, %4989) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3852-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %4991 = "dtu_hlir.shape"(%4985) {end = 2147483647 : i64, node_name = "Shape_3853-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %4992 = "dtu_hlir.dynamic_reshape"(%4990, %4991) {allowzero = 0 : i64, node_name = "Reshape_3854-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %4993 = "dtu_hlir.mul"(%4992, %521) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3855-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4994 = "dtu_hlir.add"(%4993, %522) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3856-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %4995 = "dtu_hlir.sigmoid"(%4994) {node_name = "Sigmoid_3857-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4996 = "dtu_hlir.mul"(%4994, %4995) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3858-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %4997 = "dtu_hlir.conv_bias"(%4996, %148, %149) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3859-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4998 = "dtu_hlir.conv_bias"(%4964, %150, %151) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3860-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x8x8xf32>, tensor<1280x2560x1x1xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %4999 = "dtu_hlir.add"(%4998, %4997) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3861-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %5000 = dtu_hlir.constant  {node_name = "Constant_3862-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %5001 = "dtu_hlir.broadcast_in_dim"(%5000) {node_name = "Div_3863-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x8x8xf32>
    %5002 = "dtu_hlir.div"(%4999, %5001) {node_name = "Div_3863-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %5003 = "dtu_hlir.concatenate"(%5002, %4257) {dimension = 1 : i64, node_name = "Concat_3864-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %5004 = dtu_hlir.constant  {node_name = "Constant_3865-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5005 = "dtu_hlir.dynamic_reshape"(%5003, %5004) {allowzero = 0 : i64, node_name = "Reshape_3866-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<3xi64>) -> tensor<2x32x5120xf32>
    %5006 = dtu_hlir.constant  {node_name = "Constant_3867-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5007 = dtu_hlir.constant  {node_name = "Constant_3868-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5008 = "dtu_hlir.instance_norm"(%5005, %5006, %5007) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3869-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x5120xf32>
    %5009 = "dtu_hlir.shape"(%5003) {end = 2147483647 : i64, node_name = "Shape_3870-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>) -> tensor<4xi64>
    %5010 = "dtu_hlir.dynamic_reshape"(%5008, %5009) {allowzero = 0 : i64, node_name = "Reshape_3871-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x5120xf32>, tensor<4xi64>) -> tensor<2x2560x8x8xf32>
    %5011 = "dtu_hlir.mul"(%5010, %523) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3872-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x8x8xf32>
    %5012 = "dtu_hlir.add"(%5011, %524) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3873-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x8x8xf32>
    %5013 = "dtu_hlir.sigmoid"(%5012) {node_name = "Sigmoid_3874-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %5014 = "dtu_hlir.mul"(%5012, %5013) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3875-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x8x8xf32>, tensor<2x2560x8x8xf32>) -> tensor<2x2560x8x8xf32>
    %5015 = "dtu_hlir.conv_bias"(%5014, %152, %153) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3876-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x8x8xf32>, tensor<1280x2560x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %5016 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3877-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %5017 = "dtu_hlir.mul"(%915, %5016) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3878-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %5018 = "dtu_hlir.transpose"(%154) {node_name = "Gemm_3879-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %5019 = "dtu_hlir.gemm"(%5017, %5018, %155) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3879-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %5020 = dtu_hlir.constant  {node_name = "Constant_3880-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %5021 = "dtu_hlir.unsqueeze"(%5019, %5020) {node_name = "Unsqueeze_3881-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %5022 = dtu_hlir.constant  {node_name = "Constant_3882-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %5023 = "dtu_hlir.unsqueeze"(%5021, %5022) {node_name = "Unsqueeze_3883-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %5024 = "dtu_hlir.add"(%5015, %5023) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3884-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %5025 = dtu_hlir.constant  {node_name = "Constant_3885-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5026 = "dtu_hlir.dynamic_reshape"(%5024, %5025) {allowzero = 0 : i64, node_name = "Reshape_3886-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<3xi64>) -> tensor<2x32x2560xf32>
    %5027 = dtu_hlir.constant  {node_name = "Constant_3887-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5028 = dtu_hlir.constant  {node_name = "Constant_3888-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5029 = "dtu_hlir.instance_norm"(%5026, %5027, %5028) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3889-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x2560xf32>
    %5030 = "dtu_hlir.shape"(%5024) {end = 2147483647 : i64, node_name = "Shape_3890-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<4xi64>
    %5031 = "dtu_hlir.dynamic_reshape"(%5029, %5030) {allowzero = 0 : i64, node_name = "Reshape_3891-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x2560xf32>, tensor<4xi64>) -> tensor<2x1280x8x8xf32>
    %5032 = "dtu_hlir.mul"(%5031, %525) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3892-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %5033 = "dtu_hlir.add"(%5032, %526) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3893-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x8x8xf32>
    %5034 = "dtu_hlir.sigmoid"(%5033) {node_name = "Sigmoid_3894-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %5035 = "dtu_hlir.mul"(%5033, %5034) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3895-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %5036 = "dtu_hlir.conv_bias"(%5035, %156, %157) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3896-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x8x8xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %5037 = "dtu_hlir.conv_bias"(%5003, %158, %159) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3897-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x8x8xf32>, tensor<1280x2560x1x1xf32>, tensor<1280xf32>) -> tensor<2x1280x8x8xf32>
    %5038 = "dtu_hlir.add"(%5037, %5036) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3898-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %5039 = dtu_hlir.constant  {node_name = "Constant_3899-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %5040 = "dtu_hlir.broadcast_in_dim"(%5039) {node_name = "Div_3900-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x8x8xf32>
    %5041 = "dtu_hlir.div"(%5038, %5040) {node_name = "Div_3900-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<2x1280x8x8xf32>) -> tensor<2x1280x8x8xf32>
    %5042 = dtu_hlir.constant  {node_name = "Resize_3901-Const-2", node_type = "Resize"} dense<[]> : tensor<0xi64>
    %5043 = dtu_hlir.constant  {node_name = "Resize_3901-Const-3", node_type = "Resize"} dense<[0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00]> : tensor<8xf32>
    %5044 = "dtu_hlir.resize"(%5041, %5043, %527, %5042) {coordinate_transformation_mode = 1 : i64, cubic_coeff_a = -7.500000e-01 : f32, exclude_outside = false, extrapolation_value = 0.000000e+00 : f32, mode = 0 : i64, nearest_mode = 3 : i64, node_name = "Resize_3901-4", node_type = "Resize", resize_dimensions = dense<[-2, -1]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x8x8xf32>, tensor<8xf32>, tensor<4xf32>, tensor<0xi64>) -> tensor<2x1280x16x16xf32>
    %5045 = "dtu_hlir.conv_bias"(%5044, %160, %161) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3902-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %5046 = "dtu_hlir.concatenate"(%5045, %4256) {dimension = 1 : i64, node_name = "Concat_3903-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x2560x16x16xf32>
    %5047 = dtu_hlir.constant  {node_name = "Constant_3904-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5048 = "dtu_hlir.dynamic_reshape"(%5046, %5047) {allowzero = 0 : i64, node_name = "Reshape_3905-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %5049 = dtu_hlir.constant  {node_name = "Constant_3906-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5050 = dtu_hlir.constant  {node_name = "Constant_3907-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5051 = "dtu_hlir.instance_norm"(%5048, %5049, %5050) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3908-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %5052 = "dtu_hlir.shape"(%5046) {end = 2147483647 : i64, node_name = "Shape_3909-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>) -> tensor<4xi64>
    %5053 = "dtu_hlir.dynamic_reshape"(%5051, %5052) {allowzero = 0 : i64, node_name = "Reshape_3910-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x2560x16x16xf32>
    %5054 = "dtu_hlir.mul"(%5053, %528) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3911-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x16x16xf32>
    %5055 = "dtu_hlir.add"(%5054, %529) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3912-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x16x16xf32>
    %5056 = "dtu_hlir.sigmoid"(%5055) {node_name = "Sigmoid_3913-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>) -> tensor<2x2560x16x16xf32>
    %5057 = "dtu_hlir.mul"(%5055, %5056) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3914-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<2x2560x16x16xf32>) -> tensor<2x2560x16x16xf32>
    %5058 = "dtu_hlir.conv_bias"(%5057, %198, %199) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3915-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x16x16xf32>, tensor<1280x2560x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %5059 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_3916-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %5060 = "dtu_hlir.mul"(%915, %5059) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3917-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %5061 = "dtu_hlir.transpose"(%200) {node_name = "Gemm_3918-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %5062 = "dtu_hlir.gemm"(%5060, %5061, %201) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_3918-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %5063 = dtu_hlir.constant  {node_name = "Constant_3919-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %5064 = "dtu_hlir.unsqueeze"(%5062, %5063) {node_name = "Unsqueeze_3920-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %5065 = dtu_hlir.constant  {node_name = "Constant_3921-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %5066 = "dtu_hlir.unsqueeze"(%5064, %5065) {node_name = "Unsqueeze_3922-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %5067 = "dtu_hlir.add"(%5058, %5066) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3923-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5068 = dtu_hlir.constant  {node_name = "Constant_3924-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5069 = "dtu_hlir.dynamic_reshape"(%5067, %5068) {allowzero = 0 : i64, node_name = "Reshape_3925-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %5070 = dtu_hlir.constant  {node_name = "Constant_3926-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5071 = dtu_hlir.constant  {node_name = "Constant_3927-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5072 = "dtu_hlir.instance_norm"(%5069, %5070, %5071) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3928-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %5073 = "dtu_hlir.shape"(%5067) {end = 2147483647 : i64, node_name = "Shape_3929-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5074 = "dtu_hlir.dynamic_reshape"(%5072, %5073) {allowzero = 0 : i64, node_name = "Reshape_3930-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %5075 = "dtu_hlir.mul"(%5074, %530) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3931-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5076 = "dtu_hlir.add"(%5075, %531) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3932-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5077 = "dtu_hlir.sigmoid"(%5076) {node_name = "Sigmoid_3933-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5078 = "dtu_hlir.mul"(%5076, %5077) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_3934-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5079 = "dtu_hlir.conv_bias"(%5078, %202, %203) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3935-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %5080 = "dtu_hlir.conv_bias"(%5046, %204, %205) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3936-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x16x16xf32>, tensor<1280x2560x1x1xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %5081 = "dtu_hlir.add"(%5080, %5079) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_3937-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5082 = dtu_hlir.constant  {node_name = "Constant_3938-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %5083 = "dtu_hlir.broadcast_in_dim"(%5082) {node_name = "Div_3939-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x16x16xf32>
    %5084 = "dtu_hlir.div"(%5081, %5083) {node_name = "Div_3939-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5085 = "dtu_hlir.shape"(%5084) {end = 2147483647 : i64, node_name = "Shape_3940-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5086 = dtu_hlir.constant  {node_name = "Constant_3941-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5087 = "dtu_hlir.gather"(%5085, %5086) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3942-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5088 = "dtu_hlir.shape"(%5084) {end = 2147483647 : i64, node_name = "Shape_3943-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5089 = dtu_hlir.constant  {node_name = "Constant_3944-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5090 = "dtu_hlir.gather"(%5088, %5089) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3945-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5091 = "dtu_hlir.shape"(%5084) {end = 2147483647 : i64, node_name = "Shape_3946-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5092 = dtu_hlir.constant  {node_name = "Constant_3947-0", node_type = "Constant"} dense<3> : tensor<i64>
    %5093 = "dtu_hlir.gather"(%5091, %5092) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3948-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5094 = dtu_hlir.constant  {node_name = "Constant_3949-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5095 = "dtu_hlir.dynamic_reshape"(%5084, %5094) {allowzero = 0 : i64, node_name = "Reshape_3950-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %5096 = dtu_hlir.constant  {node_name = "Constant_3951-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5097 = dtu_hlir.constant  {node_name = "Constant_3952-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5098 = "dtu_hlir.instance_norm"(%5095, %5096, %5097) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_3953-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %5099 = "dtu_hlir.shape"(%5084) {end = 2147483647 : i64, node_name = "Shape_3954-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5100 = "dtu_hlir.dynamic_reshape"(%5098, %5099) {allowzero = 0 : i64, node_name = "Reshape_3955-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %5101 = "dtu_hlir.mul"(%5100, %532) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_3956-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5102 = "dtu_hlir.add"(%5101, %533) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_3957-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5103 = "dtu_hlir.shape"(%5102) {end = 2147483647 : i64, node_name = "Shape_3958-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5104 = dtu_hlir.constant  {node_name = "Constant_3959-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5105 = "dtu_hlir.gather"(%5103, %5104) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3960-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5106 = "dtu_hlir.transpose"(%5102) {node_name = "Transpose_3961-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x16x16x1280xf32>
    %5107 = "dtu_hlir.mul"(%5090, %5093) {node_name = "Mul_3962-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5108 = dtu_hlir.constant  {node_name = "Constant_3963-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5109 = "dtu_hlir.unsqueeze"(%5087, %5108) {node_name = "Unsqueeze_3964-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5110 = dtu_hlir.constant  {node_name = "Constant_3965-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5111 = "dtu_hlir.unsqueeze"(%5107, %5110) {node_name = "Unsqueeze_3966-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5112 = dtu_hlir.constant  {node_name = "Constant_3967-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5113 = "dtu_hlir.unsqueeze"(%5105, %5112) {node_name = "Unsqueeze_3968-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5114 = "dtu_hlir.concatenate"(%5109, %5111, %5113) {dimension = 0 : i64, node_name = "Concat_3969-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5115 = "dtu_hlir.dynamic_reshape"(%5106, %5114) {allowzero = 0 : i64, node_name = "Reshape_3970-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %5116 = "dtu_hlir.dot_general"(%5115, %534) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3971-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5117 = "dtu_hlir.add"(%162, %5116) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3972-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5118 = dtu_hlir.constant  {node_name = "ReduceMean_3973-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5119 = "dtu_hlir.reshape"(%5118) {node_name = "ReduceMean_3973-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5120 = "dtu_hlir.reduce"(%5117, %5119) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3973-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3973-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5121 = dtu_hlir.constant  {node_name = "ReduceMean_3973-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5122 = "dtu_hlir.unsqueeze"(%5120, %5121) {node_name = "ReduceMean_3973-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5123 = dtu_hlir.constant  {node_name = "ReduceMean_3973-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5124 = "dtu_hlir.broadcast_in_dim"(%5123) {node_name = "ReduceMean_3973-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5125 = "dtu_hlir.div"(%5122, %5124) {node_name = "ReduceMean_3973-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5126 = "dtu_hlir.sub"(%5117, %5125) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_3974-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5127 = dtu_hlir.constant  {node_name = "Constant_3975-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %5128 = "dtu_hlir.broadcast_in_dim"(%5127) {node_name = "Pow_3976-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %5129 = "dtu_hlir.pow"(%5126, %5128) {node_name = "Pow_3976-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5130 = dtu_hlir.constant  {node_name = "ReduceMean_3977-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5131 = "dtu_hlir.reshape"(%5130) {node_name = "ReduceMean_3977-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5132 = "dtu_hlir.reduce"(%5129, %5131) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_3977-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_3977-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5133 = dtu_hlir.constant  {node_name = "ReduceMean_3977-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5134 = "dtu_hlir.unsqueeze"(%5132, %5133) {node_name = "ReduceMean_3977-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5135 = dtu_hlir.constant  {node_name = "ReduceMean_3977-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5136 = "dtu_hlir.broadcast_in_dim"(%5135) {node_name = "ReduceMean_3977-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5137 = "dtu_hlir.div"(%5134, %5136) {node_name = "ReduceMean_3977-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5138 = dtu_hlir.constant  {node_name = "Constant_3978-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %5139 = "dtu_hlir.broadcast_in_dim"(%5138) {node_name = "Add_3979-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %5140 = "dtu_hlir.add"(%5137, %5139) {node_name = "Add_3979-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5141 = "dtu_hlir.sqrt"(%5140) {node_name = "Sqrt_3980-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5142 = "dtu_hlir.div"(%5126, %5141) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_3981-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5143 = "dtu_hlir.mul"(%5142, %167) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_3982-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5144 = "dtu_hlir.add"(%5143, %168) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_3983-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5145 = "dtu_hlir.dot_general"(%5144, %535) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3984-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5146 = "dtu_hlir.dot_general"(%5144, %536) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3985-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5147 = "dtu_hlir.dot_general"(%5144, %537) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_3986-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5148 = "dtu_hlir.shape"(%5145) {end = 2147483647 : i64, node_name = "Shape_3987-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5149 = dtu_hlir.constant  {node_name = "Constant_3988-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5150 = "dtu_hlir.gather"(%5148, %5149) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3989-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5151 = "dtu_hlir.shape"(%5145) {end = 2147483647 : i64, node_name = "Shape_3990-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5152 = dtu_hlir.constant  {node_name = "Constant_3991-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5153 = "dtu_hlir.gather"(%5151, %5152) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3992-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5154 = "dtu_hlir.shape"(%5145) {end = 2147483647 : i64, node_name = "Shape_3993-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5155 = dtu_hlir.constant  {node_name = "Constant_3994-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5156 = "dtu_hlir.gather"(%5154, %5155) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_3995-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5157 = dtu_hlir.constant  {node_name = "Constant_3996-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5158 = "dtu_hlir.div"(%5156, %5157) {node_name = "Div_3997-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5159 = "dtu_hlir.convert"(%5158) {node_name = "Cast_3998-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5160 = "dtu_hlir.convert"(%5159) {node_name = "Cast_3999-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5161 = dtu_hlir.constant  {node_name = "Constant_4000-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5162 = "dtu_hlir.unsqueeze"(%5150, %5161) {node_name = "Unsqueeze_4001-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5163 = dtu_hlir.constant  {node_name = "Constant_4002-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5164 = "dtu_hlir.unsqueeze"(%5153, %5163) {node_name = "Unsqueeze_4003-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5165 = dtu_hlir.constant  {node_name = "Constant_4004-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5166 = "dtu_hlir.unsqueeze"(%5160, %5165) {node_name = "Unsqueeze_4005-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5167 = "dtu_hlir.concatenate"(%5162, %5164, %801, %5166) {dimension = 0 : i64, node_name = "Concat_4006-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5168 = "dtu_hlir.dynamic_reshape"(%5145, %5167) {allowzero = 0 : i64, node_name = "Reshape_4007-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5169 = "dtu_hlir.transpose"(%5168) {node_name = "Transpose_4008-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5170 = dtu_hlir.constant  {node_name = "Constant_4009-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5171 = "dtu_hlir.mul"(%5150, %5170) {node_name = "Mul_4010-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5172 = dtu_hlir.constant  {node_name = "Constant_4011-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5173 = "dtu_hlir.div"(%5156, %5172) {node_name = "Div_4012-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5174 = "dtu_hlir.convert"(%5173) {node_name = "Cast_4013-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5175 = "dtu_hlir.convert"(%5174) {node_name = "Cast_4014-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5176 = dtu_hlir.constant  {node_name = "Constant_4015-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5177 = "dtu_hlir.unsqueeze"(%5171, %5176) {node_name = "Unsqueeze_4016-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5178 = dtu_hlir.constant  {node_name = "Constant_4017-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5179 = "dtu_hlir.unsqueeze"(%5153, %5178) {node_name = "Unsqueeze_4018-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5180 = dtu_hlir.constant  {node_name = "Constant_4019-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5181 = "dtu_hlir.unsqueeze"(%5175, %5180) {node_name = "Unsqueeze_4020-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5182 = "dtu_hlir.concatenate"(%5177, %5179, %5181) {dimension = 0 : i64, node_name = "Concat_4021-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5183 = "dtu_hlir.dynamic_reshape"(%5169, %5182) {allowzero = 0 : i64, node_name = "Reshape_4022-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5184 = "dtu_hlir.shape"(%5146) {end = 2147483647 : i64, node_name = "Shape_4023-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5185 = dtu_hlir.constant  {node_name = "Constant_4024-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5186 = "dtu_hlir.gather"(%5184, %5185) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4025-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5187 = "dtu_hlir.shape"(%5146) {end = 2147483647 : i64, node_name = "Shape_4026-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5188 = dtu_hlir.constant  {node_name = "Constant_4027-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5189 = "dtu_hlir.gather"(%5187, %5188) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4028-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5190 = "dtu_hlir.shape"(%5146) {end = 2147483647 : i64, node_name = "Shape_4029-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5191 = dtu_hlir.constant  {node_name = "Constant_4030-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5192 = "dtu_hlir.gather"(%5190, %5191) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4031-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5193 = dtu_hlir.constant  {node_name = "Constant_4032-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5194 = "dtu_hlir.div"(%5192, %5193) {node_name = "Div_4033-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5195 = "dtu_hlir.convert"(%5194) {node_name = "Cast_4034-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5196 = "dtu_hlir.convert"(%5195) {node_name = "Cast_4035-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5197 = dtu_hlir.constant  {node_name = "Constant_4036-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5198 = "dtu_hlir.unsqueeze"(%5186, %5197) {node_name = "Unsqueeze_4037-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5199 = dtu_hlir.constant  {node_name = "Constant_4038-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5200 = "dtu_hlir.unsqueeze"(%5189, %5199) {node_name = "Unsqueeze_4039-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5201 = dtu_hlir.constant  {node_name = "Constant_4040-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5202 = "dtu_hlir.unsqueeze"(%5196, %5201) {node_name = "Unsqueeze_4041-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5203 = "dtu_hlir.concatenate"(%5198, %5200, %800, %5202) {dimension = 0 : i64, node_name = "Concat_4042-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5204 = "dtu_hlir.dynamic_reshape"(%5146, %5203) {allowzero = 0 : i64, node_name = "Reshape_4043-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5205 = "dtu_hlir.transpose"(%5204) {node_name = "Transpose_4044-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5206 = dtu_hlir.constant  {node_name = "Constant_4045-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5207 = "dtu_hlir.mul"(%5186, %5206) {node_name = "Mul_4046-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5208 = dtu_hlir.constant  {node_name = "Constant_4047-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5209 = "dtu_hlir.div"(%5192, %5208) {node_name = "Div_4048-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5210 = "dtu_hlir.convert"(%5209) {node_name = "Cast_4049-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5211 = "dtu_hlir.convert"(%5210) {node_name = "Cast_4050-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5212 = dtu_hlir.constant  {node_name = "Constant_4051-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5213 = "dtu_hlir.unsqueeze"(%5207, %5212) {node_name = "Unsqueeze_4052-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5214 = dtu_hlir.constant  {node_name = "Constant_4053-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5215 = "dtu_hlir.unsqueeze"(%5189, %5214) {node_name = "Unsqueeze_4054-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5216 = dtu_hlir.constant  {node_name = "Constant_4055-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5217 = "dtu_hlir.unsqueeze"(%5211, %5216) {node_name = "Unsqueeze_4056-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5218 = "dtu_hlir.concatenate"(%5213, %5215, %5217) {dimension = 0 : i64, node_name = "Concat_4057-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5219 = "dtu_hlir.dynamic_reshape"(%5205, %5218) {allowzero = 0 : i64, node_name = "Reshape_4058-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5220 = "dtu_hlir.shape"(%5147) {end = 2147483647 : i64, node_name = "Shape_4059-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5221 = dtu_hlir.constant  {node_name = "Constant_4060-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5222 = "dtu_hlir.gather"(%5220, %5221) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4061-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5223 = "dtu_hlir.shape"(%5147) {end = 2147483647 : i64, node_name = "Shape_4062-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5224 = dtu_hlir.constant  {node_name = "Constant_4063-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5225 = "dtu_hlir.gather"(%5223, %5224) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4064-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5226 = "dtu_hlir.shape"(%5147) {end = 2147483647 : i64, node_name = "Shape_4065-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5227 = dtu_hlir.constant  {node_name = "Constant_4066-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5228 = "dtu_hlir.gather"(%5226, %5227) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4067-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5229 = dtu_hlir.constant  {node_name = "Constant_4068-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5230 = "dtu_hlir.div"(%5228, %5229) {node_name = "Div_4069-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5231 = "dtu_hlir.convert"(%5230) {node_name = "Cast_4070-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5232 = "dtu_hlir.convert"(%5231) {node_name = "Cast_4071-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5233 = dtu_hlir.constant  {node_name = "Constant_4072-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5234 = "dtu_hlir.unsqueeze"(%5222, %5233) {node_name = "Unsqueeze_4073-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5235 = dtu_hlir.constant  {node_name = "Constant_4074-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5236 = "dtu_hlir.unsqueeze"(%5225, %5235) {node_name = "Unsqueeze_4075-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5237 = dtu_hlir.constant  {node_name = "Constant_4076-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5238 = "dtu_hlir.unsqueeze"(%5232, %5237) {node_name = "Unsqueeze_4077-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5239 = "dtu_hlir.concatenate"(%5234, %5236, %799, %5238) {dimension = 0 : i64, node_name = "Concat_4078-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5240 = "dtu_hlir.dynamic_reshape"(%5147, %5239) {allowzero = 0 : i64, node_name = "Reshape_4079-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5241 = "dtu_hlir.transpose"(%5240) {node_name = "Transpose_4080-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5242 = dtu_hlir.constant  {node_name = "Constant_4081-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5243 = "dtu_hlir.mul"(%5222, %5242) {node_name = "Mul_4082-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5244 = dtu_hlir.constant  {node_name = "Constant_4083-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5245 = "dtu_hlir.div"(%5228, %5244) {node_name = "Div_4084-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5246 = "dtu_hlir.convert"(%5245) {node_name = "Cast_4085-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5247 = "dtu_hlir.convert"(%5246) {node_name = "Cast_4086-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5248 = dtu_hlir.constant  {node_name = "Constant_4087-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5249 = "dtu_hlir.unsqueeze"(%5243, %5248) {node_name = "Unsqueeze_4088-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5250 = dtu_hlir.constant  {node_name = "Constant_4089-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5251 = "dtu_hlir.unsqueeze"(%5225, %5250) {node_name = "Unsqueeze_4090-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5252 = dtu_hlir.constant  {node_name = "Constant_4091-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5253 = "dtu_hlir.unsqueeze"(%5247, %5252) {node_name = "Unsqueeze_4092-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5254 = "dtu_hlir.concatenate"(%5249, %5251, %5253) {dimension = 0 : i64, node_name = "Concat_4093-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5255 = "dtu_hlir.dynamic_reshape"(%5241, %5254) {allowzero = 0 : i64, node_name = "Reshape_4094-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5256 = "dtu_hlir.shape"(%5183) {end = 2147483647 : i64, node_name = "Shape_4095-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5257 = dtu_hlir.constant  {node_name = "Constant_4096-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5258 = "dtu_hlir.gather"(%5256, %5257) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4097-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5259 = "dtu_hlir.shape"(%5183) {end = 2147483647 : i64, node_name = "Shape_4098-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5260 = dtu_hlir.constant  {node_name = "Constant_4099-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5261 = "dtu_hlir.gather"(%5259, %5260) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4100-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5262 = "dtu_hlir.shape"(%5219) {end = 2147483647 : i64, node_name = "Shape_4101-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5263 = dtu_hlir.constant  {node_name = "Constant_4102-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5264 = "dtu_hlir.gather"(%5262, %5263) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4103-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5265 = dtu_hlir.constant  {node_name = "Constant_4104-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5266 = "dtu_hlir.unsqueeze"(%5258, %5265) {node_name = "Unsqueeze_4105-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5267 = dtu_hlir.constant  {node_name = "Constant_4106-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5268 = "dtu_hlir.unsqueeze"(%5261, %5267) {node_name = "Unsqueeze_4107-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5269 = dtu_hlir.constant  {node_name = "Constant_4108-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5270 = "dtu_hlir.unsqueeze"(%5264, %5269) {node_name = "Unsqueeze_4109-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5271 = "dtu_hlir.concatenate"(%5266, %5268, %5270) {dimension = 0 : i64, node_name = "Concat_4110-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5272 = dtu_hlir.constant  {node_name = "ConstantOfShape_4111-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %5273 = "dtu_hlir.dynamic_broadcast_in_dim"(%5272, %5271) {node_name = "ConstantOfShape_4111-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x256xf32>
    %5274 = "dtu_hlir.transpose"(%5219) {node_name = "Transpose_4112-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<40x64x256xf32>
    %5275 = "dtu_hlir.dot_general"(%5183, %5274) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4113-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x256xf32>) -> tensor<40x256x256xf32>
    %5276 = "dtu_hlir.broadcast_in_dim"(%798) {node_name = "Mul_4114-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %5277 = "dtu_hlir.mul"(%5275, %5276) {node_name = "Mul_4114-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5278 = "dtu_hlir.broadcast_in_dim"(%797) {node_name = "Mul_4115-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %5279 = "dtu_hlir.mul"(%5273, %5278) {node_name = "Mul_4115-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5280 = "dtu_hlir.add"(%5277, %5279) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4116-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5281 = "dtu_hlir.softmax"(%5280) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4117-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5282 = "dtu_hlir.convert"(%5281) {node_name = "Cast_4118-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5283 = "dtu_hlir.dot_general"(%5282, %5255) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4119-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x64xf32>) -> tensor<40x256x64xf32>
    %5284 = "dtu_hlir.shape"(%5283) {end = 2147483647 : i64, node_name = "Shape_4120-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5285 = dtu_hlir.constant  {node_name = "Constant_4121-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5286 = "dtu_hlir.gather"(%5284, %5285) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4122-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5287 = "dtu_hlir.shape"(%5283) {end = 2147483647 : i64, node_name = "Shape_4123-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5288 = dtu_hlir.constant  {node_name = "Constant_4124-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5289 = "dtu_hlir.gather"(%5287, %5288) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4125-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5290 = "dtu_hlir.shape"(%5283) {end = 2147483647 : i64, node_name = "Shape_4126-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5291 = dtu_hlir.constant  {node_name = "Constant_4127-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5292 = "dtu_hlir.gather"(%5290, %5291) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4128-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5293 = dtu_hlir.constant  {node_name = "Constant_4129-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5294 = "dtu_hlir.div"(%5286, %5293) {node_name = "Div_4130-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5295 = "dtu_hlir.convert"(%5294) {node_name = "Cast_4131-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5296 = "dtu_hlir.convert"(%5295) {node_name = "Cast_4132-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5297 = dtu_hlir.constant  {node_name = "Constant_4133-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5298 = "dtu_hlir.unsqueeze"(%5296, %5297) {node_name = "Unsqueeze_4134-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5299 = dtu_hlir.constant  {node_name = "Constant_4135-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5300 = "dtu_hlir.unsqueeze"(%5289, %5299) {node_name = "Unsqueeze_4136-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5301 = dtu_hlir.constant  {node_name = "Constant_4137-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5302 = "dtu_hlir.unsqueeze"(%5292, %5301) {node_name = "Unsqueeze_4138-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5303 = "dtu_hlir.concatenate"(%5298, %796, %5300, %5302) {dimension = 0 : i64, node_name = "Concat_4139-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5304 = "dtu_hlir.dynamic_reshape"(%5283, %5303) {allowzero = 0 : i64, node_name = "Reshape_4140-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %5305 = "dtu_hlir.transpose"(%5304) {node_name = "Transpose_4141-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %5306 = dtu_hlir.constant  {node_name = "Constant_4142-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5307 = "dtu_hlir.div"(%5286, %5306) {node_name = "Div_4143-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5308 = "dtu_hlir.convert"(%5307) {node_name = "Cast_4144-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5309 = "dtu_hlir.convert"(%5308) {node_name = "Cast_4145-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5310 = dtu_hlir.constant  {node_name = "Constant_4146-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5311 = "dtu_hlir.mul"(%5292, %5310) {node_name = "Mul_4147-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5312 = dtu_hlir.constant  {node_name = "Constant_4148-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5313 = "dtu_hlir.unsqueeze"(%5309, %5312) {node_name = "Unsqueeze_4149-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5314 = dtu_hlir.constant  {node_name = "Constant_4150-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5315 = "dtu_hlir.unsqueeze"(%5289, %5314) {node_name = "Unsqueeze_4151-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5316 = dtu_hlir.constant  {node_name = "Constant_4152-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5317 = "dtu_hlir.unsqueeze"(%5311, %5316) {node_name = "Unsqueeze_4153-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5318 = "dtu_hlir.concatenate"(%5313, %5315, %5317) {dimension = 0 : i64, node_name = "Concat_4154-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5319 = "dtu_hlir.dynamic_reshape"(%5305, %5318) {allowzero = 0 : i64, node_name = "Reshape_4155-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %5320 = "dtu_hlir.dot_general"(%5319, %538) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4156-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5321 = "dtu_hlir.add"(%163, %5320) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4157-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5322 = "dtu_hlir.add"(%5321, %5117) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4158-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5323 = dtu_hlir.constant  {node_name = "ReduceMean_4159-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5324 = "dtu_hlir.reshape"(%5323) {node_name = "ReduceMean_4159-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5325 = "dtu_hlir.reduce"(%5322, %5324) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4159-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4159-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5326 = dtu_hlir.constant  {node_name = "ReduceMean_4159-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5327 = "dtu_hlir.unsqueeze"(%5325, %5326) {node_name = "ReduceMean_4159-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5328 = dtu_hlir.constant  {node_name = "ReduceMean_4159-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5329 = "dtu_hlir.broadcast_in_dim"(%5328) {node_name = "ReduceMean_4159-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5330 = "dtu_hlir.div"(%5327, %5329) {node_name = "ReduceMean_4159-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5331 = "dtu_hlir.sub"(%5322, %5330) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_4160-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5332 = dtu_hlir.constant  {node_name = "Constant_4161-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %5333 = "dtu_hlir.broadcast_in_dim"(%5332) {node_name = "Pow_4162-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %5334 = "dtu_hlir.pow"(%5331, %5333) {node_name = "Pow_4162-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5335 = dtu_hlir.constant  {node_name = "ReduceMean_4163-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5336 = "dtu_hlir.reshape"(%5335) {node_name = "ReduceMean_4163-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5337 = "dtu_hlir.reduce"(%5334, %5336) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4163-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4163-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5338 = dtu_hlir.constant  {node_name = "ReduceMean_4163-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5339 = "dtu_hlir.unsqueeze"(%5337, %5338) {node_name = "ReduceMean_4163-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5340 = dtu_hlir.constant  {node_name = "ReduceMean_4163-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5341 = "dtu_hlir.broadcast_in_dim"(%5340) {node_name = "ReduceMean_4163-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5342 = "dtu_hlir.div"(%5339, %5341) {node_name = "ReduceMean_4163-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5343 = dtu_hlir.constant  {node_name = "Constant_4164-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %5344 = "dtu_hlir.broadcast_in_dim"(%5343) {node_name = "Add_4165-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %5345 = "dtu_hlir.add"(%5342, %5344) {node_name = "Add_4165-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5346 = "dtu_hlir.sqrt"(%5345) {node_name = "Sqrt_4166-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5347 = "dtu_hlir.div"(%5331, %5346) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_4167-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5348 = "dtu_hlir.mul"(%5347, %169) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_4168-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5349 = "dtu_hlir.add"(%5348, %170) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4169-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5350 = "dtu_hlir.dot_general"(%5349, %539) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4170-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5351 = "dtu_hlir.dot_general"(%arg2, %540) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4171-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %5352 = "dtu_hlir.dot_general"(%arg2, %541) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4172-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %5353 = "dtu_hlir.shape"(%5350) {end = 2147483647 : i64, node_name = "Shape_4173-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5354 = dtu_hlir.constant  {node_name = "Constant_4174-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5355 = "dtu_hlir.gather"(%5353, %5354) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4175-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5356 = "dtu_hlir.shape"(%5350) {end = 2147483647 : i64, node_name = "Shape_4176-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5357 = dtu_hlir.constant  {node_name = "Constant_4177-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5358 = "dtu_hlir.gather"(%5356, %5357) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4178-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5359 = "dtu_hlir.shape"(%5350) {end = 2147483647 : i64, node_name = "Shape_4179-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5360 = dtu_hlir.constant  {node_name = "Constant_4180-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5361 = "dtu_hlir.gather"(%5359, %5360) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4181-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5362 = dtu_hlir.constant  {node_name = "Constant_4182-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5363 = "dtu_hlir.div"(%5361, %5362) {node_name = "Div_4183-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5364 = "dtu_hlir.convert"(%5363) {node_name = "Cast_4184-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5365 = "dtu_hlir.convert"(%5364) {node_name = "Cast_4185-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5366 = dtu_hlir.constant  {node_name = "Constant_4186-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5367 = "dtu_hlir.unsqueeze"(%5355, %5366) {node_name = "Unsqueeze_4187-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5368 = dtu_hlir.constant  {node_name = "Constant_4188-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5369 = "dtu_hlir.unsqueeze"(%5358, %5368) {node_name = "Unsqueeze_4189-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5370 = dtu_hlir.constant  {node_name = "Constant_4190-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5371 = "dtu_hlir.unsqueeze"(%5365, %5370) {node_name = "Unsqueeze_4191-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5372 = "dtu_hlir.concatenate"(%5367, %5369, %795, %5371) {dimension = 0 : i64, node_name = "Concat_4192-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5373 = "dtu_hlir.dynamic_reshape"(%5350, %5372) {allowzero = 0 : i64, node_name = "Reshape_4193-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5374 = "dtu_hlir.transpose"(%5373) {node_name = "Transpose_4194-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5375 = dtu_hlir.constant  {node_name = "Constant_4195-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5376 = "dtu_hlir.mul"(%5355, %5375) {node_name = "Mul_4196-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5377 = dtu_hlir.constant  {node_name = "Constant_4197-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5378 = "dtu_hlir.div"(%5361, %5377) {node_name = "Div_4198-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5379 = "dtu_hlir.convert"(%5378) {node_name = "Cast_4199-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5380 = "dtu_hlir.convert"(%5379) {node_name = "Cast_4200-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5381 = dtu_hlir.constant  {node_name = "Constant_4201-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5382 = "dtu_hlir.unsqueeze"(%5376, %5381) {node_name = "Unsqueeze_4202-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5383 = dtu_hlir.constant  {node_name = "Constant_4203-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5384 = "dtu_hlir.unsqueeze"(%5358, %5383) {node_name = "Unsqueeze_4204-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5385 = dtu_hlir.constant  {node_name = "Constant_4205-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5386 = "dtu_hlir.unsqueeze"(%5380, %5385) {node_name = "Unsqueeze_4206-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5387 = "dtu_hlir.concatenate"(%5382, %5384, %5386) {dimension = 0 : i64, node_name = "Concat_4207-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5388 = "dtu_hlir.dynamic_reshape"(%5374, %5387) {allowzero = 0 : i64, node_name = "Reshape_4208-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5389 = "dtu_hlir.shape"(%5351) {end = 2147483647 : i64, node_name = "Shape_4209-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5390 = dtu_hlir.constant  {node_name = "Constant_4210-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5391 = "dtu_hlir.gather"(%5389, %5390) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4211-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5392 = "dtu_hlir.shape"(%5351) {end = 2147483647 : i64, node_name = "Shape_4212-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5393 = dtu_hlir.constant  {node_name = "Constant_4213-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5394 = "dtu_hlir.gather"(%5392, %5393) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4214-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5395 = "dtu_hlir.shape"(%5351) {end = 2147483647 : i64, node_name = "Shape_4215-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5396 = dtu_hlir.constant  {node_name = "Constant_4216-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5397 = "dtu_hlir.gather"(%5395, %5396) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4217-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5398 = dtu_hlir.constant  {node_name = "Constant_4218-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5399 = "dtu_hlir.div"(%5397, %5398) {node_name = "Div_4219-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5400 = "dtu_hlir.convert"(%5399) {node_name = "Cast_4220-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5401 = "dtu_hlir.convert"(%5400) {node_name = "Cast_4221-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5402 = dtu_hlir.constant  {node_name = "Constant_4222-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5403 = "dtu_hlir.unsqueeze"(%5391, %5402) {node_name = "Unsqueeze_4223-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5404 = dtu_hlir.constant  {node_name = "Constant_4224-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5405 = "dtu_hlir.unsqueeze"(%5394, %5404) {node_name = "Unsqueeze_4225-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5406 = dtu_hlir.constant  {node_name = "Constant_4226-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5407 = "dtu_hlir.unsqueeze"(%5401, %5406) {node_name = "Unsqueeze_4227-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5408 = "dtu_hlir.concatenate"(%5403, %5405, %794, %5407) {dimension = 0 : i64, node_name = "Concat_4228-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5409 = "dtu_hlir.dynamic_reshape"(%5351, %5408) {allowzero = 0 : i64, node_name = "Reshape_4229-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %5410 = "dtu_hlir.transpose"(%5409) {node_name = "Transpose_4230-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %5411 = dtu_hlir.constant  {node_name = "Constant_4231-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5412 = "dtu_hlir.mul"(%5391, %5411) {node_name = "Mul_4232-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5413 = dtu_hlir.constant  {node_name = "Constant_4233-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5414 = "dtu_hlir.div"(%5397, %5413) {node_name = "Div_4234-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5415 = "dtu_hlir.convert"(%5414) {node_name = "Cast_4235-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5416 = "dtu_hlir.convert"(%5415) {node_name = "Cast_4236-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5417 = dtu_hlir.constant  {node_name = "Constant_4237-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5418 = "dtu_hlir.unsqueeze"(%5412, %5417) {node_name = "Unsqueeze_4238-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5419 = dtu_hlir.constant  {node_name = "Constant_4239-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5420 = "dtu_hlir.unsqueeze"(%5394, %5419) {node_name = "Unsqueeze_4240-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5421 = dtu_hlir.constant  {node_name = "Constant_4241-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5422 = "dtu_hlir.unsqueeze"(%5416, %5421) {node_name = "Unsqueeze_4242-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5423 = "dtu_hlir.concatenate"(%5418, %5420, %5422) {dimension = 0 : i64, node_name = "Concat_4243-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5424 = "dtu_hlir.dynamic_reshape"(%5410, %5423) {allowzero = 0 : i64, node_name = "Reshape_4244-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %5425 = "dtu_hlir.shape"(%5352) {end = 2147483647 : i64, node_name = "Shape_4245-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5426 = dtu_hlir.constant  {node_name = "Constant_4246-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5427 = "dtu_hlir.gather"(%5425, %5426) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4247-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5428 = "dtu_hlir.shape"(%5352) {end = 2147483647 : i64, node_name = "Shape_4248-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5429 = dtu_hlir.constant  {node_name = "Constant_4249-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5430 = "dtu_hlir.gather"(%5428, %5429) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4250-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5431 = "dtu_hlir.shape"(%5352) {end = 2147483647 : i64, node_name = "Shape_4251-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5432 = dtu_hlir.constant  {node_name = "Constant_4252-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5433 = "dtu_hlir.gather"(%5431, %5432) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4253-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5434 = dtu_hlir.constant  {node_name = "Constant_4254-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5435 = "dtu_hlir.div"(%5433, %5434) {node_name = "Div_4255-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5436 = "dtu_hlir.convert"(%5435) {node_name = "Cast_4256-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5437 = "dtu_hlir.convert"(%5436) {node_name = "Cast_4257-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5438 = dtu_hlir.constant  {node_name = "Constant_4258-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5439 = "dtu_hlir.unsqueeze"(%5427, %5438) {node_name = "Unsqueeze_4259-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5440 = dtu_hlir.constant  {node_name = "Constant_4260-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5441 = "dtu_hlir.unsqueeze"(%5430, %5440) {node_name = "Unsqueeze_4261-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5442 = dtu_hlir.constant  {node_name = "Constant_4262-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5443 = "dtu_hlir.unsqueeze"(%5437, %5442) {node_name = "Unsqueeze_4263-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5444 = "dtu_hlir.concatenate"(%5439, %5441, %793, %5443) {dimension = 0 : i64, node_name = "Concat_4264-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5445 = "dtu_hlir.dynamic_reshape"(%5352, %5444) {allowzero = 0 : i64, node_name = "Reshape_4265-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %5446 = "dtu_hlir.transpose"(%5445) {node_name = "Transpose_4266-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %5447 = dtu_hlir.constant  {node_name = "Constant_4267-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5448 = "dtu_hlir.mul"(%5427, %5447) {node_name = "Mul_4268-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5449 = dtu_hlir.constant  {node_name = "Constant_4269-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5450 = "dtu_hlir.div"(%5433, %5449) {node_name = "Div_4270-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5451 = "dtu_hlir.convert"(%5450) {node_name = "Cast_4271-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5452 = "dtu_hlir.convert"(%5451) {node_name = "Cast_4272-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5453 = dtu_hlir.constant  {node_name = "Constant_4273-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5454 = "dtu_hlir.unsqueeze"(%5448, %5453) {node_name = "Unsqueeze_4274-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5455 = dtu_hlir.constant  {node_name = "Constant_4275-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5456 = "dtu_hlir.unsqueeze"(%5430, %5455) {node_name = "Unsqueeze_4276-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5457 = dtu_hlir.constant  {node_name = "Constant_4277-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5458 = "dtu_hlir.unsqueeze"(%5452, %5457) {node_name = "Unsqueeze_4278-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5459 = "dtu_hlir.concatenate"(%5454, %5456, %5458) {dimension = 0 : i64, node_name = "Concat_4279-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5460 = "dtu_hlir.dynamic_reshape"(%5446, %5459) {allowzero = 0 : i64, node_name = "Reshape_4280-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %5461 = "dtu_hlir.shape"(%5388) {end = 2147483647 : i64, node_name = "Shape_4281-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5462 = dtu_hlir.constant  {node_name = "Constant_4282-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5463 = "dtu_hlir.gather"(%5461, %5462) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4283-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5464 = "dtu_hlir.shape"(%5388) {end = 2147483647 : i64, node_name = "Shape_4284-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5465 = dtu_hlir.constant  {node_name = "Constant_4285-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5466 = "dtu_hlir.gather"(%5464, %5465) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4286-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5467 = "dtu_hlir.shape"(%5424) {end = 2147483647 : i64, node_name = "Shape_4287-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<3xi64>
    %5468 = dtu_hlir.constant  {node_name = "Constant_4288-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5469 = "dtu_hlir.gather"(%5467, %5468) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4289-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5470 = dtu_hlir.constant  {node_name = "Constant_4290-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5471 = "dtu_hlir.unsqueeze"(%5463, %5470) {node_name = "Unsqueeze_4291-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5472 = dtu_hlir.constant  {node_name = "Constant_4292-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5473 = "dtu_hlir.unsqueeze"(%5466, %5472) {node_name = "Unsqueeze_4293-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5474 = dtu_hlir.constant  {node_name = "Constant_4294-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5475 = "dtu_hlir.unsqueeze"(%5469, %5474) {node_name = "Unsqueeze_4295-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5476 = "dtu_hlir.concatenate"(%5471, %5473, %5475) {dimension = 0 : i64, node_name = "Concat_4296-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5477 = dtu_hlir.constant  {node_name = "ConstantOfShape_4297-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %5478 = "dtu_hlir.dynamic_broadcast_in_dim"(%5477, %5476) {node_name = "ConstantOfShape_4297-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x77xf32>
    %5479 = "dtu_hlir.transpose"(%5424) {node_name = "Transpose_4298-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<40x64x77xf32>
    %5480 = "dtu_hlir.dot_general"(%5388, %5479) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4299-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x77xf32>) -> tensor<40x256x77xf32>
    %5481 = "dtu_hlir.broadcast_in_dim"(%792) {node_name = "Mul_4300-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %5482 = "dtu_hlir.mul"(%5480, %5481) {node_name = "Mul_4300-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %5483 = "dtu_hlir.broadcast_in_dim"(%791) {node_name = "Mul_4301-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %5484 = "dtu_hlir.mul"(%5478, %5483) {node_name = "Mul_4301-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %5485 = "dtu_hlir.add"(%5482, %5484) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4302-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %5486 = "dtu_hlir.softmax"(%5485) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4303-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %5487 = "dtu_hlir.convert"(%5486) {node_name = "Cast_4304-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %5488 = "dtu_hlir.dot_general"(%5487, %5460) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4305-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x77x64xf32>) -> tensor<40x256x64xf32>
    %5489 = "dtu_hlir.shape"(%5488) {end = 2147483647 : i64, node_name = "Shape_4306-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5490 = dtu_hlir.constant  {node_name = "Constant_4307-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5491 = "dtu_hlir.gather"(%5489, %5490) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4308-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5492 = "dtu_hlir.shape"(%5488) {end = 2147483647 : i64, node_name = "Shape_4309-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5493 = dtu_hlir.constant  {node_name = "Constant_4310-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5494 = "dtu_hlir.gather"(%5492, %5493) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4311-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5495 = "dtu_hlir.shape"(%5488) {end = 2147483647 : i64, node_name = "Shape_4312-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5496 = dtu_hlir.constant  {node_name = "Constant_4313-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5497 = "dtu_hlir.gather"(%5495, %5496) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4314-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5498 = dtu_hlir.constant  {node_name = "Constant_4315-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5499 = "dtu_hlir.div"(%5491, %5498) {node_name = "Div_4316-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5500 = "dtu_hlir.convert"(%5499) {node_name = "Cast_4317-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5501 = "dtu_hlir.convert"(%5500) {node_name = "Cast_4318-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5502 = dtu_hlir.constant  {node_name = "Constant_4319-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5503 = "dtu_hlir.unsqueeze"(%5501, %5502) {node_name = "Unsqueeze_4320-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5504 = dtu_hlir.constant  {node_name = "Constant_4321-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5505 = "dtu_hlir.unsqueeze"(%5494, %5504) {node_name = "Unsqueeze_4322-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5506 = dtu_hlir.constant  {node_name = "Constant_4323-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5507 = "dtu_hlir.unsqueeze"(%5497, %5506) {node_name = "Unsqueeze_4324-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5508 = "dtu_hlir.concatenate"(%5503, %790, %5505, %5507) {dimension = 0 : i64, node_name = "Concat_4325-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5509 = "dtu_hlir.dynamic_reshape"(%5488, %5508) {allowzero = 0 : i64, node_name = "Reshape_4326-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %5510 = "dtu_hlir.transpose"(%5509) {node_name = "Transpose_4327-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %5511 = dtu_hlir.constant  {node_name = "Constant_4328-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5512 = "dtu_hlir.div"(%5491, %5511) {node_name = "Div_4329-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5513 = "dtu_hlir.convert"(%5512) {node_name = "Cast_4330-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5514 = "dtu_hlir.convert"(%5513) {node_name = "Cast_4331-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5515 = dtu_hlir.constant  {node_name = "Constant_4332-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5516 = "dtu_hlir.mul"(%5497, %5515) {node_name = "Mul_4333-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5517 = dtu_hlir.constant  {node_name = "Constant_4334-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5518 = "dtu_hlir.unsqueeze"(%5514, %5517) {node_name = "Unsqueeze_4335-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5519 = dtu_hlir.constant  {node_name = "Constant_4336-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5520 = "dtu_hlir.unsqueeze"(%5494, %5519) {node_name = "Unsqueeze_4337-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5521 = dtu_hlir.constant  {node_name = "Constant_4338-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5522 = "dtu_hlir.unsqueeze"(%5516, %5521) {node_name = "Unsqueeze_4339-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5523 = "dtu_hlir.concatenate"(%5518, %5520, %5522) {dimension = 0 : i64, node_name = "Concat_4340-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5524 = "dtu_hlir.dynamic_reshape"(%5510, %5523) {allowzero = 0 : i64, node_name = "Reshape_4341-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %5525 = "dtu_hlir.dot_general"(%5524, %542) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4342-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5526 = "dtu_hlir.add"(%166, %5525) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4343-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5527 = "dtu_hlir.add"(%5526, %5322) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4344-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5528 = dtu_hlir.constant  {node_name = "ReduceMean_4345-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5529 = "dtu_hlir.reshape"(%5528) {node_name = "ReduceMean_4345-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5530 = "dtu_hlir.reduce"(%5527, %5529) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4345-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4345-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5531 = dtu_hlir.constant  {node_name = "ReduceMean_4345-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5532 = "dtu_hlir.unsqueeze"(%5530, %5531) {node_name = "ReduceMean_4345-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5533 = dtu_hlir.constant  {node_name = "ReduceMean_4345-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5534 = "dtu_hlir.broadcast_in_dim"(%5533) {node_name = "ReduceMean_4345-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5535 = "dtu_hlir.div"(%5532, %5534) {node_name = "ReduceMean_4345-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5536 = "dtu_hlir.sub"(%5527, %5535) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_4346-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5537 = dtu_hlir.constant  {node_name = "Constant_4347-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %5538 = "dtu_hlir.broadcast_in_dim"(%5537) {node_name = "Pow_4348-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %5539 = "dtu_hlir.pow"(%5536, %5538) {node_name = "Pow_4348-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5540 = dtu_hlir.constant  {node_name = "ReduceMean_4349-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5541 = "dtu_hlir.reshape"(%5540) {node_name = "ReduceMean_4349-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5542 = "dtu_hlir.reduce"(%5539, %5541) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4349-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4349-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5543 = dtu_hlir.constant  {node_name = "ReduceMean_4349-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5544 = "dtu_hlir.unsqueeze"(%5542, %5543) {node_name = "ReduceMean_4349-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5545 = dtu_hlir.constant  {node_name = "ReduceMean_4349-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5546 = "dtu_hlir.broadcast_in_dim"(%5545) {node_name = "ReduceMean_4349-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5547 = "dtu_hlir.div"(%5544, %5546) {node_name = "ReduceMean_4349-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5548 = dtu_hlir.constant  {node_name = "Constant_4350-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %5549 = "dtu_hlir.broadcast_in_dim"(%5548) {node_name = "Add_4351-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %5550 = "dtu_hlir.add"(%5547, %5549) {node_name = "Add_4351-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5551 = "dtu_hlir.sqrt"(%5550) {node_name = "Sqrt_4352-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5552 = "dtu_hlir.div"(%5536, %5551) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_4353-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5553 = "dtu_hlir.mul"(%5552, %171) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_4354-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5554 = "dtu_hlir.add"(%5553, %172) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4355-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5555 = "dtu_hlir.dot_general"(%5554, %543) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4356-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x10240xf32>) -> tensor<2x256x10240xf32>
    %5556 = "dtu_hlir.add"(%164, %5555) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4357-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10240xf32>, tensor<2x256x10240xf32>) -> tensor<2x256x10240xf32>
    %5557 = "dtu_hlir.shape"(%5556) {end = 2147483647 : i64, node_name = "Shape_4358-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>) -> tensor<3xi64>
    %5558 = dtu_hlir.constant  {node_name = "Constant_4359-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %5559 = "dtu_hlir.gather"(%5557, %5558) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4360-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %5560 = dtu_hlir.constant  {node_name = "Constant_4361-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5561 = dtu_hlir.constant  {node_name = "Constant_4362-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %5562 = "dtu_hlir.add"(%5559, %5561) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_4363-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %5563 = dtu_hlir.constant  {node_name = "Constant_4364-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %5564 = "dtu_hlir.div"(%5562, %5563) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_4365-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %5565 = dtu_hlir.constant  {node_name = "Constant_4366-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %5566 = "dtu_hlir.mul"(%5564, %5565) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_4367-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %5567 = dtu_hlir.constant  {node_name = "Slice_4368-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %5568 = "dtu_hlir.real_dynamic_slice"(%5556, %5560, %5566, %5567, %5558) {node_name = "Slice_4368-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %5569 = dtu_hlir.constant  {node_name = "Constant_4369-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %5570 = "dtu_hlir.mul"(%5564, %5569) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_4370-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %5571 = "dtu_hlir.shape"(%5566) {end = 2147483647 : i64, node_name = "Slice_4371-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %5572 = dtu_hlir.constant  {node_name = "Slice_4371-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %5573 = "dtu_hlir.dynamic_broadcast_in_dim"(%5572, %5571) {node_name = "Slice_4371-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5574 = "dtu_hlir.real_dynamic_slice"(%5556, %5566, %5570, %5573, %5558) {node_name = "Slice_4371-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %5575 = dtu_hlir.constant  {node_name = "Constant_4372-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %5576 = "dtu_hlir.broadcast_in_dim"(%5575) {node_name = "Div_4373-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %5577 = "dtu_hlir.div"(%5574, %5576) {node_name = "Div_4373-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %5578 = "dtu_hlir.erf"(%5577) {node_name = "Erf_4374-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %5579 = dtu_hlir.constant  {node_name = "Constant_4375-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %5580 = "dtu_hlir.broadcast_in_dim"(%5579) {node_name = "Add_4376-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %5581 = "dtu_hlir.add"(%5578, %5580) {node_name = "Add_4376-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %5582 = "dtu_hlir.mul"(%5574, %5581) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_4377-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %5583 = dtu_hlir.constant  {node_name = "Constant_4378-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %5584 = "dtu_hlir.broadcast_in_dim"(%5583) {node_name = "Mul_4379-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %5585 = "dtu_hlir.mul"(%5582, %5584) {node_name = "Mul_4379-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %5586 = "dtu_hlir.mul"(%5568, %5585) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_4380-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %5587 = "dtu_hlir.dot_general"(%5586, %544) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4381-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<5120x1280xf32>) -> tensor<2x256x1280xf32>
    %5588 = "dtu_hlir.add"(%165, %5587) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4382-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5589 = "dtu_hlir.add"(%5588, %5527) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4383-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5590 = "dtu_hlir.dot_general"(%5589, %545) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4384-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5591 = "dtu_hlir.add"(%173, %5590) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4385-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5592 = dtu_hlir.constant  {node_name = "Constant_4386-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5593 = "dtu_hlir.unsqueeze"(%5087, %5592) {node_name = "Unsqueeze_4387-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5594 = dtu_hlir.constant  {node_name = "Constant_4388-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5595 = "dtu_hlir.unsqueeze"(%5090, %5594) {node_name = "Unsqueeze_4389-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5596 = dtu_hlir.constant  {node_name = "Constant_4390-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5597 = "dtu_hlir.unsqueeze"(%5093, %5596) {node_name = "Unsqueeze_4391-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5598 = dtu_hlir.constant  {node_name = "Constant_4392-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5599 = "dtu_hlir.unsqueeze"(%5105, %5598) {node_name = "Unsqueeze_4393-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5600 = "dtu_hlir.concatenate"(%5593, %5595, %5597, %5599) {dimension = 0 : i64, node_name = "Concat_4394-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5601 = "dtu_hlir.dynamic_reshape"(%5591, %5600) {allowzero = 0 : i64, node_name = "Reshape_4395-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x16x16x1280xf32>
    %5602 = "dtu_hlir.transpose"(%5601) {node_name = "Transpose_4396-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>) -> tensor<2x1280x16x16xf32>
    %5603 = "dtu_hlir.add"(%5602, %5084) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_4397-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5604 = "dtu_hlir.concatenate"(%5603, %3700) {dimension = 1 : i64, node_name = "Concat_4398-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x2560x16x16xf32>
    %5605 = dtu_hlir.constant  {node_name = "Constant_4399-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5606 = "dtu_hlir.dynamic_reshape"(%5604, %5605) {allowzero = 0 : i64, node_name = "Reshape_4400-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %5607 = dtu_hlir.constant  {node_name = "Constant_4401-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5608 = dtu_hlir.constant  {node_name = "Constant_4402-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5609 = "dtu_hlir.instance_norm"(%5606, %5607, %5608) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_4403-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %5610 = "dtu_hlir.shape"(%5604) {end = 2147483647 : i64, node_name = "Shape_4404-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>) -> tensor<4xi64>
    %5611 = "dtu_hlir.dynamic_reshape"(%5609, %5610) {allowzero = 0 : i64, node_name = "Reshape_4405-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x2560x16x16xf32>
    %5612 = "dtu_hlir.mul"(%5611, %546) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_4406-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x16x16xf32>
    %5613 = "dtu_hlir.add"(%5612, %547) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_4407-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<2560x1x1xf32>) -> tensor<2x2560x16x16xf32>
    %5614 = "dtu_hlir.sigmoid"(%5613) {node_name = "Sigmoid_4408-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>) -> tensor<2x2560x16x16xf32>
    %5615 = "dtu_hlir.mul"(%5613, %5614) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_4409-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x2560x16x16xf32>, tensor<2x2560x16x16xf32>) -> tensor<2x2560x16x16xf32>
    %5616 = "dtu_hlir.conv_bias"(%5615, %206, %207) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4410-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x16x16xf32>, tensor<1280x2560x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %5617 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_4411-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %5618 = "dtu_hlir.mul"(%915, %5617) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4412-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %5619 = "dtu_hlir.transpose"(%208) {node_name = "Gemm_4413-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %5620 = "dtu_hlir.gemm"(%5618, %5619, %209) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_4413-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %5621 = dtu_hlir.constant  {node_name = "Constant_4414-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %5622 = "dtu_hlir.unsqueeze"(%5620, %5621) {node_name = "Unsqueeze_4415-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %5623 = dtu_hlir.constant  {node_name = "Constant_4416-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %5624 = "dtu_hlir.unsqueeze"(%5622, %5623) {node_name = "Unsqueeze_4417-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %5625 = "dtu_hlir.add"(%5616, %5624) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_4418-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5626 = dtu_hlir.constant  {node_name = "Constant_4419-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5627 = "dtu_hlir.dynamic_reshape"(%5625, %5626) {allowzero = 0 : i64, node_name = "Reshape_4420-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %5628 = dtu_hlir.constant  {node_name = "Constant_4421-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5629 = dtu_hlir.constant  {node_name = "Constant_4422-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5630 = "dtu_hlir.instance_norm"(%5627, %5628, %5629) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_4423-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %5631 = "dtu_hlir.shape"(%5625) {end = 2147483647 : i64, node_name = "Shape_4424-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5632 = "dtu_hlir.dynamic_reshape"(%5630, %5631) {allowzero = 0 : i64, node_name = "Reshape_4425-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %5633 = "dtu_hlir.mul"(%5632, %548) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_4426-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5634 = "dtu_hlir.add"(%5633, %549) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_4427-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5635 = "dtu_hlir.sigmoid"(%5634) {node_name = "Sigmoid_4428-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5636 = "dtu_hlir.mul"(%5634, %5635) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_4429-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5637 = "dtu_hlir.conv_bias"(%5636, %210, %211) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4430-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %5638 = "dtu_hlir.conv_bias"(%5604, %212, %213) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4431-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x2560x16x16xf32>, tensor<1280x2560x1x1xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %5639 = "dtu_hlir.add"(%5638, %5637) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_4432-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5640 = dtu_hlir.constant  {node_name = "Constant_4433-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %5641 = "dtu_hlir.broadcast_in_dim"(%5640) {node_name = "Div_4434-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x16x16xf32>
    %5642 = "dtu_hlir.div"(%5639, %5641) {node_name = "Div_4434-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %5643 = "dtu_hlir.shape"(%5642) {end = 2147483647 : i64, node_name = "Shape_4435-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5644 = dtu_hlir.constant  {node_name = "Constant_4436-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5645 = "dtu_hlir.gather"(%5643, %5644) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4437-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5646 = "dtu_hlir.shape"(%5642) {end = 2147483647 : i64, node_name = "Shape_4438-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5647 = dtu_hlir.constant  {node_name = "Constant_4439-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5648 = "dtu_hlir.gather"(%5646, %5647) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4440-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5649 = "dtu_hlir.shape"(%5642) {end = 2147483647 : i64, node_name = "Shape_4441-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5650 = dtu_hlir.constant  {node_name = "Constant_4442-0", node_type = "Constant"} dense<3> : tensor<i64>
    %5651 = "dtu_hlir.gather"(%5649, %5650) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4443-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5652 = dtu_hlir.constant  {node_name = "Constant_4444-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %5653 = "dtu_hlir.dynamic_reshape"(%5642, %5652) {allowzero = 0 : i64, node_name = "Reshape_4445-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %5654 = dtu_hlir.constant  {node_name = "Constant_4446-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %5655 = dtu_hlir.constant  {node_name = "Constant_4447-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %5656 = "dtu_hlir.instance_norm"(%5653, %5654, %5655) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_4448-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %5657 = "dtu_hlir.shape"(%5642) {end = 2147483647 : i64, node_name = "Shape_4449-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5658 = "dtu_hlir.dynamic_reshape"(%5656, %5657) {allowzero = 0 : i64, node_name = "Reshape_4450-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %5659 = "dtu_hlir.mul"(%5658, %550) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_4451-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5660 = "dtu_hlir.add"(%5659, %551) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_4452-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %5661 = "dtu_hlir.shape"(%5660) {end = 2147483647 : i64, node_name = "Shape_4453-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %5662 = dtu_hlir.constant  {node_name = "Constant_4454-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5663 = "dtu_hlir.gather"(%5661, %5662) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4455-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %5664 = "dtu_hlir.transpose"(%5660) {node_name = "Transpose_4456-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x16x16x1280xf32>
    %5665 = "dtu_hlir.mul"(%5648, %5651) {node_name = "Mul_4457-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5666 = dtu_hlir.constant  {node_name = "Constant_4458-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5667 = "dtu_hlir.unsqueeze"(%5645, %5666) {node_name = "Unsqueeze_4459-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5668 = dtu_hlir.constant  {node_name = "Constant_4460-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5669 = "dtu_hlir.unsqueeze"(%5665, %5668) {node_name = "Unsqueeze_4461-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5670 = dtu_hlir.constant  {node_name = "Constant_4462-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5671 = "dtu_hlir.unsqueeze"(%5663, %5670) {node_name = "Unsqueeze_4463-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5672 = "dtu_hlir.concatenate"(%5667, %5669, %5671) {dimension = 0 : i64, node_name = "Concat_4464-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5673 = "dtu_hlir.dynamic_reshape"(%5664, %5672) {allowzero = 0 : i64, node_name = "Reshape_4465-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %5674 = "dtu_hlir.dot_general"(%5673, %552) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4466-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5675 = "dtu_hlir.add"(%174, %5674) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4467-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5676 = dtu_hlir.constant  {node_name = "ReduceMean_4468-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5677 = "dtu_hlir.reshape"(%5676) {node_name = "ReduceMean_4468-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5678 = "dtu_hlir.reduce"(%5675, %5677) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4468-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4468-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5679 = dtu_hlir.constant  {node_name = "ReduceMean_4468-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5680 = "dtu_hlir.unsqueeze"(%5678, %5679) {node_name = "ReduceMean_4468-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5681 = dtu_hlir.constant  {node_name = "ReduceMean_4468-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5682 = "dtu_hlir.broadcast_in_dim"(%5681) {node_name = "ReduceMean_4468-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5683 = "dtu_hlir.div"(%5680, %5682) {node_name = "ReduceMean_4468-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5684 = "dtu_hlir.sub"(%5675, %5683) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_4469-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5685 = dtu_hlir.constant  {node_name = "Constant_4470-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %5686 = "dtu_hlir.broadcast_in_dim"(%5685) {node_name = "Pow_4471-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %5687 = "dtu_hlir.pow"(%5684, %5686) {node_name = "Pow_4471-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5688 = dtu_hlir.constant  {node_name = "ReduceMean_4472-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5689 = "dtu_hlir.reshape"(%5688) {node_name = "ReduceMean_4472-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5690 = "dtu_hlir.reduce"(%5687, %5689) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4472-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4472-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5691 = dtu_hlir.constant  {node_name = "ReduceMean_4472-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5692 = "dtu_hlir.unsqueeze"(%5690, %5691) {node_name = "ReduceMean_4472-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5693 = dtu_hlir.constant  {node_name = "ReduceMean_4472-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5694 = "dtu_hlir.broadcast_in_dim"(%5693) {node_name = "ReduceMean_4472-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5695 = "dtu_hlir.div"(%5692, %5694) {node_name = "ReduceMean_4472-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5696 = dtu_hlir.constant  {node_name = "Constant_4473-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %5697 = "dtu_hlir.broadcast_in_dim"(%5696) {node_name = "Add_4474-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %5698 = "dtu_hlir.add"(%5695, %5697) {node_name = "Add_4474-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5699 = "dtu_hlir.sqrt"(%5698) {node_name = "Sqrt_4475-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5700 = "dtu_hlir.div"(%5684, %5699) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_4476-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5701 = "dtu_hlir.mul"(%5700, %179) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_4477-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5702 = "dtu_hlir.add"(%5701, %180) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4478-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5703 = "dtu_hlir.dot_general"(%5702, %553) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4479-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5704 = "dtu_hlir.dot_general"(%5702, %554) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4480-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5705 = "dtu_hlir.dot_general"(%5702, %555) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4481-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5706 = "dtu_hlir.shape"(%5703) {end = 2147483647 : i64, node_name = "Shape_4482-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5707 = dtu_hlir.constant  {node_name = "Constant_4483-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5708 = "dtu_hlir.gather"(%5706, %5707) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4484-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5709 = "dtu_hlir.shape"(%5703) {end = 2147483647 : i64, node_name = "Shape_4485-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5710 = dtu_hlir.constant  {node_name = "Constant_4486-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5711 = "dtu_hlir.gather"(%5709, %5710) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4487-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5712 = "dtu_hlir.shape"(%5703) {end = 2147483647 : i64, node_name = "Shape_4488-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5713 = dtu_hlir.constant  {node_name = "Constant_4489-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5714 = "dtu_hlir.gather"(%5712, %5713) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4490-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5715 = dtu_hlir.constant  {node_name = "Constant_4491-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5716 = "dtu_hlir.div"(%5714, %5715) {node_name = "Div_4492-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5717 = "dtu_hlir.convert"(%5716) {node_name = "Cast_4493-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5718 = "dtu_hlir.convert"(%5717) {node_name = "Cast_4494-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5719 = dtu_hlir.constant  {node_name = "Constant_4495-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5720 = "dtu_hlir.unsqueeze"(%5708, %5719) {node_name = "Unsqueeze_4496-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5721 = dtu_hlir.constant  {node_name = "Constant_4497-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5722 = "dtu_hlir.unsqueeze"(%5711, %5721) {node_name = "Unsqueeze_4498-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5723 = dtu_hlir.constant  {node_name = "Constant_4499-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5724 = "dtu_hlir.unsqueeze"(%5718, %5723) {node_name = "Unsqueeze_4500-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5725 = "dtu_hlir.concatenate"(%5720, %5722, %789, %5724) {dimension = 0 : i64, node_name = "Concat_4501-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5726 = "dtu_hlir.dynamic_reshape"(%5703, %5725) {allowzero = 0 : i64, node_name = "Reshape_4502-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5727 = "dtu_hlir.transpose"(%5726) {node_name = "Transpose_4503-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5728 = dtu_hlir.constant  {node_name = "Constant_4504-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5729 = "dtu_hlir.mul"(%5708, %5728) {node_name = "Mul_4505-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5730 = dtu_hlir.constant  {node_name = "Constant_4506-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5731 = "dtu_hlir.div"(%5714, %5730) {node_name = "Div_4507-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5732 = "dtu_hlir.convert"(%5731) {node_name = "Cast_4508-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5733 = "dtu_hlir.convert"(%5732) {node_name = "Cast_4509-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5734 = dtu_hlir.constant  {node_name = "Constant_4510-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5735 = "dtu_hlir.unsqueeze"(%5729, %5734) {node_name = "Unsqueeze_4511-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5736 = dtu_hlir.constant  {node_name = "Constant_4512-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5737 = "dtu_hlir.unsqueeze"(%5711, %5736) {node_name = "Unsqueeze_4513-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5738 = dtu_hlir.constant  {node_name = "Constant_4514-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5739 = "dtu_hlir.unsqueeze"(%5733, %5738) {node_name = "Unsqueeze_4515-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5740 = "dtu_hlir.concatenate"(%5735, %5737, %5739) {dimension = 0 : i64, node_name = "Concat_4516-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5741 = "dtu_hlir.dynamic_reshape"(%5727, %5740) {allowzero = 0 : i64, node_name = "Reshape_4517-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5742 = "dtu_hlir.shape"(%5704) {end = 2147483647 : i64, node_name = "Shape_4518-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5743 = dtu_hlir.constant  {node_name = "Constant_4519-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5744 = "dtu_hlir.gather"(%5742, %5743) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4520-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5745 = "dtu_hlir.shape"(%5704) {end = 2147483647 : i64, node_name = "Shape_4521-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5746 = dtu_hlir.constant  {node_name = "Constant_4522-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5747 = "dtu_hlir.gather"(%5745, %5746) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4523-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5748 = "dtu_hlir.shape"(%5704) {end = 2147483647 : i64, node_name = "Shape_4524-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5749 = dtu_hlir.constant  {node_name = "Constant_4525-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5750 = "dtu_hlir.gather"(%5748, %5749) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4526-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5751 = dtu_hlir.constant  {node_name = "Constant_4527-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5752 = "dtu_hlir.div"(%5750, %5751) {node_name = "Div_4528-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5753 = "dtu_hlir.convert"(%5752) {node_name = "Cast_4529-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5754 = "dtu_hlir.convert"(%5753) {node_name = "Cast_4530-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5755 = dtu_hlir.constant  {node_name = "Constant_4531-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5756 = "dtu_hlir.unsqueeze"(%5744, %5755) {node_name = "Unsqueeze_4532-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5757 = dtu_hlir.constant  {node_name = "Constant_4533-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5758 = "dtu_hlir.unsqueeze"(%5747, %5757) {node_name = "Unsqueeze_4534-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5759 = dtu_hlir.constant  {node_name = "Constant_4535-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5760 = "dtu_hlir.unsqueeze"(%5754, %5759) {node_name = "Unsqueeze_4536-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5761 = "dtu_hlir.concatenate"(%5756, %5758, %788, %5760) {dimension = 0 : i64, node_name = "Concat_4537-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5762 = "dtu_hlir.dynamic_reshape"(%5704, %5761) {allowzero = 0 : i64, node_name = "Reshape_4538-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5763 = "dtu_hlir.transpose"(%5762) {node_name = "Transpose_4539-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5764 = dtu_hlir.constant  {node_name = "Constant_4540-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5765 = "dtu_hlir.mul"(%5744, %5764) {node_name = "Mul_4541-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5766 = dtu_hlir.constant  {node_name = "Constant_4542-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5767 = "dtu_hlir.div"(%5750, %5766) {node_name = "Div_4543-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5768 = "dtu_hlir.convert"(%5767) {node_name = "Cast_4544-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5769 = "dtu_hlir.convert"(%5768) {node_name = "Cast_4545-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5770 = dtu_hlir.constant  {node_name = "Constant_4546-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5771 = "dtu_hlir.unsqueeze"(%5765, %5770) {node_name = "Unsqueeze_4547-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5772 = dtu_hlir.constant  {node_name = "Constant_4548-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5773 = "dtu_hlir.unsqueeze"(%5747, %5772) {node_name = "Unsqueeze_4549-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5774 = dtu_hlir.constant  {node_name = "Constant_4550-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5775 = "dtu_hlir.unsqueeze"(%5769, %5774) {node_name = "Unsqueeze_4551-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5776 = "dtu_hlir.concatenate"(%5771, %5773, %5775) {dimension = 0 : i64, node_name = "Concat_4552-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5777 = "dtu_hlir.dynamic_reshape"(%5763, %5776) {allowzero = 0 : i64, node_name = "Reshape_4553-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5778 = "dtu_hlir.shape"(%5705) {end = 2147483647 : i64, node_name = "Shape_4554-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5779 = dtu_hlir.constant  {node_name = "Constant_4555-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5780 = "dtu_hlir.gather"(%5778, %5779) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4556-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5781 = "dtu_hlir.shape"(%5705) {end = 2147483647 : i64, node_name = "Shape_4557-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5782 = dtu_hlir.constant  {node_name = "Constant_4558-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5783 = "dtu_hlir.gather"(%5781, %5782) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4559-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5784 = "dtu_hlir.shape"(%5705) {end = 2147483647 : i64, node_name = "Shape_4560-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5785 = dtu_hlir.constant  {node_name = "Constant_4561-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5786 = "dtu_hlir.gather"(%5784, %5785) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4562-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5787 = dtu_hlir.constant  {node_name = "Constant_4563-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5788 = "dtu_hlir.div"(%5786, %5787) {node_name = "Div_4564-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5789 = "dtu_hlir.convert"(%5788) {node_name = "Cast_4565-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5790 = "dtu_hlir.convert"(%5789) {node_name = "Cast_4566-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5791 = dtu_hlir.constant  {node_name = "Constant_4567-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5792 = "dtu_hlir.unsqueeze"(%5780, %5791) {node_name = "Unsqueeze_4568-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5793 = dtu_hlir.constant  {node_name = "Constant_4569-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5794 = "dtu_hlir.unsqueeze"(%5783, %5793) {node_name = "Unsqueeze_4570-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5795 = dtu_hlir.constant  {node_name = "Constant_4571-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5796 = "dtu_hlir.unsqueeze"(%5790, %5795) {node_name = "Unsqueeze_4572-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5797 = "dtu_hlir.concatenate"(%5792, %5794, %787, %5796) {dimension = 0 : i64, node_name = "Concat_4573-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5798 = "dtu_hlir.dynamic_reshape"(%5705, %5797) {allowzero = 0 : i64, node_name = "Reshape_4574-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5799 = "dtu_hlir.transpose"(%5798) {node_name = "Transpose_4575-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5800 = dtu_hlir.constant  {node_name = "Constant_4576-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5801 = "dtu_hlir.mul"(%5780, %5800) {node_name = "Mul_4577-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5802 = dtu_hlir.constant  {node_name = "Constant_4578-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5803 = "dtu_hlir.div"(%5786, %5802) {node_name = "Div_4579-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5804 = "dtu_hlir.convert"(%5803) {node_name = "Cast_4580-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5805 = "dtu_hlir.convert"(%5804) {node_name = "Cast_4581-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5806 = dtu_hlir.constant  {node_name = "Constant_4582-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5807 = "dtu_hlir.unsqueeze"(%5801, %5806) {node_name = "Unsqueeze_4583-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5808 = dtu_hlir.constant  {node_name = "Constant_4584-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5809 = "dtu_hlir.unsqueeze"(%5783, %5808) {node_name = "Unsqueeze_4585-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5810 = dtu_hlir.constant  {node_name = "Constant_4586-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5811 = "dtu_hlir.unsqueeze"(%5805, %5810) {node_name = "Unsqueeze_4587-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5812 = "dtu_hlir.concatenate"(%5807, %5809, %5811) {dimension = 0 : i64, node_name = "Concat_4588-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5813 = "dtu_hlir.dynamic_reshape"(%5799, %5812) {allowzero = 0 : i64, node_name = "Reshape_4589-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5814 = "dtu_hlir.shape"(%5741) {end = 2147483647 : i64, node_name = "Shape_4590-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5815 = dtu_hlir.constant  {node_name = "Constant_4591-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5816 = "dtu_hlir.gather"(%5814, %5815) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4592-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5817 = "dtu_hlir.shape"(%5741) {end = 2147483647 : i64, node_name = "Shape_4593-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5818 = dtu_hlir.constant  {node_name = "Constant_4594-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5819 = "dtu_hlir.gather"(%5817, %5818) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4595-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5820 = "dtu_hlir.shape"(%5777) {end = 2147483647 : i64, node_name = "Shape_4596-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5821 = dtu_hlir.constant  {node_name = "Constant_4597-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5822 = "dtu_hlir.gather"(%5820, %5821) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4598-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5823 = dtu_hlir.constant  {node_name = "Constant_4599-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5824 = "dtu_hlir.unsqueeze"(%5816, %5823) {node_name = "Unsqueeze_4600-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5825 = dtu_hlir.constant  {node_name = "Constant_4601-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5826 = "dtu_hlir.unsqueeze"(%5819, %5825) {node_name = "Unsqueeze_4602-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5827 = dtu_hlir.constant  {node_name = "Constant_4603-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5828 = "dtu_hlir.unsqueeze"(%5822, %5827) {node_name = "Unsqueeze_4604-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5829 = "dtu_hlir.concatenate"(%5824, %5826, %5828) {dimension = 0 : i64, node_name = "Concat_4605-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5830 = dtu_hlir.constant  {node_name = "ConstantOfShape_4606-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %5831 = "dtu_hlir.dynamic_broadcast_in_dim"(%5830, %5829) {node_name = "ConstantOfShape_4606-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x256xf32>
    %5832 = "dtu_hlir.transpose"(%5777) {node_name = "Transpose_4607-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<40x64x256xf32>
    %5833 = "dtu_hlir.dot_general"(%5741, %5832) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4608-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x256xf32>) -> tensor<40x256x256xf32>
    %5834 = "dtu_hlir.broadcast_in_dim"(%786) {node_name = "Mul_4609-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %5835 = "dtu_hlir.mul"(%5833, %5834) {node_name = "Mul_4609-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5836 = "dtu_hlir.broadcast_in_dim"(%785) {node_name = "Mul_4610-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %5837 = "dtu_hlir.mul"(%5831, %5836) {node_name = "Mul_4610-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5838 = "dtu_hlir.add"(%5835, %5837) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4611-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5839 = "dtu_hlir.softmax"(%5838) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4612-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5840 = "dtu_hlir.convert"(%5839) {node_name = "Cast_4613-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %5841 = "dtu_hlir.dot_general"(%5840, %5813) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4614-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x64xf32>) -> tensor<40x256x64xf32>
    %5842 = "dtu_hlir.shape"(%5841) {end = 2147483647 : i64, node_name = "Shape_4615-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5843 = dtu_hlir.constant  {node_name = "Constant_4616-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5844 = "dtu_hlir.gather"(%5842, %5843) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4617-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5845 = "dtu_hlir.shape"(%5841) {end = 2147483647 : i64, node_name = "Shape_4618-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5846 = dtu_hlir.constant  {node_name = "Constant_4619-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5847 = "dtu_hlir.gather"(%5845, %5846) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4620-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5848 = "dtu_hlir.shape"(%5841) {end = 2147483647 : i64, node_name = "Shape_4621-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %5849 = dtu_hlir.constant  {node_name = "Constant_4622-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5850 = "dtu_hlir.gather"(%5848, %5849) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4623-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5851 = dtu_hlir.constant  {node_name = "Constant_4624-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5852 = "dtu_hlir.div"(%5844, %5851) {node_name = "Div_4625-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5853 = "dtu_hlir.convert"(%5852) {node_name = "Cast_4626-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5854 = "dtu_hlir.convert"(%5853) {node_name = "Cast_4627-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5855 = dtu_hlir.constant  {node_name = "Constant_4628-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5856 = "dtu_hlir.unsqueeze"(%5854, %5855) {node_name = "Unsqueeze_4629-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5857 = dtu_hlir.constant  {node_name = "Constant_4630-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5858 = "dtu_hlir.unsqueeze"(%5847, %5857) {node_name = "Unsqueeze_4631-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5859 = dtu_hlir.constant  {node_name = "Constant_4632-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5860 = "dtu_hlir.unsqueeze"(%5850, %5859) {node_name = "Unsqueeze_4633-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5861 = "dtu_hlir.concatenate"(%5856, %784, %5858, %5860) {dimension = 0 : i64, node_name = "Concat_4634-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5862 = "dtu_hlir.dynamic_reshape"(%5841, %5861) {allowzero = 0 : i64, node_name = "Reshape_4635-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %5863 = "dtu_hlir.transpose"(%5862) {node_name = "Transpose_4636-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %5864 = dtu_hlir.constant  {node_name = "Constant_4637-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5865 = "dtu_hlir.div"(%5844, %5864) {node_name = "Div_4638-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5866 = "dtu_hlir.convert"(%5865) {node_name = "Cast_4639-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5867 = "dtu_hlir.convert"(%5866) {node_name = "Cast_4640-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5868 = dtu_hlir.constant  {node_name = "Constant_4641-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5869 = "dtu_hlir.mul"(%5850, %5868) {node_name = "Mul_4642-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5870 = dtu_hlir.constant  {node_name = "Constant_4643-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5871 = "dtu_hlir.unsqueeze"(%5867, %5870) {node_name = "Unsqueeze_4644-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5872 = dtu_hlir.constant  {node_name = "Constant_4645-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5873 = "dtu_hlir.unsqueeze"(%5847, %5872) {node_name = "Unsqueeze_4646-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5874 = dtu_hlir.constant  {node_name = "Constant_4647-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5875 = "dtu_hlir.unsqueeze"(%5869, %5874) {node_name = "Unsqueeze_4648-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5876 = "dtu_hlir.concatenate"(%5871, %5873, %5875) {dimension = 0 : i64, node_name = "Concat_4649-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5877 = "dtu_hlir.dynamic_reshape"(%5863, %5876) {allowzero = 0 : i64, node_name = "Reshape_4650-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %5878 = "dtu_hlir.dot_general"(%5877, %556) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4651-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5879 = "dtu_hlir.add"(%175, %5878) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4652-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5880 = "dtu_hlir.add"(%5879, %5675) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4653-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5881 = dtu_hlir.constant  {node_name = "ReduceMean_4654-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5882 = "dtu_hlir.reshape"(%5881) {node_name = "ReduceMean_4654-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5883 = "dtu_hlir.reduce"(%5880, %5882) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4654-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4654-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5884 = dtu_hlir.constant  {node_name = "ReduceMean_4654-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5885 = "dtu_hlir.unsqueeze"(%5883, %5884) {node_name = "ReduceMean_4654-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5886 = dtu_hlir.constant  {node_name = "ReduceMean_4654-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5887 = "dtu_hlir.broadcast_in_dim"(%5886) {node_name = "ReduceMean_4654-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5888 = "dtu_hlir.div"(%5885, %5887) {node_name = "ReduceMean_4654-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5889 = "dtu_hlir.sub"(%5880, %5888) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_4655-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5890 = dtu_hlir.constant  {node_name = "Constant_4656-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %5891 = "dtu_hlir.broadcast_in_dim"(%5890) {node_name = "Pow_4657-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %5892 = "dtu_hlir.pow"(%5889, %5891) {node_name = "Pow_4657-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %5893 = dtu_hlir.constant  {node_name = "ReduceMean_4658-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %5894 = "dtu_hlir.reshape"(%5893) {node_name = "ReduceMean_4658-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %5895 = "dtu_hlir.reduce"(%5892, %5894) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4658-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4658-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %5896 = dtu_hlir.constant  {node_name = "ReduceMean_4658-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %5897 = "dtu_hlir.unsqueeze"(%5895, %5896) {node_name = "ReduceMean_4658-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %5898 = dtu_hlir.constant  {node_name = "ReduceMean_4658-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %5899 = "dtu_hlir.broadcast_in_dim"(%5898) {node_name = "ReduceMean_4658-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %5900 = "dtu_hlir.div"(%5897, %5899) {node_name = "ReduceMean_4658-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5901 = dtu_hlir.constant  {node_name = "Constant_4659-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %5902 = "dtu_hlir.broadcast_in_dim"(%5901) {node_name = "Add_4660-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %5903 = "dtu_hlir.add"(%5900, %5902) {node_name = "Add_4660-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5904 = "dtu_hlir.sqrt"(%5903) {node_name = "Sqrt_4661-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %5905 = "dtu_hlir.div"(%5889, %5904) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_4662-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %5906 = "dtu_hlir.mul"(%5905, %181) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_4663-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5907 = "dtu_hlir.add"(%5906, %182) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4664-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %5908 = "dtu_hlir.dot_general"(%5907, %557) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4665-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %5909 = "dtu_hlir.dot_general"(%arg2, %558) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4666-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %5910 = "dtu_hlir.dot_general"(%arg2, %559) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4667-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %5911 = "dtu_hlir.shape"(%5908) {end = 2147483647 : i64, node_name = "Shape_4668-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5912 = dtu_hlir.constant  {node_name = "Constant_4669-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5913 = "dtu_hlir.gather"(%5911, %5912) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4670-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5914 = "dtu_hlir.shape"(%5908) {end = 2147483647 : i64, node_name = "Shape_4671-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5915 = dtu_hlir.constant  {node_name = "Constant_4672-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5916 = "dtu_hlir.gather"(%5914, %5915) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4673-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5917 = "dtu_hlir.shape"(%5908) {end = 2147483647 : i64, node_name = "Shape_4674-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %5918 = dtu_hlir.constant  {node_name = "Constant_4675-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5919 = "dtu_hlir.gather"(%5917, %5918) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4676-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5920 = dtu_hlir.constant  {node_name = "Constant_4677-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5921 = "dtu_hlir.div"(%5919, %5920) {node_name = "Div_4678-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5922 = "dtu_hlir.convert"(%5921) {node_name = "Cast_4679-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5923 = "dtu_hlir.convert"(%5922) {node_name = "Cast_4680-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5924 = dtu_hlir.constant  {node_name = "Constant_4681-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5925 = "dtu_hlir.unsqueeze"(%5913, %5924) {node_name = "Unsqueeze_4682-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5926 = dtu_hlir.constant  {node_name = "Constant_4683-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5927 = "dtu_hlir.unsqueeze"(%5916, %5926) {node_name = "Unsqueeze_4684-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5928 = dtu_hlir.constant  {node_name = "Constant_4685-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5929 = "dtu_hlir.unsqueeze"(%5923, %5928) {node_name = "Unsqueeze_4686-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5930 = "dtu_hlir.concatenate"(%5925, %5927, %783, %5929) {dimension = 0 : i64, node_name = "Concat_4687-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5931 = "dtu_hlir.dynamic_reshape"(%5908, %5930) {allowzero = 0 : i64, node_name = "Reshape_4688-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %5932 = "dtu_hlir.transpose"(%5931) {node_name = "Transpose_4689-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %5933 = dtu_hlir.constant  {node_name = "Constant_4690-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5934 = "dtu_hlir.mul"(%5913, %5933) {node_name = "Mul_4691-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5935 = dtu_hlir.constant  {node_name = "Constant_4692-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5936 = "dtu_hlir.div"(%5919, %5935) {node_name = "Div_4693-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5937 = "dtu_hlir.convert"(%5936) {node_name = "Cast_4694-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5938 = "dtu_hlir.convert"(%5937) {node_name = "Cast_4695-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5939 = dtu_hlir.constant  {node_name = "Constant_4696-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5940 = "dtu_hlir.unsqueeze"(%5934, %5939) {node_name = "Unsqueeze_4697-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5941 = dtu_hlir.constant  {node_name = "Constant_4698-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5942 = "dtu_hlir.unsqueeze"(%5916, %5941) {node_name = "Unsqueeze_4699-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5943 = dtu_hlir.constant  {node_name = "Constant_4700-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5944 = "dtu_hlir.unsqueeze"(%5938, %5943) {node_name = "Unsqueeze_4701-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5945 = "dtu_hlir.concatenate"(%5940, %5942, %5944) {dimension = 0 : i64, node_name = "Concat_4702-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5946 = "dtu_hlir.dynamic_reshape"(%5932, %5945) {allowzero = 0 : i64, node_name = "Reshape_4703-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %5947 = "dtu_hlir.shape"(%5909) {end = 2147483647 : i64, node_name = "Shape_4704-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5948 = dtu_hlir.constant  {node_name = "Constant_4705-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5949 = "dtu_hlir.gather"(%5947, %5948) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4706-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5950 = "dtu_hlir.shape"(%5909) {end = 2147483647 : i64, node_name = "Shape_4707-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5951 = dtu_hlir.constant  {node_name = "Constant_4708-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5952 = "dtu_hlir.gather"(%5950, %5951) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4709-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5953 = "dtu_hlir.shape"(%5909) {end = 2147483647 : i64, node_name = "Shape_4710-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5954 = dtu_hlir.constant  {node_name = "Constant_4711-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5955 = "dtu_hlir.gather"(%5953, %5954) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4712-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5956 = dtu_hlir.constant  {node_name = "Constant_4713-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5957 = "dtu_hlir.div"(%5955, %5956) {node_name = "Div_4714-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5958 = "dtu_hlir.convert"(%5957) {node_name = "Cast_4715-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5959 = "dtu_hlir.convert"(%5958) {node_name = "Cast_4716-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5960 = dtu_hlir.constant  {node_name = "Constant_4717-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5961 = "dtu_hlir.unsqueeze"(%5949, %5960) {node_name = "Unsqueeze_4718-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5962 = dtu_hlir.constant  {node_name = "Constant_4719-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5963 = "dtu_hlir.unsqueeze"(%5952, %5962) {node_name = "Unsqueeze_4720-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5964 = dtu_hlir.constant  {node_name = "Constant_4721-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5965 = "dtu_hlir.unsqueeze"(%5959, %5964) {node_name = "Unsqueeze_4722-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5966 = "dtu_hlir.concatenate"(%5961, %5963, %782, %5965) {dimension = 0 : i64, node_name = "Concat_4723-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %5967 = "dtu_hlir.dynamic_reshape"(%5909, %5966) {allowzero = 0 : i64, node_name = "Reshape_4724-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %5968 = "dtu_hlir.transpose"(%5967) {node_name = "Transpose_4725-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %5969 = dtu_hlir.constant  {node_name = "Constant_4726-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5970 = "dtu_hlir.mul"(%5949, %5969) {node_name = "Mul_4727-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5971 = dtu_hlir.constant  {node_name = "Constant_4728-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5972 = "dtu_hlir.div"(%5955, %5971) {node_name = "Div_4729-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5973 = "dtu_hlir.convert"(%5972) {node_name = "Cast_4730-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5974 = "dtu_hlir.convert"(%5973) {node_name = "Cast_4731-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5975 = dtu_hlir.constant  {node_name = "Constant_4732-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5976 = "dtu_hlir.unsqueeze"(%5970, %5975) {node_name = "Unsqueeze_4733-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5977 = dtu_hlir.constant  {node_name = "Constant_4734-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5978 = "dtu_hlir.unsqueeze"(%5952, %5977) {node_name = "Unsqueeze_4735-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5979 = dtu_hlir.constant  {node_name = "Constant_4736-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5980 = "dtu_hlir.unsqueeze"(%5974, %5979) {node_name = "Unsqueeze_4737-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5981 = "dtu_hlir.concatenate"(%5976, %5978, %5980) {dimension = 0 : i64, node_name = "Concat_4738-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %5982 = "dtu_hlir.dynamic_reshape"(%5968, %5981) {allowzero = 0 : i64, node_name = "Reshape_4739-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %5983 = "dtu_hlir.shape"(%5910) {end = 2147483647 : i64, node_name = "Shape_4740-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5984 = dtu_hlir.constant  {node_name = "Constant_4741-0", node_type = "Constant"} dense<0> : tensor<i64>
    %5985 = "dtu_hlir.gather"(%5983, %5984) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4742-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5986 = "dtu_hlir.shape"(%5910) {end = 2147483647 : i64, node_name = "Shape_4743-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5987 = dtu_hlir.constant  {node_name = "Constant_4744-0", node_type = "Constant"} dense<1> : tensor<i64>
    %5988 = "dtu_hlir.gather"(%5986, %5987) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4745-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5989 = "dtu_hlir.shape"(%5910) {end = 2147483647 : i64, node_name = "Shape_4746-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %5990 = dtu_hlir.constant  {node_name = "Constant_4747-0", node_type = "Constant"} dense<2> : tensor<i64>
    %5991 = "dtu_hlir.gather"(%5989, %5990) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4748-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %5992 = dtu_hlir.constant  {node_name = "Constant_4749-0", node_type = "Constant"} dense<20> : tensor<i64>
    %5993 = "dtu_hlir.div"(%5991, %5992) {node_name = "Div_4750-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %5994 = "dtu_hlir.convert"(%5993) {node_name = "Cast_4751-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5995 = "dtu_hlir.convert"(%5994) {node_name = "Cast_4752-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %5996 = dtu_hlir.constant  {node_name = "Constant_4753-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5997 = "dtu_hlir.unsqueeze"(%5985, %5996) {node_name = "Unsqueeze_4754-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %5998 = dtu_hlir.constant  {node_name = "Constant_4755-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %5999 = "dtu_hlir.unsqueeze"(%5988, %5998) {node_name = "Unsqueeze_4756-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6000 = dtu_hlir.constant  {node_name = "Constant_4757-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6001 = "dtu_hlir.unsqueeze"(%5995, %6000) {node_name = "Unsqueeze_4758-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6002 = "dtu_hlir.concatenate"(%5997, %5999, %781, %6001) {dimension = 0 : i64, node_name = "Concat_4759-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6003 = "dtu_hlir.dynamic_reshape"(%5910, %6002) {allowzero = 0 : i64, node_name = "Reshape_4760-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %6004 = "dtu_hlir.transpose"(%6003) {node_name = "Transpose_4761-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %6005 = dtu_hlir.constant  {node_name = "Constant_4762-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6006 = "dtu_hlir.mul"(%5985, %6005) {node_name = "Mul_4763-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6007 = dtu_hlir.constant  {node_name = "Constant_4764-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6008 = "dtu_hlir.div"(%5991, %6007) {node_name = "Div_4765-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6009 = "dtu_hlir.convert"(%6008) {node_name = "Cast_4766-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6010 = "dtu_hlir.convert"(%6009) {node_name = "Cast_4767-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6011 = dtu_hlir.constant  {node_name = "Constant_4768-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6012 = "dtu_hlir.unsqueeze"(%6006, %6011) {node_name = "Unsqueeze_4769-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6013 = dtu_hlir.constant  {node_name = "Constant_4770-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6014 = "dtu_hlir.unsqueeze"(%5988, %6013) {node_name = "Unsqueeze_4771-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6015 = dtu_hlir.constant  {node_name = "Constant_4772-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6016 = "dtu_hlir.unsqueeze"(%6010, %6015) {node_name = "Unsqueeze_4773-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6017 = "dtu_hlir.concatenate"(%6012, %6014, %6016) {dimension = 0 : i64, node_name = "Concat_4774-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6018 = "dtu_hlir.dynamic_reshape"(%6004, %6017) {allowzero = 0 : i64, node_name = "Reshape_4775-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %6019 = "dtu_hlir.shape"(%5946) {end = 2147483647 : i64, node_name = "Shape_4776-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6020 = dtu_hlir.constant  {node_name = "Constant_4777-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6021 = "dtu_hlir.gather"(%6019, %6020) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4778-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6022 = "dtu_hlir.shape"(%5946) {end = 2147483647 : i64, node_name = "Shape_4779-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6023 = dtu_hlir.constant  {node_name = "Constant_4780-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6024 = "dtu_hlir.gather"(%6022, %6023) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4781-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6025 = "dtu_hlir.shape"(%5982) {end = 2147483647 : i64, node_name = "Shape_4782-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<3xi64>
    %6026 = dtu_hlir.constant  {node_name = "Constant_4783-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6027 = "dtu_hlir.gather"(%6025, %6026) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4784-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6028 = dtu_hlir.constant  {node_name = "Constant_4785-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6029 = "dtu_hlir.unsqueeze"(%6021, %6028) {node_name = "Unsqueeze_4786-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6030 = dtu_hlir.constant  {node_name = "Constant_4787-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6031 = "dtu_hlir.unsqueeze"(%6024, %6030) {node_name = "Unsqueeze_4788-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6032 = dtu_hlir.constant  {node_name = "Constant_4789-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6033 = "dtu_hlir.unsqueeze"(%6027, %6032) {node_name = "Unsqueeze_4790-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6034 = "dtu_hlir.concatenate"(%6029, %6031, %6033) {dimension = 0 : i64, node_name = "Concat_4791-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6035 = dtu_hlir.constant  {node_name = "ConstantOfShape_4792-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %6036 = "dtu_hlir.dynamic_broadcast_in_dim"(%6035, %6034) {node_name = "ConstantOfShape_4792-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x77xf32>
    %6037 = "dtu_hlir.transpose"(%5982) {node_name = "Transpose_4793-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<40x64x77xf32>
    %6038 = "dtu_hlir.dot_general"(%5946, %6037) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4794-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x77xf32>) -> tensor<40x256x77xf32>
    %6039 = "dtu_hlir.broadcast_in_dim"(%780) {node_name = "Mul_4795-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %6040 = "dtu_hlir.mul"(%6038, %6039) {node_name = "Mul_4795-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6041 = "dtu_hlir.broadcast_in_dim"(%779) {node_name = "Mul_4796-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %6042 = "dtu_hlir.mul"(%6036, %6041) {node_name = "Mul_4796-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6043 = "dtu_hlir.add"(%6040, %6042) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4797-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6044 = "dtu_hlir.softmax"(%6043) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4798-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6045 = "dtu_hlir.convert"(%6044) {node_name = "Cast_4799-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6046 = "dtu_hlir.dot_general"(%6045, %6018) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4800-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x77x64xf32>) -> tensor<40x256x64xf32>
    %6047 = "dtu_hlir.shape"(%6046) {end = 2147483647 : i64, node_name = "Shape_4801-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6048 = dtu_hlir.constant  {node_name = "Constant_4802-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6049 = "dtu_hlir.gather"(%6047, %6048) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4803-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6050 = "dtu_hlir.shape"(%6046) {end = 2147483647 : i64, node_name = "Shape_4804-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6051 = dtu_hlir.constant  {node_name = "Constant_4805-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6052 = "dtu_hlir.gather"(%6050, %6051) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4806-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6053 = "dtu_hlir.shape"(%6046) {end = 2147483647 : i64, node_name = "Shape_4807-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6054 = dtu_hlir.constant  {node_name = "Constant_4808-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6055 = "dtu_hlir.gather"(%6053, %6054) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4809-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6056 = dtu_hlir.constant  {node_name = "Constant_4810-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6057 = "dtu_hlir.div"(%6049, %6056) {node_name = "Div_4811-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6058 = "dtu_hlir.convert"(%6057) {node_name = "Cast_4812-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6059 = "dtu_hlir.convert"(%6058) {node_name = "Cast_4813-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6060 = dtu_hlir.constant  {node_name = "Constant_4814-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6061 = "dtu_hlir.unsqueeze"(%6059, %6060) {node_name = "Unsqueeze_4815-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6062 = dtu_hlir.constant  {node_name = "Constant_4816-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6063 = "dtu_hlir.unsqueeze"(%6052, %6062) {node_name = "Unsqueeze_4817-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6064 = dtu_hlir.constant  {node_name = "Constant_4818-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6065 = "dtu_hlir.unsqueeze"(%6055, %6064) {node_name = "Unsqueeze_4819-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6066 = "dtu_hlir.concatenate"(%6061, %778, %6063, %6065) {dimension = 0 : i64, node_name = "Concat_4820-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6067 = "dtu_hlir.dynamic_reshape"(%6046, %6066) {allowzero = 0 : i64, node_name = "Reshape_4821-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %6068 = "dtu_hlir.transpose"(%6067) {node_name = "Transpose_4822-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %6069 = dtu_hlir.constant  {node_name = "Constant_4823-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6070 = "dtu_hlir.div"(%6049, %6069) {node_name = "Div_4824-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6071 = "dtu_hlir.convert"(%6070) {node_name = "Cast_4825-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6072 = "dtu_hlir.convert"(%6071) {node_name = "Cast_4826-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6073 = dtu_hlir.constant  {node_name = "Constant_4827-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6074 = "dtu_hlir.mul"(%6055, %6073) {node_name = "Mul_4828-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6075 = dtu_hlir.constant  {node_name = "Constant_4829-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6076 = "dtu_hlir.unsqueeze"(%6072, %6075) {node_name = "Unsqueeze_4830-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6077 = dtu_hlir.constant  {node_name = "Constant_4831-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6078 = "dtu_hlir.unsqueeze"(%6052, %6077) {node_name = "Unsqueeze_4832-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6079 = dtu_hlir.constant  {node_name = "Constant_4833-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6080 = "dtu_hlir.unsqueeze"(%6074, %6079) {node_name = "Unsqueeze_4834-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6081 = "dtu_hlir.concatenate"(%6076, %6078, %6080) {dimension = 0 : i64, node_name = "Concat_4835-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6082 = "dtu_hlir.dynamic_reshape"(%6068, %6081) {allowzero = 0 : i64, node_name = "Reshape_4836-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %6083 = "dtu_hlir.dot_general"(%6082, %560) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4837-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6084 = "dtu_hlir.add"(%178, %6083) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4838-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6085 = "dtu_hlir.add"(%6084, %5880) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4839-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6086 = dtu_hlir.constant  {node_name = "ReduceMean_4840-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6087 = "dtu_hlir.reshape"(%6086) {node_name = "ReduceMean_4840-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6088 = "dtu_hlir.reduce"(%6085, %6087) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4840-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4840-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6089 = dtu_hlir.constant  {node_name = "ReduceMean_4840-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6090 = "dtu_hlir.unsqueeze"(%6088, %6089) {node_name = "ReduceMean_4840-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6091 = dtu_hlir.constant  {node_name = "ReduceMean_4840-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6092 = "dtu_hlir.broadcast_in_dim"(%6091) {node_name = "ReduceMean_4840-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6093 = "dtu_hlir.div"(%6090, %6092) {node_name = "ReduceMean_4840-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6094 = "dtu_hlir.sub"(%6085, %6093) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_4841-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6095 = dtu_hlir.constant  {node_name = "Constant_4842-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %6096 = "dtu_hlir.broadcast_in_dim"(%6095) {node_name = "Pow_4843-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %6097 = "dtu_hlir.pow"(%6094, %6096) {node_name = "Pow_4843-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6098 = dtu_hlir.constant  {node_name = "ReduceMean_4844-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6099 = "dtu_hlir.reshape"(%6098) {node_name = "ReduceMean_4844-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6100 = "dtu_hlir.reduce"(%6097, %6099) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4844-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4844-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6101 = dtu_hlir.constant  {node_name = "ReduceMean_4844-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6102 = "dtu_hlir.unsqueeze"(%6100, %6101) {node_name = "ReduceMean_4844-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6103 = dtu_hlir.constant  {node_name = "ReduceMean_4844-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6104 = "dtu_hlir.broadcast_in_dim"(%6103) {node_name = "ReduceMean_4844-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6105 = "dtu_hlir.div"(%6102, %6104) {node_name = "ReduceMean_4844-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6106 = dtu_hlir.constant  {node_name = "Constant_4845-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %6107 = "dtu_hlir.broadcast_in_dim"(%6106) {node_name = "Add_4846-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %6108 = "dtu_hlir.add"(%6105, %6107) {node_name = "Add_4846-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6109 = "dtu_hlir.sqrt"(%6108) {node_name = "Sqrt_4847-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6110 = "dtu_hlir.div"(%6094, %6109) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_4848-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6111 = "dtu_hlir.mul"(%6110, %183) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_4849-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6112 = "dtu_hlir.add"(%6111, %184) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4850-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6113 = "dtu_hlir.dot_general"(%6112, %561) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4851-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x10240xf32>) -> tensor<2x256x10240xf32>
    %6114 = "dtu_hlir.add"(%176, %6113) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4852-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10240xf32>, tensor<2x256x10240xf32>) -> tensor<2x256x10240xf32>
    %6115 = "dtu_hlir.shape"(%6114) {end = 2147483647 : i64, node_name = "Shape_4853-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>) -> tensor<3xi64>
    %6116 = dtu_hlir.constant  {node_name = "Constant_4854-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %6117 = "dtu_hlir.gather"(%6115, %6116) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4855-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6118 = dtu_hlir.constant  {node_name = "Constant_4856-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6119 = dtu_hlir.constant  {node_name = "Constant_4857-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %6120 = "dtu_hlir.add"(%6117, %6119) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_4858-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6121 = dtu_hlir.constant  {node_name = "Constant_4859-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %6122 = "dtu_hlir.div"(%6120, %6121) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_4860-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6123 = dtu_hlir.constant  {node_name = "Constant_4861-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %6124 = "dtu_hlir.mul"(%6122, %6123) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_4862-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6125 = dtu_hlir.constant  {node_name = "Slice_4863-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %6126 = "dtu_hlir.real_dynamic_slice"(%6114, %6118, %6124, %6125, %6116) {node_name = "Slice_4863-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %6127 = dtu_hlir.constant  {node_name = "Constant_4864-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %6128 = "dtu_hlir.mul"(%6122, %6127) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_4865-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6129 = "dtu_hlir.shape"(%6124) {end = 2147483647 : i64, node_name = "Slice_4866-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %6130 = dtu_hlir.constant  {node_name = "Slice_4866-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %6131 = "dtu_hlir.dynamic_broadcast_in_dim"(%6130, %6129) {node_name = "Slice_4866-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6132 = "dtu_hlir.real_dynamic_slice"(%6114, %6124, %6128, %6131, %6116) {node_name = "Slice_4866-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %6133 = dtu_hlir.constant  {node_name = "Constant_4867-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %6134 = "dtu_hlir.broadcast_in_dim"(%6133) {node_name = "Div_4868-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %6135 = "dtu_hlir.div"(%6132, %6134) {node_name = "Div_4868-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6136 = "dtu_hlir.erf"(%6135) {node_name = "Erf_4869-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6137 = dtu_hlir.constant  {node_name = "Constant_4870-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %6138 = "dtu_hlir.broadcast_in_dim"(%6137) {node_name = "Add_4871-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %6139 = "dtu_hlir.add"(%6136, %6138) {node_name = "Add_4871-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6140 = "dtu_hlir.mul"(%6132, %6139) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_4872-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6141 = dtu_hlir.constant  {node_name = "Constant_4873-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %6142 = "dtu_hlir.broadcast_in_dim"(%6141) {node_name = "Mul_4874-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %6143 = "dtu_hlir.mul"(%6140, %6142) {node_name = "Mul_4874-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6144 = "dtu_hlir.mul"(%6126, %6143) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_4875-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6145 = "dtu_hlir.dot_general"(%6144, %562) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4876-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<5120x1280xf32>) -> tensor<2x256x1280xf32>
    %6146 = "dtu_hlir.add"(%177, %6145) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4877-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6147 = "dtu_hlir.add"(%6146, %6085) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_4878-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6148 = "dtu_hlir.dot_general"(%6147, %563) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4879-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6149 = "dtu_hlir.add"(%185, %6148) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4880-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6150 = dtu_hlir.constant  {node_name = "Constant_4881-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6151 = "dtu_hlir.unsqueeze"(%5645, %6150) {node_name = "Unsqueeze_4882-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6152 = dtu_hlir.constant  {node_name = "Constant_4883-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6153 = "dtu_hlir.unsqueeze"(%5648, %6152) {node_name = "Unsqueeze_4884-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6154 = dtu_hlir.constant  {node_name = "Constant_4885-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6155 = "dtu_hlir.unsqueeze"(%5651, %6154) {node_name = "Unsqueeze_4886-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6156 = dtu_hlir.constant  {node_name = "Constant_4887-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6157 = "dtu_hlir.unsqueeze"(%5663, %6156) {node_name = "Unsqueeze_4888-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6158 = "dtu_hlir.concatenate"(%6151, %6153, %6155, %6157) {dimension = 0 : i64, node_name = "Concat_4889-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6159 = "dtu_hlir.dynamic_reshape"(%6149, %6158) {allowzero = 0 : i64, node_name = "Reshape_4890-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x16x16x1280xf32>
    %6160 = "dtu_hlir.transpose"(%6159) {node_name = "Transpose_4891-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>) -> tensor<2x1280x16x16xf32>
    %6161 = "dtu_hlir.add"(%6160, %5642) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_4892-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %6162 = "dtu_hlir.concatenate"(%6161, %3143) {dimension = 1 : i64, node_name = "Concat_4893-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x640x16x16xf32>) -> tensor<2x1920x16x16xf32>
    %6163 = dtu_hlir.constant  {node_name = "Constant_4894-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %6164 = "dtu_hlir.dynamic_reshape"(%6162, %6163) {allowzero = 0 : i64, node_name = "Reshape_4895-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1920x16x16xf32>, tensor<3xi64>) -> tensor<2x32x15360xf32>
    %6165 = dtu_hlir.constant  {node_name = "Constant_4896-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %6166 = dtu_hlir.constant  {node_name = "Constant_4897-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %6167 = "dtu_hlir.instance_norm"(%6164, %6165, %6166) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_4898-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x15360xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x15360xf32>
    %6168 = "dtu_hlir.shape"(%6162) {end = 2147483647 : i64, node_name = "Shape_4899-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1920x16x16xf32>) -> tensor<4xi64>
    %6169 = "dtu_hlir.dynamic_reshape"(%6167, %6168) {allowzero = 0 : i64, node_name = "Reshape_4900-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x15360xf32>, tensor<4xi64>) -> tensor<2x1920x16x16xf32>
    %6170 = "dtu_hlir.mul"(%6169, %564) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_4901-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1920x16x16xf32>, tensor<1920x1x1xf32>) -> tensor<2x1920x16x16xf32>
    %6171 = "dtu_hlir.add"(%6170, %565) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_4902-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1920x16x16xf32>, tensor<1920x1x1xf32>) -> tensor<2x1920x16x16xf32>
    %6172 = "dtu_hlir.sigmoid"(%6171) {node_name = "Sigmoid_4903-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1920x16x16xf32>) -> tensor<2x1920x16x16xf32>
    %6173 = "dtu_hlir.mul"(%6171, %6172) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_4904-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1920x16x16xf32>, tensor<2x1920x16x16xf32>) -> tensor<2x1920x16x16xf32>
    %6174 = "dtu_hlir.conv_bias"(%6173, %214, %215) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4905-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1920x16x16xf32>, tensor<1280x1920x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %6175 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_4906-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %6176 = "dtu_hlir.mul"(%915, %6175) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4907-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %6177 = "dtu_hlir.transpose"(%216) {node_name = "Gemm_4908-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<1280x1280xf32>) -> tensor<1280x1280xf32>
    %6178 = "dtu_hlir.gemm"(%6176, %6177, %217) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_4908-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x1280xf32>, tensor<1280xf32>) -> tensor<2x1280xf32>
    %6179 = dtu_hlir.constant  {node_name = "Constant_4909-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %6180 = "dtu_hlir.unsqueeze"(%6178, %6179) {node_name = "Unsqueeze_4910-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<1xi64>) -> tensor<2x1280x1xf32>
    %6181 = dtu_hlir.constant  {node_name = "Constant_4911-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %6182 = "dtu_hlir.unsqueeze"(%6180, %6181) {node_name = "Unsqueeze_4912-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x1280x1xf32>, tensor<1xi64>) -> tensor<2x1280x1x1xf32>
    %6183 = "dtu_hlir.add"(%6174, %6182) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_4913-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %6184 = dtu_hlir.constant  {node_name = "Constant_4914-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %6185 = "dtu_hlir.dynamic_reshape"(%6183, %6184) {allowzero = 0 : i64, node_name = "Reshape_4915-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %6186 = dtu_hlir.constant  {node_name = "Constant_4916-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %6187 = dtu_hlir.constant  {node_name = "Constant_4917-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %6188 = "dtu_hlir.instance_norm"(%6185, %6186, %6187) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_4918-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %6189 = "dtu_hlir.shape"(%6183) {end = 2147483647 : i64, node_name = "Shape_4919-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %6190 = "dtu_hlir.dynamic_reshape"(%6188, %6189) {allowzero = 0 : i64, node_name = "Reshape_4920-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %6191 = "dtu_hlir.mul"(%6190, %566) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_4921-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %6192 = "dtu_hlir.add"(%6191, %567) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_4922-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %6193 = "dtu_hlir.sigmoid"(%6192) {node_name = "Sigmoid_4923-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %6194 = "dtu_hlir.mul"(%6192, %6193) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_4924-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %6195 = "dtu_hlir.conv_bias"(%6194, %218, %219) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4925-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x16x16xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %6196 = "dtu_hlir.conv_bias"(%6162, %220, %221) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4926-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1920x16x16xf32>, tensor<1280x1920x1x1xf32>, tensor<1280xf32>) -> tensor<2x1280x16x16xf32>
    %6197 = "dtu_hlir.add"(%6196, %6195) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_4927-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %6198 = dtu_hlir.constant  {node_name = "Constant_4928-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %6199 = "dtu_hlir.broadcast_in_dim"(%6198) {node_name = "Div_4929-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1280x16x16xf32>
    %6200 = "dtu_hlir.div"(%6197, %6199) {node_name = "Div_4929-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %6201 = "dtu_hlir.shape"(%6200) {end = 2147483647 : i64, node_name = "Shape_4930-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %6202 = dtu_hlir.constant  {node_name = "Constant_4931-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6203 = "dtu_hlir.gather"(%6201, %6202) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4932-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6204 = "dtu_hlir.shape"(%6200) {end = 2147483647 : i64, node_name = "Shape_4933-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %6205 = dtu_hlir.constant  {node_name = "Constant_4934-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6206 = "dtu_hlir.gather"(%6204, %6205) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4935-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6207 = "dtu_hlir.shape"(%6200) {end = 2147483647 : i64, node_name = "Shape_4936-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %6208 = dtu_hlir.constant  {node_name = "Constant_4937-0", node_type = "Constant"} dense<3> : tensor<i64>
    %6209 = "dtu_hlir.gather"(%6207, %6208) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4938-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6210 = dtu_hlir.constant  {node_name = "Constant_4939-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %6211 = "dtu_hlir.dynamic_reshape"(%6200, %6210) {allowzero = 0 : i64, node_name = "Reshape_4940-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<3xi64>) -> tensor<2x32x10240xf32>
    %6212 = dtu_hlir.constant  {node_name = "Constant_4941-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %6213 = dtu_hlir.constant  {node_name = "Constant_4942-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %6214 = "dtu_hlir.instance_norm"(%6211, %6212, %6213) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_4943-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x10240xf32>
    %6215 = "dtu_hlir.shape"(%6200) {end = 2147483647 : i64, node_name = "Shape_4944-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %6216 = "dtu_hlir.dynamic_reshape"(%6214, %6215) {allowzero = 0 : i64, node_name = "Reshape_4945-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x10240xf32>, tensor<4xi64>) -> tensor<2x1280x16x16xf32>
    %6217 = "dtu_hlir.mul"(%6216, %568) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_4946-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %6218 = "dtu_hlir.add"(%6217, %569) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_4947-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x16x16xf32>
    %6219 = "dtu_hlir.shape"(%6218) {end = 2147483647 : i64, node_name = "Shape_4948-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<4xi64>
    %6220 = dtu_hlir.constant  {node_name = "Constant_4949-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6221 = "dtu_hlir.gather"(%6219, %6220) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4950-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6222 = "dtu_hlir.transpose"(%6218) {node_name = "Transpose_4951-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>) -> tensor<2x16x16x1280xf32>
    %6223 = "dtu_hlir.mul"(%6206, %6209) {node_name = "Mul_4952-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6224 = dtu_hlir.constant  {node_name = "Constant_4953-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6225 = "dtu_hlir.unsqueeze"(%6203, %6224) {node_name = "Unsqueeze_4954-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6226 = dtu_hlir.constant  {node_name = "Constant_4955-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6227 = "dtu_hlir.unsqueeze"(%6223, %6226) {node_name = "Unsqueeze_4956-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6228 = dtu_hlir.constant  {node_name = "Constant_4957-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6229 = "dtu_hlir.unsqueeze"(%6221, %6228) {node_name = "Unsqueeze_4958-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6230 = "dtu_hlir.concatenate"(%6225, %6227, %6229) {dimension = 0 : i64, node_name = "Concat_4959-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6231 = "dtu_hlir.dynamic_reshape"(%6222, %6230) {allowzero = 0 : i64, node_name = "Reshape_4960-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %6232 = "dtu_hlir.dot_general"(%6231, %570) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4961-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6233 = "dtu_hlir.add"(%186, %6232) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4962-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6234 = dtu_hlir.constant  {node_name = "ReduceMean_4963-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6235 = "dtu_hlir.reshape"(%6234) {node_name = "ReduceMean_4963-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6236 = "dtu_hlir.reduce"(%6233, %6235) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4963-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4963-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6237 = dtu_hlir.constant  {node_name = "ReduceMean_4963-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6238 = "dtu_hlir.unsqueeze"(%6236, %6237) {node_name = "ReduceMean_4963-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6239 = dtu_hlir.constant  {node_name = "ReduceMean_4963-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6240 = "dtu_hlir.broadcast_in_dim"(%6239) {node_name = "ReduceMean_4963-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6241 = "dtu_hlir.div"(%6238, %6240) {node_name = "ReduceMean_4963-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6242 = "dtu_hlir.sub"(%6233, %6241) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_4964-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6243 = dtu_hlir.constant  {node_name = "Constant_4965-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %6244 = "dtu_hlir.broadcast_in_dim"(%6243) {node_name = "Pow_4966-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %6245 = "dtu_hlir.pow"(%6242, %6244) {node_name = "Pow_4966-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6246 = dtu_hlir.constant  {node_name = "ReduceMean_4967-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6247 = "dtu_hlir.reshape"(%6246) {node_name = "ReduceMean_4967-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6248 = "dtu_hlir.reduce"(%6245, %6247) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_4967-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_4967-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6249 = dtu_hlir.constant  {node_name = "ReduceMean_4967-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6250 = "dtu_hlir.unsqueeze"(%6248, %6249) {node_name = "ReduceMean_4967-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6251 = dtu_hlir.constant  {node_name = "ReduceMean_4967-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6252 = "dtu_hlir.broadcast_in_dim"(%6251) {node_name = "ReduceMean_4967-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6253 = "dtu_hlir.div"(%6250, %6252) {node_name = "ReduceMean_4967-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6254 = dtu_hlir.constant  {node_name = "Constant_4968-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %6255 = "dtu_hlir.broadcast_in_dim"(%6254) {node_name = "Add_4969-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %6256 = "dtu_hlir.add"(%6253, %6255) {node_name = "Add_4969-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6257 = "dtu_hlir.sqrt"(%6256) {node_name = "Sqrt_4970-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6258 = "dtu_hlir.div"(%6242, %6257) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_4971-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6259 = "dtu_hlir.mul"(%6258, %191) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_4972-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6260 = "dtu_hlir.add"(%6259, %192) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_4973-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6261 = "dtu_hlir.dot_general"(%6260, %571) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4974-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6262 = "dtu_hlir.dot_general"(%6260, %572) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4975-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6263 = "dtu_hlir.dot_general"(%6260, %573) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_4976-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6264 = "dtu_hlir.shape"(%6261) {end = 2147483647 : i64, node_name = "Shape_4977-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6265 = dtu_hlir.constant  {node_name = "Constant_4978-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6266 = "dtu_hlir.gather"(%6264, %6265) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4979-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6267 = "dtu_hlir.shape"(%6261) {end = 2147483647 : i64, node_name = "Shape_4980-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6268 = dtu_hlir.constant  {node_name = "Constant_4981-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6269 = "dtu_hlir.gather"(%6267, %6268) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4982-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6270 = "dtu_hlir.shape"(%6261) {end = 2147483647 : i64, node_name = "Shape_4983-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6271 = dtu_hlir.constant  {node_name = "Constant_4984-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6272 = "dtu_hlir.gather"(%6270, %6271) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_4985-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6273 = dtu_hlir.constant  {node_name = "Constant_4986-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6274 = "dtu_hlir.div"(%6272, %6273) {node_name = "Div_4987-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6275 = "dtu_hlir.convert"(%6274) {node_name = "Cast_4988-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6276 = "dtu_hlir.convert"(%6275) {node_name = "Cast_4989-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6277 = dtu_hlir.constant  {node_name = "Constant_4990-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6278 = "dtu_hlir.unsqueeze"(%6266, %6277) {node_name = "Unsqueeze_4991-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6279 = dtu_hlir.constant  {node_name = "Constant_4992-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6280 = "dtu_hlir.unsqueeze"(%6269, %6279) {node_name = "Unsqueeze_4993-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6281 = dtu_hlir.constant  {node_name = "Constant_4994-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6282 = "dtu_hlir.unsqueeze"(%6276, %6281) {node_name = "Unsqueeze_4995-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6283 = "dtu_hlir.concatenate"(%6278, %6280, %777, %6282) {dimension = 0 : i64, node_name = "Concat_4996-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6284 = "dtu_hlir.dynamic_reshape"(%6261, %6283) {allowzero = 0 : i64, node_name = "Reshape_4997-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %6285 = "dtu_hlir.transpose"(%6284) {node_name = "Transpose_4998-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %6286 = dtu_hlir.constant  {node_name = "Constant_4999-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6287 = "dtu_hlir.mul"(%6266, %6286) {node_name = "Mul_5000-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6288 = dtu_hlir.constant  {node_name = "Constant_5001-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6289 = "dtu_hlir.div"(%6272, %6288) {node_name = "Div_5002-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6290 = "dtu_hlir.convert"(%6289) {node_name = "Cast_5003-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6291 = "dtu_hlir.convert"(%6290) {node_name = "Cast_5004-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6292 = dtu_hlir.constant  {node_name = "Constant_5005-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6293 = "dtu_hlir.unsqueeze"(%6287, %6292) {node_name = "Unsqueeze_5006-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6294 = dtu_hlir.constant  {node_name = "Constant_5007-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6295 = "dtu_hlir.unsqueeze"(%6269, %6294) {node_name = "Unsqueeze_5008-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6296 = dtu_hlir.constant  {node_name = "Constant_5009-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6297 = "dtu_hlir.unsqueeze"(%6291, %6296) {node_name = "Unsqueeze_5010-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6298 = "dtu_hlir.concatenate"(%6293, %6295, %6297) {dimension = 0 : i64, node_name = "Concat_5011-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6299 = "dtu_hlir.dynamic_reshape"(%6285, %6298) {allowzero = 0 : i64, node_name = "Reshape_5012-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %6300 = "dtu_hlir.shape"(%6262) {end = 2147483647 : i64, node_name = "Shape_5013-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6301 = dtu_hlir.constant  {node_name = "Constant_5014-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6302 = "dtu_hlir.gather"(%6300, %6301) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5015-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6303 = "dtu_hlir.shape"(%6262) {end = 2147483647 : i64, node_name = "Shape_5016-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6304 = dtu_hlir.constant  {node_name = "Constant_5017-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6305 = "dtu_hlir.gather"(%6303, %6304) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5018-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6306 = "dtu_hlir.shape"(%6262) {end = 2147483647 : i64, node_name = "Shape_5019-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6307 = dtu_hlir.constant  {node_name = "Constant_5020-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6308 = "dtu_hlir.gather"(%6306, %6307) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5021-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6309 = dtu_hlir.constant  {node_name = "Constant_5022-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6310 = "dtu_hlir.div"(%6308, %6309) {node_name = "Div_5023-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6311 = "dtu_hlir.convert"(%6310) {node_name = "Cast_5024-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6312 = "dtu_hlir.convert"(%6311) {node_name = "Cast_5025-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6313 = dtu_hlir.constant  {node_name = "Constant_5026-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6314 = "dtu_hlir.unsqueeze"(%6302, %6313) {node_name = "Unsqueeze_5027-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6315 = dtu_hlir.constant  {node_name = "Constant_5028-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6316 = "dtu_hlir.unsqueeze"(%6305, %6315) {node_name = "Unsqueeze_5029-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6317 = dtu_hlir.constant  {node_name = "Constant_5030-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6318 = "dtu_hlir.unsqueeze"(%6312, %6317) {node_name = "Unsqueeze_5031-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6319 = "dtu_hlir.concatenate"(%6314, %6316, %776, %6318) {dimension = 0 : i64, node_name = "Concat_5032-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6320 = "dtu_hlir.dynamic_reshape"(%6262, %6319) {allowzero = 0 : i64, node_name = "Reshape_5033-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %6321 = "dtu_hlir.transpose"(%6320) {node_name = "Transpose_5034-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %6322 = dtu_hlir.constant  {node_name = "Constant_5035-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6323 = "dtu_hlir.mul"(%6302, %6322) {node_name = "Mul_5036-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6324 = dtu_hlir.constant  {node_name = "Constant_5037-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6325 = "dtu_hlir.div"(%6308, %6324) {node_name = "Div_5038-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6326 = "dtu_hlir.convert"(%6325) {node_name = "Cast_5039-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6327 = "dtu_hlir.convert"(%6326) {node_name = "Cast_5040-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6328 = dtu_hlir.constant  {node_name = "Constant_5041-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6329 = "dtu_hlir.unsqueeze"(%6323, %6328) {node_name = "Unsqueeze_5042-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6330 = dtu_hlir.constant  {node_name = "Constant_5043-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6331 = "dtu_hlir.unsqueeze"(%6305, %6330) {node_name = "Unsqueeze_5044-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6332 = dtu_hlir.constant  {node_name = "Constant_5045-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6333 = "dtu_hlir.unsqueeze"(%6327, %6332) {node_name = "Unsqueeze_5046-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6334 = "dtu_hlir.concatenate"(%6329, %6331, %6333) {dimension = 0 : i64, node_name = "Concat_5047-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6335 = "dtu_hlir.dynamic_reshape"(%6321, %6334) {allowzero = 0 : i64, node_name = "Reshape_5048-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %6336 = "dtu_hlir.shape"(%6263) {end = 2147483647 : i64, node_name = "Shape_5049-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6337 = dtu_hlir.constant  {node_name = "Constant_5050-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6338 = "dtu_hlir.gather"(%6336, %6337) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5051-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6339 = "dtu_hlir.shape"(%6263) {end = 2147483647 : i64, node_name = "Shape_5052-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6340 = dtu_hlir.constant  {node_name = "Constant_5053-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6341 = "dtu_hlir.gather"(%6339, %6340) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5054-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6342 = "dtu_hlir.shape"(%6263) {end = 2147483647 : i64, node_name = "Shape_5055-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6343 = dtu_hlir.constant  {node_name = "Constant_5056-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6344 = "dtu_hlir.gather"(%6342, %6343) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5057-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6345 = dtu_hlir.constant  {node_name = "Constant_5058-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6346 = "dtu_hlir.div"(%6344, %6345) {node_name = "Div_5059-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6347 = "dtu_hlir.convert"(%6346) {node_name = "Cast_5060-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6348 = "dtu_hlir.convert"(%6347) {node_name = "Cast_5061-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6349 = dtu_hlir.constant  {node_name = "Constant_5062-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6350 = "dtu_hlir.unsqueeze"(%6338, %6349) {node_name = "Unsqueeze_5063-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6351 = dtu_hlir.constant  {node_name = "Constant_5064-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6352 = "dtu_hlir.unsqueeze"(%6341, %6351) {node_name = "Unsqueeze_5065-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6353 = dtu_hlir.constant  {node_name = "Constant_5066-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6354 = "dtu_hlir.unsqueeze"(%6348, %6353) {node_name = "Unsqueeze_5067-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6355 = "dtu_hlir.concatenate"(%6350, %6352, %775, %6354) {dimension = 0 : i64, node_name = "Concat_5068-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6356 = "dtu_hlir.dynamic_reshape"(%6263, %6355) {allowzero = 0 : i64, node_name = "Reshape_5069-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %6357 = "dtu_hlir.transpose"(%6356) {node_name = "Transpose_5070-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %6358 = dtu_hlir.constant  {node_name = "Constant_5071-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6359 = "dtu_hlir.mul"(%6338, %6358) {node_name = "Mul_5072-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6360 = dtu_hlir.constant  {node_name = "Constant_5073-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6361 = "dtu_hlir.div"(%6344, %6360) {node_name = "Div_5074-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6362 = "dtu_hlir.convert"(%6361) {node_name = "Cast_5075-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6363 = "dtu_hlir.convert"(%6362) {node_name = "Cast_5076-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6364 = dtu_hlir.constant  {node_name = "Constant_5077-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6365 = "dtu_hlir.unsqueeze"(%6359, %6364) {node_name = "Unsqueeze_5078-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6366 = dtu_hlir.constant  {node_name = "Constant_5079-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6367 = "dtu_hlir.unsqueeze"(%6341, %6366) {node_name = "Unsqueeze_5080-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6368 = dtu_hlir.constant  {node_name = "Constant_5081-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6369 = "dtu_hlir.unsqueeze"(%6363, %6368) {node_name = "Unsqueeze_5082-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6370 = "dtu_hlir.concatenate"(%6365, %6367, %6369) {dimension = 0 : i64, node_name = "Concat_5083-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6371 = "dtu_hlir.dynamic_reshape"(%6357, %6370) {allowzero = 0 : i64, node_name = "Reshape_5084-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %6372 = "dtu_hlir.shape"(%6299) {end = 2147483647 : i64, node_name = "Shape_5085-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6373 = dtu_hlir.constant  {node_name = "Constant_5086-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6374 = "dtu_hlir.gather"(%6372, %6373) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5087-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6375 = "dtu_hlir.shape"(%6299) {end = 2147483647 : i64, node_name = "Shape_5088-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6376 = dtu_hlir.constant  {node_name = "Constant_5089-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6377 = "dtu_hlir.gather"(%6375, %6376) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5090-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6378 = "dtu_hlir.shape"(%6335) {end = 2147483647 : i64, node_name = "Shape_5091-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6379 = dtu_hlir.constant  {node_name = "Constant_5092-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6380 = "dtu_hlir.gather"(%6378, %6379) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5093-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6381 = dtu_hlir.constant  {node_name = "Constant_5094-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6382 = "dtu_hlir.unsqueeze"(%6374, %6381) {node_name = "Unsqueeze_5095-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6383 = dtu_hlir.constant  {node_name = "Constant_5096-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6384 = "dtu_hlir.unsqueeze"(%6377, %6383) {node_name = "Unsqueeze_5097-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6385 = dtu_hlir.constant  {node_name = "Constant_5098-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6386 = "dtu_hlir.unsqueeze"(%6380, %6385) {node_name = "Unsqueeze_5099-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6387 = "dtu_hlir.concatenate"(%6382, %6384, %6386) {dimension = 0 : i64, node_name = "Concat_5100-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6388 = dtu_hlir.constant  {node_name = "ConstantOfShape_5101-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %6389 = "dtu_hlir.dynamic_broadcast_in_dim"(%6388, %6387) {node_name = "ConstantOfShape_5101-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x256xf32>
    %6390 = "dtu_hlir.transpose"(%6335) {node_name = "Transpose_5102-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<40x64x256xf32>
    %6391 = "dtu_hlir.dot_general"(%6299, %6390) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5103-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x256xf32>) -> tensor<40x256x256xf32>
    %6392 = "dtu_hlir.broadcast_in_dim"(%774) {node_name = "Mul_5104-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %6393 = "dtu_hlir.mul"(%6391, %6392) {node_name = "Mul_5104-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %6394 = "dtu_hlir.broadcast_in_dim"(%773) {node_name = "Mul_5105-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x256xf32>
    %6395 = "dtu_hlir.mul"(%6389, %6394) {node_name = "Mul_5105-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %6396 = "dtu_hlir.add"(%6393, %6395) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5106-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %6397 = "dtu_hlir.softmax"(%6396) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5107-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %6398 = "dtu_hlir.convert"(%6397) {node_name = "Cast_5108-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>) -> tensor<40x256x256xf32>
    %6399 = "dtu_hlir.dot_general"(%6398, %6371) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5109-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x256xf32>, tensor<40x256x64xf32>) -> tensor<40x256x64xf32>
    %6400 = "dtu_hlir.shape"(%6399) {end = 2147483647 : i64, node_name = "Shape_5110-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6401 = dtu_hlir.constant  {node_name = "Constant_5111-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6402 = "dtu_hlir.gather"(%6400, %6401) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5112-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6403 = "dtu_hlir.shape"(%6399) {end = 2147483647 : i64, node_name = "Shape_5113-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6404 = dtu_hlir.constant  {node_name = "Constant_5114-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6405 = "dtu_hlir.gather"(%6403, %6404) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5115-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6406 = "dtu_hlir.shape"(%6399) {end = 2147483647 : i64, node_name = "Shape_5116-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6407 = dtu_hlir.constant  {node_name = "Constant_5117-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6408 = "dtu_hlir.gather"(%6406, %6407) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5118-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6409 = dtu_hlir.constant  {node_name = "Constant_5119-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6410 = "dtu_hlir.div"(%6402, %6409) {node_name = "Div_5120-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6411 = "dtu_hlir.convert"(%6410) {node_name = "Cast_5121-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6412 = "dtu_hlir.convert"(%6411) {node_name = "Cast_5122-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6413 = dtu_hlir.constant  {node_name = "Constant_5123-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6414 = "dtu_hlir.unsqueeze"(%6412, %6413) {node_name = "Unsqueeze_5124-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6415 = dtu_hlir.constant  {node_name = "Constant_5125-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6416 = "dtu_hlir.unsqueeze"(%6405, %6415) {node_name = "Unsqueeze_5126-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6417 = dtu_hlir.constant  {node_name = "Constant_5127-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6418 = "dtu_hlir.unsqueeze"(%6408, %6417) {node_name = "Unsqueeze_5128-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6419 = "dtu_hlir.concatenate"(%6414, %772, %6416, %6418) {dimension = 0 : i64, node_name = "Concat_5129-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6420 = "dtu_hlir.dynamic_reshape"(%6399, %6419) {allowzero = 0 : i64, node_name = "Reshape_5130-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %6421 = "dtu_hlir.transpose"(%6420) {node_name = "Transpose_5131-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %6422 = dtu_hlir.constant  {node_name = "Constant_5132-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6423 = "dtu_hlir.div"(%6402, %6422) {node_name = "Div_5133-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6424 = "dtu_hlir.convert"(%6423) {node_name = "Cast_5134-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6425 = "dtu_hlir.convert"(%6424) {node_name = "Cast_5135-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6426 = dtu_hlir.constant  {node_name = "Constant_5136-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6427 = "dtu_hlir.mul"(%6408, %6426) {node_name = "Mul_5137-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6428 = dtu_hlir.constant  {node_name = "Constant_5138-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6429 = "dtu_hlir.unsqueeze"(%6425, %6428) {node_name = "Unsqueeze_5139-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6430 = dtu_hlir.constant  {node_name = "Constant_5140-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6431 = "dtu_hlir.unsqueeze"(%6405, %6430) {node_name = "Unsqueeze_5141-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6432 = dtu_hlir.constant  {node_name = "Constant_5142-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6433 = "dtu_hlir.unsqueeze"(%6427, %6432) {node_name = "Unsqueeze_5143-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6434 = "dtu_hlir.concatenate"(%6429, %6431, %6433) {dimension = 0 : i64, node_name = "Concat_5144-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6435 = "dtu_hlir.dynamic_reshape"(%6421, %6434) {allowzero = 0 : i64, node_name = "Reshape_5145-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %6436 = "dtu_hlir.dot_general"(%6435, %574) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5146-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6437 = "dtu_hlir.add"(%187, %6436) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5147-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6438 = "dtu_hlir.add"(%6437, %6233) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5148-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6439 = dtu_hlir.constant  {node_name = "ReduceMean_5149-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6440 = "dtu_hlir.reshape"(%6439) {node_name = "ReduceMean_5149-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6441 = "dtu_hlir.reduce"(%6438, %6440) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5149-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5149-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6442 = dtu_hlir.constant  {node_name = "ReduceMean_5149-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6443 = "dtu_hlir.unsqueeze"(%6441, %6442) {node_name = "ReduceMean_5149-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6444 = dtu_hlir.constant  {node_name = "ReduceMean_5149-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6445 = "dtu_hlir.broadcast_in_dim"(%6444) {node_name = "ReduceMean_5149-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6446 = "dtu_hlir.div"(%6443, %6445) {node_name = "ReduceMean_5149-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6447 = "dtu_hlir.sub"(%6438, %6446) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_5150-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6448 = dtu_hlir.constant  {node_name = "Constant_5151-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %6449 = "dtu_hlir.broadcast_in_dim"(%6448) {node_name = "Pow_5152-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %6450 = "dtu_hlir.pow"(%6447, %6449) {node_name = "Pow_5152-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6451 = dtu_hlir.constant  {node_name = "ReduceMean_5153-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6452 = "dtu_hlir.reshape"(%6451) {node_name = "ReduceMean_5153-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6453 = "dtu_hlir.reduce"(%6450, %6452) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5153-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5153-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6454 = dtu_hlir.constant  {node_name = "ReduceMean_5153-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6455 = "dtu_hlir.unsqueeze"(%6453, %6454) {node_name = "ReduceMean_5153-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6456 = dtu_hlir.constant  {node_name = "ReduceMean_5153-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6457 = "dtu_hlir.broadcast_in_dim"(%6456) {node_name = "ReduceMean_5153-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6458 = "dtu_hlir.div"(%6455, %6457) {node_name = "ReduceMean_5153-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6459 = dtu_hlir.constant  {node_name = "Constant_5154-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %6460 = "dtu_hlir.broadcast_in_dim"(%6459) {node_name = "Add_5155-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %6461 = "dtu_hlir.add"(%6458, %6460) {node_name = "Add_5155-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6462 = "dtu_hlir.sqrt"(%6461) {node_name = "Sqrt_5156-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6463 = "dtu_hlir.div"(%6447, %6462) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_5157-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6464 = "dtu_hlir.mul"(%6463, %193) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_5158-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6465 = "dtu_hlir.add"(%6464, %194) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5159-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6466 = "dtu_hlir.dot_general"(%6465, %575) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5160-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6467 = "dtu_hlir.dot_general"(%arg2, %576) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5161-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %6468 = "dtu_hlir.dot_general"(%arg2, %577) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5162-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x1280xf32>) -> tensor<2x77x1280xf32>
    %6469 = "dtu_hlir.shape"(%6466) {end = 2147483647 : i64, node_name = "Shape_5163-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6470 = dtu_hlir.constant  {node_name = "Constant_5164-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6471 = "dtu_hlir.gather"(%6469, %6470) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5165-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6472 = "dtu_hlir.shape"(%6466) {end = 2147483647 : i64, node_name = "Shape_5166-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6473 = dtu_hlir.constant  {node_name = "Constant_5167-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6474 = "dtu_hlir.gather"(%6472, %6473) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5168-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6475 = "dtu_hlir.shape"(%6466) {end = 2147483647 : i64, node_name = "Shape_5169-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>) -> tensor<3xi64>
    %6476 = dtu_hlir.constant  {node_name = "Constant_5170-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6477 = "dtu_hlir.gather"(%6475, %6476) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5171-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6478 = dtu_hlir.constant  {node_name = "Constant_5172-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6479 = "dtu_hlir.div"(%6477, %6478) {node_name = "Div_5173-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6480 = "dtu_hlir.convert"(%6479) {node_name = "Cast_5174-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6481 = "dtu_hlir.convert"(%6480) {node_name = "Cast_5175-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6482 = dtu_hlir.constant  {node_name = "Constant_5176-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6483 = "dtu_hlir.unsqueeze"(%6471, %6482) {node_name = "Unsqueeze_5177-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6484 = dtu_hlir.constant  {node_name = "Constant_5178-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6485 = "dtu_hlir.unsqueeze"(%6474, %6484) {node_name = "Unsqueeze_5179-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6486 = dtu_hlir.constant  {node_name = "Constant_5180-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6487 = "dtu_hlir.unsqueeze"(%6481, %6486) {node_name = "Unsqueeze_5181-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6488 = "dtu_hlir.concatenate"(%6483, %6485, %771, %6487) {dimension = 0 : i64, node_name = "Concat_5182-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6489 = "dtu_hlir.dynamic_reshape"(%6466, %6488) {allowzero = 0 : i64, node_name = "Reshape_5183-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x256x20x64xf32>
    %6490 = "dtu_hlir.transpose"(%6489) {node_name = "Transpose_5184-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>) -> tensor<2x20x256x64xf32>
    %6491 = dtu_hlir.constant  {node_name = "Constant_5185-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6492 = "dtu_hlir.mul"(%6471, %6491) {node_name = "Mul_5186-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6493 = dtu_hlir.constant  {node_name = "Constant_5187-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6494 = "dtu_hlir.div"(%6477, %6493) {node_name = "Div_5188-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6495 = "dtu_hlir.convert"(%6494) {node_name = "Cast_5189-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6496 = "dtu_hlir.convert"(%6495) {node_name = "Cast_5190-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6497 = dtu_hlir.constant  {node_name = "Constant_5191-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6498 = "dtu_hlir.unsqueeze"(%6492, %6497) {node_name = "Unsqueeze_5192-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6499 = dtu_hlir.constant  {node_name = "Constant_5193-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6500 = "dtu_hlir.unsqueeze"(%6474, %6499) {node_name = "Unsqueeze_5194-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6501 = dtu_hlir.constant  {node_name = "Constant_5195-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6502 = "dtu_hlir.unsqueeze"(%6496, %6501) {node_name = "Unsqueeze_5196-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6503 = "dtu_hlir.concatenate"(%6498, %6500, %6502) {dimension = 0 : i64, node_name = "Concat_5197-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6504 = "dtu_hlir.dynamic_reshape"(%6490, %6503) {allowzero = 0 : i64, node_name = "Reshape_5198-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>, tensor<3xi64>) -> tensor<40x256x64xf32>
    %6505 = "dtu_hlir.shape"(%6467) {end = 2147483647 : i64, node_name = "Shape_5199-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %6506 = dtu_hlir.constant  {node_name = "Constant_5200-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6507 = "dtu_hlir.gather"(%6505, %6506) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5201-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6508 = "dtu_hlir.shape"(%6467) {end = 2147483647 : i64, node_name = "Shape_5202-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %6509 = dtu_hlir.constant  {node_name = "Constant_5203-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6510 = "dtu_hlir.gather"(%6508, %6509) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5204-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6511 = "dtu_hlir.shape"(%6467) {end = 2147483647 : i64, node_name = "Shape_5205-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %6512 = dtu_hlir.constant  {node_name = "Constant_5206-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6513 = "dtu_hlir.gather"(%6511, %6512) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5207-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6514 = dtu_hlir.constant  {node_name = "Constant_5208-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6515 = "dtu_hlir.div"(%6513, %6514) {node_name = "Div_5209-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6516 = "dtu_hlir.convert"(%6515) {node_name = "Cast_5210-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6517 = "dtu_hlir.convert"(%6516) {node_name = "Cast_5211-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6518 = dtu_hlir.constant  {node_name = "Constant_5212-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6519 = "dtu_hlir.unsqueeze"(%6507, %6518) {node_name = "Unsqueeze_5213-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6520 = dtu_hlir.constant  {node_name = "Constant_5214-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6521 = "dtu_hlir.unsqueeze"(%6510, %6520) {node_name = "Unsqueeze_5215-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6522 = dtu_hlir.constant  {node_name = "Constant_5216-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6523 = "dtu_hlir.unsqueeze"(%6517, %6522) {node_name = "Unsqueeze_5217-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6524 = "dtu_hlir.concatenate"(%6519, %6521, %770, %6523) {dimension = 0 : i64, node_name = "Concat_5218-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6525 = "dtu_hlir.dynamic_reshape"(%6467, %6524) {allowzero = 0 : i64, node_name = "Reshape_5219-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %6526 = "dtu_hlir.transpose"(%6525) {node_name = "Transpose_5220-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %6527 = dtu_hlir.constant  {node_name = "Constant_5221-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6528 = "dtu_hlir.mul"(%6507, %6527) {node_name = "Mul_5222-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6529 = dtu_hlir.constant  {node_name = "Constant_5223-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6530 = "dtu_hlir.div"(%6513, %6529) {node_name = "Div_5224-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6531 = "dtu_hlir.convert"(%6530) {node_name = "Cast_5225-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6532 = "dtu_hlir.convert"(%6531) {node_name = "Cast_5226-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6533 = dtu_hlir.constant  {node_name = "Constant_5227-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6534 = "dtu_hlir.unsqueeze"(%6528, %6533) {node_name = "Unsqueeze_5228-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6535 = dtu_hlir.constant  {node_name = "Constant_5229-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6536 = "dtu_hlir.unsqueeze"(%6510, %6535) {node_name = "Unsqueeze_5230-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6537 = dtu_hlir.constant  {node_name = "Constant_5231-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6538 = "dtu_hlir.unsqueeze"(%6532, %6537) {node_name = "Unsqueeze_5232-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6539 = "dtu_hlir.concatenate"(%6534, %6536, %6538) {dimension = 0 : i64, node_name = "Concat_5233-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6540 = "dtu_hlir.dynamic_reshape"(%6526, %6539) {allowzero = 0 : i64, node_name = "Reshape_5234-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %6541 = "dtu_hlir.shape"(%6468) {end = 2147483647 : i64, node_name = "Shape_5235-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %6542 = dtu_hlir.constant  {node_name = "Constant_5236-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6543 = "dtu_hlir.gather"(%6541, %6542) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5237-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6544 = "dtu_hlir.shape"(%6468) {end = 2147483647 : i64, node_name = "Shape_5238-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %6545 = dtu_hlir.constant  {node_name = "Constant_5239-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6546 = "dtu_hlir.gather"(%6544, %6545) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5240-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6547 = "dtu_hlir.shape"(%6468) {end = 2147483647 : i64, node_name = "Shape_5241-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>) -> tensor<3xi64>
    %6548 = dtu_hlir.constant  {node_name = "Constant_5242-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6549 = "dtu_hlir.gather"(%6547, %6548) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5243-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6550 = dtu_hlir.constant  {node_name = "Constant_5244-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6551 = "dtu_hlir.div"(%6549, %6550) {node_name = "Div_5245-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6552 = "dtu_hlir.convert"(%6551) {node_name = "Cast_5246-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6553 = "dtu_hlir.convert"(%6552) {node_name = "Cast_5247-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6554 = dtu_hlir.constant  {node_name = "Constant_5248-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6555 = "dtu_hlir.unsqueeze"(%6543, %6554) {node_name = "Unsqueeze_5249-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6556 = dtu_hlir.constant  {node_name = "Constant_5250-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6557 = "dtu_hlir.unsqueeze"(%6546, %6556) {node_name = "Unsqueeze_5251-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6558 = dtu_hlir.constant  {node_name = "Constant_5252-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6559 = "dtu_hlir.unsqueeze"(%6553, %6558) {node_name = "Unsqueeze_5253-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6560 = "dtu_hlir.concatenate"(%6555, %6557, %769, %6559) {dimension = 0 : i64, node_name = "Concat_5254-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6561 = "dtu_hlir.dynamic_reshape"(%6468, %6560) {allowzero = 0 : i64, node_name = "Reshape_5255-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x1280xf32>, tensor<4xi64>) -> tensor<2x77x20x64xf32>
    %6562 = "dtu_hlir.transpose"(%6561) {node_name = "Transpose_5256-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x20x64xf32>) -> tensor<2x20x77x64xf32>
    %6563 = dtu_hlir.constant  {node_name = "Constant_5257-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6564 = "dtu_hlir.mul"(%6543, %6563) {node_name = "Mul_5258-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6565 = dtu_hlir.constant  {node_name = "Constant_5259-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6566 = "dtu_hlir.div"(%6549, %6565) {node_name = "Div_5260-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6567 = "dtu_hlir.convert"(%6566) {node_name = "Cast_5261-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6568 = "dtu_hlir.convert"(%6567) {node_name = "Cast_5262-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6569 = dtu_hlir.constant  {node_name = "Constant_5263-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6570 = "dtu_hlir.unsqueeze"(%6564, %6569) {node_name = "Unsqueeze_5264-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6571 = dtu_hlir.constant  {node_name = "Constant_5265-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6572 = "dtu_hlir.unsqueeze"(%6546, %6571) {node_name = "Unsqueeze_5266-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6573 = dtu_hlir.constant  {node_name = "Constant_5267-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6574 = "dtu_hlir.unsqueeze"(%6568, %6573) {node_name = "Unsqueeze_5268-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6575 = "dtu_hlir.concatenate"(%6570, %6572, %6574) {dimension = 0 : i64, node_name = "Concat_5269-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6576 = "dtu_hlir.dynamic_reshape"(%6562, %6575) {allowzero = 0 : i64, node_name = "Reshape_5270-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x20x77x64xf32>, tensor<3xi64>) -> tensor<40x77x64xf32>
    %6577 = "dtu_hlir.shape"(%6504) {end = 2147483647 : i64, node_name = "Shape_5271-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6578 = dtu_hlir.constant  {node_name = "Constant_5272-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6579 = "dtu_hlir.gather"(%6577, %6578) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5273-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6580 = "dtu_hlir.shape"(%6504) {end = 2147483647 : i64, node_name = "Shape_5274-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6581 = dtu_hlir.constant  {node_name = "Constant_5275-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6582 = "dtu_hlir.gather"(%6580, %6581) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5276-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6583 = "dtu_hlir.shape"(%6540) {end = 2147483647 : i64, node_name = "Shape_5277-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<3xi64>
    %6584 = dtu_hlir.constant  {node_name = "Constant_5278-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6585 = "dtu_hlir.gather"(%6583, %6584) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5279-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6586 = dtu_hlir.constant  {node_name = "Constant_5280-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6587 = "dtu_hlir.unsqueeze"(%6579, %6586) {node_name = "Unsqueeze_5281-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6588 = dtu_hlir.constant  {node_name = "Constant_5282-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6589 = "dtu_hlir.unsqueeze"(%6582, %6588) {node_name = "Unsqueeze_5283-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6590 = dtu_hlir.constant  {node_name = "Constant_5284-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6591 = "dtu_hlir.unsqueeze"(%6585, %6590) {node_name = "Unsqueeze_5285-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6592 = "dtu_hlir.concatenate"(%6587, %6589, %6591) {dimension = 0 : i64, node_name = "Concat_5286-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6593 = dtu_hlir.constant  {node_name = "ConstantOfShape_5287-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %6594 = "dtu_hlir.dynamic_broadcast_in_dim"(%6593, %6592) {node_name = "ConstantOfShape_5287-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<40x256x77xf32>
    %6595 = "dtu_hlir.transpose"(%6540) {node_name = "Transpose_5288-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<40x77x64xf32>) -> tensor<40x64x77xf32>
    %6596 = "dtu_hlir.dot_general"(%6504, %6595) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5289-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<40x64x77xf32>) -> tensor<40x256x77xf32>
    %6597 = "dtu_hlir.broadcast_in_dim"(%768) {node_name = "Mul_5290-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %6598 = "dtu_hlir.mul"(%6596, %6597) {node_name = "Mul_5290-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6599 = "dtu_hlir.broadcast_in_dim"(%767) {node_name = "Mul_5291-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<40x256x77xf32>
    %6600 = "dtu_hlir.mul"(%6594, %6599) {node_name = "Mul_5291-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6601 = "dtu_hlir.add"(%6598, %6600) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5292-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6602 = "dtu_hlir.softmax"(%6601) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5293-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6603 = "dtu_hlir.convert"(%6602) {node_name = "Cast_5294-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>) -> tensor<40x256x77xf32>
    %6604 = "dtu_hlir.dot_general"(%6603, %6576) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5295-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<40x256x77xf32>, tensor<40x77x64xf32>) -> tensor<40x256x64xf32>
    %6605 = "dtu_hlir.shape"(%6604) {end = 2147483647 : i64, node_name = "Shape_5296-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6606 = dtu_hlir.constant  {node_name = "Constant_5297-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6607 = "dtu_hlir.gather"(%6605, %6606) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5298-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6608 = "dtu_hlir.shape"(%6604) {end = 2147483647 : i64, node_name = "Shape_5299-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6609 = dtu_hlir.constant  {node_name = "Constant_5300-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6610 = "dtu_hlir.gather"(%6608, %6609) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5301-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6611 = "dtu_hlir.shape"(%6604) {end = 2147483647 : i64, node_name = "Shape_5302-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<40x256x64xf32>) -> tensor<3xi64>
    %6612 = dtu_hlir.constant  {node_name = "Constant_5303-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6613 = "dtu_hlir.gather"(%6611, %6612) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5304-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6614 = dtu_hlir.constant  {node_name = "Constant_5305-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6615 = "dtu_hlir.div"(%6607, %6614) {node_name = "Div_5306-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6616 = "dtu_hlir.convert"(%6615) {node_name = "Cast_5307-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6617 = "dtu_hlir.convert"(%6616) {node_name = "Cast_5308-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6618 = dtu_hlir.constant  {node_name = "Constant_5309-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6619 = "dtu_hlir.unsqueeze"(%6617, %6618) {node_name = "Unsqueeze_5310-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6620 = dtu_hlir.constant  {node_name = "Constant_5311-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6621 = "dtu_hlir.unsqueeze"(%6610, %6620) {node_name = "Unsqueeze_5312-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6622 = dtu_hlir.constant  {node_name = "Constant_5313-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6623 = "dtu_hlir.unsqueeze"(%6613, %6622) {node_name = "Unsqueeze_5314-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6624 = "dtu_hlir.concatenate"(%6619, %766, %6621, %6623) {dimension = 0 : i64, node_name = "Concat_5315-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6625 = "dtu_hlir.dynamic_reshape"(%6604, %6624) {allowzero = 0 : i64, node_name = "Reshape_5316-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<40x256x64xf32>, tensor<4xi64>) -> tensor<2x20x256x64xf32>
    %6626 = "dtu_hlir.transpose"(%6625) {node_name = "Transpose_5317-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x20x256x64xf32>) -> tensor<2x256x20x64xf32>
    %6627 = dtu_hlir.constant  {node_name = "Constant_5318-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6628 = "dtu_hlir.div"(%6607, %6627) {node_name = "Div_5319-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6629 = "dtu_hlir.convert"(%6628) {node_name = "Cast_5320-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6630 = "dtu_hlir.convert"(%6629) {node_name = "Cast_5321-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6631 = dtu_hlir.constant  {node_name = "Constant_5322-0", node_type = "Constant"} dense<20> : tensor<i64>
    %6632 = "dtu_hlir.mul"(%6613, %6631) {node_name = "Mul_5323-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6633 = dtu_hlir.constant  {node_name = "Constant_5324-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6634 = "dtu_hlir.unsqueeze"(%6630, %6633) {node_name = "Unsqueeze_5325-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6635 = dtu_hlir.constant  {node_name = "Constant_5326-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6636 = "dtu_hlir.unsqueeze"(%6610, %6635) {node_name = "Unsqueeze_5327-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6637 = dtu_hlir.constant  {node_name = "Constant_5328-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6638 = "dtu_hlir.unsqueeze"(%6632, %6637) {node_name = "Unsqueeze_5329-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6639 = "dtu_hlir.concatenate"(%6634, %6636, %6638) {dimension = 0 : i64, node_name = "Concat_5330-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6640 = "dtu_hlir.dynamic_reshape"(%6626, %6639) {allowzero = 0 : i64, node_name = "Reshape_5331-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x20x64xf32>, tensor<3xi64>) -> tensor<2x256x1280xf32>
    %6641 = "dtu_hlir.dot_general"(%6640, %578) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5332-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6642 = "dtu_hlir.add"(%190, %6641) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5333-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6643 = "dtu_hlir.add"(%6642, %6438) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5334-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6644 = dtu_hlir.constant  {node_name = "ReduceMean_5335-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6645 = "dtu_hlir.reshape"(%6644) {node_name = "ReduceMean_5335-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6646 = "dtu_hlir.reduce"(%6643, %6645) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5335-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5335-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6647 = dtu_hlir.constant  {node_name = "ReduceMean_5335-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6648 = "dtu_hlir.unsqueeze"(%6646, %6647) {node_name = "ReduceMean_5335-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6649 = dtu_hlir.constant  {node_name = "ReduceMean_5335-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6650 = "dtu_hlir.broadcast_in_dim"(%6649) {node_name = "ReduceMean_5335-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6651 = "dtu_hlir.div"(%6648, %6650) {node_name = "ReduceMean_5335-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6652 = "dtu_hlir.sub"(%6643, %6651) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_5336-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6653 = dtu_hlir.constant  {node_name = "Constant_5337-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %6654 = "dtu_hlir.broadcast_in_dim"(%6653) {node_name = "Pow_5338-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1280xf32>
    %6655 = "dtu_hlir.pow"(%6652, %6654) {node_name = "Pow_5338-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6656 = dtu_hlir.constant  {node_name = "ReduceMean_5339-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6657 = "dtu_hlir.reshape"(%6656) {node_name = "ReduceMean_5339-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6658 = "dtu_hlir.reduce"(%6655, %6657) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5339-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5339-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<f32>) -> tensor<2x256xf32>
    %6659 = dtu_hlir.constant  {node_name = "ReduceMean_5339-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6660 = "dtu_hlir.unsqueeze"(%6658, %6659) {node_name = "ReduceMean_5339-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256xf32>, tensor<1xi64>) -> tensor<2x256x1xf32>
    %6661 = dtu_hlir.constant  {node_name = "ReduceMean_5339-Const-6", node_type = "ReduceMean"} dense<1.280000e+03> : tensor<1xf32>
    %6662 = "dtu_hlir.broadcast_in_dim"(%6661) {node_name = "ReduceMean_5339-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x256x1xf32>
    %6663 = "dtu_hlir.div"(%6660, %6662) {node_name = "ReduceMean_5339-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6664 = dtu_hlir.constant  {node_name = "Constant_5340-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %6665 = "dtu_hlir.broadcast_in_dim"(%6664) {node_name = "Add_5341-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x1xf32>
    %6666 = "dtu_hlir.add"(%6663, %6665) {node_name = "Add_5341-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6667 = "dtu_hlir.sqrt"(%6666) {node_name = "Sqrt_5342-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x256x1xf32>) -> tensor<2x256x1xf32>
    %6668 = "dtu_hlir.div"(%6652, %6667) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_5343-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1xf32>) -> tensor<2x256x1280xf32>
    %6669 = "dtu_hlir.mul"(%6668, %195) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_5344-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6670 = "dtu_hlir.add"(%6669, %196) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5345-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf32>
    %6671 = "dtu_hlir.dot_general"(%6670, %579) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5346-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x10240xf32>) -> tensor<2x256x10240xf32>
    %6672 = "dtu_hlir.add"(%188, %6671) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5347-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10240xf32>, tensor<2x256x10240xf32>) -> tensor<2x256x10240xf32>
    %6673 = "dtu_hlir.shape"(%6672) {end = 2147483647 : i64, node_name = "Shape_5348-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>) -> tensor<3xi64>
    %6674 = dtu_hlir.constant  {node_name = "Constant_5349-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %6675 = "dtu_hlir.gather"(%6673, %6674) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5350-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6676 = dtu_hlir.constant  {node_name = "Constant_5351-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6677 = dtu_hlir.constant  {node_name = "Constant_5352-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %6678 = "dtu_hlir.add"(%6675, %6677) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_5353-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6679 = dtu_hlir.constant  {node_name = "Constant_5354-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %6680 = "dtu_hlir.div"(%6678, %6679) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_5355-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6681 = dtu_hlir.constant  {node_name = "Constant_5356-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %6682 = "dtu_hlir.mul"(%6680, %6681) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_5357-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6683 = dtu_hlir.constant  {node_name = "Slice_5358-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %6684 = "dtu_hlir.real_dynamic_slice"(%6672, %6676, %6682, %6683, %6674) {node_name = "Slice_5358-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %6685 = dtu_hlir.constant  {node_name = "Constant_5359-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %6686 = "dtu_hlir.mul"(%6680, %6685) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_5360-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %6687 = "dtu_hlir.shape"(%6682) {end = 2147483647 : i64, node_name = "Slice_5361-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %6688 = dtu_hlir.constant  {node_name = "Slice_5361-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %6689 = "dtu_hlir.dynamic_broadcast_in_dim"(%6688, %6687) {node_name = "Slice_5361-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6690 = "dtu_hlir.real_dynamic_slice"(%6672, %6682, %6686, %6689, %6674) {node_name = "Slice_5361-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x256x10240xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x256x5120xf32>
    %6691 = dtu_hlir.constant  {node_name = "Constant_5362-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %6692 = "dtu_hlir.broadcast_in_dim"(%6691) {node_name = "Div_5363-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %6693 = "dtu_hlir.div"(%6690, %6692) {node_name = "Div_5363-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6694 = "dtu_hlir.erf"(%6693) {node_name = "Erf_5364-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6695 = dtu_hlir.constant  {node_name = "Constant_5365-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %6696 = "dtu_hlir.broadcast_in_dim"(%6695) {node_name = "Add_5366-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %6697 = "dtu_hlir.add"(%6694, %6696) {node_name = "Add_5366-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6698 = "dtu_hlir.mul"(%6690, %6697) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_5367-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6699 = dtu_hlir.constant  {node_name = "Constant_5368-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %6700 = "dtu_hlir.broadcast_in_dim"(%6699) {node_name = "Mul_5369-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x256x5120xf32>
    %6701 = "dtu_hlir.mul"(%6698, %6700) {node_name = "Mul_5369-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6702 = "dtu_hlir.mul"(%6684, %6701) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_5370-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<2x256x5120xf32>) -> tensor<2x256x5120xf32>
    %6703 = "dtu_hlir.dot_general"(%6702, %580) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5371-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x5120xf32>, tensor<5120x1280xf32>) -> tensor<2x256x1280xf32>
    %6704 = "dtu_hlir.add"(%189, %6703) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5372-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6705 = "dtu_hlir.add"(%6704, %6643) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5373-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6706 = "dtu_hlir.dot_general"(%6705, %581) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5374-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<1280x1280xf32>) -> tensor<2x256x1280xf32>
    %6707 = "dtu_hlir.add"(%197, %6706) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5375-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1280xf32>, tensor<2x256x1280xf32>) -> tensor<2x256x1280xf32>
    %6708 = dtu_hlir.constant  {node_name = "Constant_5376-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6709 = "dtu_hlir.unsqueeze"(%6203, %6708) {node_name = "Unsqueeze_5377-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6710 = dtu_hlir.constant  {node_name = "Constant_5378-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6711 = "dtu_hlir.unsqueeze"(%6206, %6710) {node_name = "Unsqueeze_5379-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6712 = dtu_hlir.constant  {node_name = "Constant_5380-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6713 = "dtu_hlir.unsqueeze"(%6209, %6712) {node_name = "Unsqueeze_5381-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6714 = dtu_hlir.constant  {node_name = "Constant_5382-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6715 = "dtu_hlir.unsqueeze"(%6221, %6714) {node_name = "Unsqueeze_5383-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6716 = "dtu_hlir.concatenate"(%6709, %6711, %6713, %6715) {dimension = 0 : i64, node_name = "Concat_5384-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6717 = "dtu_hlir.dynamic_reshape"(%6707, %6716) {allowzero = 0 : i64, node_name = "Reshape_5385-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x256x1280xf32>, tensor<4xi64>) -> tensor<2x16x16x1280xf32>
    %6718 = "dtu_hlir.transpose"(%6717) {node_name = "Transpose_5386-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x16x16x1280xf32>) -> tensor<2x1280x16x16xf32>
    %6719 = "dtu_hlir.add"(%6718, %6200) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_5387-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<2x1280x16x16xf32>) -> tensor<2x1280x16x16xf32>
    %6720 = dtu_hlir.constant  {node_name = "Resize_5388-Const-2", node_type = "Resize"} dense<[]> : tensor<0xi64>
    %6721 = dtu_hlir.constant  {node_name = "Resize_5388-Const-3", node_type = "Resize"} dense<[0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00]> : tensor<8xf32>
    %6722 = "dtu_hlir.resize"(%6719, %6721, %765, %6720) {coordinate_transformation_mode = 1 : i64, cubic_coeff_a = -7.500000e-01 : f32, exclude_outside = false, extrapolation_value = 0.000000e+00 : f32, mode = 0 : i64, nearest_mode = 3 : i64, node_name = "Resize_5388-4", node_type = "Resize", resize_dimensions = dense<[-2, -1]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<2x1280x16x16xf32>, tensor<8xf32>, tensor<4xf32>, tensor<0xi64>) -> tensor<2x1280x32x32xf32>
    %6723 = "dtu_hlir.conv_bias"(%6722, %222, %223) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5389-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x32x32xf32>, tensor<1280x1280x3x3xf32>, tensor<1280xf32>) -> tensor<2x1280x32x32xf32>
    %6724 = "dtu_hlir.concatenate"(%6723, %3142) {dimension = 1 : i64, node_name = "Concat_5390-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x1280x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x1920x32x32xf32>
    %6725 = dtu_hlir.constant  {node_name = "Constant_5391-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %6726 = "dtu_hlir.dynamic_reshape"(%6724, %6725) {allowzero = 0 : i64, node_name = "Reshape_5392-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1920x32x32xf32>, tensor<3xi64>) -> tensor<2x32x61440xf32>
    %6727 = dtu_hlir.constant  {node_name = "Constant_5393-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %6728 = dtu_hlir.constant  {node_name = "Constant_5394-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %6729 = "dtu_hlir.instance_norm"(%6726, %6727, %6728) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_5395-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x61440xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x61440xf32>
    %6730 = "dtu_hlir.shape"(%6724) {end = 2147483647 : i64, node_name = "Shape_5396-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1920x32x32xf32>) -> tensor<4xi64>
    %6731 = "dtu_hlir.dynamic_reshape"(%6729, %6730) {allowzero = 0 : i64, node_name = "Reshape_5397-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x61440xf32>, tensor<4xi64>) -> tensor<2x1920x32x32xf32>
    %6732 = "dtu_hlir.mul"(%6731, %582) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_5398-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1920x32x32xf32>, tensor<1920x1x1xf32>) -> tensor<2x1920x32x32xf32>
    %6733 = "dtu_hlir.add"(%6732, %583) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_5399-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1920x32x32xf32>, tensor<1920x1x1xf32>) -> tensor<2x1920x32x32xf32>
    %6734 = "dtu_hlir.sigmoid"(%6733) {node_name = "Sigmoid_5400-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1920x32x32xf32>) -> tensor<2x1920x32x32xf32>
    %6735 = "dtu_hlir.mul"(%6733, %6734) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_5401-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1920x32x32xf32>, tensor<2x1920x32x32xf32>) -> tensor<2x1920x32x32xf32>
    %6736 = "dtu_hlir.conv_bias"(%6735, %260, %261) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5402-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1920x32x32xf32>, tensor<640x1920x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %6737 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_5403-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %6738 = "dtu_hlir.mul"(%915, %6737) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5404-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %6739 = "dtu_hlir.transpose"(%262) {node_name = "Gemm_5405-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<640x1280xf32>) -> tensor<1280x640xf32>
    %6740 = "dtu_hlir.gemm"(%6738, %6739, %263) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_5405-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x640xf32>, tensor<640xf32>) -> tensor<2x640xf32>
    %6741 = dtu_hlir.constant  {node_name = "Constant_5406-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %6742 = "dtu_hlir.unsqueeze"(%6740, %6741) {node_name = "Unsqueeze_5407-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640xf32>, tensor<1xi64>) -> tensor<2x640x1xf32>
    %6743 = dtu_hlir.constant  {node_name = "Constant_5408-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %6744 = "dtu_hlir.unsqueeze"(%6742, %6743) {node_name = "Unsqueeze_5409-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640x1xf32>, tensor<1xi64>) -> tensor<2x640x1x1xf32>
    %6745 = "dtu_hlir.add"(%6736, %6744) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_5410-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %6746 = dtu_hlir.constant  {node_name = "Constant_5411-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %6747 = "dtu_hlir.dynamic_reshape"(%6745, %6746) {allowzero = 0 : i64, node_name = "Reshape_5412-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %6748 = dtu_hlir.constant  {node_name = "Constant_5413-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %6749 = dtu_hlir.constant  {node_name = "Constant_5414-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %6750 = "dtu_hlir.instance_norm"(%6747, %6748, %6749) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_5415-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %6751 = "dtu_hlir.shape"(%6745) {end = 2147483647 : i64, node_name = "Shape_5416-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %6752 = "dtu_hlir.dynamic_reshape"(%6750, %6751) {allowzero = 0 : i64, node_name = "Reshape_5417-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %6753 = "dtu_hlir.mul"(%6752, %584) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_5418-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %6754 = "dtu_hlir.add"(%6753, %585) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_5419-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %6755 = "dtu_hlir.sigmoid"(%6754) {node_name = "Sigmoid_5420-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %6756 = "dtu_hlir.mul"(%6754, %6755) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_5421-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %6757 = "dtu_hlir.conv_bias"(%6756, %264, %265) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5422-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x32x32xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %6758 = "dtu_hlir.conv_bias"(%6724, %266, %267) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5423-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1920x32x32xf32>, tensor<640x1920x1x1xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %6759 = "dtu_hlir.add"(%6758, %6757) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_5424-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %6760 = dtu_hlir.constant  {node_name = "Constant_5425-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %6761 = "dtu_hlir.broadcast_in_dim"(%6760) {node_name = "Div_5426-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x640x32x32xf32>
    %6762 = "dtu_hlir.div"(%6759, %6761) {node_name = "Div_5426-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %6763 = "dtu_hlir.shape"(%6762) {end = 2147483647 : i64, node_name = "Shape_5427-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %6764 = dtu_hlir.constant  {node_name = "Constant_5428-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6765 = "dtu_hlir.gather"(%6763, %6764) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5429-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6766 = "dtu_hlir.shape"(%6762) {end = 2147483647 : i64, node_name = "Shape_5430-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %6767 = dtu_hlir.constant  {node_name = "Constant_5431-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6768 = "dtu_hlir.gather"(%6766, %6767) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5432-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6769 = "dtu_hlir.shape"(%6762) {end = 2147483647 : i64, node_name = "Shape_5433-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %6770 = dtu_hlir.constant  {node_name = "Constant_5434-0", node_type = "Constant"} dense<3> : tensor<i64>
    %6771 = "dtu_hlir.gather"(%6769, %6770) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5435-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6772 = dtu_hlir.constant  {node_name = "Constant_5436-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %6773 = "dtu_hlir.dynamic_reshape"(%6762, %6772) {allowzero = 0 : i64, node_name = "Reshape_5437-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %6774 = dtu_hlir.constant  {node_name = "Constant_5438-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %6775 = dtu_hlir.constant  {node_name = "Constant_5439-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %6776 = "dtu_hlir.instance_norm"(%6773, %6774, %6775) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_5440-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %6777 = "dtu_hlir.shape"(%6762) {end = 2147483647 : i64, node_name = "Shape_5441-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %6778 = "dtu_hlir.dynamic_reshape"(%6776, %6777) {allowzero = 0 : i64, node_name = "Reshape_5442-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %6779 = "dtu_hlir.mul"(%6778, %586) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_5443-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %6780 = "dtu_hlir.add"(%6779, %587) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_5444-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %6781 = "dtu_hlir.shape"(%6780) {end = 2147483647 : i64, node_name = "Shape_5445-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %6782 = dtu_hlir.constant  {node_name = "Constant_5446-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6783 = "dtu_hlir.gather"(%6781, %6782) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5447-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %6784 = "dtu_hlir.transpose"(%6780) {node_name = "Transpose_5448-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x32x32x640xf32>
    %6785 = "dtu_hlir.mul"(%6768, %6771) {node_name = "Mul_5449-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6786 = dtu_hlir.constant  {node_name = "Constant_5450-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6787 = "dtu_hlir.unsqueeze"(%6765, %6786) {node_name = "Unsqueeze_5451-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6788 = dtu_hlir.constant  {node_name = "Constant_5452-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6789 = "dtu_hlir.unsqueeze"(%6785, %6788) {node_name = "Unsqueeze_5453-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6790 = dtu_hlir.constant  {node_name = "Constant_5454-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6791 = "dtu_hlir.unsqueeze"(%6783, %6790) {node_name = "Unsqueeze_5455-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6792 = "dtu_hlir.concatenate"(%6787, %6789, %6791) {dimension = 0 : i64, node_name = "Concat_5456-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6793 = "dtu_hlir.dynamic_reshape"(%6784, %6792) {allowzero = 0 : i64, node_name = "Reshape_5457-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %6794 = "dtu_hlir.dot_general"(%6793, %588) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5458-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %6795 = "dtu_hlir.add"(%224, %6794) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5459-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %6796 = dtu_hlir.constant  {node_name = "ReduceMean_5460-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6797 = "dtu_hlir.reshape"(%6796) {node_name = "ReduceMean_5460-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6798 = "dtu_hlir.reduce"(%6795, %6797) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5460-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5460-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %6799 = dtu_hlir.constant  {node_name = "ReduceMean_5460-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6800 = "dtu_hlir.unsqueeze"(%6798, %6799) {node_name = "ReduceMean_5460-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %6801 = dtu_hlir.constant  {node_name = "ReduceMean_5460-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %6802 = "dtu_hlir.broadcast_in_dim"(%6801) {node_name = "ReduceMean_5460-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %6803 = "dtu_hlir.div"(%6800, %6802) {node_name = "ReduceMean_5460-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %6804 = "dtu_hlir.sub"(%6795, %6803) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_5461-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %6805 = dtu_hlir.constant  {node_name = "Constant_5462-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %6806 = "dtu_hlir.broadcast_in_dim"(%6805) {node_name = "Pow_5463-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %6807 = "dtu_hlir.pow"(%6804, %6806) {node_name = "Pow_5463-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %6808 = dtu_hlir.constant  {node_name = "ReduceMean_5464-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %6809 = "dtu_hlir.reshape"(%6808) {node_name = "ReduceMean_5464-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %6810 = "dtu_hlir.reduce"(%6807, %6809) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5464-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5464-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %6811 = dtu_hlir.constant  {node_name = "ReduceMean_5464-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %6812 = "dtu_hlir.unsqueeze"(%6810, %6811) {node_name = "ReduceMean_5464-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %6813 = dtu_hlir.constant  {node_name = "ReduceMean_5464-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %6814 = "dtu_hlir.broadcast_in_dim"(%6813) {node_name = "ReduceMean_5464-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %6815 = "dtu_hlir.div"(%6812, %6814) {node_name = "ReduceMean_5464-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %6816 = dtu_hlir.constant  {node_name = "Constant_5465-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %6817 = "dtu_hlir.broadcast_in_dim"(%6816) {node_name = "Add_5466-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %6818 = "dtu_hlir.add"(%6815, %6817) {node_name = "Add_5466-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %6819 = "dtu_hlir.sqrt"(%6818) {node_name = "Sqrt_5467-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %6820 = "dtu_hlir.div"(%6804, %6819) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_5468-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %6821 = "dtu_hlir.mul"(%6820, %229) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_5469-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %6822 = "dtu_hlir.add"(%6821, %230) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5470-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %6823 = "dtu_hlir.dot_general"(%6822, %589) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5471-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %6824 = "dtu_hlir.dot_general"(%6822, %590) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5472-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %6825 = "dtu_hlir.dot_general"(%6822, %591) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5473-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %6826 = "dtu_hlir.shape"(%6823) {end = 2147483647 : i64, node_name = "Shape_5474-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6827 = dtu_hlir.constant  {node_name = "Constant_5475-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6828 = "dtu_hlir.gather"(%6826, %6827) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5476-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6829 = "dtu_hlir.shape"(%6823) {end = 2147483647 : i64, node_name = "Shape_5477-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6830 = dtu_hlir.constant  {node_name = "Constant_5478-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6831 = "dtu_hlir.gather"(%6829, %6830) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5479-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6832 = "dtu_hlir.shape"(%6823) {end = 2147483647 : i64, node_name = "Shape_5480-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6833 = dtu_hlir.constant  {node_name = "Constant_5481-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6834 = "dtu_hlir.gather"(%6832, %6833) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5482-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6835 = dtu_hlir.constant  {node_name = "Constant_5483-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6836 = "dtu_hlir.div"(%6834, %6835) {node_name = "Div_5484-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6837 = "dtu_hlir.convert"(%6836) {node_name = "Cast_5485-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6838 = "dtu_hlir.convert"(%6837) {node_name = "Cast_5486-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6839 = dtu_hlir.constant  {node_name = "Constant_5487-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6840 = "dtu_hlir.unsqueeze"(%6828, %6839) {node_name = "Unsqueeze_5488-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6841 = dtu_hlir.constant  {node_name = "Constant_5489-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6842 = "dtu_hlir.unsqueeze"(%6831, %6841) {node_name = "Unsqueeze_5490-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6843 = dtu_hlir.constant  {node_name = "Constant_5491-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6844 = "dtu_hlir.unsqueeze"(%6838, %6843) {node_name = "Unsqueeze_5492-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6845 = "dtu_hlir.concatenate"(%6840, %6842, %764, %6844) {dimension = 0 : i64, node_name = "Concat_5493-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6846 = "dtu_hlir.dynamic_reshape"(%6823, %6845) {allowzero = 0 : i64, node_name = "Reshape_5494-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %6847 = "dtu_hlir.transpose"(%6846) {node_name = "Transpose_5495-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %6848 = dtu_hlir.constant  {node_name = "Constant_5496-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6849 = "dtu_hlir.mul"(%6828, %6848) {node_name = "Mul_5497-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6850 = dtu_hlir.constant  {node_name = "Constant_5498-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6851 = "dtu_hlir.div"(%6834, %6850) {node_name = "Div_5499-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6852 = "dtu_hlir.convert"(%6851) {node_name = "Cast_5500-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6853 = "dtu_hlir.convert"(%6852) {node_name = "Cast_5501-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6854 = dtu_hlir.constant  {node_name = "Constant_5502-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6855 = "dtu_hlir.unsqueeze"(%6849, %6854) {node_name = "Unsqueeze_5503-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6856 = dtu_hlir.constant  {node_name = "Constant_5504-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6857 = "dtu_hlir.unsqueeze"(%6831, %6856) {node_name = "Unsqueeze_5505-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6858 = dtu_hlir.constant  {node_name = "Constant_5506-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6859 = "dtu_hlir.unsqueeze"(%6853, %6858) {node_name = "Unsqueeze_5507-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6860 = "dtu_hlir.concatenate"(%6855, %6857, %6859) {dimension = 0 : i64, node_name = "Concat_5508-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6861 = "dtu_hlir.dynamic_reshape"(%6847, %6860) {allowzero = 0 : i64, node_name = "Reshape_5509-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %6862 = "dtu_hlir.shape"(%6824) {end = 2147483647 : i64, node_name = "Shape_5510-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6863 = dtu_hlir.constant  {node_name = "Constant_5511-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6864 = "dtu_hlir.gather"(%6862, %6863) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5512-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6865 = "dtu_hlir.shape"(%6824) {end = 2147483647 : i64, node_name = "Shape_5513-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6866 = dtu_hlir.constant  {node_name = "Constant_5514-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6867 = "dtu_hlir.gather"(%6865, %6866) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5515-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6868 = "dtu_hlir.shape"(%6824) {end = 2147483647 : i64, node_name = "Shape_5516-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6869 = dtu_hlir.constant  {node_name = "Constant_5517-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6870 = "dtu_hlir.gather"(%6868, %6869) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5518-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6871 = dtu_hlir.constant  {node_name = "Constant_5519-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6872 = "dtu_hlir.div"(%6870, %6871) {node_name = "Div_5520-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6873 = "dtu_hlir.convert"(%6872) {node_name = "Cast_5521-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6874 = "dtu_hlir.convert"(%6873) {node_name = "Cast_5522-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6875 = dtu_hlir.constant  {node_name = "Constant_5523-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6876 = "dtu_hlir.unsqueeze"(%6864, %6875) {node_name = "Unsqueeze_5524-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6877 = dtu_hlir.constant  {node_name = "Constant_5525-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6878 = "dtu_hlir.unsqueeze"(%6867, %6877) {node_name = "Unsqueeze_5526-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6879 = dtu_hlir.constant  {node_name = "Constant_5527-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6880 = "dtu_hlir.unsqueeze"(%6874, %6879) {node_name = "Unsqueeze_5528-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6881 = "dtu_hlir.concatenate"(%6876, %6878, %763, %6880) {dimension = 0 : i64, node_name = "Concat_5529-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6882 = "dtu_hlir.dynamic_reshape"(%6824, %6881) {allowzero = 0 : i64, node_name = "Reshape_5530-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %6883 = "dtu_hlir.transpose"(%6882) {node_name = "Transpose_5531-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %6884 = dtu_hlir.constant  {node_name = "Constant_5532-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6885 = "dtu_hlir.mul"(%6864, %6884) {node_name = "Mul_5533-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6886 = dtu_hlir.constant  {node_name = "Constant_5534-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6887 = "dtu_hlir.div"(%6870, %6886) {node_name = "Div_5535-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6888 = "dtu_hlir.convert"(%6887) {node_name = "Cast_5536-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6889 = "dtu_hlir.convert"(%6888) {node_name = "Cast_5537-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6890 = dtu_hlir.constant  {node_name = "Constant_5538-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6891 = "dtu_hlir.unsqueeze"(%6885, %6890) {node_name = "Unsqueeze_5539-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6892 = dtu_hlir.constant  {node_name = "Constant_5540-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6893 = "dtu_hlir.unsqueeze"(%6867, %6892) {node_name = "Unsqueeze_5541-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6894 = dtu_hlir.constant  {node_name = "Constant_5542-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6895 = "dtu_hlir.unsqueeze"(%6889, %6894) {node_name = "Unsqueeze_5543-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6896 = "dtu_hlir.concatenate"(%6891, %6893, %6895) {dimension = 0 : i64, node_name = "Concat_5544-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6897 = "dtu_hlir.dynamic_reshape"(%6883, %6896) {allowzero = 0 : i64, node_name = "Reshape_5545-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %6898 = "dtu_hlir.shape"(%6825) {end = 2147483647 : i64, node_name = "Shape_5546-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6899 = dtu_hlir.constant  {node_name = "Constant_5547-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6900 = "dtu_hlir.gather"(%6898, %6899) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5548-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6901 = "dtu_hlir.shape"(%6825) {end = 2147483647 : i64, node_name = "Shape_5549-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6902 = dtu_hlir.constant  {node_name = "Constant_5550-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6903 = "dtu_hlir.gather"(%6901, %6902) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5551-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6904 = "dtu_hlir.shape"(%6825) {end = 2147483647 : i64, node_name = "Shape_5552-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %6905 = dtu_hlir.constant  {node_name = "Constant_5553-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6906 = "dtu_hlir.gather"(%6904, %6905) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5554-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6907 = dtu_hlir.constant  {node_name = "Constant_5555-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6908 = "dtu_hlir.div"(%6906, %6907) {node_name = "Div_5556-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6909 = "dtu_hlir.convert"(%6908) {node_name = "Cast_5557-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6910 = "dtu_hlir.convert"(%6909) {node_name = "Cast_5558-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6911 = dtu_hlir.constant  {node_name = "Constant_5559-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6912 = "dtu_hlir.unsqueeze"(%6900, %6911) {node_name = "Unsqueeze_5560-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6913 = dtu_hlir.constant  {node_name = "Constant_5561-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6914 = "dtu_hlir.unsqueeze"(%6903, %6913) {node_name = "Unsqueeze_5562-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6915 = dtu_hlir.constant  {node_name = "Constant_5563-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6916 = "dtu_hlir.unsqueeze"(%6910, %6915) {node_name = "Unsqueeze_5564-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6917 = "dtu_hlir.concatenate"(%6912, %6914, %762, %6916) {dimension = 0 : i64, node_name = "Concat_5565-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6918 = "dtu_hlir.dynamic_reshape"(%6825, %6917) {allowzero = 0 : i64, node_name = "Reshape_5566-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %6919 = "dtu_hlir.transpose"(%6918) {node_name = "Transpose_5567-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %6920 = dtu_hlir.constant  {node_name = "Constant_5568-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6921 = "dtu_hlir.mul"(%6900, %6920) {node_name = "Mul_5569-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6922 = dtu_hlir.constant  {node_name = "Constant_5570-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6923 = "dtu_hlir.div"(%6906, %6922) {node_name = "Div_5571-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6924 = "dtu_hlir.convert"(%6923) {node_name = "Cast_5572-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6925 = "dtu_hlir.convert"(%6924) {node_name = "Cast_5573-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6926 = dtu_hlir.constant  {node_name = "Constant_5574-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6927 = "dtu_hlir.unsqueeze"(%6921, %6926) {node_name = "Unsqueeze_5575-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6928 = dtu_hlir.constant  {node_name = "Constant_5576-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6929 = "dtu_hlir.unsqueeze"(%6903, %6928) {node_name = "Unsqueeze_5577-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6930 = dtu_hlir.constant  {node_name = "Constant_5578-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6931 = "dtu_hlir.unsqueeze"(%6925, %6930) {node_name = "Unsqueeze_5579-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6932 = "dtu_hlir.concatenate"(%6927, %6929, %6931) {dimension = 0 : i64, node_name = "Concat_5580-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6933 = "dtu_hlir.dynamic_reshape"(%6919, %6932) {allowzero = 0 : i64, node_name = "Reshape_5581-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %6934 = "dtu_hlir.shape"(%6861) {end = 2147483647 : i64, node_name = "Shape_5582-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %6935 = dtu_hlir.constant  {node_name = "Constant_5583-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6936 = "dtu_hlir.gather"(%6934, %6935) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5584-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6937 = "dtu_hlir.shape"(%6861) {end = 2147483647 : i64, node_name = "Shape_5585-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %6938 = dtu_hlir.constant  {node_name = "Constant_5586-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6939 = "dtu_hlir.gather"(%6937, %6938) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5587-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6940 = "dtu_hlir.shape"(%6897) {end = 2147483647 : i64, node_name = "Shape_5588-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %6941 = dtu_hlir.constant  {node_name = "Constant_5589-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6942 = "dtu_hlir.gather"(%6940, %6941) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5590-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6943 = dtu_hlir.constant  {node_name = "Constant_5591-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6944 = "dtu_hlir.unsqueeze"(%6936, %6943) {node_name = "Unsqueeze_5592-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6945 = dtu_hlir.constant  {node_name = "Constant_5593-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6946 = "dtu_hlir.unsqueeze"(%6939, %6945) {node_name = "Unsqueeze_5594-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6947 = dtu_hlir.constant  {node_name = "Constant_5595-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6948 = "dtu_hlir.unsqueeze"(%6942, %6947) {node_name = "Unsqueeze_5596-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6949 = "dtu_hlir.concatenate"(%6944, %6946, %6948) {dimension = 0 : i64, node_name = "Concat_5597-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6950 = dtu_hlir.constant  {node_name = "ConstantOfShape_5598-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %6951 = "dtu_hlir.dynamic_broadcast_in_dim"(%6950, %6949) {node_name = "ConstantOfShape_5598-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x1024xf32>
    %6952 = "dtu_hlir.transpose"(%6897) {node_name = "Transpose_5599-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<20x64x1024xf32>
    %6953 = "dtu_hlir.dot_general"(%6861, %6952) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5600-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x1024xf32>) -> tensor<20x1024x1024xf32>
    %6954 = "dtu_hlir.broadcast_in_dim"(%761) {node_name = "Mul_5601-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %6955 = "dtu_hlir.mul"(%6953, %6954) {node_name = "Mul_5601-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %6956 = "dtu_hlir.broadcast_in_dim"(%760) {node_name = "Mul_5602-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %6957 = "dtu_hlir.mul"(%6951, %6956) {node_name = "Mul_5602-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %6958 = "dtu_hlir.add"(%6955, %6957) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5603-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %6959 = "dtu_hlir.softmax"(%6958) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5604-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %6960 = "dtu_hlir.convert"(%6959) {node_name = "Cast_5605-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %6961 = "dtu_hlir.dot_general"(%6960, %6933) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5606-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x64xf32>) -> tensor<20x1024x64xf32>
    %6962 = "dtu_hlir.shape"(%6961) {end = 2147483647 : i64, node_name = "Shape_5607-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %6963 = dtu_hlir.constant  {node_name = "Constant_5608-0", node_type = "Constant"} dense<0> : tensor<i64>
    %6964 = "dtu_hlir.gather"(%6962, %6963) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5609-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6965 = "dtu_hlir.shape"(%6961) {end = 2147483647 : i64, node_name = "Shape_5610-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %6966 = dtu_hlir.constant  {node_name = "Constant_5611-0", node_type = "Constant"} dense<1> : tensor<i64>
    %6967 = "dtu_hlir.gather"(%6965, %6966) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5612-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6968 = "dtu_hlir.shape"(%6961) {end = 2147483647 : i64, node_name = "Shape_5613-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %6969 = dtu_hlir.constant  {node_name = "Constant_5614-0", node_type = "Constant"} dense<2> : tensor<i64>
    %6970 = "dtu_hlir.gather"(%6968, %6969) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5615-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %6971 = dtu_hlir.constant  {node_name = "Constant_5616-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6972 = "dtu_hlir.div"(%6964, %6971) {node_name = "Div_5617-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6973 = "dtu_hlir.convert"(%6972) {node_name = "Cast_5618-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6974 = "dtu_hlir.convert"(%6973) {node_name = "Cast_5619-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6975 = dtu_hlir.constant  {node_name = "Constant_5620-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6976 = "dtu_hlir.unsqueeze"(%6974, %6975) {node_name = "Unsqueeze_5621-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6977 = dtu_hlir.constant  {node_name = "Constant_5622-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6978 = "dtu_hlir.unsqueeze"(%6967, %6977) {node_name = "Unsqueeze_5623-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6979 = dtu_hlir.constant  {node_name = "Constant_5624-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6980 = "dtu_hlir.unsqueeze"(%6970, %6979) {node_name = "Unsqueeze_5625-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6981 = "dtu_hlir.concatenate"(%6976, %759, %6978, %6980) {dimension = 0 : i64, node_name = "Concat_5626-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %6982 = "dtu_hlir.dynamic_reshape"(%6961, %6981) {allowzero = 0 : i64, node_name = "Reshape_5627-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %6983 = "dtu_hlir.transpose"(%6982) {node_name = "Transpose_5628-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %6984 = dtu_hlir.constant  {node_name = "Constant_5629-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6985 = "dtu_hlir.div"(%6964, %6984) {node_name = "Div_5630-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6986 = "dtu_hlir.convert"(%6985) {node_name = "Cast_5631-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6987 = "dtu_hlir.convert"(%6986) {node_name = "Cast_5632-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %6988 = dtu_hlir.constant  {node_name = "Constant_5633-0", node_type = "Constant"} dense<10> : tensor<i64>
    %6989 = "dtu_hlir.mul"(%6970, %6988) {node_name = "Mul_5634-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %6990 = dtu_hlir.constant  {node_name = "Constant_5635-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6991 = "dtu_hlir.unsqueeze"(%6987, %6990) {node_name = "Unsqueeze_5636-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6992 = dtu_hlir.constant  {node_name = "Constant_5637-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6993 = "dtu_hlir.unsqueeze"(%6967, %6992) {node_name = "Unsqueeze_5638-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6994 = dtu_hlir.constant  {node_name = "Constant_5639-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %6995 = "dtu_hlir.unsqueeze"(%6989, %6994) {node_name = "Unsqueeze_5640-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %6996 = "dtu_hlir.concatenate"(%6991, %6993, %6995) {dimension = 0 : i64, node_name = "Concat_5641-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %6997 = "dtu_hlir.dynamic_reshape"(%6983, %6996) {allowzero = 0 : i64, node_name = "Reshape_5642-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %6998 = "dtu_hlir.dot_general"(%6997, %592) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5643-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %6999 = "dtu_hlir.add"(%225, %6998) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5644-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7000 = "dtu_hlir.add"(%6999, %6795) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5645-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7001 = dtu_hlir.constant  {node_name = "ReduceMean_5646-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7002 = "dtu_hlir.reshape"(%7001) {node_name = "ReduceMean_5646-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7003 = "dtu_hlir.reduce"(%7000, %7002) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5646-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5646-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7004 = dtu_hlir.constant  {node_name = "ReduceMean_5646-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7005 = "dtu_hlir.unsqueeze"(%7003, %7004) {node_name = "ReduceMean_5646-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7006 = dtu_hlir.constant  {node_name = "ReduceMean_5646-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7007 = "dtu_hlir.broadcast_in_dim"(%7006) {node_name = "ReduceMean_5646-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7008 = "dtu_hlir.div"(%7005, %7007) {node_name = "ReduceMean_5646-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7009 = "dtu_hlir.sub"(%7000, %7008) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_5647-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7010 = dtu_hlir.constant  {node_name = "Constant_5648-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %7011 = "dtu_hlir.broadcast_in_dim"(%7010) {node_name = "Pow_5649-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %7012 = "dtu_hlir.pow"(%7009, %7011) {node_name = "Pow_5649-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7013 = dtu_hlir.constant  {node_name = "ReduceMean_5650-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7014 = "dtu_hlir.reshape"(%7013) {node_name = "ReduceMean_5650-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7015 = "dtu_hlir.reduce"(%7012, %7014) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5650-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5650-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7016 = dtu_hlir.constant  {node_name = "ReduceMean_5650-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7017 = "dtu_hlir.unsqueeze"(%7015, %7016) {node_name = "ReduceMean_5650-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7018 = dtu_hlir.constant  {node_name = "ReduceMean_5650-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7019 = "dtu_hlir.broadcast_in_dim"(%7018) {node_name = "ReduceMean_5650-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7020 = "dtu_hlir.div"(%7017, %7019) {node_name = "ReduceMean_5650-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7021 = dtu_hlir.constant  {node_name = "Constant_5651-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %7022 = "dtu_hlir.broadcast_in_dim"(%7021) {node_name = "Add_5652-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %7023 = "dtu_hlir.add"(%7020, %7022) {node_name = "Add_5652-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7024 = "dtu_hlir.sqrt"(%7023) {node_name = "Sqrt_5653-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7025 = "dtu_hlir.div"(%7009, %7024) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_5654-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7026 = "dtu_hlir.mul"(%7025, %231) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_5655-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7027 = "dtu_hlir.add"(%7026, %232) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5656-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7028 = "dtu_hlir.dot_general"(%7027, %593) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5657-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7029 = "dtu_hlir.dot_general"(%arg2, %594) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5658-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %7030 = "dtu_hlir.dot_general"(%arg2, %595) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5659-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %7031 = "dtu_hlir.shape"(%7028) {end = 2147483647 : i64, node_name = "Shape_5660-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7032 = dtu_hlir.constant  {node_name = "Constant_5661-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7033 = "dtu_hlir.gather"(%7031, %7032) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5662-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7034 = "dtu_hlir.shape"(%7028) {end = 2147483647 : i64, node_name = "Shape_5663-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7035 = dtu_hlir.constant  {node_name = "Constant_5664-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7036 = "dtu_hlir.gather"(%7034, %7035) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5665-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7037 = "dtu_hlir.shape"(%7028) {end = 2147483647 : i64, node_name = "Shape_5666-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7038 = dtu_hlir.constant  {node_name = "Constant_5667-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7039 = "dtu_hlir.gather"(%7037, %7038) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5668-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7040 = dtu_hlir.constant  {node_name = "Constant_5669-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7041 = "dtu_hlir.div"(%7039, %7040) {node_name = "Div_5670-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7042 = "dtu_hlir.convert"(%7041) {node_name = "Cast_5671-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7043 = "dtu_hlir.convert"(%7042) {node_name = "Cast_5672-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7044 = dtu_hlir.constant  {node_name = "Constant_5673-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7045 = "dtu_hlir.unsqueeze"(%7033, %7044) {node_name = "Unsqueeze_5674-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7046 = dtu_hlir.constant  {node_name = "Constant_5675-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7047 = "dtu_hlir.unsqueeze"(%7036, %7046) {node_name = "Unsqueeze_5676-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7048 = dtu_hlir.constant  {node_name = "Constant_5677-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7049 = "dtu_hlir.unsqueeze"(%7043, %7048) {node_name = "Unsqueeze_5678-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7050 = "dtu_hlir.concatenate"(%7045, %7047, %758, %7049) {dimension = 0 : i64, node_name = "Concat_5679-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7051 = "dtu_hlir.dynamic_reshape"(%7028, %7050) {allowzero = 0 : i64, node_name = "Reshape_5680-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %7052 = "dtu_hlir.transpose"(%7051) {node_name = "Transpose_5681-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %7053 = dtu_hlir.constant  {node_name = "Constant_5682-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7054 = "dtu_hlir.mul"(%7033, %7053) {node_name = "Mul_5683-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7055 = dtu_hlir.constant  {node_name = "Constant_5684-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7056 = "dtu_hlir.div"(%7039, %7055) {node_name = "Div_5685-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7057 = "dtu_hlir.convert"(%7056) {node_name = "Cast_5686-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7058 = "dtu_hlir.convert"(%7057) {node_name = "Cast_5687-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7059 = dtu_hlir.constant  {node_name = "Constant_5688-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7060 = "dtu_hlir.unsqueeze"(%7054, %7059) {node_name = "Unsqueeze_5689-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7061 = dtu_hlir.constant  {node_name = "Constant_5690-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7062 = "dtu_hlir.unsqueeze"(%7036, %7061) {node_name = "Unsqueeze_5691-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7063 = dtu_hlir.constant  {node_name = "Constant_5692-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7064 = "dtu_hlir.unsqueeze"(%7058, %7063) {node_name = "Unsqueeze_5693-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7065 = "dtu_hlir.concatenate"(%7060, %7062, %7064) {dimension = 0 : i64, node_name = "Concat_5694-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7066 = "dtu_hlir.dynamic_reshape"(%7052, %7065) {allowzero = 0 : i64, node_name = "Reshape_5695-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %7067 = "dtu_hlir.shape"(%7029) {end = 2147483647 : i64, node_name = "Shape_5696-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7068 = dtu_hlir.constant  {node_name = "Constant_5697-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7069 = "dtu_hlir.gather"(%7067, %7068) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5698-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7070 = "dtu_hlir.shape"(%7029) {end = 2147483647 : i64, node_name = "Shape_5699-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7071 = dtu_hlir.constant  {node_name = "Constant_5700-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7072 = "dtu_hlir.gather"(%7070, %7071) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5701-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7073 = "dtu_hlir.shape"(%7029) {end = 2147483647 : i64, node_name = "Shape_5702-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7074 = dtu_hlir.constant  {node_name = "Constant_5703-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7075 = "dtu_hlir.gather"(%7073, %7074) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5704-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7076 = dtu_hlir.constant  {node_name = "Constant_5705-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7077 = "dtu_hlir.div"(%7075, %7076) {node_name = "Div_5706-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7078 = "dtu_hlir.convert"(%7077) {node_name = "Cast_5707-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7079 = "dtu_hlir.convert"(%7078) {node_name = "Cast_5708-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7080 = dtu_hlir.constant  {node_name = "Constant_5709-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7081 = "dtu_hlir.unsqueeze"(%7069, %7080) {node_name = "Unsqueeze_5710-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7082 = dtu_hlir.constant  {node_name = "Constant_5711-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7083 = "dtu_hlir.unsqueeze"(%7072, %7082) {node_name = "Unsqueeze_5712-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7084 = dtu_hlir.constant  {node_name = "Constant_5713-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7085 = "dtu_hlir.unsqueeze"(%7079, %7084) {node_name = "Unsqueeze_5714-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7086 = "dtu_hlir.concatenate"(%7081, %7083, %757, %7085) {dimension = 0 : i64, node_name = "Concat_5715-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7087 = "dtu_hlir.dynamic_reshape"(%7029, %7086) {allowzero = 0 : i64, node_name = "Reshape_5716-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %7088 = "dtu_hlir.transpose"(%7087) {node_name = "Transpose_5717-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %7089 = dtu_hlir.constant  {node_name = "Constant_5718-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7090 = "dtu_hlir.mul"(%7069, %7089) {node_name = "Mul_5719-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7091 = dtu_hlir.constant  {node_name = "Constant_5720-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7092 = "dtu_hlir.div"(%7075, %7091) {node_name = "Div_5721-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7093 = "dtu_hlir.convert"(%7092) {node_name = "Cast_5722-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7094 = "dtu_hlir.convert"(%7093) {node_name = "Cast_5723-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7095 = dtu_hlir.constant  {node_name = "Constant_5724-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7096 = "dtu_hlir.unsqueeze"(%7090, %7095) {node_name = "Unsqueeze_5725-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7097 = dtu_hlir.constant  {node_name = "Constant_5726-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7098 = "dtu_hlir.unsqueeze"(%7072, %7097) {node_name = "Unsqueeze_5727-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7099 = dtu_hlir.constant  {node_name = "Constant_5728-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7100 = "dtu_hlir.unsqueeze"(%7094, %7099) {node_name = "Unsqueeze_5729-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7101 = "dtu_hlir.concatenate"(%7096, %7098, %7100) {dimension = 0 : i64, node_name = "Concat_5730-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7102 = "dtu_hlir.dynamic_reshape"(%7088, %7101) {allowzero = 0 : i64, node_name = "Reshape_5731-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %7103 = "dtu_hlir.shape"(%7030) {end = 2147483647 : i64, node_name = "Shape_5732-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7104 = dtu_hlir.constant  {node_name = "Constant_5733-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7105 = "dtu_hlir.gather"(%7103, %7104) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5734-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7106 = "dtu_hlir.shape"(%7030) {end = 2147483647 : i64, node_name = "Shape_5735-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7107 = dtu_hlir.constant  {node_name = "Constant_5736-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7108 = "dtu_hlir.gather"(%7106, %7107) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5737-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7109 = "dtu_hlir.shape"(%7030) {end = 2147483647 : i64, node_name = "Shape_5738-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7110 = dtu_hlir.constant  {node_name = "Constant_5739-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7111 = "dtu_hlir.gather"(%7109, %7110) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5740-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7112 = dtu_hlir.constant  {node_name = "Constant_5741-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7113 = "dtu_hlir.div"(%7111, %7112) {node_name = "Div_5742-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7114 = "dtu_hlir.convert"(%7113) {node_name = "Cast_5743-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7115 = "dtu_hlir.convert"(%7114) {node_name = "Cast_5744-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7116 = dtu_hlir.constant  {node_name = "Constant_5745-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7117 = "dtu_hlir.unsqueeze"(%7105, %7116) {node_name = "Unsqueeze_5746-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7118 = dtu_hlir.constant  {node_name = "Constant_5747-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7119 = "dtu_hlir.unsqueeze"(%7108, %7118) {node_name = "Unsqueeze_5748-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7120 = dtu_hlir.constant  {node_name = "Constant_5749-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7121 = "dtu_hlir.unsqueeze"(%7115, %7120) {node_name = "Unsqueeze_5750-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7122 = "dtu_hlir.concatenate"(%7117, %7119, %756, %7121) {dimension = 0 : i64, node_name = "Concat_5751-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7123 = "dtu_hlir.dynamic_reshape"(%7030, %7122) {allowzero = 0 : i64, node_name = "Reshape_5752-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %7124 = "dtu_hlir.transpose"(%7123) {node_name = "Transpose_5753-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %7125 = dtu_hlir.constant  {node_name = "Constant_5754-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7126 = "dtu_hlir.mul"(%7105, %7125) {node_name = "Mul_5755-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7127 = dtu_hlir.constant  {node_name = "Constant_5756-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7128 = "dtu_hlir.div"(%7111, %7127) {node_name = "Div_5757-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7129 = "dtu_hlir.convert"(%7128) {node_name = "Cast_5758-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7130 = "dtu_hlir.convert"(%7129) {node_name = "Cast_5759-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7131 = dtu_hlir.constant  {node_name = "Constant_5760-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7132 = "dtu_hlir.unsqueeze"(%7126, %7131) {node_name = "Unsqueeze_5761-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7133 = dtu_hlir.constant  {node_name = "Constant_5762-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7134 = "dtu_hlir.unsqueeze"(%7108, %7133) {node_name = "Unsqueeze_5763-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7135 = dtu_hlir.constant  {node_name = "Constant_5764-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7136 = "dtu_hlir.unsqueeze"(%7130, %7135) {node_name = "Unsqueeze_5765-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7137 = "dtu_hlir.concatenate"(%7132, %7134, %7136) {dimension = 0 : i64, node_name = "Concat_5766-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7138 = "dtu_hlir.dynamic_reshape"(%7124, %7137) {allowzero = 0 : i64, node_name = "Reshape_5767-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %7139 = "dtu_hlir.shape"(%7066) {end = 2147483647 : i64, node_name = "Shape_5768-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7140 = dtu_hlir.constant  {node_name = "Constant_5769-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7141 = "dtu_hlir.gather"(%7139, %7140) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5770-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7142 = "dtu_hlir.shape"(%7066) {end = 2147483647 : i64, node_name = "Shape_5771-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7143 = dtu_hlir.constant  {node_name = "Constant_5772-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7144 = "dtu_hlir.gather"(%7142, %7143) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5773-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7145 = "dtu_hlir.shape"(%7102) {end = 2147483647 : i64, node_name = "Shape_5774-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<3xi64>
    %7146 = dtu_hlir.constant  {node_name = "Constant_5775-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7147 = "dtu_hlir.gather"(%7145, %7146) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5776-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7148 = dtu_hlir.constant  {node_name = "Constant_5777-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7149 = "dtu_hlir.unsqueeze"(%7141, %7148) {node_name = "Unsqueeze_5778-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7150 = dtu_hlir.constant  {node_name = "Constant_5779-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7151 = "dtu_hlir.unsqueeze"(%7144, %7150) {node_name = "Unsqueeze_5780-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7152 = dtu_hlir.constant  {node_name = "Constant_5781-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7153 = "dtu_hlir.unsqueeze"(%7147, %7152) {node_name = "Unsqueeze_5782-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7154 = "dtu_hlir.concatenate"(%7149, %7151, %7153) {dimension = 0 : i64, node_name = "Concat_5783-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7155 = dtu_hlir.constant  {node_name = "ConstantOfShape_5784-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %7156 = "dtu_hlir.dynamic_broadcast_in_dim"(%7155, %7154) {node_name = "ConstantOfShape_5784-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x77xf32>
    %7157 = "dtu_hlir.transpose"(%7102) {node_name = "Transpose_5785-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<20x64x77xf32>
    %7158 = "dtu_hlir.dot_general"(%7066, %7157) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5786-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x77xf32>) -> tensor<20x1024x77xf32>
    %7159 = "dtu_hlir.broadcast_in_dim"(%755) {node_name = "Mul_5787-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %7160 = "dtu_hlir.mul"(%7158, %7159) {node_name = "Mul_5787-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7161 = "dtu_hlir.broadcast_in_dim"(%754) {node_name = "Mul_5788-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %7162 = "dtu_hlir.mul"(%7156, %7161) {node_name = "Mul_5788-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7163 = "dtu_hlir.add"(%7160, %7162) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5789-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7164 = "dtu_hlir.softmax"(%7163) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5790-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7165 = "dtu_hlir.convert"(%7164) {node_name = "Cast_5791-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7166 = "dtu_hlir.dot_general"(%7165, %7138) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5792-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x77x64xf32>) -> tensor<20x1024x64xf32>
    %7167 = "dtu_hlir.shape"(%7166) {end = 2147483647 : i64, node_name = "Shape_5793-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7168 = dtu_hlir.constant  {node_name = "Constant_5794-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7169 = "dtu_hlir.gather"(%7167, %7168) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5795-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7170 = "dtu_hlir.shape"(%7166) {end = 2147483647 : i64, node_name = "Shape_5796-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7171 = dtu_hlir.constant  {node_name = "Constant_5797-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7172 = "dtu_hlir.gather"(%7170, %7171) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5798-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7173 = "dtu_hlir.shape"(%7166) {end = 2147483647 : i64, node_name = "Shape_5799-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7174 = dtu_hlir.constant  {node_name = "Constant_5800-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7175 = "dtu_hlir.gather"(%7173, %7174) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5801-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7176 = dtu_hlir.constant  {node_name = "Constant_5802-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7177 = "dtu_hlir.div"(%7169, %7176) {node_name = "Div_5803-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7178 = "dtu_hlir.convert"(%7177) {node_name = "Cast_5804-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7179 = "dtu_hlir.convert"(%7178) {node_name = "Cast_5805-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7180 = dtu_hlir.constant  {node_name = "Constant_5806-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7181 = "dtu_hlir.unsqueeze"(%7179, %7180) {node_name = "Unsqueeze_5807-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7182 = dtu_hlir.constant  {node_name = "Constant_5808-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7183 = "dtu_hlir.unsqueeze"(%7172, %7182) {node_name = "Unsqueeze_5809-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7184 = dtu_hlir.constant  {node_name = "Constant_5810-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7185 = "dtu_hlir.unsqueeze"(%7175, %7184) {node_name = "Unsqueeze_5811-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7186 = "dtu_hlir.concatenate"(%7181, %753, %7183, %7185) {dimension = 0 : i64, node_name = "Concat_5812-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7187 = "dtu_hlir.dynamic_reshape"(%7166, %7186) {allowzero = 0 : i64, node_name = "Reshape_5813-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %7188 = "dtu_hlir.transpose"(%7187) {node_name = "Transpose_5814-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %7189 = dtu_hlir.constant  {node_name = "Constant_5815-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7190 = "dtu_hlir.div"(%7169, %7189) {node_name = "Div_5816-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7191 = "dtu_hlir.convert"(%7190) {node_name = "Cast_5817-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7192 = "dtu_hlir.convert"(%7191) {node_name = "Cast_5818-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7193 = dtu_hlir.constant  {node_name = "Constant_5819-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7194 = "dtu_hlir.mul"(%7175, %7193) {node_name = "Mul_5820-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7195 = dtu_hlir.constant  {node_name = "Constant_5821-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7196 = "dtu_hlir.unsqueeze"(%7192, %7195) {node_name = "Unsqueeze_5822-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7197 = dtu_hlir.constant  {node_name = "Constant_5823-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7198 = "dtu_hlir.unsqueeze"(%7172, %7197) {node_name = "Unsqueeze_5824-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7199 = dtu_hlir.constant  {node_name = "Constant_5825-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7200 = "dtu_hlir.unsqueeze"(%7194, %7199) {node_name = "Unsqueeze_5826-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7201 = "dtu_hlir.concatenate"(%7196, %7198, %7200) {dimension = 0 : i64, node_name = "Concat_5827-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7202 = "dtu_hlir.dynamic_reshape"(%7188, %7201) {allowzero = 0 : i64, node_name = "Reshape_5828-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %7203 = "dtu_hlir.dot_general"(%7202, %596) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5829-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7204 = "dtu_hlir.add"(%228, %7203) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5830-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7205 = "dtu_hlir.add"(%7204, %7000) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5831-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7206 = dtu_hlir.constant  {node_name = "ReduceMean_5832-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7207 = "dtu_hlir.reshape"(%7206) {node_name = "ReduceMean_5832-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7208 = "dtu_hlir.reduce"(%7205, %7207) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5832-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5832-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7209 = dtu_hlir.constant  {node_name = "ReduceMean_5832-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7210 = "dtu_hlir.unsqueeze"(%7208, %7209) {node_name = "ReduceMean_5832-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7211 = dtu_hlir.constant  {node_name = "ReduceMean_5832-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7212 = "dtu_hlir.broadcast_in_dim"(%7211) {node_name = "ReduceMean_5832-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7213 = "dtu_hlir.div"(%7210, %7212) {node_name = "ReduceMean_5832-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7214 = "dtu_hlir.sub"(%7205, %7213) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_5833-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7215 = dtu_hlir.constant  {node_name = "Constant_5834-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %7216 = "dtu_hlir.broadcast_in_dim"(%7215) {node_name = "Pow_5835-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %7217 = "dtu_hlir.pow"(%7214, %7216) {node_name = "Pow_5835-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7218 = dtu_hlir.constant  {node_name = "ReduceMean_5836-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7219 = "dtu_hlir.reshape"(%7218) {node_name = "ReduceMean_5836-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7220 = "dtu_hlir.reduce"(%7217, %7219) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5836-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5836-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7221 = dtu_hlir.constant  {node_name = "ReduceMean_5836-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7222 = "dtu_hlir.unsqueeze"(%7220, %7221) {node_name = "ReduceMean_5836-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7223 = dtu_hlir.constant  {node_name = "ReduceMean_5836-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7224 = "dtu_hlir.broadcast_in_dim"(%7223) {node_name = "ReduceMean_5836-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7225 = "dtu_hlir.div"(%7222, %7224) {node_name = "ReduceMean_5836-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7226 = dtu_hlir.constant  {node_name = "Constant_5837-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %7227 = "dtu_hlir.broadcast_in_dim"(%7226) {node_name = "Add_5838-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %7228 = "dtu_hlir.add"(%7225, %7227) {node_name = "Add_5838-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7229 = "dtu_hlir.sqrt"(%7228) {node_name = "Sqrt_5839-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7230 = "dtu_hlir.div"(%7214, %7229) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_5840-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7231 = "dtu_hlir.mul"(%7230, %233) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_5841-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7232 = "dtu_hlir.add"(%7231, %234) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5842-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7233 = "dtu_hlir.dot_general"(%7232, %597) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5843-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x5120xf32>) -> tensor<2x1024x5120xf32>
    %7234 = "dtu_hlir.add"(%226, %7233) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5844-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<5120xf32>, tensor<2x1024x5120xf32>) -> tensor<2x1024x5120xf32>
    %7235 = "dtu_hlir.shape"(%7234) {end = 2147483647 : i64, node_name = "Shape_5845-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>) -> tensor<3xi64>
    %7236 = dtu_hlir.constant  {node_name = "Constant_5846-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %7237 = "dtu_hlir.gather"(%7235, %7236) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5847-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7238 = dtu_hlir.constant  {node_name = "Constant_5848-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7239 = dtu_hlir.constant  {node_name = "Constant_5849-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %7240 = "dtu_hlir.add"(%7237, %7239) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_5850-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7241 = dtu_hlir.constant  {node_name = "Constant_5851-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %7242 = "dtu_hlir.div"(%7240, %7241) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_5852-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7243 = dtu_hlir.constant  {node_name = "Constant_5853-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %7244 = "dtu_hlir.mul"(%7242, %7243) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_5854-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7245 = dtu_hlir.constant  {node_name = "Slice_5855-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %7246 = "dtu_hlir.real_dynamic_slice"(%7234, %7238, %7244, %7245, %7236) {node_name = "Slice_5855-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %7247 = dtu_hlir.constant  {node_name = "Constant_5856-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %7248 = "dtu_hlir.mul"(%7242, %7247) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_5857-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7249 = "dtu_hlir.shape"(%7244) {end = 2147483647 : i64, node_name = "Slice_5858-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %7250 = dtu_hlir.constant  {node_name = "Slice_5858-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %7251 = "dtu_hlir.dynamic_broadcast_in_dim"(%7250, %7249) {node_name = "Slice_5858-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7252 = "dtu_hlir.real_dynamic_slice"(%7234, %7244, %7248, %7251, %7236) {node_name = "Slice_5858-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %7253 = dtu_hlir.constant  {node_name = "Constant_5859-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %7254 = "dtu_hlir.broadcast_in_dim"(%7253) {node_name = "Div_5860-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %7255 = "dtu_hlir.div"(%7252, %7254) {node_name = "Div_5860-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7256 = "dtu_hlir.erf"(%7255) {node_name = "Erf_5861-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7257 = dtu_hlir.constant  {node_name = "Constant_5862-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %7258 = "dtu_hlir.broadcast_in_dim"(%7257) {node_name = "Add_5863-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %7259 = "dtu_hlir.add"(%7256, %7258) {node_name = "Add_5863-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7260 = "dtu_hlir.mul"(%7252, %7259) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_5864-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7261 = dtu_hlir.constant  {node_name = "Constant_5865-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %7262 = "dtu_hlir.broadcast_in_dim"(%7261) {node_name = "Mul_5866-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %7263 = "dtu_hlir.mul"(%7260, %7262) {node_name = "Mul_5866-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7264 = "dtu_hlir.mul"(%7246, %7263) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_5867-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7265 = "dtu_hlir.dot_general"(%7264, %598) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5868-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2560x640xf32>) -> tensor<2x1024x640xf32>
    %7266 = "dtu_hlir.add"(%227, %7265) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5869-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7267 = "dtu_hlir.add"(%7266, %7205) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_5870-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7268 = "dtu_hlir.dot_general"(%7267, %599) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5871-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7269 = "dtu_hlir.add"(%235, %7268) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5872-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7270 = dtu_hlir.constant  {node_name = "Constant_5873-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7271 = "dtu_hlir.unsqueeze"(%6765, %7270) {node_name = "Unsqueeze_5874-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7272 = dtu_hlir.constant  {node_name = "Constant_5875-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7273 = "dtu_hlir.unsqueeze"(%6768, %7272) {node_name = "Unsqueeze_5876-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7274 = dtu_hlir.constant  {node_name = "Constant_5877-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7275 = "dtu_hlir.unsqueeze"(%6771, %7274) {node_name = "Unsqueeze_5878-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7276 = dtu_hlir.constant  {node_name = "Constant_5879-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7277 = "dtu_hlir.unsqueeze"(%6783, %7276) {node_name = "Unsqueeze_5880-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7278 = "dtu_hlir.concatenate"(%7271, %7273, %7275, %7277) {dimension = 0 : i64, node_name = "Concat_5881-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7279 = "dtu_hlir.dynamic_reshape"(%7269, %7278) {allowzero = 0 : i64, node_name = "Reshape_5882-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x32x32x640xf32>
    %7280 = "dtu_hlir.transpose"(%7279) {node_name = "Transpose_5883-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>) -> tensor<2x640x32x32xf32>
    %7281 = "dtu_hlir.add"(%7280, %6762) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_5884-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7282 = "dtu_hlir.concatenate"(%7281, %2586) {dimension = 1 : i64, node_name = "Concat_5885-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x1280x32x32xf32>
    %7283 = dtu_hlir.constant  {node_name = "Constant_5886-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %7284 = "dtu_hlir.dynamic_reshape"(%7282, %7283) {allowzero = 0 : i64, node_name = "Reshape_5887-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1280x32x32xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %7285 = dtu_hlir.constant  {node_name = "Constant_5888-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %7286 = dtu_hlir.constant  {node_name = "Constant_5889-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %7287 = "dtu_hlir.instance_norm"(%7284, %7285, %7286) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_5890-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %7288 = "dtu_hlir.shape"(%7282) {end = 2147483647 : i64, node_name = "Shape_5891-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1280x32x32xf32>) -> tensor<4xi64>
    %7289 = "dtu_hlir.dynamic_reshape"(%7287, %7288) {allowzero = 0 : i64, node_name = "Reshape_5892-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x1280x32x32xf32>
    %7290 = "dtu_hlir.mul"(%7289, %600) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_5893-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x32x32xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x32x32xf32>
    %7291 = "dtu_hlir.add"(%7290, %601) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_5894-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1280x32x32xf32>, tensor<1280x1x1xf32>) -> tensor<2x1280x32x32xf32>
    %7292 = "dtu_hlir.sigmoid"(%7291) {node_name = "Sigmoid_5895-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280x32x32xf32>) -> tensor<2x1280x32x32xf32>
    %7293 = "dtu_hlir.mul"(%7291, %7292) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_5896-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280x32x32xf32>, tensor<2x1280x32x32xf32>) -> tensor<2x1280x32x32xf32>
    %7294 = "dtu_hlir.conv_bias"(%7293, %268, %269) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5897-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x32x32xf32>, tensor<640x1280x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %7295 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_5898-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %7296 = "dtu_hlir.mul"(%915, %7295) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5899-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %7297 = "dtu_hlir.transpose"(%270) {node_name = "Gemm_5900-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<640x1280xf32>) -> tensor<1280x640xf32>
    %7298 = "dtu_hlir.gemm"(%7296, %7297, %271) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_5900-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x640xf32>, tensor<640xf32>) -> tensor<2x640xf32>
    %7299 = dtu_hlir.constant  {node_name = "Constant_5901-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %7300 = "dtu_hlir.unsqueeze"(%7298, %7299) {node_name = "Unsqueeze_5902-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640xf32>, tensor<1xi64>) -> tensor<2x640x1xf32>
    %7301 = dtu_hlir.constant  {node_name = "Constant_5903-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %7302 = "dtu_hlir.unsqueeze"(%7300, %7301) {node_name = "Unsqueeze_5904-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640x1xf32>, tensor<1xi64>) -> tensor<2x640x1x1xf32>
    %7303 = "dtu_hlir.add"(%7294, %7302) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_5905-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7304 = dtu_hlir.constant  {node_name = "Constant_5906-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %7305 = "dtu_hlir.dynamic_reshape"(%7303, %7304) {allowzero = 0 : i64, node_name = "Reshape_5907-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %7306 = dtu_hlir.constant  {node_name = "Constant_5908-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %7307 = dtu_hlir.constant  {node_name = "Constant_5909-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %7308 = "dtu_hlir.instance_norm"(%7305, %7306, %7307) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_5910-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %7309 = "dtu_hlir.shape"(%7303) {end = 2147483647 : i64, node_name = "Shape_5911-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7310 = "dtu_hlir.dynamic_reshape"(%7308, %7309) {allowzero = 0 : i64, node_name = "Reshape_5912-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %7311 = "dtu_hlir.mul"(%7310, %602) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_5913-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7312 = "dtu_hlir.add"(%7311, %603) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_5914-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7313 = "dtu_hlir.sigmoid"(%7312) {node_name = "Sigmoid_5915-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7314 = "dtu_hlir.mul"(%7312, %7313) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_5916-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7315 = "dtu_hlir.conv_bias"(%7314, %272, %273) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5917-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x32x32xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %7316 = "dtu_hlir.conv_bias"(%7282, %274, %275) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5918-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x1280x32x32xf32>, tensor<640x1280x1x1xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %7317 = "dtu_hlir.add"(%7316, %7315) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_5919-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7318 = dtu_hlir.constant  {node_name = "Constant_5920-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %7319 = "dtu_hlir.broadcast_in_dim"(%7318) {node_name = "Div_5921-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x640x32x32xf32>
    %7320 = "dtu_hlir.div"(%7317, %7319) {node_name = "Div_5921-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7321 = "dtu_hlir.shape"(%7320) {end = 2147483647 : i64, node_name = "Shape_5922-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7322 = dtu_hlir.constant  {node_name = "Constant_5923-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7323 = "dtu_hlir.gather"(%7321, %7322) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5924-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7324 = "dtu_hlir.shape"(%7320) {end = 2147483647 : i64, node_name = "Shape_5925-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7325 = dtu_hlir.constant  {node_name = "Constant_5926-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7326 = "dtu_hlir.gather"(%7324, %7325) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5927-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7327 = "dtu_hlir.shape"(%7320) {end = 2147483647 : i64, node_name = "Shape_5928-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7328 = dtu_hlir.constant  {node_name = "Constant_5929-0", node_type = "Constant"} dense<3> : tensor<i64>
    %7329 = "dtu_hlir.gather"(%7327, %7328) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5930-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7330 = dtu_hlir.constant  {node_name = "Constant_5931-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %7331 = "dtu_hlir.dynamic_reshape"(%7320, %7330) {allowzero = 0 : i64, node_name = "Reshape_5932-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %7332 = dtu_hlir.constant  {node_name = "Constant_5933-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %7333 = dtu_hlir.constant  {node_name = "Constant_5934-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %7334 = "dtu_hlir.instance_norm"(%7331, %7332, %7333) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_5935-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %7335 = "dtu_hlir.shape"(%7320) {end = 2147483647 : i64, node_name = "Shape_5936-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7336 = "dtu_hlir.dynamic_reshape"(%7334, %7335) {allowzero = 0 : i64, node_name = "Reshape_5937-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %7337 = "dtu_hlir.mul"(%7336, %604) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_5938-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7338 = "dtu_hlir.add"(%7337, %605) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_5939-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7339 = "dtu_hlir.shape"(%7338) {end = 2147483647 : i64, node_name = "Shape_5940-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7340 = dtu_hlir.constant  {node_name = "Constant_5941-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7341 = "dtu_hlir.gather"(%7339, %7340) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5942-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7342 = "dtu_hlir.transpose"(%7338) {node_name = "Transpose_5943-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x32x32x640xf32>
    %7343 = "dtu_hlir.mul"(%7326, %7329) {node_name = "Mul_5944-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7344 = dtu_hlir.constant  {node_name = "Constant_5945-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7345 = "dtu_hlir.unsqueeze"(%7323, %7344) {node_name = "Unsqueeze_5946-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7346 = dtu_hlir.constant  {node_name = "Constant_5947-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7347 = "dtu_hlir.unsqueeze"(%7343, %7346) {node_name = "Unsqueeze_5948-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7348 = dtu_hlir.constant  {node_name = "Constant_5949-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7349 = "dtu_hlir.unsqueeze"(%7341, %7348) {node_name = "Unsqueeze_5950-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7350 = "dtu_hlir.concatenate"(%7345, %7347, %7349) {dimension = 0 : i64, node_name = "Concat_5951-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7351 = "dtu_hlir.dynamic_reshape"(%7342, %7350) {allowzero = 0 : i64, node_name = "Reshape_5952-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %7352 = "dtu_hlir.dot_general"(%7351, %606) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5953-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7353 = "dtu_hlir.add"(%236, %7352) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5954-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7354 = dtu_hlir.constant  {node_name = "ReduceMean_5955-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7355 = "dtu_hlir.reshape"(%7354) {node_name = "ReduceMean_5955-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7356 = "dtu_hlir.reduce"(%7353, %7355) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5955-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5955-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7357 = dtu_hlir.constant  {node_name = "ReduceMean_5955-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7358 = "dtu_hlir.unsqueeze"(%7356, %7357) {node_name = "ReduceMean_5955-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7359 = dtu_hlir.constant  {node_name = "ReduceMean_5955-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7360 = "dtu_hlir.broadcast_in_dim"(%7359) {node_name = "ReduceMean_5955-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7361 = "dtu_hlir.div"(%7358, %7360) {node_name = "ReduceMean_5955-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7362 = "dtu_hlir.sub"(%7353, %7361) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_5956-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7363 = dtu_hlir.constant  {node_name = "Constant_5957-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %7364 = "dtu_hlir.broadcast_in_dim"(%7363) {node_name = "Pow_5958-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %7365 = "dtu_hlir.pow"(%7362, %7364) {node_name = "Pow_5958-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7366 = dtu_hlir.constant  {node_name = "ReduceMean_5959-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7367 = "dtu_hlir.reshape"(%7366) {node_name = "ReduceMean_5959-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7368 = "dtu_hlir.reduce"(%7365, %7367) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_5959-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_5959-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7369 = dtu_hlir.constant  {node_name = "ReduceMean_5959-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7370 = "dtu_hlir.unsqueeze"(%7368, %7369) {node_name = "ReduceMean_5959-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7371 = dtu_hlir.constant  {node_name = "ReduceMean_5959-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7372 = "dtu_hlir.broadcast_in_dim"(%7371) {node_name = "ReduceMean_5959-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7373 = "dtu_hlir.div"(%7370, %7372) {node_name = "ReduceMean_5959-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7374 = dtu_hlir.constant  {node_name = "Constant_5960-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %7375 = "dtu_hlir.broadcast_in_dim"(%7374) {node_name = "Add_5961-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %7376 = "dtu_hlir.add"(%7373, %7375) {node_name = "Add_5961-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7377 = "dtu_hlir.sqrt"(%7376) {node_name = "Sqrt_5962-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7378 = "dtu_hlir.div"(%7362, %7377) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_5963-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7379 = "dtu_hlir.mul"(%7378, %241) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_5964-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7380 = "dtu_hlir.add"(%7379, %242) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_5965-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7381 = "dtu_hlir.dot_general"(%7380, %607) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5966-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7382 = "dtu_hlir.dot_general"(%7380, %608) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5967-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7383 = "dtu_hlir.dot_general"(%7380, %609) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_5968-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7384 = "dtu_hlir.shape"(%7381) {end = 2147483647 : i64, node_name = "Shape_5969-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7385 = dtu_hlir.constant  {node_name = "Constant_5970-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7386 = "dtu_hlir.gather"(%7384, %7385) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5971-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7387 = "dtu_hlir.shape"(%7381) {end = 2147483647 : i64, node_name = "Shape_5972-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7388 = dtu_hlir.constant  {node_name = "Constant_5973-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7389 = "dtu_hlir.gather"(%7387, %7388) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5974-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7390 = "dtu_hlir.shape"(%7381) {end = 2147483647 : i64, node_name = "Shape_5975-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7391 = dtu_hlir.constant  {node_name = "Constant_5976-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7392 = "dtu_hlir.gather"(%7390, %7391) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_5977-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7393 = dtu_hlir.constant  {node_name = "Constant_5978-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7394 = "dtu_hlir.div"(%7392, %7393) {node_name = "Div_5979-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7395 = "dtu_hlir.convert"(%7394) {node_name = "Cast_5980-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7396 = "dtu_hlir.convert"(%7395) {node_name = "Cast_5981-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7397 = dtu_hlir.constant  {node_name = "Constant_5982-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7398 = "dtu_hlir.unsqueeze"(%7386, %7397) {node_name = "Unsqueeze_5983-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7399 = dtu_hlir.constant  {node_name = "Constant_5984-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7400 = "dtu_hlir.unsqueeze"(%7389, %7399) {node_name = "Unsqueeze_5985-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7401 = dtu_hlir.constant  {node_name = "Constant_5986-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7402 = "dtu_hlir.unsqueeze"(%7396, %7401) {node_name = "Unsqueeze_5987-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7403 = "dtu_hlir.concatenate"(%7398, %7400, %752, %7402) {dimension = 0 : i64, node_name = "Concat_5988-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7404 = "dtu_hlir.dynamic_reshape"(%7381, %7403) {allowzero = 0 : i64, node_name = "Reshape_5989-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %7405 = "dtu_hlir.transpose"(%7404) {node_name = "Transpose_5990-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %7406 = dtu_hlir.constant  {node_name = "Constant_5991-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7407 = "dtu_hlir.mul"(%7386, %7406) {node_name = "Mul_5992-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7408 = dtu_hlir.constant  {node_name = "Constant_5993-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7409 = "dtu_hlir.div"(%7392, %7408) {node_name = "Div_5994-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7410 = "dtu_hlir.convert"(%7409) {node_name = "Cast_5995-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7411 = "dtu_hlir.convert"(%7410) {node_name = "Cast_5996-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7412 = dtu_hlir.constant  {node_name = "Constant_5997-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7413 = "dtu_hlir.unsqueeze"(%7407, %7412) {node_name = "Unsqueeze_5998-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7414 = dtu_hlir.constant  {node_name = "Constant_5999-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7415 = "dtu_hlir.unsqueeze"(%7389, %7414) {node_name = "Unsqueeze_6000-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7416 = dtu_hlir.constant  {node_name = "Constant_6001-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7417 = "dtu_hlir.unsqueeze"(%7411, %7416) {node_name = "Unsqueeze_6002-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7418 = "dtu_hlir.concatenate"(%7413, %7415, %7417) {dimension = 0 : i64, node_name = "Concat_6003-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7419 = "dtu_hlir.dynamic_reshape"(%7405, %7418) {allowzero = 0 : i64, node_name = "Reshape_6004-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %7420 = "dtu_hlir.shape"(%7382) {end = 2147483647 : i64, node_name = "Shape_6005-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7421 = dtu_hlir.constant  {node_name = "Constant_6006-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7422 = "dtu_hlir.gather"(%7420, %7421) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6007-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7423 = "dtu_hlir.shape"(%7382) {end = 2147483647 : i64, node_name = "Shape_6008-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7424 = dtu_hlir.constant  {node_name = "Constant_6009-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7425 = "dtu_hlir.gather"(%7423, %7424) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6010-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7426 = "dtu_hlir.shape"(%7382) {end = 2147483647 : i64, node_name = "Shape_6011-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7427 = dtu_hlir.constant  {node_name = "Constant_6012-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7428 = "dtu_hlir.gather"(%7426, %7427) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6013-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7429 = dtu_hlir.constant  {node_name = "Constant_6014-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7430 = "dtu_hlir.div"(%7428, %7429) {node_name = "Div_6015-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7431 = "dtu_hlir.convert"(%7430) {node_name = "Cast_6016-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7432 = "dtu_hlir.convert"(%7431) {node_name = "Cast_6017-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7433 = dtu_hlir.constant  {node_name = "Constant_6018-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7434 = "dtu_hlir.unsqueeze"(%7422, %7433) {node_name = "Unsqueeze_6019-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7435 = dtu_hlir.constant  {node_name = "Constant_6020-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7436 = "dtu_hlir.unsqueeze"(%7425, %7435) {node_name = "Unsqueeze_6021-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7437 = dtu_hlir.constant  {node_name = "Constant_6022-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7438 = "dtu_hlir.unsqueeze"(%7432, %7437) {node_name = "Unsqueeze_6023-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7439 = "dtu_hlir.concatenate"(%7434, %7436, %751, %7438) {dimension = 0 : i64, node_name = "Concat_6024-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7440 = "dtu_hlir.dynamic_reshape"(%7382, %7439) {allowzero = 0 : i64, node_name = "Reshape_6025-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %7441 = "dtu_hlir.transpose"(%7440) {node_name = "Transpose_6026-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %7442 = dtu_hlir.constant  {node_name = "Constant_6027-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7443 = "dtu_hlir.mul"(%7422, %7442) {node_name = "Mul_6028-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7444 = dtu_hlir.constant  {node_name = "Constant_6029-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7445 = "dtu_hlir.div"(%7428, %7444) {node_name = "Div_6030-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7446 = "dtu_hlir.convert"(%7445) {node_name = "Cast_6031-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7447 = "dtu_hlir.convert"(%7446) {node_name = "Cast_6032-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7448 = dtu_hlir.constant  {node_name = "Constant_6033-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7449 = "dtu_hlir.unsqueeze"(%7443, %7448) {node_name = "Unsqueeze_6034-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7450 = dtu_hlir.constant  {node_name = "Constant_6035-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7451 = "dtu_hlir.unsqueeze"(%7425, %7450) {node_name = "Unsqueeze_6036-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7452 = dtu_hlir.constant  {node_name = "Constant_6037-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7453 = "dtu_hlir.unsqueeze"(%7447, %7452) {node_name = "Unsqueeze_6038-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7454 = "dtu_hlir.concatenate"(%7449, %7451, %7453) {dimension = 0 : i64, node_name = "Concat_6039-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7455 = "dtu_hlir.dynamic_reshape"(%7441, %7454) {allowzero = 0 : i64, node_name = "Reshape_6040-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %7456 = "dtu_hlir.shape"(%7383) {end = 2147483647 : i64, node_name = "Shape_6041-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7457 = dtu_hlir.constant  {node_name = "Constant_6042-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7458 = "dtu_hlir.gather"(%7456, %7457) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6043-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7459 = "dtu_hlir.shape"(%7383) {end = 2147483647 : i64, node_name = "Shape_6044-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7460 = dtu_hlir.constant  {node_name = "Constant_6045-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7461 = "dtu_hlir.gather"(%7459, %7460) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6046-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7462 = "dtu_hlir.shape"(%7383) {end = 2147483647 : i64, node_name = "Shape_6047-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7463 = dtu_hlir.constant  {node_name = "Constant_6048-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7464 = "dtu_hlir.gather"(%7462, %7463) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6049-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7465 = dtu_hlir.constant  {node_name = "Constant_6050-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7466 = "dtu_hlir.div"(%7464, %7465) {node_name = "Div_6051-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7467 = "dtu_hlir.convert"(%7466) {node_name = "Cast_6052-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7468 = "dtu_hlir.convert"(%7467) {node_name = "Cast_6053-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7469 = dtu_hlir.constant  {node_name = "Constant_6054-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7470 = "dtu_hlir.unsqueeze"(%7458, %7469) {node_name = "Unsqueeze_6055-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7471 = dtu_hlir.constant  {node_name = "Constant_6056-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7472 = "dtu_hlir.unsqueeze"(%7461, %7471) {node_name = "Unsqueeze_6057-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7473 = dtu_hlir.constant  {node_name = "Constant_6058-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7474 = "dtu_hlir.unsqueeze"(%7468, %7473) {node_name = "Unsqueeze_6059-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7475 = "dtu_hlir.concatenate"(%7470, %7472, %750, %7474) {dimension = 0 : i64, node_name = "Concat_6060-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7476 = "dtu_hlir.dynamic_reshape"(%7383, %7475) {allowzero = 0 : i64, node_name = "Reshape_6061-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %7477 = "dtu_hlir.transpose"(%7476) {node_name = "Transpose_6062-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %7478 = dtu_hlir.constant  {node_name = "Constant_6063-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7479 = "dtu_hlir.mul"(%7458, %7478) {node_name = "Mul_6064-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7480 = dtu_hlir.constant  {node_name = "Constant_6065-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7481 = "dtu_hlir.div"(%7464, %7480) {node_name = "Div_6066-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7482 = "dtu_hlir.convert"(%7481) {node_name = "Cast_6067-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7483 = "dtu_hlir.convert"(%7482) {node_name = "Cast_6068-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7484 = dtu_hlir.constant  {node_name = "Constant_6069-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7485 = "dtu_hlir.unsqueeze"(%7479, %7484) {node_name = "Unsqueeze_6070-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7486 = dtu_hlir.constant  {node_name = "Constant_6071-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7487 = "dtu_hlir.unsqueeze"(%7461, %7486) {node_name = "Unsqueeze_6072-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7488 = dtu_hlir.constant  {node_name = "Constant_6073-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7489 = "dtu_hlir.unsqueeze"(%7483, %7488) {node_name = "Unsqueeze_6074-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7490 = "dtu_hlir.concatenate"(%7485, %7487, %7489) {dimension = 0 : i64, node_name = "Concat_6075-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7491 = "dtu_hlir.dynamic_reshape"(%7477, %7490) {allowzero = 0 : i64, node_name = "Reshape_6076-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %7492 = "dtu_hlir.shape"(%7419) {end = 2147483647 : i64, node_name = "Shape_6077-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7493 = dtu_hlir.constant  {node_name = "Constant_6078-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7494 = "dtu_hlir.gather"(%7492, %7493) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6079-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7495 = "dtu_hlir.shape"(%7419) {end = 2147483647 : i64, node_name = "Shape_6080-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7496 = dtu_hlir.constant  {node_name = "Constant_6081-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7497 = "dtu_hlir.gather"(%7495, %7496) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6082-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7498 = "dtu_hlir.shape"(%7455) {end = 2147483647 : i64, node_name = "Shape_6083-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7499 = dtu_hlir.constant  {node_name = "Constant_6084-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7500 = "dtu_hlir.gather"(%7498, %7499) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6085-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7501 = dtu_hlir.constant  {node_name = "Constant_6086-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7502 = "dtu_hlir.unsqueeze"(%7494, %7501) {node_name = "Unsqueeze_6087-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7503 = dtu_hlir.constant  {node_name = "Constant_6088-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7504 = "dtu_hlir.unsqueeze"(%7497, %7503) {node_name = "Unsqueeze_6089-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7505 = dtu_hlir.constant  {node_name = "Constant_6090-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7506 = "dtu_hlir.unsqueeze"(%7500, %7505) {node_name = "Unsqueeze_6091-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7507 = "dtu_hlir.concatenate"(%7502, %7504, %7506) {dimension = 0 : i64, node_name = "Concat_6092-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7508 = dtu_hlir.constant  {node_name = "ConstantOfShape_6093-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %7509 = "dtu_hlir.dynamic_broadcast_in_dim"(%7508, %7507) {node_name = "ConstantOfShape_6093-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x1024xf32>
    %7510 = "dtu_hlir.transpose"(%7455) {node_name = "Transpose_6094-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<20x64x1024xf32>
    %7511 = "dtu_hlir.dot_general"(%7419, %7510) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6095-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x1024xf32>) -> tensor<20x1024x1024xf32>
    %7512 = "dtu_hlir.broadcast_in_dim"(%749) {node_name = "Mul_6096-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %7513 = "dtu_hlir.mul"(%7511, %7512) {node_name = "Mul_6096-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %7514 = "dtu_hlir.broadcast_in_dim"(%748) {node_name = "Mul_6097-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %7515 = "dtu_hlir.mul"(%7509, %7514) {node_name = "Mul_6097-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %7516 = "dtu_hlir.add"(%7513, %7515) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6098-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %7517 = "dtu_hlir.softmax"(%7516) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6099-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %7518 = "dtu_hlir.convert"(%7517) {node_name = "Cast_6100-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %7519 = "dtu_hlir.dot_general"(%7518, %7491) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6101-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x64xf32>) -> tensor<20x1024x64xf32>
    %7520 = "dtu_hlir.shape"(%7519) {end = 2147483647 : i64, node_name = "Shape_6102-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7521 = dtu_hlir.constant  {node_name = "Constant_6103-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7522 = "dtu_hlir.gather"(%7520, %7521) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6104-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7523 = "dtu_hlir.shape"(%7519) {end = 2147483647 : i64, node_name = "Shape_6105-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7524 = dtu_hlir.constant  {node_name = "Constant_6106-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7525 = "dtu_hlir.gather"(%7523, %7524) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6107-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7526 = "dtu_hlir.shape"(%7519) {end = 2147483647 : i64, node_name = "Shape_6108-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7527 = dtu_hlir.constant  {node_name = "Constant_6109-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7528 = "dtu_hlir.gather"(%7526, %7527) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6110-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7529 = dtu_hlir.constant  {node_name = "Constant_6111-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7530 = "dtu_hlir.div"(%7522, %7529) {node_name = "Div_6112-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7531 = "dtu_hlir.convert"(%7530) {node_name = "Cast_6113-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7532 = "dtu_hlir.convert"(%7531) {node_name = "Cast_6114-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7533 = dtu_hlir.constant  {node_name = "Constant_6115-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7534 = "dtu_hlir.unsqueeze"(%7532, %7533) {node_name = "Unsqueeze_6116-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7535 = dtu_hlir.constant  {node_name = "Constant_6117-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7536 = "dtu_hlir.unsqueeze"(%7525, %7535) {node_name = "Unsqueeze_6118-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7537 = dtu_hlir.constant  {node_name = "Constant_6119-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7538 = "dtu_hlir.unsqueeze"(%7528, %7537) {node_name = "Unsqueeze_6120-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7539 = "dtu_hlir.concatenate"(%7534, %747, %7536, %7538) {dimension = 0 : i64, node_name = "Concat_6121-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7540 = "dtu_hlir.dynamic_reshape"(%7519, %7539) {allowzero = 0 : i64, node_name = "Reshape_6122-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %7541 = "dtu_hlir.transpose"(%7540) {node_name = "Transpose_6123-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %7542 = dtu_hlir.constant  {node_name = "Constant_6124-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7543 = "dtu_hlir.div"(%7522, %7542) {node_name = "Div_6125-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7544 = "dtu_hlir.convert"(%7543) {node_name = "Cast_6126-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7545 = "dtu_hlir.convert"(%7544) {node_name = "Cast_6127-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7546 = dtu_hlir.constant  {node_name = "Constant_6128-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7547 = "dtu_hlir.mul"(%7528, %7546) {node_name = "Mul_6129-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7548 = dtu_hlir.constant  {node_name = "Constant_6130-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7549 = "dtu_hlir.unsqueeze"(%7545, %7548) {node_name = "Unsqueeze_6131-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7550 = dtu_hlir.constant  {node_name = "Constant_6132-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7551 = "dtu_hlir.unsqueeze"(%7525, %7550) {node_name = "Unsqueeze_6133-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7552 = dtu_hlir.constant  {node_name = "Constant_6134-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7553 = "dtu_hlir.unsqueeze"(%7547, %7552) {node_name = "Unsqueeze_6135-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7554 = "dtu_hlir.concatenate"(%7549, %7551, %7553) {dimension = 0 : i64, node_name = "Concat_6136-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7555 = "dtu_hlir.dynamic_reshape"(%7541, %7554) {allowzero = 0 : i64, node_name = "Reshape_6137-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %7556 = "dtu_hlir.dot_general"(%7555, %610) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6138-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7557 = "dtu_hlir.add"(%237, %7556) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6139-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7558 = "dtu_hlir.add"(%7557, %7353) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6140-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7559 = dtu_hlir.constant  {node_name = "ReduceMean_6141-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7560 = "dtu_hlir.reshape"(%7559) {node_name = "ReduceMean_6141-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7561 = "dtu_hlir.reduce"(%7558, %7560) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6141-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6141-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7562 = dtu_hlir.constant  {node_name = "ReduceMean_6141-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7563 = "dtu_hlir.unsqueeze"(%7561, %7562) {node_name = "ReduceMean_6141-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7564 = dtu_hlir.constant  {node_name = "ReduceMean_6141-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7565 = "dtu_hlir.broadcast_in_dim"(%7564) {node_name = "ReduceMean_6141-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7566 = "dtu_hlir.div"(%7563, %7565) {node_name = "ReduceMean_6141-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7567 = "dtu_hlir.sub"(%7558, %7566) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_6142-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7568 = dtu_hlir.constant  {node_name = "Constant_6143-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %7569 = "dtu_hlir.broadcast_in_dim"(%7568) {node_name = "Pow_6144-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %7570 = "dtu_hlir.pow"(%7567, %7569) {node_name = "Pow_6144-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7571 = dtu_hlir.constant  {node_name = "ReduceMean_6145-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7572 = "dtu_hlir.reshape"(%7571) {node_name = "ReduceMean_6145-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7573 = "dtu_hlir.reduce"(%7570, %7572) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6145-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6145-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7574 = dtu_hlir.constant  {node_name = "ReduceMean_6145-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7575 = "dtu_hlir.unsqueeze"(%7573, %7574) {node_name = "ReduceMean_6145-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7576 = dtu_hlir.constant  {node_name = "ReduceMean_6145-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7577 = "dtu_hlir.broadcast_in_dim"(%7576) {node_name = "ReduceMean_6145-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7578 = "dtu_hlir.div"(%7575, %7577) {node_name = "ReduceMean_6145-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7579 = dtu_hlir.constant  {node_name = "Constant_6146-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %7580 = "dtu_hlir.broadcast_in_dim"(%7579) {node_name = "Add_6147-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %7581 = "dtu_hlir.add"(%7578, %7580) {node_name = "Add_6147-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7582 = "dtu_hlir.sqrt"(%7581) {node_name = "Sqrt_6148-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7583 = "dtu_hlir.div"(%7567, %7582) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_6149-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7584 = "dtu_hlir.mul"(%7583, %243) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_6150-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7585 = "dtu_hlir.add"(%7584, %244) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6151-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7586 = "dtu_hlir.dot_general"(%7585, %611) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6152-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7587 = "dtu_hlir.dot_general"(%arg2, %612) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6153-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %7588 = "dtu_hlir.dot_general"(%arg2, %613) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6154-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %7589 = "dtu_hlir.shape"(%7586) {end = 2147483647 : i64, node_name = "Shape_6155-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7590 = dtu_hlir.constant  {node_name = "Constant_6156-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7591 = "dtu_hlir.gather"(%7589, %7590) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6157-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7592 = "dtu_hlir.shape"(%7586) {end = 2147483647 : i64, node_name = "Shape_6158-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7593 = dtu_hlir.constant  {node_name = "Constant_6159-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7594 = "dtu_hlir.gather"(%7592, %7593) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6160-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7595 = "dtu_hlir.shape"(%7586) {end = 2147483647 : i64, node_name = "Shape_6161-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7596 = dtu_hlir.constant  {node_name = "Constant_6162-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7597 = "dtu_hlir.gather"(%7595, %7596) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6163-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7598 = dtu_hlir.constant  {node_name = "Constant_6164-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7599 = "dtu_hlir.div"(%7597, %7598) {node_name = "Div_6165-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7600 = "dtu_hlir.convert"(%7599) {node_name = "Cast_6166-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7601 = "dtu_hlir.convert"(%7600) {node_name = "Cast_6167-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7602 = dtu_hlir.constant  {node_name = "Constant_6168-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7603 = "dtu_hlir.unsqueeze"(%7591, %7602) {node_name = "Unsqueeze_6169-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7604 = dtu_hlir.constant  {node_name = "Constant_6170-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7605 = "dtu_hlir.unsqueeze"(%7594, %7604) {node_name = "Unsqueeze_6171-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7606 = dtu_hlir.constant  {node_name = "Constant_6172-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7607 = "dtu_hlir.unsqueeze"(%7601, %7606) {node_name = "Unsqueeze_6173-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7608 = "dtu_hlir.concatenate"(%7603, %7605, %746, %7607) {dimension = 0 : i64, node_name = "Concat_6174-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7609 = "dtu_hlir.dynamic_reshape"(%7586, %7608) {allowzero = 0 : i64, node_name = "Reshape_6175-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %7610 = "dtu_hlir.transpose"(%7609) {node_name = "Transpose_6176-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %7611 = dtu_hlir.constant  {node_name = "Constant_6177-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7612 = "dtu_hlir.mul"(%7591, %7611) {node_name = "Mul_6178-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7613 = dtu_hlir.constant  {node_name = "Constant_6179-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7614 = "dtu_hlir.div"(%7597, %7613) {node_name = "Div_6180-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7615 = "dtu_hlir.convert"(%7614) {node_name = "Cast_6181-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7616 = "dtu_hlir.convert"(%7615) {node_name = "Cast_6182-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7617 = dtu_hlir.constant  {node_name = "Constant_6183-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7618 = "dtu_hlir.unsqueeze"(%7612, %7617) {node_name = "Unsqueeze_6184-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7619 = dtu_hlir.constant  {node_name = "Constant_6185-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7620 = "dtu_hlir.unsqueeze"(%7594, %7619) {node_name = "Unsqueeze_6186-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7621 = dtu_hlir.constant  {node_name = "Constant_6187-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7622 = "dtu_hlir.unsqueeze"(%7616, %7621) {node_name = "Unsqueeze_6188-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7623 = "dtu_hlir.concatenate"(%7618, %7620, %7622) {dimension = 0 : i64, node_name = "Concat_6189-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7624 = "dtu_hlir.dynamic_reshape"(%7610, %7623) {allowzero = 0 : i64, node_name = "Reshape_6190-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %7625 = "dtu_hlir.shape"(%7587) {end = 2147483647 : i64, node_name = "Shape_6191-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7626 = dtu_hlir.constant  {node_name = "Constant_6192-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7627 = "dtu_hlir.gather"(%7625, %7626) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6193-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7628 = "dtu_hlir.shape"(%7587) {end = 2147483647 : i64, node_name = "Shape_6194-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7629 = dtu_hlir.constant  {node_name = "Constant_6195-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7630 = "dtu_hlir.gather"(%7628, %7629) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6196-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7631 = "dtu_hlir.shape"(%7587) {end = 2147483647 : i64, node_name = "Shape_6197-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7632 = dtu_hlir.constant  {node_name = "Constant_6198-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7633 = "dtu_hlir.gather"(%7631, %7632) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6199-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7634 = dtu_hlir.constant  {node_name = "Constant_6200-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7635 = "dtu_hlir.div"(%7633, %7634) {node_name = "Div_6201-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7636 = "dtu_hlir.convert"(%7635) {node_name = "Cast_6202-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7637 = "dtu_hlir.convert"(%7636) {node_name = "Cast_6203-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7638 = dtu_hlir.constant  {node_name = "Constant_6204-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7639 = "dtu_hlir.unsqueeze"(%7627, %7638) {node_name = "Unsqueeze_6205-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7640 = dtu_hlir.constant  {node_name = "Constant_6206-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7641 = "dtu_hlir.unsqueeze"(%7630, %7640) {node_name = "Unsqueeze_6207-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7642 = dtu_hlir.constant  {node_name = "Constant_6208-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7643 = "dtu_hlir.unsqueeze"(%7637, %7642) {node_name = "Unsqueeze_6209-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7644 = "dtu_hlir.concatenate"(%7639, %7641, %745, %7643) {dimension = 0 : i64, node_name = "Concat_6210-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7645 = "dtu_hlir.dynamic_reshape"(%7587, %7644) {allowzero = 0 : i64, node_name = "Reshape_6211-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %7646 = "dtu_hlir.transpose"(%7645) {node_name = "Transpose_6212-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %7647 = dtu_hlir.constant  {node_name = "Constant_6213-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7648 = "dtu_hlir.mul"(%7627, %7647) {node_name = "Mul_6214-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7649 = dtu_hlir.constant  {node_name = "Constant_6215-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7650 = "dtu_hlir.div"(%7633, %7649) {node_name = "Div_6216-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7651 = "dtu_hlir.convert"(%7650) {node_name = "Cast_6217-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7652 = "dtu_hlir.convert"(%7651) {node_name = "Cast_6218-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7653 = dtu_hlir.constant  {node_name = "Constant_6219-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7654 = "dtu_hlir.unsqueeze"(%7648, %7653) {node_name = "Unsqueeze_6220-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7655 = dtu_hlir.constant  {node_name = "Constant_6221-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7656 = "dtu_hlir.unsqueeze"(%7630, %7655) {node_name = "Unsqueeze_6222-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7657 = dtu_hlir.constant  {node_name = "Constant_6223-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7658 = "dtu_hlir.unsqueeze"(%7652, %7657) {node_name = "Unsqueeze_6224-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7659 = "dtu_hlir.concatenate"(%7654, %7656, %7658) {dimension = 0 : i64, node_name = "Concat_6225-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7660 = "dtu_hlir.dynamic_reshape"(%7646, %7659) {allowzero = 0 : i64, node_name = "Reshape_6226-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %7661 = "dtu_hlir.shape"(%7588) {end = 2147483647 : i64, node_name = "Shape_6227-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7662 = dtu_hlir.constant  {node_name = "Constant_6228-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7663 = "dtu_hlir.gather"(%7661, %7662) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6229-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7664 = "dtu_hlir.shape"(%7588) {end = 2147483647 : i64, node_name = "Shape_6230-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7665 = dtu_hlir.constant  {node_name = "Constant_6231-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7666 = "dtu_hlir.gather"(%7664, %7665) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6232-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7667 = "dtu_hlir.shape"(%7588) {end = 2147483647 : i64, node_name = "Shape_6233-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %7668 = dtu_hlir.constant  {node_name = "Constant_6234-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7669 = "dtu_hlir.gather"(%7667, %7668) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6235-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7670 = dtu_hlir.constant  {node_name = "Constant_6236-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7671 = "dtu_hlir.div"(%7669, %7670) {node_name = "Div_6237-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7672 = "dtu_hlir.convert"(%7671) {node_name = "Cast_6238-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7673 = "dtu_hlir.convert"(%7672) {node_name = "Cast_6239-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7674 = dtu_hlir.constant  {node_name = "Constant_6240-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7675 = "dtu_hlir.unsqueeze"(%7663, %7674) {node_name = "Unsqueeze_6241-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7676 = dtu_hlir.constant  {node_name = "Constant_6242-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7677 = "dtu_hlir.unsqueeze"(%7666, %7676) {node_name = "Unsqueeze_6243-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7678 = dtu_hlir.constant  {node_name = "Constant_6244-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7679 = "dtu_hlir.unsqueeze"(%7673, %7678) {node_name = "Unsqueeze_6245-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7680 = "dtu_hlir.concatenate"(%7675, %7677, %744, %7679) {dimension = 0 : i64, node_name = "Concat_6246-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7681 = "dtu_hlir.dynamic_reshape"(%7588, %7680) {allowzero = 0 : i64, node_name = "Reshape_6247-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %7682 = "dtu_hlir.transpose"(%7681) {node_name = "Transpose_6248-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %7683 = dtu_hlir.constant  {node_name = "Constant_6249-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7684 = "dtu_hlir.mul"(%7663, %7683) {node_name = "Mul_6250-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7685 = dtu_hlir.constant  {node_name = "Constant_6251-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7686 = "dtu_hlir.div"(%7669, %7685) {node_name = "Div_6252-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7687 = "dtu_hlir.convert"(%7686) {node_name = "Cast_6253-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7688 = "dtu_hlir.convert"(%7687) {node_name = "Cast_6254-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7689 = dtu_hlir.constant  {node_name = "Constant_6255-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7690 = "dtu_hlir.unsqueeze"(%7684, %7689) {node_name = "Unsqueeze_6256-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7691 = dtu_hlir.constant  {node_name = "Constant_6257-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7692 = "dtu_hlir.unsqueeze"(%7666, %7691) {node_name = "Unsqueeze_6258-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7693 = dtu_hlir.constant  {node_name = "Constant_6259-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7694 = "dtu_hlir.unsqueeze"(%7688, %7693) {node_name = "Unsqueeze_6260-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7695 = "dtu_hlir.concatenate"(%7690, %7692, %7694) {dimension = 0 : i64, node_name = "Concat_6261-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7696 = "dtu_hlir.dynamic_reshape"(%7682, %7695) {allowzero = 0 : i64, node_name = "Reshape_6262-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %7697 = "dtu_hlir.shape"(%7624) {end = 2147483647 : i64, node_name = "Shape_6263-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7698 = dtu_hlir.constant  {node_name = "Constant_6264-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7699 = "dtu_hlir.gather"(%7697, %7698) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6265-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7700 = "dtu_hlir.shape"(%7624) {end = 2147483647 : i64, node_name = "Shape_6266-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7701 = dtu_hlir.constant  {node_name = "Constant_6267-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7702 = "dtu_hlir.gather"(%7700, %7701) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6268-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7703 = "dtu_hlir.shape"(%7660) {end = 2147483647 : i64, node_name = "Shape_6269-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<3xi64>
    %7704 = dtu_hlir.constant  {node_name = "Constant_6270-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7705 = "dtu_hlir.gather"(%7703, %7704) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6271-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7706 = dtu_hlir.constant  {node_name = "Constant_6272-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7707 = "dtu_hlir.unsqueeze"(%7699, %7706) {node_name = "Unsqueeze_6273-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7708 = dtu_hlir.constant  {node_name = "Constant_6274-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7709 = "dtu_hlir.unsqueeze"(%7702, %7708) {node_name = "Unsqueeze_6275-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7710 = dtu_hlir.constant  {node_name = "Constant_6276-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7711 = "dtu_hlir.unsqueeze"(%7705, %7710) {node_name = "Unsqueeze_6277-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7712 = "dtu_hlir.concatenate"(%7707, %7709, %7711) {dimension = 0 : i64, node_name = "Concat_6278-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7713 = dtu_hlir.constant  {node_name = "ConstantOfShape_6279-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %7714 = "dtu_hlir.dynamic_broadcast_in_dim"(%7713, %7712) {node_name = "ConstantOfShape_6279-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x77xf32>
    %7715 = "dtu_hlir.transpose"(%7660) {node_name = "Transpose_6280-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<20x64x77xf32>
    %7716 = "dtu_hlir.dot_general"(%7624, %7715) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6281-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x77xf32>) -> tensor<20x1024x77xf32>
    %7717 = "dtu_hlir.broadcast_in_dim"(%743) {node_name = "Mul_6282-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %7718 = "dtu_hlir.mul"(%7716, %7717) {node_name = "Mul_6282-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7719 = "dtu_hlir.broadcast_in_dim"(%742) {node_name = "Mul_6283-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %7720 = "dtu_hlir.mul"(%7714, %7719) {node_name = "Mul_6283-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7721 = "dtu_hlir.add"(%7718, %7720) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6284-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7722 = "dtu_hlir.softmax"(%7721) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6285-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7723 = "dtu_hlir.convert"(%7722) {node_name = "Cast_6286-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %7724 = "dtu_hlir.dot_general"(%7723, %7696) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6287-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x77x64xf32>) -> tensor<20x1024x64xf32>
    %7725 = "dtu_hlir.shape"(%7724) {end = 2147483647 : i64, node_name = "Shape_6288-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7726 = dtu_hlir.constant  {node_name = "Constant_6289-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7727 = "dtu_hlir.gather"(%7725, %7726) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6290-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7728 = "dtu_hlir.shape"(%7724) {end = 2147483647 : i64, node_name = "Shape_6291-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7729 = dtu_hlir.constant  {node_name = "Constant_6292-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7730 = "dtu_hlir.gather"(%7728, %7729) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6293-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7731 = "dtu_hlir.shape"(%7724) {end = 2147483647 : i64, node_name = "Shape_6294-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %7732 = dtu_hlir.constant  {node_name = "Constant_6295-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7733 = "dtu_hlir.gather"(%7731, %7732) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6296-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7734 = dtu_hlir.constant  {node_name = "Constant_6297-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7735 = "dtu_hlir.div"(%7727, %7734) {node_name = "Div_6298-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7736 = "dtu_hlir.convert"(%7735) {node_name = "Cast_6299-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7737 = "dtu_hlir.convert"(%7736) {node_name = "Cast_6300-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7738 = dtu_hlir.constant  {node_name = "Constant_6301-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7739 = "dtu_hlir.unsqueeze"(%7737, %7738) {node_name = "Unsqueeze_6302-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7740 = dtu_hlir.constant  {node_name = "Constant_6303-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7741 = "dtu_hlir.unsqueeze"(%7730, %7740) {node_name = "Unsqueeze_6304-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7742 = dtu_hlir.constant  {node_name = "Constant_6305-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7743 = "dtu_hlir.unsqueeze"(%7733, %7742) {node_name = "Unsqueeze_6306-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7744 = "dtu_hlir.concatenate"(%7739, %741, %7741, %7743) {dimension = 0 : i64, node_name = "Concat_6307-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7745 = "dtu_hlir.dynamic_reshape"(%7724, %7744) {allowzero = 0 : i64, node_name = "Reshape_6308-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %7746 = "dtu_hlir.transpose"(%7745) {node_name = "Transpose_6309-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %7747 = dtu_hlir.constant  {node_name = "Constant_6310-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7748 = "dtu_hlir.div"(%7727, %7747) {node_name = "Div_6311-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7749 = "dtu_hlir.convert"(%7748) {node_name = "Cast_6312-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7750 = "dtu_hlir.convert"(%7749) {node_name = "Cast_6313-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7751 = dtu_hlir.constant  {node_name = "Constant_6314-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7752 = "dtu_hlir.mul"(%7733, %7751) {node_name = "Mul_6315-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7753 = dtu_hlir.constant  {node_name = "Constant_6316-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7754 = "dtu_hlir.unsqueeze"(%7750, %7753) {node_name = "Unsqueeze_6317-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7755 = dtu_hlir.constant  {node_name = "Constant_6318-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7756 = "dtu_hlir.unsqueeze"(%7730, %7755) {node_name = "Unsqueeze_6319-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7757 = dtu_hlir.constant  {node_name = "Constant_6320-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7758 = "dtu_hlir.unsqueeze"(%7752, %7757) {node_name = "Unsqueeze_6321-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7759 = "dtu_hlir.concatenate"(%7754, %7756, %7758) {dimension = 0 : i64, node_name = "Concat_6322-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7760 = "dtu_hlir.dynamic_reshape"(%7746, %7759) {allowzero = 0 : i64, node_name = "Reshape_6323-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %7761 = "dtu_hlir.dot_general"(%7760, %614) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6324-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7762 = "dtu_hlir.add"(%240, %7761) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6325-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7763 = "dtu_hlir.add"(%7762, %7558) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6326-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7764 = dtu_hlir.constant  {node_name = "ReduceMean_6327-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7765 = "dtu_hlir.reshape"(%7764) {node_name = "ReduceMean_6327-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7766 = "dtu_hlir.reduce"(%7763, %7765) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6327-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6327-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7767 = dtu_hlir.constant  {node_name = "ReduceMean_6327-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7768 = "dtu_hlir.unsqueeze"(%7766, %7767) {node_name = "ReduceMean_6327-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7769 = dtu_hlir.constant  {node_name = "ReduceMean_6327-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7770 = "dtu_hlir.broadcast_in_dim"(%7769) {node_name = "ReduceMean_6327-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7771 = "dtu_hlir.div"(%7768, %7770) {node_name = "ReduceMean_6327-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7772 = "dtu_hlir.sub"(%7763, %7771) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_6328-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7773 = dtu_hlir.constant  {node_name = "Constant_6329-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %7774 = "dtu_hlir.broadcast_in_dim"(%7773) {node_name = "Pow_6330-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %7775 = "dtu_hlir.pow"(%7772, %7774) {node_name = "Pow_6330-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7776 = dtu_hlir.constant  {node_name = "ReduceMean_6331-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7777 = "dtu_hlir.reshape"(%7776) {node_name = "ReduceMean_6331-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7778 = "dtu_hlir.reduce"(%7775, %7777) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6331-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6331-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7779 = dtu_hlir.constant  {node_name = "ReduceMean_6331-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7780 = "dtu_hlir.unsqueeze"(%7778, %7779) {node_name = "ReduceMean_6331-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7781 = dtu_hlir.constant  {node_name = "ReduceMean_6331-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7782 = "dtu_hlir.broadcast_in_dim"(%7781) {node_name = "ReduceMean_6331-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7783 = "dtu_hlir.div"(%7780, %7782) {node_name = "ReduceMean_6331-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7784 = dtu_hlir.constant  {node_name = "Constant_6332-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %7785 = "dtu_hlir.broadcast_in_dim"(%7784) {node_name = "Add_6333-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %7786 = "dtu_hlir.add"(%7783, %7785) {node_name = "Add_6333-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7787 = "dtu_hlir.sqrt"(%7786) {node_name = "Sqrt_6334-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7788 = "dtu_hlir.div"(%7772, %7787) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_6335-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7789 = "dtu_hlir.mul"(%7788, %245) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_6336-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7790 = "dtu_hlir.add"(%7789, %246) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6337-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7791 = "dtu_hlir.dot_general"(%7790, %615) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6338-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x5120xf32>) -> tensor<2x1024x5120xf32>
    %7792 = "dtu_hlir.add"(%238, %7791) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6339-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<5120xf32>, tensor<2x1024x5120xf32>) -> tensor<2x1024x5120xf32>
    %7793 = "dtu_hlir.shape"(%7792) {end = 2147483647 : i64, node_name = "Shape_6340-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>) -> tensor<3xi64>
    %7794 = dtu_hlir.constant  {node_name = "Constant_6341-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %7795 = "dtu_hlir.gather"(%7793, %7794) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6342-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7796 = dtu_hlir.constant  {node_name = "Constant_6343-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7797 = dtu_hlir.constant  {node_name = "Constant_6344-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %7798 = "dtu_hlir.add"(%7795, %7797) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_6345-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7799 = dtu_hlir.constant  {node_name = "Constant_6346-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %7800 = "dtu_hlir.div"(%7798, %7799) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_6347-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7801 = dtu_hlir.constant  {node_name = "Constant_6348-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %7802 = "dtu_hlir.mul"(%7800, %7801) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_6349-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7803 = dtu_hlir.constant  {node_name = "Slice_6350-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %7804 = "dtu_hlir.real_dynamic_slice"(%7792, %7796, %7802, %7803, %7794) {node_name = "Slice_6350-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %7805 = dtu_hlir.constant  {node_name = "Constant_6351-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %7806 = "dtu_hlir.mul"(%7800, %7805) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_6352-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %7807 = "dtu_hlir.shape"(%7802) {end = 2147483647 : i64, node_name = "Slice_6353-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %7808 = dtu_hlir.constant  {node_name = "Slice_6353-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %7809 = "dtu_hlir.dynamic_broadcast_in_dim"(%7808, %7807) {node_name = "Slice_6353-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7810 = "dtu_hlir.real_dynamic_slice"(%7792, %7802, %7806, %7809, %7794) {node_name = "Slice_6353-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %7811 = dtu_hlir.constant  {node_name = "Constant_6354-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %7812 = "dtu_hlir.broadcast_in_dim"(%7811) {node_name = "Div_6355-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %7813 = "dtu_hlir.div"(%7810, %7812) {node_name = "Div_6355-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7814 = "dtu_hlir.erf"(%7813) {node_name = "Erf_6356-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7815 = dtu_hlir.constant  {node_name = "Constant_6357-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %7816 = "dtu_hlir.broadcast_in_dim"(%7815) {node_name = "Add_6358-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %7817 = "dtu_hlir.add"(%7814, %7816) {node_name = "Add_6358-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7818 = "dtu_hlir.mul"(%7810, %7817) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_6359-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7819 = dtu_hlir.constant  {node_name = "Constant_6360-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %7820 = "dtu_hlir.broadcast_in_dim"(%7819) {node_name = "Mul_6361-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %7821 = "dtu_hlir.mul"(%7818, %7820) {node_name = "Mul_6361-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7822 = "dtu_hlir.mul"(%7804, %7821) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_6362-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %7823 = "dtu_hlir.dot_general"(%7822, %616) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6363-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2560x640xf32>) -> tensor<2x1024x640xf32>
    %7824 = "dtu_hlir.add"(%239, %7823) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6364-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7825 = "dtu_hlir.add"(%7824, %7763) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6365-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7826 = "dtu_hlir.dot_general"(%7825, %617) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6366-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7827 = "dtu_hlir.add"(%247, %7826) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6367-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7828 = dtu_hlir.constant  {node_name = "Constant_6368-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7829 = "dtu_hlir.unsqueeze"(%7323, %7828) {node_name = "Unsqueeze_6369-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7830 = dtu_hlir.constant  {node_name = "Constant_6370-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7831 = "dtu_hlir.unsqueeze"(%7326, %7830) {node_name = "Unsqueeze_6371-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7832 = dtu_hlir.constant  {node_name = "Constant_6372-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7833 = "dtu_hlir.unsqueeze"(%7329, %7832) {node_name = "Unsqueeze_6373-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7834 = dtu_hlir.constant  {node_name = "Constant_6374-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7835 = "dtu_hlir.unsqueeze"(%7341, %7834) {node_name = "Unsqueeze_6375-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7836 = "dtu_hlir.concatenate"(%7829, %7831, %7833, %7835) {dimension = 0 : i64, node_name = "Concat_6376-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7837 = "dtu_hlir.dynamic_reshape"(%7827, %7836) {allowzero = 0 : i64, node_name = "Reshape_6377-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x32x32x640xf32>
    %7838 = "dtu_hlir.transpose"(%7837) {node_name = "Transpose_6378-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>) -> tensor<2x640x32x32xf32>
    %7839 = "dtu_hlir.add"(%7838, %7320) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_6379-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7840 = "dtu_hlir.concatenate"(%7839, %2029) {dimension = 1 : i64, node_name = "Concat_6380-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x320x32x32xf32>) -> tensor<2x960x32x32xf32>
    %7841 = dtu_hlir.constant  {node_name = "Constant_6381-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %7842 = "dtu_hlir.dynamic_reshape"(%7840, %7841) {allowzero = 0 : i64, node_name = "Reshape_6382-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x960x32x32xf32>, tensor<3xi64>) -> tensor<2x32x30720xf32>
    %7843 = dtu_hlir.constant  {node_name = "Constant_6383-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %7844 = dtu_hlir.constant  {node_name = "Constant_6384-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %7845 = "dtu_hlir.instance_norm"(%7842, %7843, %7844) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_6385-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x30720xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x30720xf32>
    %7846 = "dtu_hlir.shape"(%7840) {end = 2147483647 : i64, node_name = "Shape_6386-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x960x32x32xf32>) -> tensor<4xi64>
    %7847 = "dtu_hlir.dynamic_reshape"(%7845, %7846) {allowzero = 0 : i64, node_name = "Reshape_6387-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x30720xf32>, tensor<4xi64>) -> tensor<2x960x32x32xf32>
    %7848 = "dtu_hlir.mul"(%7847, %618) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_6388-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x960x32x32xf32>, tensor<960x1x1xf32>) -> tensor<2x960x32x32xf32>
    %7849 = "dtu_hlir.add"(%7848, %619) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_6389-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x960x32x32xf32>, tensor<960x1x1xf32>) -> tensor<2x960x32x32xf32>
    %7850 = "dtu_hlir.sigmoid"(%7849) {node_name = "Sigmoid_6390-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x960x32x32xf32>) -> tensor<2x960x32x32xf32>
    %7851 = "dtu_hlir.mul"(%7849, %7850) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_6391-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x960x32x32xf32>, tensor<2x960x32x32xf32>) -> tensor<2x960x32x32xf32>
    %7852 = "dtu_hlir.conv_bias"(%7851, %276, %277) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6392-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x960x32x32xf32>, tensor<640x960x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %7853 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_6393-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %7854 = "dtu_hlir.mul"(%915, %7853) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6394-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %7855 = "dtu_hlir.transpose"(%278) {node_name = "Gemm_6395-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<640x1280xf32>) -> tensor<1280x640xf32>
    %7856 = "dtu_hlir.gemm"(%7854, %7855, %279) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_6395-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x640xf32>, tensor<640xf32>) -> tensor<2x640xf32>
    %7857 = dtu_hlir.constant  {node_name = "Constant_6396-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %7858 = "dtu_hlir.unsqueeze"(%7856, %7857) {node_name = "Unsqueeze_6397-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640xf32>, tensor<1xi64>) -> tensor<2x640x1xf32>
    %7859 = dtu_hlir.constant  {node_name = "Constant_6398-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %7860 = "dtu_hlir.unsqueeze"(%7858, %7859) {node_name = "Unsqueeze_6399-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x640x1xf32>, tensor<1xi64>) -> tensor<2x640x1x1xf32>
    %7861 = "dtu_hlir.add"(%7852, %7860) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_6400-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7862 = dtu_hlir.constant  {node_name = "Constant_6401-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %7863 = "dtu_hlir.dynamic_reshape"(%7861, %7862) {allowzero = 0 : i64, node_name = "Reshape_6402-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %7864 = dtu_hlir.constant  {node_name = "Constant_6403-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %7865 = dtu_hlir.constant  {node_name = "Constant_6404-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %7866 = "dtu_hlir.instance_norm"(%7863, %7864, %7865) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_6405-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %7867 = "dtu_hlir.shape"(%7861) {end = 2147483647 : i64, node_name = "Shape_6406-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7868 = "dtu_hlir.dynamic_reshape"(%7866, %7867) {allowzero = 0 : i64, node_name = "Reshape_6407-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %7869 = "dtu_hlir.mul"(%7868, %620) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_6408-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7870 = "dtu_hlir.add"(%7869, %621) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_6409-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7871 = "dtu_hlir.sigmoid"(%7870) {node_name = "Sigmoid_6410-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7872 = "dtu_hlir.mul"(%7870, %7871) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_6411-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7873 = "dtu_hlir.conv_bias"(%7872, %280, %281) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6412-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x32x32xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %7874 = "dtu_hlir.conv_bias"(%7840, %282, %283) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6413-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x960x32x32xf32>, tensor<640x960x1x1xf32>, tensor<640xf32>) -> tensor<2x640x32x32xf32>
    %7875 = "dtu_hlir.add"(%7874, %7873) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_6414-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7876 = dtu_hlir.constant  {node_name = "Constant_6415-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %7877 = "dtu_hlir.broadcast_in_dim"(%7876) {node_name = "Div_6416-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x640x32x32xf32>
    %7878 = "dtu_hlir.div"(%7875, %7877) {node_name = "Div_6416-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %7879 = "dtu_hlir.shape"(%7878) {end = 2147483647 : i64, node_name = "Shape_6417-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7880 = dtu_hlir.constant  {node_name = "Constant_6418-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7881 = "dtu_hlir.gather"(%7879, %7880) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6419-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7882 = "dtu_hlir.shape"(%7878) {end = 2147483647 : i64, node_name = "Shape_6420-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7883 = dtu_hlir.constant  {node_name = "Constant_6421-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7884 = "dtu_hlir.gather"(%7882, %7883) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6422-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7885 = "dtu_hlir.shape"(%7878) {end = 2147483647 : i64, node_name = "Shape_6423-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7886 = dtu_hlir.constant  {node_name = "Constant_6424-0", node_type = "Constant"} dense<3> : tensor<i64>
    %7887 = "dtu_hlir.gather"(%7885, %7886) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6425-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7888 = dtu_hlir.constant  {node_name = "Constant_6426-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %7889 = "dtu_hlir.dynamic_reshape"(%7878, %7888) {allowzero = 0 : i64, node_name = "Reshape_6427-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<3xi64>) -> tensor<2x32x20480xf32>
    %7890 = dtu_hlir.constant  {node_name = "Constant_6428-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %7891 = dtu_hlir.constant  {node_name = "Constant_6429-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %7892 = "dtu_hlir.instance_norm"(%7889, %7890, %7891) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_6430-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x20480xf32>
    %7893 = "dtu_hlir.shape"(%7878) {end = 2147483647 : i64, node_name = "Shape_6431-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7894 = "dtu_hlir.dynamic_reshape"(%7892, %7893) {allowzero = 0 : i64, node_name = "Reshape_6432-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x20480xf32>, tensor<4xi64>) -> tensor<2x640x32x32xf32>
    %7895 = "dtu_hlir.mul"(%7894, %622) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_6433-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7896 = "dtu_hlir.add"(%7895, %623) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_6434-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<640x1x1xf32>) -> tensor<2x640x32x32xf32>
    %7897 = "dtu_hlir.shape"(%7896) {end = 2147483647 : i64, node_name = "Shape_6435-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<4xi64>
    %7898 = dtu_hlir.constant  {node_name = "Constant_6436-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7899 = "dtu_hlir.gather"(%7897, %7898) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6437-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %7900 = "dtu_hlir.transpose"(%7896) {node_name = "Transpose_6438-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>) -> tensor<2x32x32x640xf32>
    %7901 = "dtu_hlir.mul"(%7884, %7887) {node_name = "Mul_6439-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7902 = dtu_hlir.constant  {node_name = "Constant_6440-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7903 = "dtu_hlir.unsqueeze"(%7881, %7902) {node_name = "Unsqueeze_6441-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7904 = dtu_hlir.constant  {node_name = "Constant_6442-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7905 = "dtu_hlir.unsqueeze"(%7901, %7904) {node_name = "Unsqueeze_6443-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7906 = dtu_hlir.constant  {node_name = "Constant_6444-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7907 = "dtu_hlir.unsqueeze"(%7899, %7906) {node_name = "Unsqueeze_6445-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7908 = "dtu_hlir.concatenate"(%7903, %7905, %7907) {dimension = 0 : i64, node_name = "Concat_6446-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7909 = "dtu_hlir.dynamic_reshape"(%7900, %7908) {allowzero = 0 : i64, node_name = "Reshape_6447-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %7910 = "dtu_hlir.dot_general"(%7909, %624) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6448-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7911 = "dtu_hlir.add"(%248, %7910) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6449-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7912 = dtu_hlir.constant  {node_name = "ReduceMean_6450-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7913 = "dtu_hlir.reshape"(%7912) {node_name = "ReduceMean_6450-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7914 = "dtu_hlir.reduce"(%7911, %7913) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6450-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6450-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7915 = dtu_hlir.constant  {node_name = "ReduceMean_6450-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7916 = "dtu_hlir.unsqueeze"(%7914, %7915) {node_name = "ReduceMean_6450-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7917 = dtu_hlir.constant  {node_name = "ReduceMean_6450-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7918 = "dtu_hlir.broadcast_in_dim"(%7917) {node_name = "ReduceMean_6450-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7919 = "dtu_hlir.div"(%7916, %7918) {node_name = "ReduceMean_6450-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7920 = "dtu_hlir.sub"(%7911, %7919) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_6451-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7921 = dtu_hlir.constant  {node_name = "Constant_6452-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %7922 = "dtu_hlir.broadcast_in_dim"(%7921) {node_name = "Pow_6453-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %7923 = "dtu_hlir.pow"(%7920, %7922) {node_name = "Pow_6453-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %7924 = dtu_hlir.constant  {node_name = "ReduceMean_6454-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %7925 = "dtu_hlir.reshape"(%7924) {node_name = "ReduceMean_6454-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %7926 = "dtu_hlir.reduce"(%7923, %7925) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6454-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6454-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %7927 = dtu_hlir.constant  {node_name = "ReduceMean_6454-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %7928 = "dtu_hlir.unsqueeze"(%7926, %7927) {node_name = "ReduceMean_6454-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %7929 = dtu_hlir.constant  {node_name = "ReduceMean_6454-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %7930 = "dtu_hlir.broadcast_in_dim"(%7929) {node_name = "ReduceMean_6454-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %7931 = "dtu_hlir.div"(%7928, %7930) {node_name = "ReduceMean_6454-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7932 = dtu_hlir.constant  {node_name = "Constant_6455-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %7933 = "dtu_hlir.broadcast_in_dim"(%7932) {node_name = "Add_6456-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %7934 = "dtu_hlir.add"(%7931, %7933) {node_name = "Add_6456-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7935 = "dtu_hlir.sqrt"(%7934) {node_name = "Sqrt_6457-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %7936 = "dtu_hlir.div"(%7920, %7935) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_6458-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %7937 = "dtu_hlir.mul"(%7936, %253) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_6459-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7938 = "dtu_hlir.add"(%7937, %254) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6460-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %7939 = "dtu_hlir.dot_general"(%7938, %625) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6461-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7940 = "dtu_hlir.dot_general"(%7938, %626) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6462-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7941 = "dtu_hlir.dot_general"(%7938, %627) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6463-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %7942 = "dtu_hlir.shape"(%7939) {end = 2147483647 : i64, node_name = "Shape_6464-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7943 = dtu_hlir.constant  {node_name = "Constant_6465-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7944 = "dtu_hlir.gather"(%7942, %7943) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6466-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7945 = "dtu_hlir.shape"(%7939) {end = 2147483647 : i64, node_name = "Shape_6467-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7946 = dtu_hlir.constant  {node_name = "Constant_6468-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7947 = "dtu_hlir.gather"(%7945, %7946) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6469-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7948 = "dtu_hlir.shape"(%7939) {end = 2147483647 : i64, node_name = "Shape_6470-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7949 = dtu_hlir.constant  {node_name = "Constant_6471-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7950 = "dtu_hlir.gather"(%7948, %7949) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6472-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7951 = dtu_hlir.constant  {node_name = "Constant_6473-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7952 = "dtu_hlir.div"(%7950, %7951) {node_name = "Div_6474-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7953 = "dtu_hlir.convert"(%7952) {node_name = "Cast_6475-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7954 = "dtu_hlir.convert"(%7953) {node_name = "Cast_6476-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7955 = dtu_hlir.constant  {node_name = "Constant_6477-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7956 = "dtu_hlir.unsqueeze"(%7944, %7955) {node_name = "Unsqueeze_6478-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7957 = dtu_hlir.constant  {node_name = "Constant_6479-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7958 = "dtu_hlir.unsqueeze"(%7947, %7957) {node_name = "Unsqueeze_6480-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7959 = dtu_hlir.constant  {node_name = "Constant_6481-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7960 = "dtu_hlir.unsqueeze"(%7954, %7959) {node_name = "Unsqueeze_6482-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7961 = "dtu_hlir.concatenate"(%7956, %7958, %740, %7960) {dimension = 0 : i64, node_name = "Concat_6483-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7962 = "dtu_hlir.dynamic_reshape"(%7939, %7961) {allowzero = 0 : i64, node_name = "Reshape_6484-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %7963 = "dtu_hlir.transpose"(%7962) {node_name = "Transpose_6485-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %7964 = dtu_hlir.constant  {node_name = "Constant_6486-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7965 = "dtu_hlir.mul"(%7944, %7964) {node_name = "Mul_6487-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7966 = dtu_hlir.constant  {node_name = "Constant_6488-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7967 = "dtu_hlir.div"(%7950, %7966) {node_name = "Div_6489-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7968 = "dtu_hlir.convert"(%7967) {node_name = "Cast_6490-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7969 = "dtu_hlir.convert"(%7968) {node_name = "Cast_6491-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7970 = dtu_hlir.constant  {node_name = "Constant_6492-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7971 = "dtu_hlir.unsqueeze"(%7965, %7970) {node_name = "Unsqueeze_6493-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7972 = dtu_hlir.constant  {node_name = "Constant_6494-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7973 = "dtu_hlir.unsqueeze"(%7947, %7972) {node_name = "Unsqueeze_6495-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7974 = dtu_hlir.constant  {node_name = "Constant_6496-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7975 = "dtu_hlir.unsqueeze"(%7969, %7974) {node_name = "Unsqueeze_6497-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7976 = "dtu_hlir.concatenate"(%7971, %7973, %7975) {dimension = 0 : i64, node_name = "Concat_6498-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %7977 = "dtu_hlir.dynamic_reshape"(%7963, %7976) {allowzero = 0 : i64, node_name = "Reshape_6499-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %7978 = "dtu_hlir.shape"(%7940) {end = 2147483647 : i64, node_name = "Shape_6500-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7979 = dtu_hlir.constant  {node_name = "Constant_6501-0", node_type = "Constant"} dense<0> : tensor<i64>
    %7980 = "dtu_hlir.gather"(%7978, %7979) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6502-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7981 = "dtu_hlir.shape"(%7940) {end = 2147483647 : i64, node_name = "Shape_6503-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7982 = dtu_hlir.constant  {node_name = "Constant_6504-0", node_type = "Constant"} dense<1> : tensor<i64>
    %7983 = "dtu_hlir.gather"(%7981, %7982) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6505-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7984 = "dtu_hlir.shape"(%7940) {end = 2147483647 : i64, node_name = "Shape_6506-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %7985 = dtu_hlir.constant  {node_name = "Constant_6507-0", node_type = "Constant"} dense<2> : tensor<i64>
    %7986 = "dtu_hlir.gather"(%7984, %7985) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6508-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %7987 = dtu_hlir.constant  {node_name = "Constant_6509-0", node_type = "Constant"} dense<10> : tensor<i64>
    %7988 = "dtu_hlir.div"(%7986, %7987) {node_name = "Div_6510-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %7989 = "dtu_hlir.convert"(%7988) {node_name = "Cast_6511-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7990 = "dtu_hlir.convert"(%7989) {node_name = "Cast_6512-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %7991 = dtu_hlir.constant  {node_name = "Constant_6513-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7992 = "dtu_hlir.unsqueeze"(%7980, %7991) {node_name = "Unsqueeze_6514-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7993 = dtu_hlir.constant  {node_name = "Constant_6515-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7994 = "dtu_hlir.unsqueeze"(%7983, %7993) {node_name = "Unsqueeze_6516-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7995 = dtu_hlir.constant  {node_name = "Constant_6517-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %7996 = "dtu_hlir.unsqueeze"(%7990, %7995) {node_name = "Unsqueeze_6518-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %7997 = "dtu_hlir.concatenate"(%7992, %7994, %739, %7996) {dimension = 0 : i64, node_name = "Concat_6519-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %7998 = "dtu_hlir.dynamic_reshape"(%7940, %7997) {allowzero = 0 : i64, node_name = "Reshape_6520-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %7999 = "dtu_hlir.transpose"(%7998) {node_name = "Transpose_6521-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %8000 = dtu_hlir.constant  {node_name = "Constant_6522-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8001 = "dtu_hlir.mul"(%7980, %8000) {node_name = "Mul_6523-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8002 = dtu_hlir.constant  {node_name = "Constant_6524-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8003 = "dtu_hlir.div"(%7986, %8002) {node_name = "Div_6525-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8004 = "dtu_hlir.convert"(%8003) {node_name = "Cast_6526-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8005 = "dtu_hlir.convert"(%8004) {node_name = "Cast_6527-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8006 = dtu_hlir.constant  {node_name = "Constant_6528-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8007 = "dtu_hlir.unsqueeze"(%8001, %8006) {node_name = "Unsqueeze_6529-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8008 = dtu_hlir.constant  {node_name = "Constant_6530-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8009 = "dtu_hlir.unsqueeze"(%7983, %8008) {node_name = "Unsqueeze_6531-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8010 = dtu_hlir.constant  {node_name = "Constant_6532-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8011 = "dtu_hlir.unsqueeze"(%8005, %8010) {node_name = "Unsqueeze_6533-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8012 = "dtu_hlir.concatenate"(%8007, %8009, %8011) {dimension = 0 : i64, node_name = "Concat_6534-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8013 = "dtu_hlir.dynamic_reshape"(%7999, %8012) {allowzero = 0 : i64, node_name = "Reshape_6535-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %8014 = "dtu_hlir.shape"(%7941) {end = 2147483647 : i64, node_name = "Shape_6536-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %8015 = dtu_hlir.constant  {node_name = "Constant_6537-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8016 = "dtu_hlir.gather"(%8014, %8015) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6538-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8017 = "dtu_hlir.shape"(%7941) {end = 2147483647 : i64, node_name = "Shape_6539-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %8018 = dtu_hlir.constant  {node_name = "Constant_6540-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8019 = "dtu_hlir.gather"(%8017, %8018) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6541-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8020 = "dtu_hlir.shape"(%7941) {end = 2147483647 : i64, node_name = "Shape_6542-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %8021 = dtu_hlir.constant  {node_name = "Constant_6543-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8022 = "dtu_hlir.gather"(%8020, %8021) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6544-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8023 = dtu_hlir.constant  {node_name = "Constant_6545-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8024 = "dtu_hlir.div"(%8022, %8023) {node_name = "Div_6546-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8025 = "dtu_hlir.convert"(%8024) {node_name = "Cast_6547-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8026 = "dtu_hlir.convert"(%8025) {node_name = "Cast_6548-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8027 = dtu_hlir.constant  {node_name = "Constant_6549-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8028 = "dtu_hlir.unsqueeze"(%8016, %8027) {node_name = "Unsqueeze_6550-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8029 = dtu_hlir.constant  {node_name = "Constant_6551-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8030 = "dtu_hlir.unsqueeze"(%8019, %8029) {node_name = "Unsqueeze_6552-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8031 = dtu_hlir.constant  {node_name = "Constant_6553-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8032 = "dtu_hlir.unsqueeze"(%8026, %8031) {node_name = "Unsqueeze_6554-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8033 = "dtu_hlir.concatenate"(%8028, %8030, %738, %8032) {dimension = 0 : i64, node_name = "Concat_6555-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8034 = "dtu_hlir.dynamic_reshape"(%7941, %8033) {allowzero = 0 : i64, node_name = "Reshape_6556-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %8035 = "dtu_hlir.transpose"(%8034) {node_name = "Transpose_6557-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %8036 = dtu_hlir.constant  {node_name = "Constant_6558-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8037 = "dtu_hlir.mul"(%8016, %8036) {node_name = "Mul_6559-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8038 = dtu_hlir.constant  {node_name = "Constant_6560-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8039 = "dtu_hlir.div"(%8022, %8038) {node_name = "Div_6561-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8040 = "dtu_hlir.convert"(%8039) {node_name = "Cast_6562-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8041 = "dtu_hlir.convert"(%8040) {node_name = "Cast_6563-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8042 = dtu_hlir.constant  {node_name = "Constant_6564-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8043 = "dtu_hlir.unsqueeze"(%8037, %8042) {node_name = "Unsqueeze_6565-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8044 = dtu_hlir.constant  {node_name = "Constant_6566-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8045 = "dtu_hlir.unsqueeze"(%8019, %8044) {node_name = "Unsqueeze_6567-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8046 = dtu_hlir.constant  {node_name = "Constant_6568-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8047 = "dtu_hlir.unsqueeze"(%8041, %8046) {node_name = "Unsqueeze_6569-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8048 = "dtu_hlir.concatenate"(%8043, %8045, %8047) {dimension = 0 : i64, node_name = "Concat_6570-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8049 = "dtu_hlir.dynamic_reshape"(%8035, %8048) {allowzero = 0 : i64, node_name = "Reshape_6571-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %8050 = "dtu_hlir.shape"(%7977) {end = 2147483647 : i64, node_name = "Shape_6572-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8051 = dtu_hlir.constant  {node_name = "Constant_6573-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8052 = "dtu_hlir.gather"(%8050, %8051) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6574-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8053 = "dtu_hlir.shape"(%7977) {end = 2147483647 : i64, node_name = "Shape_6575-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8054 = dtu_hlir.constant  {node_name = "Constant_6576-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8055 = "dtu_hlir.gather"(%8053, %8054) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6577-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8056 = "dtu_hlir.shape"(%8013) {end = 2147483647 : i64, node_name = "Shape_6578-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8057 = dtu_hlir.constant  {node_name = "Constant_6579-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8058 = "dtu_hlir.gather"(%8056, %8057) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6580-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8059 = dtu_hlir.constant  {node_name = "Constant_6581-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8060 = "dtu_hlir.unsqueeze"(%8052, %8059) {node_name = "Unsqueeze_6582-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8061 = dtu_hlir.constant  {node_name = "Constant_6583-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8062 = "dtu_hlir.unsqueeze"(%8055, %8061) {node_name = "Unsqueeze_6584-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8063 = dtu_hlir.constant  {node_name = "Constant_6585-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8064 = "dtu_hlir.unsqueeze"(%8058, %8063) {node_name = "Unsqueeze_6586-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8065 = "dtu_hlir.concatenate"(%8060, %8062, %8064) {dimension = 0 : i64, node_name = "Concat_6587-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8066 = dtu_hlir.constant  {node_name = "ConstantOfShape_6588-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %8067 = "dtu_hlir.dynamic_broadcast_in_dim"(%8066, %8065) {node_name = "ConstantOfShape_6588-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x1024xf32>
    %8068 = "dtu_hlir.transpose"(%8013) {node_name = "Transpose_6589-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<20x64x1024xf32>
    %8069 = "dtu_hlir.dot_general"(%7977, %8068) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6590-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x1024xf32>) -> tensor<20x1024x1024xf32>
    %8070 = "dtu_hlir.broadcast_in_dim"(%737) {node_name = "Mul_6591-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %8071 = "dtu_hlir.mul"(%8069, %8070) {node_name = "Mul_6591-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %8072 = "dtu_hlir.broadcast_in_dim"(%736) {node_name = "Mul_6592-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x1024xf32>
    %8073 = "dtu_hlir.mul"(%8067, %8072) {node_name = "Mul_6592-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %8074 = "dtu_hlir.add"(%8071, %8073) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6593-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %8075 = "dtu_hlir.softmax"(%8074) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6594-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %8076 = "dtu_hlir.convert"(%8075) {node_name = "Cast_6595-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>) -> tensor<20x1024x1024xf32>
    %8077 = "dtu_hlir.dot_general"(%8076, %8049) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6596-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x1024xf32>, tensor<20x1024x64xf32>) -> tensor<20x1024x64xf32>
    %8078 = "dtu_hlir.shape"(%8077) {end = 2147483647 : i64, node_name = "Shape_6597-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8079 = dtu_hlir.constant  {node_name = "Constant_6598-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8080 = "dtu_hlir.gather"(%8078, %8079) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6599-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8081 = "dtu_hlir.shape"(%8077) {end = 2147483647 : i64, node_name = "Shape_6600-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8082 = dtu_hlir.constant  {node_name = "Constant_6601-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8083 = "dtu_hlir.gather"(%8081, %8082) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6602-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8084 = "dtu_hlir.shape"(%8077) {end = 2147483647 : i64, node_name = "Shape_6603-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8085 = dtu_hlir.constant  {node_name = "Constant_6604-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8086 = "dtu_hlir.gather"(%8084, %8085) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6605-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8087 = dtu_hlir.constant  {node_name = "Constant_6606-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8088 = "dtu_hlir.div"(%8080, %8087) {node_name = "Div_6607-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8089 = "dtu_hlir.convert"(%8088) {node_name = "Cast_6608-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8090 = "dtu_hlir.convert"(%8089) {node_name = "Cast_6609-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8091 = dtu_hlir.constant  {node_name = "Constant_6610-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8092 = "dtu_hlir.unsqueeze"(%8090, %8091) {node_name = "Unsqueeze_6611-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8093 = dtu_hlir.constant  {node_name = "Constant_6612-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8094 = "dtu_hlir.unsqueeze"(%8083, %8093) {node_name = "Unsqueeze_6613-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8095 = dtu_hlir.constant  {node_name = "Constant_6614-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8096 = "dtu_hlir.unsqueeze"(%8086, %8095) {node_name = "Unsqueeze_6615-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8097 = "dtu_hlir.concatenate"(%8092, %735, %8094, %8096) {dimension = 0 : i64, node_name = "Concat_6616-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8098 = "dtu_hlir.dynamic_reshape"(%8077, %8097) {allowzero = 0 : i64, node_name = "Reshape_6617-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %8099 = "dtu_hlir.transpose"(%8098) {node_name = "Transpose_6618-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %8100 = dtu_hlir.constant  {node_name = "Constant_6619-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8101 = "dtu_hlir.div"(%8080, %8100) {node_name = "Div_6620-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8102 = "dtu_hlir.convert"(%8101) {node_name = "Cast_6621-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8103 = "dtu_hlir.convert"(%8102) {node_name = "Cast_6622-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8104 = dtu_hlir.constant  {node_name = "Constant_6623-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8105 = "dtu_hlir.mul"(%8086, %8104) {node_name = "Mul_6624-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8106 = dtu_hlir.constant  {node_name = "Constant_6625-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8107 = "dtu_hlir.unsqueeze"(%8103, %8106) {node_name = "Unsqueeze_6626-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8108 = dtu_hlir.constant  {node_name = "Constant_6627-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8109 = "dtu_hlir.unsqueeze"(%8083, %8108) {node_name = "Unsqueeze_6628-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8110 = dtu_hlir.constant  {node_name = "Constant_6629-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8111 = "dtu_hlir.unsqueeze"(%8105, %8110) {node_name = "Unsqueeze_6630-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8112 = "dtu_hlir.concatenate"(%8107, %8109, %8111) {dimension = 0 : i64, node_name = "Concat_6631-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8113 = "dtu_hlir.dynamic_reshape"(%8099, %8112) {allowzero = 0 : i64, node_name = "Reshape_6632-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %8114 = "dtu_hlir.dot_general"(%8113, %628) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6633-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %8115 = "dtu_hlir.add"(%249, %8114) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6634-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8116 = "dtu_hlir.add"(%8115, %7911) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6635-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8117 = dtu_hlir.constant  {node_name = "ReduceMean_6636-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8118 = "dtu_hlir.reshape"(%8117) {node_name = "ReduceMean_6636-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8119 = "dtu_hlir.reduce"(%8116, %8118) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6636-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6636-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %8120 = dtu_hlir.constant  {node_name = "ReduceMean_6636-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8121 = "dtu_hlir.unsqueeze"(%8119, %8120) {node_name = "ReduceMean_6636-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %8122 = dtu_hlir.constant  {node_name = "ReduceMean_6636-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %8123 = "dtu_hlir.broadcast_in_dim"(%8122) {node_name = "ReduceMean_6636-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %8124 = "dtu_hlir.div"(%8121, %8123) {node_name = "ReduceMean_6636-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8125 = "dtu_hlir.sub"(%8116, %8124) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_6637-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %8126 = dtu_hlir.constant  {node_name = "Constant_6638-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %8127 = "dtu_hlir.broadcast_in_dim"(%8126) {node_name = "Pow_6639-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %8128 = "dtu_hlir.pow"(%8125, %8127) {node_name = "Pow_6639-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8129 = dtu_hlir.constant  {node_name = "ReduceMean_6640-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8130 = "dtu_hlir.reshape"(%8129) {node_name = "ReduceMean_6640-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8131 = "dtu_hlir.reduce"(%8128, %8130) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6640-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6640-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %8132 = dtu_hlir.constant  {node_name = "ReduceMean_6640-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8133 = "dtu_hlir.unsqueeze"(%8131, %8132) {node_name = "ReduceMean_6640-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %8134 = dtu_hlir.constant  {node_name = "ReduceMean_6640-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %8135 = "dtu_hlir.broadcast_in_dim"(%8134) {node_name = "ReduceMean_6640-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %8136 = "dtu_hlir.div"(%8133, %8135) {node_name = "ReduceMean_6640-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8137 = dtu_hlir.constant  {node_name = "Constant_6641-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %8138 = "dtu_hlir.broadcast_in_dim"(%8137) {node_name = "Add_6642-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %8139 = "dtu_hlir.add"(%8136, %8138) {node_name = "Add_6642-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8140 = "dtu_hlir.sqrt"(%8139) {node_name = "Sqrt_6643-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8141 = "dtu_hlir.div"(%8125, %8140) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_6644-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %8142 = "dtu_hlir.mul"(%8141, %255) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_6645-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %8143 = "dtu_hlir.add"(%8142, %256) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6646-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %8144 = "dtu_hlir.dot_general"(%8143, %629) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6647-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %8145 = "dtu_hlir.dot_general"(%arg2, %630) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6648-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %8146 = "dtu_hlir.dot_general"(%arg2, %631) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6649-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x640xf32>) -> tensor<2x77x640xf32>
    %8147 = "dtu_hlir.shape"(%8144) {end = 2147483647 : i64, node_name = "Shape_6650-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %8148 = dtu_hlir.constant  {node_name = "Constant_6651-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8149 = "dtu_hlir.gather"(%8147, %8148) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6652-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8150 = "dtu_hlir.shape"(%8144) {end = 2147483647 : i64, node_name = "Shape_6653-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %8151 = dtu_hlir.constant  {node_name = "Constant_6654-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8152 = "dtu_hlir.gather"(%8150, %8151) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6655-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8153 = "dtu_hlir.shape"(%8144) {end = 2147483647 : i64, node_name = "Shape_6656-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>) -> tensor<3xi64>
    %8154 = dtu_hlir.constant  {node_name = "Constant_6657-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8155 = "dtu_hlir.gather"(%8153, %8154) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6658-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8156 = dtu_hlir.constant  {node_name = "Constant_6659-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8157 = "dtu_hlir.div"(%8155, %8156) {node_name = "Div_6660-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8158 = "dtu_hlir.convert"(%8157) {node_name = "Cast_6661-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8159 = "dtu_hlir.convert"(%8158) {node_name = "Cast_6662-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8160 = dtu_hlir.constant  {node_name = "Constant_6663-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8161 = "dtu_hlir.unsqueeze"(%8149, %8160) {node_name = "Unsqueeze_6664-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8162 = dtu_hlir.constant  {node_name = "Constant_6665-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8163 = "dtu_hlir.unsqueeze"(%8152, %8162) {node_name = "Unsqueeze_6666-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8164 = dtu_hlir.constant  {node_name = "Constant_6667-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8165 = "dtu_hlir.unsqueeze"(%8159, %8164) {node_name = "Unsqueeze_6668-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8166 = "dtu_hlir.concatenate"(%8161, %8163, %734, %8165) {dimension = 0 : i64, node_name = "Concat_6669-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8167 = "dtu_hlir.dynamic_reshape"(%8144, %8166) {allowzero = 0 : i64, node_name = "Reshape_6670-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x1024x10x64xf32>
    %8168 = "dtu_hlir.transpose"(%8167) {node_name = "Transpose_6671-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>) -> tensor<2x10x1024x64xf32>
    %8169 = dtu_hlir.constant  {node_name = "Constant_6672-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8170 = "dtu_hlir.mul"(%8149, %8169) {node_name = "Mul_6673-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8171 = dtu_hlir.constant  {node_name = "Constant_6674-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8172 = "dtu_hlir.div"(%8155, %8171) {node_name = "Div_6675-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8173 = "dtu_hlir.convert"(%8172) {node_name = "Cast_6676-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8174 = "dtu_hlir.convert"(%8173) {node_name = "Cast_6677-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8175 = dtu_hlir.constant  {node_name = "Constant_6678-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8176 = "dtu_hlir.unsqueeze"(%8170, %8175) {node_name = "Unsqueeze_6679-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8177 = dtu_hlir.constant  {node_name = "Constant_6680-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8178 = "dtu_hlir.unsqueeze"(%8152, %8177) {node_name = "Unsqueeze_6681-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8179 = dtu_hlir.constant  {node_name = "Constant_6682-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8180 = "dtu_hlir.unsqueeze"(%8174, %8179) {node_name = "Unsqueeze_6683-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8181 = "dtu_hlir.concatenate"(%8176, %8178, %8180) {dimension = 0 : i64, node_name = "Concat_6684-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8182 = "dtu_hlir.dynamic_reshape"(%8168, %8181) {allowzero = 0 : i64, node_name = "Reshape_6685-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>, tensor<3xi64>) -> tensor<20x1024x64xf32>
    %8183 = "dtu_hlir.shape"(%8145) {end = 2147483647 : i64, node_name = "Shape_6686-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %8184 = dtu_hlir.constant  {node_name = "Constant_6687-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8185 = "dtu_hlir.gather"(%8183, %8184) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6688-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8186 = "dtu_hlir.shape"(%8145) {end = 2147483647 : i64, node_name = "Shape_6689-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %8187 = dtu_hlir.constant  {node_name = "Constant_6690-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8188 = "dtu_hlir.gather"(%8186, %8187) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6691-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8189 = "dtu_hlir.shape"(%8145) {end = 2147483647 : i64, node_name = "Shape_6692-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %8190 = dtu_hlir.constant  {node_name = "Constant_6693-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8191 = "dtu_hlir.gather"(%8189, %8190) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6694-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8192 = dtu_hlir.constant  {node_name = "Constant_6695-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8193 = "dtu_hlir.div"(%8191, %8192) {node_name = "Div_6696-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8194 = "dtu_hlir.convert"(%8193) {node_name = "Cast_6697-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8195 = "dtu_hlir.convert"(%8194) {node_name = "Cast_6698-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8196 = dtu_hlir.constant  {node_name = "Constant_6699-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8197 = "dtu_hlir.unsqueeze"(%8185, %8196) {node_name = "Unsqueeze_6700-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8198 = dtu_hlir.constant  {node_name = "Constant_6701-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8199 = "dtu_hlir.unsqueeze"(%8188, %8198) {node_name = "Unsqueeze_6702-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8200 = dtu_hlir.constant  {node_name = "Constant_6703-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8201 = "dtu_hlir.unsqueeze"(%8195, %8200) {node_name = "Unsqueeze_6704-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8202 = "dtu_hlir.concatenate"(%8197, %8199, %733, %8201) {dimension = 0 : i64, node_name = "Concat_6705-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8203 = "dtu_hlir.dynamic_reshape"(%8145, %8202) {allowzero = 0 : i64, node_name = "Reshape_6706-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %8204 = "dtu_hlir.transpose"(%8203) {node_name = "Transpose_6707-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %8205 = dtu_hlir.constant  {node_name = "Constant_6708-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8206 = "dtu_hlir.mul"(%8185, %8205) {node_name = "Mul_6709-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8207 = dtu_hlir.constant  {node_name = "Constant_6710-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8208 = "dtu_hlir.div"(%8191, %8207) {node_name = "Div_6711-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8209 = "dtu_hlir.convert"(%8208) {node_name = "Cast_6712-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8210 = "dtu_hlir.convert"(%8209) {node_name = "Cast_6713-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8211 = dtu_hlir.constant  {node_name = "Constant_6714-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8212 = "dtu_hlir.unsqueeze"(%8206, %8211) {node_name = "Unsqueeze_6715-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8213 = dtu_hlir.constant  {node_name = "Constant_6716-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8214 = "dtu_hlir.unsqueeze"(%8188, %8213) {node_name = "Unsqueeze_6717-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8215 = dtu_hlir.constant  {node_name = "Constant_6718-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8216 = "dtu_hlir.unsqueeze"(%8210, %8215) {node_name = "Unsqueeze_6719-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8217 = "dtu_hlir.concatenate"(%8212, %8214, %8216) {dimension = 0 : i64, node_name = "Concat_6720-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8218 = "dtu_hlir.dynamic_reshape"(%8204, %8217) {allowzero = 0 : i64, node_name = "Reshape_6721-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %8219 = "dtu_hlir.shape"(%8146) {end = 2147483647 : i64, node_name = "Shape_6722-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %8220 = dtu_hlir.constant  {node_name = "Constant_6723-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8221 = "dtu_hlir.gather"(%8219, %8220) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6724-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8222 = "dtu_hlir.shape"(%8146) {end = 2147483647 : i64, node_name = "Shape_6725-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %8223 = dtu_hlir.constant  {node_name = "Constant_6726-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8224 = "dtu_hlir.gather"(%8222, %8223) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6727-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8225 = "dtu_hlir.shape"(%8146) {end = 2147483647 : i64, node_name = "Shape_6728-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x640xf32>) -> tensor<3xi64>
    %8226 = dtu_hlir.constant  {node_name = "Constant_6729-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8227 = "dtu_hlir.gather"(%8225, %8226) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6730-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8228 = dtu_hlir.constant  {node_name = "Constant_6731-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8229 = "dtu_hlir.div"(%8227, %8228) {node_name = "Div_6732-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8230 = "dtu_hlir.convert"(%8229) {node_name = "Cast_6733-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8231 = "dtu_hlir.convert"(%8230) {node_name = "Cast_6734-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8232 = dtu_hlir.constant  {node_name = "Constant_6735-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8233 = "dtu_hlir.unsqueeze"(%8221, %8232) {node_name = "Unsqueeze_6736-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8234 = dtu_hlir.constant  {node_name = "Constant_6737-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8235 = "dtu_hlir.unsqueeze"(%8224, %8234) {node_name = "Unsqueeze_6738-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8236 = dtu_hlir.constant  {node_name = "Constant_6739-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8237 = "dtu_hlir.unsqueeze"(%8231, %8236) {node_name = "Unsqueeze_6740-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8238 = "dtu_hlir.concatenate"(%8233, %8235, %732, %8237) {dimension = 0 : i64, node_name = "Concat_6741-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8239 = "dtu_hlir.dynamic_reshape"(%8146, %8238) {allowzero = 0 : i64, node_name = "Reshape_6742-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x640xf32>, tensor<4xi64>) -> tensor<2x77x10x64xf32>
    %8240 = "dtu_hlir.transpose"(%8239) {node_name = "Transpose_6743-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x10x64xf32>) -> tensor<2x10x77x64xf32>
    %8241 = dtu_hlir.constant  {node_name = "Constant_6744-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8242 = "dtu_hlir.mul"(%8221, %8241) {node_name = "Mul_6745-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8243 = dtu_hlir.constant  {node_name = "Constant_6746-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8244 = "dtu_hlir.div"(%8227, %8243) {node_name = "Div_6747-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8245 = "dtu_hlir.convert"(%8244) {node_name = "Cast_6748-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8246 = "dtu_hlir.convert"(%8245) {node_name = "Cast_6749-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8247 = dtu_hlir.constant  {node_name = "Constant_6750-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8248 = "dtu_hlir.unsqueeze"(%8242, %8247) {node_name = "Unsqueeze_6751-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8249 = dtu_hlir.constant  {node_name = "Constant_6752-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8250 = "dtu_hlir.unsqueeze"(%8224, %8249) {node_name = "Unsqueeze_6753-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8251 = dtu_hlir.constant  {node_name = "Constant_6754-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8252 = "dtu_hlir.unsqueeze"(%8246, %8251) {node_name = "Unsqueeze_6755-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8253 = "dtu_hlir.concatenate"(%8248, %8250, %8252) {dimension = 0 : i64, node_name = "Concat_6756-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8254 = "dtu_hlir.dynamic_reshape"(%8240, %8253) {allowzero = 0 : i64, node_name = "Reshape_6757-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x10x77x64xf32>, tensor<3xi64>) -> tensor<20x77x64xf32>
    %8255 = "dtu_hlir.shape"(%8182) {end = 2147483647 : i64, node_name = "Shape_6758-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8256 = dtu_hlir.constant  {node_name = "Constant_6759-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8257 = "dtu_hlir.gather"(%8255, %8256) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6760-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8258 = "dtu_hlir.shape"(%8182) {end = 2147483647 : i64, node_name = "Shape_6761-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8259 = dtu_hlir.constant  {node_name = "Constant_6762-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8260 = "dtu_hlir.gather"(%8258, %8259) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6763-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8261 = "dtu_hlir.shape"(%8218) {end = 2147483647 : i64, node_name = "Shape_6764-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<3xi64>
    %8262 = dtu_hlir.constant  {node_name = "Constant_6765-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8263 = "dtu_hlir.gather"(%8261, %8262) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6766-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8264 = dtu_hlir.constant  {node_name = "Constant_6767-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8265 = "dtu_hlir.unsqueeze"(%8257, %8264) {node_name = "Unsqueeze_6768-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8266 = dtu_hlir.constant  {node_name = "Constant_6769-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8267 = "dtu_hlir.unsqueeze"(%8260, %8266) {node_name = "Unsqueeze_6770-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8268 = dtu_hlir.constant  {node_name = "Constant_6771-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8269 = "dtu_hlir.unsqueeze"(%8263, %8268) {node_name = "Unsqueeze_6772-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8270 = "dtu_hlir.concatenate"(%8265, %8267, %8269) {dimension = 0 : i64, node_name = "Concat_6773-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8271 = dtu_hlir.constant  {node_name = "ConstantOfShape_6774-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %8272 = "dtu_hlir.dynamic_broadcast_in_dim"(%8271, %8270) {node_name = "ConstantOfShape_6774-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<20x1024x77xf32>
    %8273 = "dtu_hlir.transpose"(%8218) {node_name = "Transpose_6775-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<20x77x64xf32>) -> tensor<20x64x77xf32>
    %8274 = "dtu_hlir.dot_general"(%8182, %8273) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6776-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<20x64x77xf32>) -> tensor<20x1024x77xf32>
    %8275 = "dtu_hlir.broadcast_in_dim"(%731) {node_name = "Mul_6777-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %8276 = "dtu_hlir.mul"(%8274, %8275) {node_name = "Mul_6777-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %8277 = "dtu_hlir.broadcast_in_dim"(%730) {node_name = "Mul_6778-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<20x1024x77xf32>
    %8278 = "dtu_hlir.mul"(%8272, %8277) {node_name = "Mul_6778-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %8279 = "dtu_hlir.add"(%8276, %8278) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6779-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %8280 = "dtu_hlir.softmax"(%8279) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6780-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %8281 = "dtu_hlir.convert"(%8280) {node_name = "Cast_6781-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>) -> tensor<20x1024x77xf32>
    %8282 = "dtu_hlir.dot_general"(%8281, %8254) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6782-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<20x1024x77xf32>, tensor<20x77x64xf32>) -> tensor<20x1024x64xf32>
    %8283 = "dtu_hlir.shape"(%8282) {end = 2147483647 : i64, node_name = "Shape_6783-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8284 = dtu_hlir.constant  {node_name = "Constant_6784-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8285 = "dtu_hlir.gather"(%8283, %8284) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6785-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8286 = "dtu_hlir.shape"(%8282) {end = 2147483647 : i64, node_name = "Shape_6786-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8287 = dtu_hlir.constant  {node_name = "Constant_6787-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8288 = "dtu_hlir.gather"(%8286, %8287) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6788-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8289 = "dtu_hlir.shape"(%8282) {end = 2147483647 : i64, node_name = "Shape_6789-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>) -> tensor<3xi64>
    %8290 = dtu_hlir.constant  {node_name = "Constant_6790-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8291 = "dtu_hlir.gather"(%8289, %8290) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6791-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8292 = dtu_hlir.constant  {node_name = "Constant_6792-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8293 = "dtu_hlir.div"(%8285, %8292) {node_name = "Div_6793-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8294 = "dtu_hlir.convert"(%8293) {node_name = "Cast_6794-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8295 = "dtu_hlir.convert"(%8294) {node_name = "Cast_6795-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8296 = dtu_hlir.constant  {node_name = "Constant_6796-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8297 = "dtu_hlir.unsqueeze"(%8295, %8296) {node_name = "Unsqueeze_6797-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8298 = dtu_hlir.constant  {node_name = "Constant_6798-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8299 = "dtu_hlir.unsqueeze"(%8288, %8298) {node_name = "Unsqueeze_6799-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8300 = dtu_hlir.constant  {node_name = "Constant_6800-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8301 = "dtu_hlir.unsqueeze"(%8291, %8300) {node_name = "Unsqueeze_6801-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8302 = "dtu_hlir.concatenate"(%8297, %729, %8299, %8301) {dimension = 0 : i64, node_name = "Concat_6802-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8303 = "dtu_hlir.dynamic_reshape"(%8282, %8302) {allowzero = 0 : i64, node_name = "Reshape_6803-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<20x1024x64xf32>, tensor<4xi64>) -> tensor<2x10x1024x64xf32>
    %8304 = "dtu_hlir.transpose"(%8303) {node_name = "Transpose_6804-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x10x1024x64xf32>) -> tensor<2x1024x10x64xf32>
    %8305 = dtu_hlir.constant  {node_name = "Constant_6805-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8306 = "dtu_hlir.div"(%8285, %8305) {node_name = "Div_6806-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8307 = "dtu_hlir.convert"(%8306) {node_name = "Cast_6807-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8308 = "dtu_hlir.convert"(%8307) {node_name = "Cast_6808-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8309 = dtu_hlir.constant  {node_name = "Constant_6809-0", node_type = "Constant"} dense<10> : tensor<i64>
    %8310 = "dtu_hlir.mul"(%8291, %8309) {node_name = "Mul_6810-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8311 = dtu_hlir.constant  {node_name = "Constant_6811-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8312 = "dtu_hlir.unsqueeze"(%8308, %8311) {node_name = "Unsqueeze_6812-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8313 = dtu_hlir.constant  {node_name = "Constant_6813-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8314 = "dtu_hlir.unsqueeze"(%8288, %8313) {node_name = "Unsqueeze_6814-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8315 = dtu_hlir.constant  {node_name = "Constant_6815-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8316 = "dtu_hlir.unsqueeze"(%8310, %8315) {node_name = "Unsqueeze_6816-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8317 = "dtu_hlir.concatenate"(%8312, %8314, %8316) {dimension = 0 : i64, node_name = "Concat_6817-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8318 = "dtu_hlir.dynamic_reshape"(%8304, %8317) {allowzero = 0 : i64, node_name = "Reshape_6818-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x10x64xf32>, tensor<3xi64>) -> tensor<2x1024x640xf32>
    %8319 = "dtu_hlir.dot_general"(%8318, %632) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6819-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %8320 = "dtu_hlir.add"(%252, %8319) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6820-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8321 = "dtu_hlir.add"(%8320, %8116) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6821-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8322 = dtu_hlir.constant  {node_name = "ReduceMean_6822-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8323 = "dtu_hlir.reshape"(%8322) {node_name = "ReduceMean_6822-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8324 = "dtu_hlir.reduce"(%8321, %8323) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6822-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6822-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %8325 = dtu_hlir.constant  {node_name = "ReduceMean_6822-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8326 = "dtu_hlir.unsqueeze"(%8324, %8325) {node_name = "ReduceMean_6822-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %8327 = dtu_hlir.constant  {node_name = "ReduceMean_6822-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %8328 = "dtu_hlir.broadcast_in_dim"(%8327) {node_name = "ReduceMean_6822-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %8329 = "dtu_hlir.div"(%8326, %8328) {node_name = "ReduceMean_6822-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8330 = "dtu_hlir.sub"(%8321, %8329) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_6823-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %8331 = dtu_hlir.constant  {node_name = "Constant_6824-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %8332 = "dtu_hlir.broadcast_in_dim"(%8331) {node_name = "Pow_6825-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x640xf32>
    %8333 = "dtu_hlir.pow"(%8330, %8332) {node_name = "Pow_6825-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8334 = dtu_hlir.constant  {node_name = "ReduceMean_6826-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8335 = "dtu_hlir.reshape"(%8334) {node_name = "ReduceMean_6826-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8336 = "dtu_hlir.reduce"(%8333, %8335) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6826-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6826-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<f32>) -> tensor<2x1024xf32>
    %8337 = dtu_hlir.constant  {node_name = "ReduceMean_6826-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8338 = "dtu_hlir.unsqueeze"(%8336, %8337) {node_name = "ReduceMean_6826-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024xf32>, tensor<1xi64>) -> tensor<2x1024x1xf32>
    %8339 = dtu_hlir.constant  {node_name = "ReduceMean_6826-Const-6", node_type = "ReduceMean"} dense<6.400000e+02> : tensor<1xf32>
    %8340 = "dtu_hlir.broadcast_in_dim"(%8339) {node_name = "ReduceMean_6826-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x1024x1xf32>
    %8341 = "dtu_hlir.div"(%8338, %8340) {node_name = "ReduceMean_6826-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8342 = dtu_hlir.constant  {node_name = "Constant_6827-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %8343 = "dtu_hlir.broadcast_in_dim"(%8342) {node_name = "Add_6828-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x1xf32>
    %8344 = "dtu_hlir.add"(%8341, %8343) {node_name = "Add_6828-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8345 = "dtu_hlir.sqrt"(%8344) {node_name = "Sqrt_6829-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x1024x1xf32>) -> tensor<2x1024x1xf32>
    %8346 = "dtu_hlir.div"(%8330, %8345) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_6830-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x1xf32>) -> tensor<2x1024x640xf32>
    %8347 = "dtu_hlir.mul"(%8346, %257) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_6831-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %8348 = "dtu_hlir.add"(%8347, %258) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6832-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf32>
    %8349 = "dtu_hlir.dot_general"(%8348, %633) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6833-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x5120xf32>) -> tensor<2x1024x5120xf32>
    %8350 = "dtu_hlir.add"(%250, %8349) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6834-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<5120xf32>, tensor<2x1024x5120xf32>) -> tensor<2x1024x5120xf32>
    %8351 = "dtu_hlir.shape"(%8350) {end = 2147483647 : i64, node_name = "Shape_6835-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>) -> tensor<3xi64>
    %8352 = dtu_hlir.constant  {node_name = "Constant_6836-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %8353 = "dtu_hlir.gather"(%8351, %8352) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6837-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8354 = dtu_hlir.constant  {node_name = "Constant_6838-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8355 = dtu_hlir.constant  {node_name = "Constant_6839-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %8356 = "dtu_hlir.add"(%8353, %8355) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_6840-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8357 = dtu_hlir.constant  {node_name = "Constant_6841-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %8358 = "dtu_hlir.div"(%8356, %8357) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_6842-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8359 = dtu_hlir.constant  {node_name = "Constant_6843-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %8360 = "dtu_hlir.mul"(%8358, %8359) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_6844-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8361 = dtu_hlir.constant  {node_name = "Slice_6845-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %8362 = "dtu_hlir.real_dynamic_slice"(%8350, %8354, %8360, %8361, %8352) {node_name = "Slice_6845-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %8363 = dtu_hlir.constant  {node_name = "Constant_6846-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %8364 = "dtu_hlir.mul"(%8358, %8363) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_6847-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8365 = "dtu_hlir.shape"(%8360) {end = 2147483647 : i64, node_name = "Slice_6848-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %8366 = dtu_hlir.constant  {node_name = "Slice_6848-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %8367 = "dtu_hlir.dynamic_broadcast_in_dim"(%8366, %8365) {node_name = "Slice_6848-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8368 = "dtu_hlir.real_dynamic_slice"(%8350, %8360, %8364, %8367, %8352) {node_name = "Slice_6848-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x1024x5120xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x1024x2560xf32>
    %8369 = dtu_hlir.constant  {node_name = "Constant_6849-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %8370 = "dtu_hlir.broadcast_in_dim"(%8369) {node_name = "Div_6850-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %8371 = "dtu_hlir.div"(%8368, %8370) {node_name = "Div_6850-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %8372 = "dtu_hlir.erf"(%8371) {node_name = "Erf_6851-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %8373 = dtu_hlir.constant  {node_name = "Constant_6852-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %8374 = "dtu_hlir.broadcast_in_dim"(%8373) {node_name = "Add_6853-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %8375 = "dtu_hlir.add"(%8372, %8374) {node_name = "Add_6853-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %8376 = "dtu_hlir.mul"(%8368, %8375) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_6854-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %8377 = dtu_hlir.constant  {node_name = "Constant_6855-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %8378 = "dtu_hlir.broadcast_in_dim"(%8377) {node_name = "Mul_6856-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x1024x2560xf32>
    %8379 = "dtu_hlir.mul"(%8376, %8378) {node_name = "Mul_6856-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %8380 = "dtu_hlir.mul"(%8362, %8379) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_6857-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2x1024x2560xf32>) -> tensor<2x1024x2560xf32>
    %8381 = "dtu_hlir.dot_general"(%8380, %634) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6858-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x2560xf32>, tensor<2560x640xf32>) -> tensor<2x1024x640xf32>
    %8382 = "dtu_hlir.add"(%251, %8381) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6859-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8383 = "dtu_hlir.add"(%8382, %8321) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_6860-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8384 = "dtu_hlir.dot_general"(%8383, %635) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6861-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<640x640xf32>) -> tensor<2x1024x640xf32>
    %8385 = "dtu_hlir.add"(%259, %8384) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6862-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<640xf32>, tensor<2x1024x640xf32>) -> tensor<2x1024x640xf32>
    %8386 = dtu_hlir.constant  {node_name = "Constant_6863-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8387 = "dtu_hlir.unsqueeze"(%7881, %8386) {node_name = "Unsqueeze_6864-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8388 = dtu_hlir.constant  {node_name = "Constant_6865-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8389 = "dtu_hlir.unsqueeze"(%7884, %8388) {node_name = "Unsqueeze_6866-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8390 = dtu_hlir.constant  {node_name = "Constant_6867-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8391 = "dtu_hlir.unsqueeze"(%7887, %8390) {node_name = "Unsqueeze_6868-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8392 = dtu_hlir.constant  {node_name = "Constant_6869-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8393 = "dtu_hlir.unsqueeze"(%7899, %8392) {node_name = "Unsqueeze_6870-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8394 = "dtu_hlir.concatenate"(%8387, %8389, %8391, %8393) {dimension = 0 : i64, node_name = "Concat_6871-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8395 = "dtu_hlir.dynamic_reshape"(%8385, %8394) {allowzero = 0 : i64, node_name = "Reshape_6872-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x1024x640xf32>, tensor<4xi64>) -> tensor<2x32x32x640xf32>
    %8396 = "dtu_hlir.transpose"(%8395) {node_name = "Transpose_6873-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x32x32x640xf32>) -> tensor<2x640x32x32xf32>
    %8397 = "dtu_hlir.add"(%8396, %7878) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_6874-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<2x640x32x32xf32>) -> tensor<2x640x32x32xf32>
    %8398 = dtu_hlir.constant  {node_name = "Resize_6875-Const-2", node_type = "Resize"} dense<[]> : tensor<0xi64>
    %8399 = dtu_hlir.constant  {node_name = "Resize_6875-Const-3", node_type = "Resize"} dense<[0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00]> : tensor<8xf32>
    %8400 = "dtu_hlir.resize"(%8397, %8399, %728, %8398) {coordinate_transformation_mode = 1 : i64, cubic_coeff_a = -7.500000e-01 : f32, exclude_outside = false, extrapolation_value = 0.000000e+00 : f32, mode = 0 : i64, nearest_mode = 3 : i64, node_name = "Resize_6875-4", node_type = "Resize", resize_dimensions = dense<[-2, -1]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<2x640x32x32xf32>, tensor<8xf32>, tensor<4xf32>, tensor<0xi64>) -> tensor<2x640x64x64xf32>
    %8401 = "dtu_hlir.conv_bias"(%8400, %284, %285) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6876-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x64x64xf32>, tensor<640x640x3x3xf32>, tensor<640xf32>) -> tensor<2x640x64x64xf32>
    %8402 = "dtu_hlir.concatenate"(%8401, %2028) {dimension = 1 : i64, node_name = "Concat_6877-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x960x64x64xf32>
    %8403 = dtu_hlir.constant  {node_name = "Constant_6878-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %8404 = "dtu_hlir.dynamic_reshape"(%8402, %8403) {allowzero = 0 : i64, node_name = "Reshape_6879-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x960x64x64xf32>, tensor<3xi64>) -> tensor<2x32x122880xf32>
    %8405 = dtu_hlir.constant  {node_name = "Constant_6880-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %8406 = dtu_hlir.constant  {node_name = "Constant_6881-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %8407 = "dtu_hlir.instance_norm"(%8404, %8405, %8406) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_6882-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x122880xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x122880xf32>
    %8408 = "dtu_hlir.shape"(%8402) {end = 2147483647 : i64, node_name = "Shape_6883-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x960x64x64xf32>) -> tensor<4xi64>
    %8409 = "dtu_hlir.dynamic_reshape"(%8407, %8408) {allowzero = 0 : i64, node_name = "Reshape_6884-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x122880xf32>, tensor<4xi64>) -> tensor<2x960x64x64xf32>
    %8410 = "dtu_hlir.mul"(%8409, %636) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_6885-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x960x64x64xf32>, tensor<960x1x1xf32>) -> tensor<2x960x64x64xf32>
    %8411 = "dtu_hlir.add"(%8410, %637) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_6886-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x960x64x64xf32>, tensor<960x1x1xf32>) -> tensor<2x960x64x64xf32>
    %8412 = "dtu_hlir.sigmoid"(%8411) {node_name = "Sigmoid_6887-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x960x64x64xf32>) -> tensor<2x960x64x64xf32>
    %8413 = "dtu_hlir.mul"(%8411, %8412) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_6888-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x960x64x64xf32>, tensor<2x960x64x64xf32>) -> tensor<2x960x64x64xf32>
    %8414 = "dtu_hlir.conv_bias"(%8413, %322, %323) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6889-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x960x64x64xf32>, tensor<320x960x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %8415 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_6890-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %8416 = "dtu_hlir.mul"(%915, %8415) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6891-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %8417 = "dtu_hlir.transpose"(%324) {node_name = "Gemm_6892-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<320x1280xf32>) -> tensor<1280x320xf32>
    %8418 = "dtu_hlir.gemm"(%8416, %8417, %325) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_6892-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x320xf32>, tensor<320xf32>) -> tensor<2x320xf32>
    %8419 = dtu_hlir.constant  {node_name = "Constant_6893-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %8420 = "dtu_hlir.unsqueeze"(%8418, %8419) {node_name = "Unsqueeze_6894-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320xf32>, tensor<1xi64>) -> tensor<2x320x1xf32>
    %8421 = dtu_hlir.constant  {node_name = "Constant_6895-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %8422 = "dtu_hlir.unsqueeze"(%8420, %8421) {node_name = "Unsqueeze_6896-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320x1xf32>, tensor<1xi64>) -> tensor<2x320x1x1xf32>
    %8423 = "dtu_hlir.add"(%8414, %8422) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_6897-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8424 = dtu_hlir.constant  {node_name = "Constant_6898-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %8425 = "dtu_hlir.dynamic_reshape"(%8423, %8424) {allowzero = 0 : i64, node_name = "Reshape_6899-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %8426 = dtu_hlir.constant  {node_name = "Constant_6900-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %8427 = dtu_hlir.constant  {node_name = "Constant_6901-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %8428 = "dtu_hlir.instance_norm"(%8425, %8426, %8427) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_6902-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %8429 = "dtu_hlir.shape"(%8423) {end = 2147483647 : i64, node_name = "Shape_6903-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %8430 = "dtu_hlir.dynamic_reshape"(%8428, %8429) {allowzero = 0 : i64, node_name = "Reshape_6904-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %8431 = "dtu_hlir.mul"(%8430, %638) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_6905-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8432 = "dtu_hlir.add"(%8431, %639) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_6906-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8433 = "dtu_hlir.sigmoid"(%8432) {node_name = "Sigmoid_6907-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8434 = "dtu_hlir.mul"(%8432, %8433) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_6908-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8435 = "dtu_hlir.conv_bias"(%8434, %326, %327) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6909-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %8436 = "dtu_hlir.conv_bias"(%8402, %328, %329) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6910-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x960x64x64xf32>, tensor<320x960x1x1xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %8437 = "dtu_hlir.add"(%8436, %8435) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_6911-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8438 = dtu_hlir.constant  {node_name = "Constant_6912-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %8439 = "dtu_hlir.broadcast_in_dim"(%8438) {node_name = "Div_6913-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x320x64x64xf32>
    %8440 = "dtu_hlir.div"(%8437, %8439) {node_name = "Div_6913-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8441 = "dtu_hlir.shape"(%8440) {end = 2147483647 : i64, node_name = "Shape_6914-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %8442 = dtu_hlir.constant  {node_name = "Constant_6915-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8443 = "dtu_hlir.gather"(%8441, %8442) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6916-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %8444 = "dtu_hlir.shape"(%8440) {end = 2147483647 : i64, node_name = "Shape_6917-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %8445 = dtu_hlir.constant  {node_name = "Constant_6918-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8446 = "dtu_hlir.gather"(%8444, %8445) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6919-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %8447 = "dtu_hlir.shape"(%8440) {end = 2147483647 : i64, node_name = "Shape_6920-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %8448 = dtu_hlir.constant  {node_name = "Constant_6921-0", node_type = "Constant"} dense<3> : tensor<i64>
    %8449 = "dtu_hlir.gather"(%8447, %8448) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6922-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %8450 = dtu_hlir.constant  {node_name = "Constant_6923-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %8451 = "dtu_hlir.dynamic_reshape"(%8440, %8450) {allowzero = 0 : i64, node_name = "Reshape_6924-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %8452 = dtu_hlir.constant  {node_name = "Constant_6925-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %8453 = dtu_hlir.constant  {node_name = "Constant_6926-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %8454 = "dtu_hlir.instance_norm"(%8451, %8452, %8453) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_6927-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %8455 = "dtu_hlir.shape"(%8440) {end = 2147483647 : i64, node_name = "Shape_6928-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %8456 = "dtu_hlir.dynamic_reshape"(%8454, %8455) {allowzero = 0 : i64, node_name = "Reshape_6929-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %8457 = "dtu_hlir.mul"(%8456, %640) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_6930-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8458 = "dtu_hlir.add"(%8457, %641) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_6931-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8459 = "dtu_hlir.shape"(%8458) {end = 2147483647 : i64, node_name = "Shape_6932-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %8460 = dtu_hlir.constant  {node_name = "Constant_6933-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8461 = "dtu_hlir.gather"(%8459, %8460) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6934-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %8462 = "dtu_hlir.transpose"(%8458) {node_name = "Transpose_6935-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x64x64x320xf32>
    %8463 = "dtu_hlir.mul"(%8446, %8449) {node_name = "Mul_6936-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8464 = dtu_hlir.constant  {node_name = "Constant_6937-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8465 = "dtu_hlir.unsqueeze"(%8443, %8464) {node_name = "Unsqueeze_6938-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8466 = dtu_hlir.constant  {node_name = "Constant_6939-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8467 = "dtu_hlir.unsqueeze"(%8463, %8466) {node_name = "Unsqueeze_6940-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8468 = dtu_hlir.constant  {node_name = "Constant_6941-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8469 = "dtu_hlir.unsqueeze"(%8461, %8468) {node_name = "Unsqueeze_6942-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8470 = "dtu_hlir.concatenate"(%8465, %8467, %8469) {dimension = 0 : i64, node_name = "Concat_6943-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8471 = "dtu_hlir.dynamic_reshape"(%8462, %8470) {allowzero = 0 : i64, node_name = "Reshape_6944-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %8472 = "dtu_hlir.dot_general"(%8471, %642) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6945-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8473 = "dtu_hlir.add"(%286, %8472) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6946-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8474 = dtu_hlir.constant  {node_name = "ReduceMean_6947-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8475 = "dtu_hlir.reshape"(%8474) {node_name = "ReduceMean_6947-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8476 = "dtu_hlir.reduce"(%8473, %8475) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6947-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6947-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %8477 = dtu_hlir.constant  {node_name = "ReduceMean_6947-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8478 = "dtu_hlir.unsqueeze"(%8476, %8477) {node_name = "ReduceMean_6947-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %8479 = dtu_hlir.constant  {node_name = "ReduceMean_6947-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %8480 = "dtu_hlir.broadcast_in_dim"(%8479) {node_name = "ReduceMean_6947-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %8481 = "dtu_hlir.div"(%8478, %8480) {node_name = "ReduceMean_6947-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8482 = "dtu_hlir.sub"(%8473, %8481) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_6948-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %8483 = dtu_hlir.constant  {node_name = "Constant_6949-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %8484 = "dtu_hlir.broadcast_in_dim"(%8483) {node_name = "Pow_6950-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %8485 = "dtu_hlir.pow"(%8482, %8484) {node_name = "Pow_6950-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8486 = dtu_hlir.constant  {node_name = "ReduceMean_6951-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8487 = "dtu_hlir.reshape"(%8486) {node_name = "ReduceMean_6951-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8488 = "dtu_hlir.reduce"(%8485, %8487) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_6951-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_6951-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %8489 = dtu_hlir.constant  {node_name = "ReduceMean_6951-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8490 = "dtu_hlir.unsqueeze"(%8488, %8489) {node_name = "ReduceMean_6951-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %8491 = dtu_hlir.constant  {node_name = "ReduceMean_6951-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %8492 = "dtu_hlir.broadcast_in_dim"(%8491) {node_name = "ReduceMean_6951-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %8493 = "dtu_hlir.div"(%8490, %8492) {node_name = "ReduceMean_6951-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8494 = dtu_hlir.constant  {node_name = "Constant_6952-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %8495 = "dtu_hlir.broadcast_in_dim"(%8494) {node_name = "Add_6953-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %8496 = "dtu_hlir.add"(%8493, %8495) {node_name = "Add_6953-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8497 = "dtu_hlir.sqrt"(%8496) {node_name = "Sqrt_6954-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8498 = "dtu_hlir.div"(%8482, %8497) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_6955-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %8499 = "dtu_hlir.mul"(%8498, %291) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_6956-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %8500 = "dtu_hlir.add"(%8499, %292) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_6957-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %8501 = "dtu_hlir.dot_general"(%8500, %643) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6958-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8502 = "dtu_hlir.dot_general"(%8500, %644) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6959-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8503 = "dtu_hlir.dot_general"(%8500, %645) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_6960-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8504 = "dtu_hlir.shape"(%8501) {end = 2147483647 : i64, node_name = "Shape_6961-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8505 = dtu_hlir.constant  {node_name = "Constant_6962-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8506 = "dtu_hlir.gather"(%8504, %8505) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6963-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8507 = "dtu_hlir.shape"(%8501) {end = 2147483647 : i64, node_name = "Shape_6964-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8508 = dtu_hlir.constant  {node_name = "Constant_6965-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8509 = "dtu_hlir.gather"(%8507, %8508) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6966-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8510 = "dtu_hlir.shape"(%8501) {end = 2147483647 : i64, node_name = "Shape_6967-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8511 = dtu_hlir.constant  {node_name = "Constant_6968-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8512 = "dtu_hlir.gather"(%8510, %8511) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6969-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8513 = dtu_hlir.constant  {node_name = "Constant_6970-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8514 = "dtu_hlir.div"(%8512, %8513) {node_name = "Div_6971-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8515 = "dtu_hlir.convert"(%8514) {node_name = "Cast_6972-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8516 = "dtu_hlir.convert"(%8515) {node_name = "Cast_6973-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8517 = dtu_hlir.constant  {node_name = "Constant_6974-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8518 = "dtu_hlir.unsqueeze"(%8506, %8517) {node_name = "Unsqueeze_6975-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8519 = dtu_hlir.constant  {node_name = "Constant_6976-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8520 = "dtu_hlir.unsqueeze"(%8509, %8519) {node_name = "Unsqueeze_6977-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8521 = dtu_hlir.constant  {node_name = "Constant_6978-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8522 = "dtu_hlir.unsqueeze"(%8516, %8521) {node_name = "Unsqueeze_6979-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8523 = "dtu_hlir.concatenate"(%8518, %8520, %727, %8522) {dimension = 0 : i64, node_name = "Concat_6980-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8524 = "dtu_hlir.dynamic_reshape"(%8501, %8523) {allowzero = 0 : i64, node_name = "Reshape_6981-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %8525 = "dtu_hlir.transpose"(%8524) {node_name = "Transpose_6982-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %8526 = dtu_hlir.constant  {node_name = "Constant_6983-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8527 = "dtu_hlir.mul"(%8506, %8526) {node_name = "Mul_6984-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8528 = dtu_hlir.constant  {node_name = "Constant_6985-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8529 = "dtu_hlir.div"(%8512, %8528) {node_name = "Div_6986-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8530 = "dtu_hlir.convert"(%8529) {node_name = "Cast_6987-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8531 = "dtu_hlir.convert"(%8530) {node_name = "Cast_6988-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8532 = dtu_hlir.constant  {node_name = "Constant_6989-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8533 = "dtu_hlir.unsqueeze"(%8527, %8532) {node_name = "Unsqueeze_6990-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8534 = dtu_hlir.constant  {node_name = "Constant_6991-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8535 = "dtu_hlir.unsqueeze"(%8509, %8534) {node_name = "Unsqueeze_6992-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8536 = dtu_hlir.constant  {node_name = "Constant_6993-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8537 = "dtu_hlir.unsqueeze"(%8531, %8536) {node_name = "Unsqueeze_6994-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8538 = "dtu_hlir.concatenate"(%8533, %8535, %8537) {dimension = 0 : i64, node_name = "Concat_6995-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8539 = "dtu_hlir.dynamic_reshape"(%8525, %8538) {allowzero = 0 : i64, node_name = "Reshape_6996-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %8540 = "dtu_hlir.shape"(%8502) {end = 2147483647 : i64, node_name = "Shape_6997-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8541 = dtu_hlir.constant  {node_name = "Constant_6998-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8542 = "dtu_hlir.gather"(%8540, %8541) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_6999-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8543 = "dtu_hlir.shape"(%8502) {end = 2147483647 : i64, node_name = "Shape_7000-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8544 = dtu_hlir.constant  {node_name = "Constant_7001-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8545 = "dtu_hlir.gather"(%8543, %8544) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7002-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8546 = "dtu_hlir.shape"(%8502) {end = 2147483647 : i64, node_name = "Shape_7003-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8547 = dtu_hlir.constant  {node_name = "Constant_7004-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8548 = "dtu_hlir.gather"(%8546, %8547) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7005-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8549 = dtu_hlir.constant  {node_name = "Constant_7006-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8550 = "dtu_hlir.div"(%8548, %8549) {node_name = "Div_7007-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8551 = "dtu_hlir.convert"(%8550) {node_name = "Cast_7008-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8552 = "dtu_hlir.convert"(%8551) {node_name = "Cast_7009-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8553 = dtu_hlir.constant  {node_name = "Constant_7010-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8554 = "dtu_hlir.unsqueeze"(%8542, %8553) {node_name = "Unsqueeze_7011-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8555 = dtu_hlir.constant  {node_name = "Constant_7012-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8556 = "dtu_hlir.unsqueeze"(%8545, %8555) {node_name = "Unsqueeze_7013-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8557 = dtu_hlir.constant  {node_name = "Constant_7014-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8558 = "dtu_hlir.unsqueeze"(%8552, %8557) {node_name = "Unsqueeze_7015-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8559 = "dtu_hlir.concatenate"(%8554, %8556, %726, %8558) {dimension = 0 : i64, node_name = "Concat_7016-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8560 = "dtu_hlir.dynamic_reshape"(%8502, %8559) {allowzero = 0 : i64, node_name = "Reshape_7017-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %8561 = "dtu_hlir.transpose"(%8560) {node_name = "Transpose_7018-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %8562 = dtu_hlir.constant  {node_name = "Constant_7019-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8563 = "dtu_hlir.mul"(%8542, %8562) {node_name = "Mul_7020-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8564 = dtu_hlir.constant  {node_name = "Constant_7021-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8565 = "dtu_hlir.div"(%8548, %8564) {node_name = "Div_7022-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8566 = "dtu_hlir.convert"(%8565) {node_name = "Cast_7023-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8567 = "dtu_hlir.convert"(%8566) {node_name = "Cast_7024-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8568 = dtu_hlir.constant  {node_name = "Constant_7025-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8569 = "dtu_hlir.unsqueeze"(%8563, %8568) {node_name = "Unsqueeze_7026-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8570 = dtu_hlir.constant  {node_name = "Constant_7027-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8571 = "dtu_hlir.unsqueeze"(%8545, %8570) {node_name = "Unsqueeze_7028-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8572 = dtu_hlir.constant  {node_name = "Constant_7029-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8573 = "dtu_hlir.unsqueeze"(%8567, %8572) {node_name = "Unsqueeze_7030-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8574 = "dtu_hlir.concatenate"(%8569, %8571, %8573) {dimension = 0 : i64, node_name = "Concat_7031-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8575 = "dtu_hlir.dynamic_reshape"(%8561, %8574) {allowzero = 0 : i64, node_name = "Reshape_7032-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %8576 = "dtu_hlir.shape"(%8503) {end = 2147483647 : i64, node_name = "Shape_7033-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8577 = dtu_hlir.constant  {node_name = "Constant_7034-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8578 = "dtu_hlir.gather"(%8576, %8577) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7035-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8579 = "dtu_hlir.shape"(%8503) {end = 2147483647 : i64, node_name = "Shape_7036-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8580 = dtu_hlir.constant  {node_name = "Constant_7037-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8581 = "dtu_hlir.gather"(%8579, %8580) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7038-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8582 = "dtu_hlir.shape"(%8503) {end = 2147483647 : i64, node_name = "Shape_7039-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8583 = dtu_hlir.constant  {node_name = "Constant_7040-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8584 = "dtu_hlir.gather"(%8582, %8583) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7041-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8585 = dtu_hlir.constant  {node_name = "Constant_7042-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8586 = "dtu_hlir.div"(%8584, %8585) {node_name = "Div_7043-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8587 = "dtu_hlir.convert"(%8586) {node_name = "Cast_7044-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8588 = "dtu_hlir.convert"(%8587) {node_name = "Cast_7045-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8589 = dtu_hlir.constant  {node_name = "Constant_7046-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8590 = "dtu_hlir.unsqueeze"(%8578, %8589) {node_name = "Unsqueeze_7047-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8591 = dtu_hlir.constant  {node_name = "Constant_7048-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8592 = "dtu_hlir.unsqueeze"(%8581, %8591) {node_name = "Unsqueeze_7049-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8593 = dtu_hlir.constant  {node_name = "Constant_7050-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8594 = "dtu_hlir.unsqueeze"(%8588, %8593) {node_name = "Unsqueeze_7051-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8595 = "dtu_hlir.concatenate"(%8590, %8592, %725, %8594) {dimension = 0 : i64, node_name = "Concat_7052-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8596 = "dtu_hlir.dynamic_reshape"(%8503, %8595) {allowzero = 0 : i64, node_name = "Reshape_7053-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %8597 = "dtu_hlir.transpose"(%8596) {node_name = "Transpose_7054-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %8598 = dtu_hlir.constant  {node_name = "Constant_7055-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8599 = "dtu_hlir.mul"(%8578, %8598) {node_name = "Mul_7056-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8600 = dtu_hlir.constant  {node_name = "Constant_7057-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8601 = "dtu_hlir.div"(%8584, %8600) {node_name = "Div_7058-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8602 = "dtu_hlir.convert"(%8601) {node_name = "Cast_7059-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8603 = "dtu_hlir.convert"(%8602) {node_name = "Cast_7060-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8604 = dtu_hlir.constant  {node_name = "Constant_7061-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8605 = "dtu_hlir.unsqueeze"(%8599, %8604) {node_name = "Unsqueeze_7062-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8606 = dtu_hlir.constant  {node_name = "Constant_7063-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8607 = "dtu_hlir.unsqueeze"(%8581, %8606) {node_name = "Unsqueeze_7064-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8608 = dtu_hlir.constant  {node_name = "Constant_7065-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8609 = "dtu_hlir.unsqueeze"(%8603, %8608) {node_name = "Unsqueeze_7066-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8610 = "dtu_hlir.concatenate"(%8605, %8607, %8609) {dimension = 0 : i64, node_name = "Concat_7067-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8611 = "dtu_hlir.dynamic_reshape"(%8597, %8610) {allowzero = 0 : i64, node_name = "Reshape_7068-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %8612 = "dtu_hlir.shape"(%8539) {end = 2147483647 : i64, node_name = "Shape_7069-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8613 = dtu_hlir.constant  {node_name = "Constant_7070-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8614 = "dtu_hlir.gather"(%8612, %8613) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7071-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8615 = "dtu_hlir.shape"(%8539) {end = 2147483647 : i64, node_name = "Shape_7072-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8616 = dtu_hlir.constant  {node_name = "Constant_7073-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8617 = "dtu_hlir.gather"(%8615, %8616) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7074-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8618 = "dtu_hlir.shape"(%8575) {end = 2147483647 : i64, node_name = "Shape_7075-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8619 = dtu_hlir.constant  {node_name = "Constant_7076-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8620 = "dtu_hlir.gather"(%8618, %8619) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7077-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8621 = dtu_hlir.constant  {node_name = "Constant_7078-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8622 = "dtu_hlir.unsqueeze"(%8614, %8621) {node_name = "Unsqueeze_7079-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8623 = dtu_hlir.constant  {node_name = "Constant_7080-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8624 = "dtu_hlir.unsqueeze"(%8617, %8623) {node_name = "Unsqueeze_7081-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8625 = dtu_hlir.constant  {node_name = "Constant_7082-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8626 = "dtu_hlir.unsqueeze"(%8620, %8625) {node_name = "Unsqueeze_7083-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8627 = "dtu_hlir.concatenate"(%8622, %8624, %8626) {dimension = 0 : i64, node_name = "Concat_7084-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8628 = dtu_hlir.constant  {node_name = "ConstantOfShape_7085-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %8629 = "dtu_hlir.dynamic_broadcast_in_dim"(%8628, %8627) {node_name = "ConstantOfShape_7085-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x4096xf32>
    %8630 = "dtu_hlir.transpose"(%8575) {node_name = "Transpose_7086-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<10x64x4096xf32>
    %8631 = "dtu_hlir.dot_general"(%8539, %8630) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7087-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x4096xf32>) -> tensor<10x4096x4096xf32>
    %8632 = "dtu_hlir.broadcast_in_dim"(%724) {node_name = "Mul_7088-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %8633 = "dtu_hlir.mul"(%8631, %8632) {node_name = "Mul_7088-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %8634 = "dtu_hlir.broadcast_in_dim"(%723) {node_name = "Mul_7089-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %8635 = "dtu_hlir.mul"(%8629, %8634) {node_name = "Mul_7089-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %8636 = "dtu_hlir.add"(%8633, %8635) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7090-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %8637 = "dtu_hlir.softmax"(%8636) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7091-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %8638 = "dtu_hlir.convert"(%8637) {node_name = "Cast_7092-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %8639 = "dtu_hlir.dot_general"(%8638, %8611) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7093-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x64xf32>) -> tensor<10x4096x64xf32>
    %8640 = "dtu_hlir.shape"(%8639) {end = 2147483647 : i64, node_name = "Shape_7094-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8641 = dtu_hlir.constant  {node_name = "Constant_7095-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8642 = "dtu_hlir.gather"(%8640, %8641) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7096-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8643 = "dtu_hlir.shape"(%8639) {end = 2147483647 : i64, node_name = "Shape_7097-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8644 = dtu_hlir.constant  {node_name = "Constant_7098-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8645 = "dtu_hlir.gather"(%8643, %8644) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7099-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8646 = "dtu_hlir.shape"(%8639) {end = 2147483647 : i64, node_name = "Shape_7100-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8647 = dtu_hlir.constant  {node_name = "Constant_7101-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8648 = "dtu_hlir.gather"(%8646, %8647) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7102-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8649 = dtu_hlir.constant  {node_name = "Constant_7103-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8650 = "dtu_hlir.div"(%8642, %8649) {node_name = "Div_7104-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8651 = "dtu_hlir.convert"(%8650) {node_name = "Cast_7105-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8652 = "dtu_hlir.convert"(%8651) {node_name = "Cast_7106-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8653 = dtu_hlir.constant  {node_name = "Constant_7107-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8654 = "dtu_hlir.unsqueeze"(%8652, %8653) {node_name = "Unsqueeze_7108-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8655 = dtu_hlir.constant  {node_name = "Constant_7109-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8656 = "dtu_hlir.unsqueeze"(%8645, %8655) {node_name = "Unsqueeze_7110-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8657 = dtu_hlir.constant  {node_name = "Constant_7111-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8658 = "dtu_hlir.unsqueeze"(%8648, %8657) {node_name = "Unsqueeze_7112-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8659 = "dtu_hlir.concatenate"(%8654, %722, %8656, %8658) {dimension = 0 : i64, node_name = "Concat_7113-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8660 = "dtu_hlir.dynamic_reshape"(%8639, %8659) {allowzero = 0 : i64, node_name = "Reshape_7114-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %8661 = "dtu_hlir.transpose"(%8660) {node_name = "Transpose_7115-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %8662 = dtu_hlir.constant  {node_name = "Constant_7116-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8663 = "dtu_hlir.div"(%8642, %8662) {node_name = "Div_7117-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8664 = "dtu_hlir.convert"(%8663) {node_name = "Cast_7118-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8665 = "dtu_hlir.convert"(%8664) {node_name = "Cast_7119-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8666 = dtu_hlir.constant  {node_name = "Constant_7120-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8667 = "dtu_hlir.mul"(%8648, %8666) {node_name = "Mul_7121-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8668 = dtu_hlir.constant  {node_name = "Constant_7122-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8669 = "dtu_hlir.unsqueeze"(%8665, %8668) {node_name = "Unsqueeze_7123-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8670 = dtu_hlir.constant  {node_name = "Constant_7124-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8671 = "dtu_hlir.unsqueeze"(%8645, %8670) {node_name = "Unsqueeze_7125-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8672 = dtu_hlir.constant  {node_name = "Constant_7126-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8673 = "dtu_hlir.unsqueeze"(%8667, %8672) {node_name = "Unsqueeze_7127-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8674 = "dtu_hlir.concatenate"(%8669, %8671, %8673) {dimension = 0 : i64, node_name = "Concat_7128-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8675 = "dtu_hlir.dynamic_reshape"(%8661, %8674) {allowzero = 0 : i64, node_name = "Reshape_7129-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %8676 = "dtu_hlir.dot_general"(%8675, %646) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7130-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8677 = "dtu_hlir.add"(%287, %8676) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7131-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8678 = "dtu_hlir.add"(%8677, %8473) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7132-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8679 = dtu_hlir.constant  {node_name = "ReduceMean_7133-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8680 = "dtu_hlir.reshape"(%8679) {node_name = "ReduceMean_7133-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8681 = "dtu_hlir.reduce"(%8678, %8680) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7133-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7133-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %8682 = dtu_hlir.constant  {node_name = "ReduceMean_7133-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8683 = "dtu_hlir.unsqueeze"(%8681, %8682) {node_name = "ReduceMean_7133-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %8684 = dtu_hlir.constant  {node_name = "ReduceMean_7133-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %8685 = "dtu_hlir.broadcast_in_dim"(%8684) {node_name = "ReduceMean_7133-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %8686 = "dtu_hlir.div"(%8683, %8685) {node_name = "ReduceMean_7133-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8687 = "dtu_hlir.sub"(%8678, %8686) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_7134-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %8688 = dtu_hlir.constant  {node_name = "Constant_7135-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %8689 = "dtu_hlir.broadcast_in_dim"(%8688) {node_name = "Pow_7136-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %8690 = "dtu_hlir.pow"(%8687, %8689) {node_name = "Pow_7136-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8691 = dtu_hlir.constant  {node_name = "ReduceMean_7137-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8692 = "dtu_hlir.reshape"(%8691) {node_name = "ReduceMean_7137-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8693 = "dtu_hlir.reduce"(%8690, %8692) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7137-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7137-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %8694 = dtu_hlir.constant  {node_name = "ReduceMean_7137-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8695 = "dtu_hlir.unsqueeze"(%8693, %8694) {node_name = "ReduceMean_7137-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %8696 = dtu_hlir.constant  {node_name = "ReduceMean_7137-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %8697 = "dtu_hlir.broadcast_in_dim"(%8696) {node_name = "ReduceMean_7137-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %8698 = "dtu_hlir.div"(%8695, %8697) {node_name = "ReduceMean_7137-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8699 = dtu_hlir.constant  {node_name = "Constant_7138-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %8700 = "dtu_hlir.broadcast_in_dim"(%8699) {node_name = "Add_7139-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %8701 = "dtu_hlir.add"(%8698, %8700) {node_name = "Add_7139-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8702 = "dtu_hlir.sqrt"(%8701) {node_name = "Sqrt_7140-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8703 = "dtu_hlir.div"(%8687, %8702) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_7141-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %8704 = "dtu_hlir.mul"(%8703, %293) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_7142-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %8705 = "dtu_hlir.add"(%8704, %294) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7143-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %8706 = "dtu_hlir.dot_general"(%8705, %647) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7144-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8707 = "dtu_hlir.dot_general"(%arg2, %648) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7145-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %8708 = "dtu_hlir.dot_general"(%arg2, %649) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7146-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %8709 = "dtu_hlir.shape"(%8706) {end = 2147483647 : i64, node_name = "Shape_7147-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8710 = dtu_hlir.constant  {node_name = "Constant_7148-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8711 = "dtu_hlir.gather"(%8709, %8710) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7149-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8712 = "dtu_hlir.shape"(%8706) {end = 2147483647 : i64, node_name = "Shape_7150-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8713 = dtu_hlir.constant  {node_name = "Constant_7151-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8714 = "dtu_hlir.gather"(%8712, %8713) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7152-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8715 = "dtu_hlir.shape"(%8706) {end = 2147483647 : i64, node_name = "Shape_7153-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %8716 = dtu_hlir.constant  {node_name = "Constant_7154-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8717 = "dtu_hlir.gather"(%8715, %8716) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7155-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8718 = dtu_hlir.constant  {node_name = "Constant_7156-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8719 = "dtu_hlir.div"(%8717, %8718) {node_name = "Div_7157-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8720 = "dtu_hlir.convert"(%8719) {node_name = "Cast_7158-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8721 = "dtu_hlir.convert"(%8720) {node_name = "Cast_7159-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8722 = dtu_hlir.constant  {node_name = "Constant_7160-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8723 = "dtu_hlir.unsqueeze"(%8711, %8722) {node_name = "Unsqueeze_7161-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8724 = dtu_hlir.constant  {node_name = "Constant_7162-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8725 = "dtu_hlir.unsqueeze"(%8714, %8724) {node_name = "Unsqueeze_7163-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8726 = dtu_hlir.constant  {node_name = "Constant_7164-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8727 = "dtu_hlir.unsqueeze"(%8721, %8726) {node_name = "Unsqueeze_7165-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8728 = "dtu_hlir.concatenate"(%8723, %8725, %721, %8727) {dimension = 0 : i64, node_name = "Concat_7166-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8729 = "dtu_hlir.dynamic_reshape"(%8706, %8728) {allowzero = 0 : i64, node_name = "Reshape_7167-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %8730 = "dtu_hlir.transpose"(%8729) {node_name = "Transpose_7168-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %8731 = dtu_hlir.constant  {node_name = "Constant_7169-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8732 = "dtu_hlir.mul"(%8711, %8731) {node_name = "Mul_7170-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8733 = dtu_hlir.constant  {node_name = "Constant_7171-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8734 = "dtu_hlir.div"(%8717, %8733) {node_name = "Div_7172-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8735 = "dtu_hlir.convert"(%8734) {node_name = "Cast_7173-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8736 = "dtu_hlir.convert"(%8735) {node_name = "Cast_7174-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8737 = dtu_hlir.constant  {node_name = "Constant_7175-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8738 = "dtu_hlir.unsqueeze"(%8732, %8737) {node_name = "Unsqueeze_7176-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8739 = dtu_hlir.constant  {node_name = "Constant_7177-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8740 = "dtu_hlir.unsqueeze"(%8714, %8739) {node_name = "Unsqueeze_7178-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8741 = dtu_hlir.constant  {node_name = "Constant_7179-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8742 = "dtu_hlir.unsqueeze"(%8736, %8741) {node_name = "Unsqueeze_7180-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8743 = "dtu_hlir.concatenate"(%8738, %8740, %8742) {dimension = 0 : i64, node_name = "Concat_7181-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8744 = "dtu_hlir.dynamic_reshape"(%8730, %8743) {allowzero = 0 : i64, node_name = "Reshape_7182-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %8745 = "dtu_hlir.shape"(%8707) {end = 2147483647 : i64, node_name = "Shape_7183-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %8746 = dtu_hlir.constant  {node_name = "Constant_7184-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8747 = "dtu_hlir.gather"(%8745, %8746) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7185-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8748 = "dtu_hlir.shape"(%8707) {end = 2147483647 : i64, node_name = "Shape_7186-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %8749 = dtu_hlir.constant  {node_name = "Constant_7187-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8750 = "dtu_hlir.gather"(%8748, %8749) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7188-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8751 = "dtu_hlir.shape"(%8707) {end = 2147483647 : i64, node_name = "Shape_7189-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %8752 = dtu_hlir.constant  {node_name = "Constant_7190-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8753 = "dtu_hlir.gather"(%8751, %8752) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7191-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8754 = dtu_hlir.constant  {node_name = "Constant_7192-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8755 = "dtu_hlir.div"(%8753, %8754) {node_name = "Div_7193-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8756 = "dtu_hlir.convert"(%8755) {node_name = "Cast_7194-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8757 = "dtu_hlir.convert"(%8756) {node_name = "Cast_7195-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8758 = dtu_hlir.constant  {node_name = "Constant_7196-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8759 = "dtu_hlir.unsqueeze"(%8747, %8758) {node_name = "Unsqueeze_7197-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8760 = dtu_hlir.constant  {node_name = "Constant_7198-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8761 = "dtu_hlir.unsqueeze"(%8750, %8760) {node_name = "Unsqueeze_7199-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8762 = dtu_hlir.constant  {node_name = "Constant_7200-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8763 = "dtu_hlir.unsqueeze"(%8757, %8762) {node_name = "Unsqueeze_7201-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8764 = "dtu_hlir.concatenate"(%8759, %8761, %720, %8763) {dimension = 0 : i64, node_name = "Concat_7202-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8765 = "dtu_hlir.dynamic_reshape"(%8707, %8764) {allowzero = 0 : i64, node_name = "Reshape_7203-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %8766 = "dtu_hlir.transpose"(%8765) {node_name = "Transpose_7204-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %8767 = dtu_hlir.constant  {node_name = "Constant_7205-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8768 = "dtu_hlir.mul"(%8747, %8767) {node_name = "Mul_7206-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8769 = dtu_hlir.constant  {node_name = "Constant_7207-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8770 = "dtu_hlir.div"(%8753, %8769) {node_name = "Div_7208-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8771 = "dtu_hlir.convert"(%8770) {node_name = "Cast_7209-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8772 = "dtu_hlir.convert"(%8771) {node_name = "Cast_7210-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8773 = dtu_hlir.constant  {node_name = "Constant_7211-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8774 = "dtu_hlir.unsqueeze"(%8768, %8773) {node_name = "Unsqueeze_7212-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8775 = dtu_hlir.constant  {node_name = "Constant_7213-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8776 = "dtu_hlir.unsqueeze"(%8750, %8775) {node_name = "Unsqueeze_7214-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8777 = dtu_hlir.constant  {node_name = "Constant_7215-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8778 = "dtu_hlir.unsqueeze"(%8772, %8777) {node_name = "Unsqueeze_7216-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8779 = "dtu_hlir.concatenate"(%8774, %8776, %8778) {dimension = 0 : i64, node_name = "Concat_7217-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8780 = "dtu_hlir.dynamic_reshape"(%8766, %8779) {allowzero = 0 : i64, node_name = "Reshape_7218-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %8781 = "dtu_hlir.shape"(%8708) {end = 2147483647 : i64, node_name = "Shape_7219-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %8782 = dtu_hlir.constant  {node_name = "Constant_7220-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8783 = "dtu_hlir.gather"(%8781, %8782) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7221-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8784 = "dtu_hlir.shape"(%8708) {end = 2147483647 : i64, node_name = "Shape_7222-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %8785 = dtu_hlir.constant  {node_name = "Constant_7223-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8786 = "dtu_hlir.gather"(%8784, %8785) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7224-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8787 = "dtu_hlir.shape"(%8708) {end = 2147483647 : i64, node_name = "Shape_7225-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %8788 = dtu_hlir.constant  {node_name = "Constant_7226-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8789 = "dtu_hlir.gather"(%8787, %8788) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7227-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8790 = dtu_hlir.constant  {node_name = "Constant_7228-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8791 = "dtu_hlir.div"(%8789, %8790) {node_name = "Div_7229-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8792 = "dtu_hlir.convert"(%8791) {node_name = "Cast_7230-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8793 = "dtu_hlir.convert"(%8792) {node_name = "Cast_7231-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8794 = dtu_hlir.constant  {node_name = "Constant_7232-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8795 = "dtu_hlir.unsqueeze"(%8783, %8794) {node_name = "Unsqueeze_7233-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8796 = dtu_hlir.constant  {node_name = "Constant_7234-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8797 = "dtu_hlir.unsqueeze"(%8786, %8796) {node_name = "Unsqueeze_7235-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8798 = dtu_hlir.constant  {node_name = "Constant_7236-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8799 = "dtu_hlir.unsqueeze"(%8793, %8798) {node_name = "Unsqueeze_7237-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8800 = "dtu_hlir.concatenate"(%8795, %8797, %719, %8799) {dimension = 0 : i64, node_name = "Concat_7238-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8801 = "dtu_hlir.dynamic_reshape"(%8708, %8800) {allowzero = 0 : i64, node_name = "Reshape_7239-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %8802 = "dtu_hlir.transpose"(%8801) {node_name = "Transpose_7240-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %8803 = dtu_hlir.constant  {node_name = "Constant_7241-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8804 = "dtu_hlir.mul"(%8783, %8803) {node_name = "Mul_7242-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8805 = dtu_hlir.constant  {node_name = "Constant_7243-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8806 = "dtu_hlir.div"(%8789, %8805) {node_name = "Div_7244-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8807 = "dtu_hlir.convert"(%8806) {node_name = "Cast_7245-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8808 = "dtu_hlir.convert"(%8807) {node_name = "Cast_7246-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8809 = dtu_hlir.constant  {node_name = "Constant_7247-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8810 = "dtu_hlir.unsqueeze"(%8804, %8809) {node_name = "Unsqueeze_7248-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8811 = dtu_hlir.constant  {node_name = "Constant_7249-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8812 = "dtu_hlir.unsqueeze"(%8786, %8811) {node_name = "Unsqueeze_7250-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8813 = dtu_hlir.constant  {node_name = "Constant_7251-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8814 = "dtu_hlir.unsqueeze"(%8808, %8813) {node_name = "Unsqueeze_7252-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8815 = "dtu_hlir.concatenate"(%8810, %8812, %8814) {dimension = 0 : i64, node_name = "Concat_7253-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8816 = "dtu_hlir.dynamic_reshape"(%8802, %8815) {allowzero = 0 : i64, node_name = "Reshape_7254-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %8817 = "dtu_hlir.shape"(%8744) {end = 2147483647 : i64, node_name = "Shape_7255-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8818 = dtu_hlir.constant  {node_name = "Constant_7256-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8819 = "dtu_hlir.gather"(%8817, %8818) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7257-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8820 = "dtu_hlir.shape"(%8744) {end = 2147483647 : i64, node_name = "Shape_7258-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8821 = dtu_hlir.constant  {node_name = "Constant_7259-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8822 = "dtu_hlir.gather"(%8820, %8821) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7260-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8823 = "dtu_hlir.shape"(%8780) {end = 2147483647 : i64, node_name = "Shape_7261-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<3xi64>
    %8824 = dtu_hlir.constant  {node_name = "Constant_7262-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8825 = "dtu_hlir.gather"(%8823, %8824) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7263-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8826 = dtu_hlir.constant  {node_name = "Constant_7264-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8827 = "dtu_hlir.unsqueeze"(%8819, %8826) {node_name = "Unsqueeze_7265-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8828 = dtu_hlir.constant  {node_name = "Constant_7266-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8829 = "dtu_hlir.unsqueeze"(%8822, %8828) {node_name = "Unsqueeze_7267-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8830 = dtu_hlir.constant  {node_name = "Constant_7268-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8831 = "dtu_hlir.unsqueeze"(%8825, %8830) {node_name = "Unsqueeze_7269-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8832 = "dtu_hlir.concatenate"(%8827, %8829, %8831) {dimension = 0 : i64, node_name = "Concat_7270-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8833 = dtu_hlir.constant  {node_name = "ConstantOfShape_7271-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %8834 = "dtu_hlir.dynamic_broadcast_in_dim"(%8833, %8832) {node_name = "ConstantOfShape_7271-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x77xf32>
    %8835 = "dtu_hlir.transpose"(%8780) {node_name = "Transpose_7272-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<10x64x77xf32>
    %8836 = "dtu_hlir.dot_general"(%8744, %8835) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7273-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x77xf32>) -> tensor<10x4096x77xf32>
    %8837 = "dtu_hlir.broadcast_in_dim"(%718) {node_name = "Mul_7274-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %8838 = "dtu_hlir.mul"(%8836, %8837) {node_name = "Mul_7274-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %8839 = "dtu_hlir.broadcast_in_dim"(%717) {node_name = "Mul_7275-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %8840 = "dtu_hlir.mul"(%8834, %8839) {node_name = "Mul_7275-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %8841 = "dtu_hlir.add"(%8838, %8840) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7276-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %8842 = "dtu_hlir.softmax"(%8841) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7277-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %8843 = "dtu_hlir.convert"(%8842) {node_name = "Cast_7278-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %8844 = "dtu_hlir.dot_general"(%8843, %8816) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7279-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x77x64xf32>) -> tensor<10x4096x64xf32>
    %8845 = "dtu_hlir.shape"(%8844) {end = 2147483647 : i64, node_name = "Shape_7280-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8846 = dtu_hlir.constant  {node_name = "Constant_7281-0", node_type = "Constant"} dense<0> : tensor<i64>
    %8847 = "dtu_hlir.gather"(%8845, %8846) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7282-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8848 = "dtu_hlir.shape"(%8844) {end = 2147483647 : i64, node_name = "Shape_7283-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8849 = dtu_hlir.constant  {node_name = "Constant_7284-0", node_type = "Constant"} dense<1> : tensor<i64>
    %8850 = "dtu_hlir.gather"(%8848, %8849) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7285-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8851 = "dtu_hlir.shape"(%8844) {end = 2147483647 : i64, node_name = "Shape_7286-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %8852 = dtu_hlir.constant  {node_name = "Constant_7287-0", node_type = "Constant"} dense<2> : tensor<i64>
    %8853 = "dtu_hlir.gather"(%8851, %8852) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7288-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %8854 = dtu_hlir.constant  {node_name = "Constant_7289-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8855 = "dtu_hlir.div"(%8847, %8854) {node_name = "Div_7290-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8856 = "dtu_hlir.convert"(%8855) {node_name = "Cast_7291-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8857 = "dtu_hlir.convert"(%8856) {node_name = "Cast_7292-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8858 = dtu_hlir.constant  {node_name = "Constant_7293-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8859 = "dtu_hlir.unsqueeze"(%8857, %8858) {node_name = "Unsqueeze_7294-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8860 = dtu_hlir.constant  {node_name = "Constant_7295-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8861 = "dtu_hlir.unsqueeze"(%8850, %8860) {node_name = "Unsqueeze_7296-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8862 = dtu_hlir.constant  {node_name = "Constant_7297-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8863 = "dtu_hlir.unsqueeze"(%8853, %8862) {node_name = "Unsqueeze_7298-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8864 = "dtu_hlir.concatenate"(%8859, %716, %8861, %8863) {dimension = 0 : i64, node_name = "Concat_7299-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8865 = "dtu_hlir.dynamic_reshape"(%8844, %8864) {allowzero = 0 : i64, node_name = "Reshape_7300-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %8866 = "dtu_hlir.transpose"(%8865) {node_name = "Transpose_7301-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %8867 = dtu_hlir.constant  {node_name = "Constant_7302-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8868 = "dtu_hlir.div"(%8847, %8867) {node_name = "Div_7303-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8869 = "dtu_hlir.convert"(%8868) {node_name = "Cast_7304-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8870 = "dtu_hlir.convert"(%8869) {node_name = "Cast_7305-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %8871 = dtu_hlir.constant  {node_name = "Constant_7306-0", node_type = "Constant"} dense<5> : tensor<i64>
    %8872 = "dtu_hlir.mul"(%8853, %8871) {node_name = "Mul_7307-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %8873 = dtu_hlir.constant  {node_name = "Constant_7308-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8874 = "dtu_hlir.unsqueeze"(%8870, %8873) {node_name = "Unsqueeze_7309-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8875 = dtu_hlir.constant  {node_name = "Constant_7310-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8876 = "dtu_hlir.unsqueeze"(%8850, %8875) {node_name = "Unsqueeze_7311-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8877 = dtu_hlir.constant  {node_name = "Constant_7312-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8878 = "dtu_hlir.unsqueeze"(%8872, %8877) {node_name = "Unsqueeze_7313-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8879 = "dtu_hlir.concatenate"(%8874, %8876, %8878) {dimension = 0 : i64, node_name = "Concat_7314-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %8880 = "dtu_hlir.dynamic_reshape"(%8866, %8879) {allowzero = 0 : i64, node_name = "Reshape_7315-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %8881 = "dtu_hlir.dot_general"(%8880, %650) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7316-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8882 = "dtu_hlir.add"(%290, %8881) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7317-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8883 = "dtu_hlir.add"(%8882, %8678) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7318-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8884 = dtu_hlir.constant  {node_name = "ReduceMean_7319-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8885 = "dtu_hlir.reshape"(%8884) {node_name = "ReduceMean_7319-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8886 = "dtu_hlir.reduce"(%8883, %8885) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7319-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7319-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %8887 = dtu_hlir.constant  {node_name = "ReduceMean_7319-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8888 = "dtu_hlir.unsqueeze"(%8886, %8887) {node_name = "ReduceMean_7319-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %8889 = dtu_hlir.constant  {node_name = "ReduceMean_7319-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %8890 = "dtu_hlir.broadcast_in_dim"(%8889) {node_name = "ReduceMean_7319-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %8891 = "dtu_hlir.div"(%8888, %8890) {node_name = "ReduceMean_7319-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8892 = "dtu_hlir.sub"(%8883, %8891) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_7320-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %8893 = dtu_hlir.constant  {node_name = "Constant_7321-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %8894 = "dtu_hlir.broadcast_in_dim"(%8893) {node_name = "Pow_7322-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %8895 = "dtu_hlir.pow"(%8892, %8894) {node_name = "Pow_7322-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8896 = dtu_hlir.constant  {node_name = "ReduceMean_7323-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %8897 = "dtu_hlir.reshape"(%8896) {node_name = "ReduceMean_7323-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %8898 = "dtu_hlir.reduce"(%8895, %8897) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7323-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7323-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %8899 = dtu_hlir.constant  {node_name = "ReduceMean_7323-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %8900 = "dtu_hlir.unsqueeze"(%8898, %8899) {node_name = "ReduceMean_7323-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %8901 = dtu_hlir.constant  {node_name = "ReduceMean_7323-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %8902 = "dtu_hlir.broadcast_in_dim"(%8901) {node_name = "ReduceMean_7323-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %8903 = "dtu_hlir.div"(%8900, %8902) {node_name = "ReduceMean_7323-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8904 = dtu_hlir.constant  {node_name = "Constant_7324-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %8905 = "dtu_hlir.broadcast_in_dim"(%8904) {node_name = "Add_7325-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %8906 = "dtu_hlir.add"(%8903, %8905) {node_name = "Add_7325-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8907 = "dtu_hlir.sqrt"(%8906) {node_name = "Sqrt_7326-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %8908 = "dtu_hlir.div"(%8892, %8907) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_7327-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %8909 = "dtu_hlir.mul"(%8908, %295) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_7328-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %8910 = "dtu_hlir.add"(%8909, %296) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7329-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %8911 = "dtu_hlir.dot_general"(%8910, %651) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7330-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x2560xf32>) -> tensor<2x4096x2560xf32>
    %8912 = "dtu_hlir.add"(%288, %8911) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7331-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2560xf32>, tensor<2x4096x2560xf32>) -> tensor<2x4096x2560xf32>
    %8913 = "dtu_hlir.shape"(%8912) {end = 2147483647 : i64, node_name = "Shape_7332-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>) -> tensor<3xi64>
    %8914 = dtu_hlir.constant  {node_name = "Constant_7333-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %8915 = "dtu_hlir.gather"(%8913, %8914) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7334-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8916 = dtu_hlir.constant  {node_name = "Constant_7335-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8917 = dtu_hlir.constant  {node_name = "Constant_7336-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %8918 = "dtu_hlir.add"(%8915, %8917) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_7337-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8919 = dtu_hlir.constant  {node_name = "Constant_7338-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %8920 = "dtu_hlir.div"(%8918, %8919) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_7339-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8921 = dtu_hlir.constant  {node_name = "Constant_7340-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %8922 = "dtu_hlir.mul"(%8920, %8921) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_7341-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8923 = dtu_hlir.constant  {node_name = "Slice_7342-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %8924 = "dtu_hlir.real_dynamic_slice"(%8912, %8916, %8922, %8923, %8914) {node_name = "Slice_7342-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %8925 = dtu_hlir.constant  {node_name = "Constant_7343-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %8926 = "dtu_hlir.mul"(%8920, %8925) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_7344-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %8927 = "dtu_hlir.shape"(%8922) {end = 2147483647 : i64, node_name = "Slice_7345-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %8928 = dtu_hlir.constant  {node_name = "Slice_7345-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %8929 = "dtu_hlir.dynamic_broadcast_in_dim"(%8928, %8927) {node_name = "Slice_7345-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8930 = "dtu_hlir.real_dynamic_slice"(%8912, %8922, %8926, %8929, %8914) {node_name = "Slice_7345-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %8931 = dtu_hlir.constant  {node_name = "Constant_7346-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %8932 = "dtu_hlir.broadcast_in_dim"(%8931) {node_name = "Div_7347-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %8933 = "dtu_hlir.div"(%8930, %8932) {node_name = "Div_7347-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %8934 = "dtu_hlir.erf"(%8933) {node_name = "Erf_7348-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %8935 = dtu_hlir.constant  {node_name = "Constant_7349-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %8936 = "dtu_hlir.broadcast_in_dim"(%8935) {node_name = "Add_7350-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %8937 = "dtu_hlir.add"(%8934, %8936) {node_name = "Add_7350-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %8938 = "dtu_hlir.mul"(%8930, %8937) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_7351-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %8939 = dtu_hlir.constant  {node_name = "Constant_7352-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %8940 = "dtu_hlir.broadcast_in_dim"(%8939) {node_name = "Mul_7353-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %8941 = "dtu_hlir.mul"(%8938, %8940) {node_name = "Mul_7353-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %8942 = "dtu_hlir.mul"(%8924, %8941) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_7354-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %8943 = "dtu_hlir.dot_general"(%8942, %652) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7355-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<1280x320xf32>) -> tensor<2x4096x320xf32>
    %8944 = "dtu_hlir.add"(%289, %8943) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7356-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8945 = "dtu_hlir.add"(%8944, %8883) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7357-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8946 = "dtu_hlir.dot_general"(%8945, %653) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7358-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %8947 = "dtu_hlir.add"(%297, %8946) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7359-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %8948 = dtu_hlir.constant  {node_name = "Constant_7360-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8949 = "dtu_hlir.unsqueeze"(%8443, %8948) {node_name = "Unsqueeze_7361-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8950 = dtu_hlir.constant  {node_name = "Constant_7362-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8951 = "dtu_hlir.unsqueeze"(%8446, %8950) {node_name = "Unsqueeze_7363-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8952 = dtu_hlir.constant  {node_name = "Constant_7364-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8953 = "dtu_hlir.unsqueeze"(%8449, %8952) {node_name = "Unsqueeze_7365-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8954 = dtu_hlir.constant  {node_name = "Constant_7366-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %8955 = "dtu_hlir.unsqueeze"(%8461, %8954) {node_name = "Unsqueeze_7367-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %8956 = "dtu_hlir.concatenate"(%8949, %8951, %8953, %8955) {dimension = 0 : i64, node_name = "Concat_7368-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %8957 = "dtu_hlir.dynamic_reshape"(%8947, %8956) {allowzero = 0 : i64, node_name = "Reshape_7369-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x64x64x320xf32>
    %8958 = "dtu_hlir.transpose"(%8957) {node_name = "Transpose_7370-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>) -> tensor<2x320x64x64xf32>
    %8959 = "dtu_hlir.add"(%8958, %8440) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_7371-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8960 = "dtu_hlir.concatenate"(%8959, %1472) {dimension = 1 : i64, node_name = "Concat_7372-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x640x64x64xf32>
    %8961 = dtu_hlir.constant  {node_name = "Constant_7373-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %8962 = "dtu_hlir.dynamic_reshape"(%8960, %8961) {allowzero = 0 : i64, node_name = "Reshape_7374-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<3xi64>) -> tensor<2x32x81920xf32>
    %8963 = dtu_hlir.constant  {node_name = "Constant_7375-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %8964 = dtu_hlir.constant  {node_name = "Constant_7376-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %8965 = "dtu_hlir.instance_norm"(%8962, %8963, %8964) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_7377-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x81920xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x81920xf32>
    %8966 = "dtu_hlir.shape"(%8960) {end = 2147483647 : i64, node_name = "Shape_7378-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>) -> tensor<4xi64>
    %8967 = "dtu_hlir.dynamic_reshape"(%8965, %8966) {allowzero = 0 : i64, node_name = "Reshape_7379-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x81920xf32>, tensor<4xi64>) -> tensor<2x640x64x64xf32>
    %8968 = "dtu_hlir.mul"(%8967, %654) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_7380-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<640x1x1xf32>) -> tensor<2x640x64x64xf32>
    %8969 = "dtu_hlir.add"(%8968, %655) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_7381-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<640x1x1xf32>) -> tensor<2x640x64x64xf32>
    %8970 = "dtu_hlir.sigmoid"(%8969) {node_name = "Sigmoid_7382-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>) -> tensor<2x640x64x64xf32>
    %8971 = "dtu_hlir.mul"(%8969, %8970) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_7383-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<2x640x64x64xf32>) -> tensor<2x640x64x64xf32>
    %8972 = "dtu_hlir.conv_bias"(%8971, %330, %331) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7384-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x64x64xf32>, tensor<320x640x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %8973 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_7385-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %8974 = "dtu_hlir.mul"(%915, %8973) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7386-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %8975 = "dtu_hlir.transpose"(%332) {node_name = "Gemm_7387-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<320x1280xf32>) -> tensor<1280x320xf32>
    %8976 = "dtu_hlir.gemm"(%8974, %8975, %333) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_7387-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x320xf32>, tensor<320xf32>) -> tensor<2x320xf32>
    %8977 = dtu_hlir.constant  {node_name = "Constant_7388-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %8978 = "dtu_hlir.unsqueeze"(%8976, %8977) {node_name = "Unsqueeze_7389-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320xf32>, tensor<1xi64>) -> tensor<2x320x1xf32>
    %8979 = dtu_hlir.constant  {node_name = "Constant_7390-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %8980 = "dtu_hlir.unsqueeze"(%8978, %8979) {node_name = "Unsqueeze_7391-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320x1xf32>, tensor<1xi64>) -> tensor<2x320x1x1xf32>
    %8981 = "dtu_hlir.add"(%8972, %8980) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_7392-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8982 = dtu_hlir.constant  {node_name = "Constant_7393-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %8983 = "dtu_hlir.dynamic_reshape"(%8981, %8982) {allowzero = 0 : i64, node_name = "Reshape_7394-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %8984 = dtu_hlir.constant  {node_name = "Constant_7395-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %8985 = dtu_hlir.constant  {node_name = "Constant_7396-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %8986 = "dtu_hlir.instance_norm"(%8983, %8984, %8985) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_7397-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %8987 = "dtu_hlir.shape"(%8981) {end = 2147483647 : i64, node_name = "Shape_7398-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %8988 = "dtu_hlir.dynamic_reshape"(%8986, %8987) {allowzero = 0 : i64, node_name = "Reshape_7399-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %8989 = "dtu_hlir.mul"(%8988, %656) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_7400-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8990 = "dtu_hlir.add"(%8989, %657) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_7401-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %8991 = "dtu_hlir.sigmoid"(%8990) {node_name = "Sigmoid_7402-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8992 = "dtu_hlir.mul"(%8990, %8991) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_7403-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8993 = "dtu_hlir.conv_bias"(%8992, %334, %335) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7404-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %8994 = "dtu_hlir.conv_bias"(%8960, %336, %337) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7405-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x64x64xf32>, tensor<320x640x1x1xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %8995 = "dtu_hlir.add"(%8994, %8993) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_7406-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8996 = dtu_hlir.constant  {node_name = "Constant_7407-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %8997 = "dtu_hlir.broadcast_in_dim"(%8996) {node_name = "Div_7408-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x320x64x64xf32>
    %8998 = "dtu_hlir.div"(%8995, %8997) {node_name = "Div_7408-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %8999 = "dtu_hlir.shape"(%8998) {end = 2147483647 : i64, node_name = "Shape_7409-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9000 = dtu_hlir.constant  {node_name = "Constant_7410-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9001 = "dtu_hlir.gather"(%8999, %9000) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7411-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9002 = "dtu_hlir.shape"(%8998) {end = 2147483647 : i64, node_name = "Shape_7412-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9003 = dtu_hlir.constant  {node_name = "Constant_7413-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9004 = "dtu_hlir.gather"(%9002, %9003) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7414-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9005 = "dtu_hlir.shape"(%8998) {end = 2147483647 : i64, node_name = "Shape_7415-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9006 = dtu_hlir.constant  {node_name = "Constant_7416-0", node_type = "Constant"} dense<3> : tensor<i64>
    %9007 = "dtu_hlir.gather"(%9005, %9006) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7417-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9008 = dtu_hlir.constant  {node_name = "Constant_7418-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %9009 = "dtu_hlir.dynamic_reshape"(%8998, %9008) {allowzero = 0 : i64, node_name = "Reshape_7419-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %9010 = dtu_hlir.constant  {node_name = "Constant_7420-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %9011 = dtu_hlir.constant  {node_name = "Constant_7421-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %9012 = "dtu_hlir.instance_norm"(%9009, %9010, %9011) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_7422-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %9013 = "dtu_hlir.shape"(%8998) {end = 2147483647 : i64, node_name = "Shape_7423-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9014 = "dtu_hlir.dynamic_reshape"(%9012, %9013) {allowzero = 0 : i64, node_name = "Reshape_7424-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %9015 = "dtu_hlir.mul"(%9014, %658) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_7425-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %9016 = "dtu_hlir.add"(%9015, %659) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_7426-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %9017 = "dtu_hlir.shape"(%9016) {end = 2147483647 : i64, node_name = "Shape_7427-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9018 = dtu_hlir.constant  {node_name = "Constant_7428-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9019 = "dtu_hlir.gather"(%9017, %9018) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7429-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9020 = "dtu_hlir.transpose"(%9016) {node_name = "Transpose_7430-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x64x64x320xf32>
    %9021 = "dtu_hlir.mul"(%9004, %9007) {node_name = "Mul_7431-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9022 = dtu_hlir.constant  {node_name = "Constant_7432-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9023 = "dtu_hlir.unsqueeze"(%9001, %9022) {node_name = "Unsqueeze_7433-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9024 = dtu_hlir.constant  {node_name = "Constant_7434-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9025 = "dtu_hlir.unsqueeze"(%9021, %9024) {node_name = "Unsqueeze_7435-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9026 = dtu_hlir.constant  {node_name = "Constant_7436-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9027 = "dtu_hlir.unsqueeze"(%9019, %9026) {node_name = "Unsqueeze_7437-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9028 = "dtu_hlir.concatenate"(%9023, %9025, %9027) {dimension = 0 : i64, node_name = "Concat_7438-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9029 = "dtu_hlir.dynamic_reshape"(%9020, %9028) {allowzero = 0 : i64, node_name = "Reshape_7439-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %9030 = "dtu_hlir.dot_general"(%9029, %660) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7440-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9031 = "dtu_hlir.add"(%298, %9030) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7441-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9032 = dtu_hlir.constant  {node_name = "ReduceMean_7442-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9033 = "dtu_hlir.reshape"(%9032) {node_name = "ReduceMean_7442-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9034 = "dtu_hlir.reduce"(%9031, %9033) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7442-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7442-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9035 = dtu_hlir.constant  {node_name = "ReduceMean_7442-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9036 = "dtu_hlir.unsqueeze"(%9034, %9035) {node_name = "ReduceMean_7442-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9037 = dtu_hlir.constant  {node_name = "ReduceMean_7442-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9038 = "dtu_hlir.broadcast_in_dim"(%9037) {node_name = "ReduceMean_7442-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9039 = "dtu_hlir.div"(%9036, %9038) {node_name = "ReduceMean_7442-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9040 = "dtu_hlir.sub"(%9031, %9039) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_7443-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9041 = dtu_hlir.constant  {node_name = "Constant_7444-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %9042 = "dtu_hlir.broadcast_in_dim"(%9041) {node_name = "Pow_7445-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %9043 = "dtu_hlir.pow"(%9040, %9042) {node_name = "Pow_7445-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9044 = dtu_hlir.constant  {node_name = "ReduceMean_7446-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9045 = "dtu_hlir.reshape"(%9044) {node_name = "ReduceMean_7446-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9046 = "dtu_hlir.reduce"(%9043, %9045) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7446-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7446-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9047 = dtu_hlir.constant  {node_name = "ReduceMean_7446-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9048 = "dtu_hlir.unsqueeze"(%9046, %9047) {node_name = "ReduceMean_7446-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9049 = dtu_hlir.constant  {node_name = "ReduceMean_7446-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9050 = "dtu_hlir.broadcast_in_dim"(%9049) {node_name = "ReduceMean_7446-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9051 = "dtu_hlir.div"(%9048, %9050) {node_name = "ReduceMean_7446-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9052 = dtu_hlir.constant  {node_name = "Constant_7447-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %9053 = "dtu_hlir.broadcast_in_dim"(%9052) {node_name = "Add_7448-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %9054 = "dtu_hlir.add"(%9051, %9053) {node_name = "Add_7448-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9055 = "dtu_hlir.sqrt"(%9054) {node_name = "Sqrt_7449-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9056 = "dtu_hlir.div"(%9040, %9055) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_7450-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9057 = "dtu_hlir.mul"(%9056, %303) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_7451-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9058 = "dtu_hlir.add"(%9057, %304) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7452-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9059 = "dtu_hlir.dot_general"(%9058, %661) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7453-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9060 = "dtu_hlir.dot_general"(%9058, %662) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7454-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9061 = "dtu_hlir.dot_general"(%9058, %663) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7455-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9062 = "dtu_hlir.shape"(%9059) {end = 2147483647 : i64, node_name = "Shape_7456-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9063 = dtu_hlir.constant  {node_name = "Constant_7457-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9064 = "dtu_hlir.gather"(%9062, %9063) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7458-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9065 = "dtu_hlir.shape"(%9059) {end = 2147483647 : i64, node_name = "Shape_7459-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9066 = dtu_hlir.constant  {node_name = "Constant_7460-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9067 = "dtu_hlir.gather"(%9065, %9066) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7461-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9068 = "dtu_hlir.shape"(%9059) {end = 2147483647 : i64, node_name = "Shape_7462-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9069 = dtu_hlir.constant  {node_name = "Constant_7463-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9070 = "dtu_hlir.gather"(%9068, %9069) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7464-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9071 = dtu_hlir.constant  {node_name = "Constant_7465-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9072 = "dtu_hlir.div"(%9070, %9071) {node_name = "Div_7466-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9073 = "dtu_hlir.convert"(%9072) {node_name = "Cast_7467-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9074 = "dtu_hlir.convert"(%9073) {node_name = "Cast_7468-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9075 = dtu_hlir.constant  {node_name = "Constant_7469-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9076 = "dtu_hlir.unsqueeze"(%9064, %9075) {node_name = "Unsqueeze_7470-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9077 = dtu_hlir.constant  {node_name = "Constant_7471-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9078 = "dtu_hlir.unsqueeze"(%9067, %9077) {node_name = "Unsqueeze_7472-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9079 = dtu_hlir.constant  {node_name = "Constant_7473-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9080 = "dtu_hlir.unsqueeze"(%9074, %9079) {node_name = "Unsqueeze_7474-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9081 = "dtu_hlir.concatenate"(%9076, %9078, %715, %9080) {dimension = 0 : i64, node_name = "Concat_7475-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9082 = "dtu_hlir.dynamic_reshape"(%9059, %9081) {allowzero = 0 : i64, node_name = "Reshape_7476-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9083 = "dtu_hlir.transpose"(%9082) {node_name = "Transpose_7477-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9084 = dtu_hlir.constant  {node_name = "Constant_7478-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9085 = "dtu_hlir.mul"(%9064, %9084) {node_name = "Mul_7479-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9086 = dtu_hlir.constant  {node_name = "Constant_7480-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9087 = "dtu_hlir.div"(%9070, %9086) {node_name = "Div_7481-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9088 = "dtu_hlir.convert"(%9087) {node_name = "Cast_7482-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9089 = "dtu_hlir.convert"(%9088) {node_name = "Cast_7483-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9090 = dtu_hlir.constant  {node_name = "Constant_7484-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9091 = "dtu_hlir.unsqueeze"(%9085, %9090) {node_name = "Unsqueeze_7485-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9092 = dtu_hlir.constant  {node_name = "Constant_7486-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9093 = "dtu_hlir.unsqueeze"(%9067, %9092) {node_name = "Unsqueeze_7487-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9094 = dtu_hlir.constant  {node_name = "Constant_7488-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9095 = "dtu_hlir.unsqueeze"(%9089, %9094) {node_name = "Unsqueeze_7489-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9096 = "dtu_hlir.concatenate"(%9091, %9093, %9095) {dimension = 0 : i64, node_name = "Concat_7490-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9097 = "dtu_hlir.dynamic_reshape"(%9083, %9096) {allowzero = 0 : i64, node_name = "Reshape_7491-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9098 = "dtu_hlir.shape"(%9060) {end = 2147483647 : i64, node_name = "Shape_7492-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9099 = dtu_hlir.constant  {node_name = "Constant_7493-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9100 = "dtu_hlir.gather"(%9098, %9099) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7494-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9101 = "dtu_hlir.shape"(%9060) {end = 2147483647 : i64, node_name = "Shape_7495-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9102 = dtu_hlir.constant  {node_name = "Constant_7496-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9103 = "dtu_hlir.gather"(%9101, %9102) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7497-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9104 = "dtu_hlir.shape"(%9060) {end = 2147483647 : i64, node_name = "Shape_7498-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9105 = dtu_hlir.constant  {node_name = "Constant_7499-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9106 = "dtu_hlir.gather"(%9104, %9105) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7500-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9107 = dtu_hlir.constant  {node_name = "Constant_7501-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9108 = "dtu_hlir.div"(%9106, %9107) {node_name = "Div_7502-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9109 = "dtu_hlir.convert"(%9108) {node_name = "Cast_7503-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9110 = "dtu_hlir.convert"(%9109) {node_name = "Cast_7504-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9111 = dtu_hlir.constant  {node_name = "Constant_7505-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9112 = "dtu_hlir.unsqueeze"(%9100, %9111) {node_name = "Unsqueeze_7506-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9113 = dtu_hlir.constant  {node_name = "Constant_7507-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9114 = "dtu_hlir.unsqueeze"(%9103, %9113) {node_name = "Unsqueeze_7508-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9115 = dtu_hlir.constant  {node_name = "Constant_7509-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9116 = "dtu_hlir.unsqueeze"(%9110, %9115) {node_name = "Unsqueeze_7510-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9117 = "dtu_hlir.concatenate"(%9112, %9114, %714, %9116) {dimension = 0 : i64, node_name = "Concat_7511-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9118 = "dtu_hlir.dynamic_reshape"(%9060, %9117) {allowzero = 0 : i64, node_name = "Reshape_7512-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9119 = "dtu_hlir.transpose"(%9118) {node_name = "Transpose_7513-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9120 = dtu_hlir.constant  {node_name = "Constant_7514-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9121 = "dtu_hlir.mul"(%9100, %9120) {node_name = "Mul_7515-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9122 = dtu_hlir.constant  {node_name = "Constant_7516-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9123 = "dtu_hlir.div"(%9106, %9122) {node_name = "Div_7517-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9124 = "dtu_hlir.convert"(%9123) {node_name = "Cast_7518-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9125 = "dtu_hlir.convert"(%9124) {node_name = "Cast_7519-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9126 = dtu_hlir.constant  {node_name = "Constant_7520-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9127 = "dtu_hlir.unsqueeze"(%9121, %9126) {node_name = "Unsqueeze_7521-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9128 = dtu_hlir.constant  {node_name = "Constant_7522-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9129 = "dtu_hlir.unsqueeze"(%9103, %9128) {node_name = "Unsqueeze_7523-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9130 = dtu_hlir.constant  {node_name = "Constant_7524-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9131 = "dtu_hlir.unsqueeze"(%9125, %9130) {node_name = "Unsqueeze_7525-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9132 = "dtu_hlir.concatenate"(%9127, %9129, %9131) {dimension = 0 : i64, node_name = "Concat_7526-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9133 = "dtu_hlir.dynamic_reshape"(%9119, %9132) {allowzero = 0 : i64, node_name = "Reshape_7527-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9134 = "dtu_hlir.shape"(%9061) {end = 2147483647 : i64, node_name = "Shape_7528-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9135 = dtu_hlir.constant  {node_name = "Constant_7529-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9136 = "dtu_hlir.gather"(%9134, %9135) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7530-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9137 = "dtu_hlir.shape"(%9061) {end = 2147483647 : i64, node_name = "Shape_7531-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9138 = dtu_hlir.constant  {node_name = "Constant_7532-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9139 = "dtu_hlir.gather"(%9137, %9138) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7533-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9140 = "dtu_hlir.shape"(%9061) {end = 2147483647 : i64, node_name = "Shape_7534-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9141 = dtu_hlir.constant  {node_name = "Constant_7535-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9142 = "dtu_hlir.gather"(%9140, %9141) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7536-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9143 = dtu_hlir.constant  {node_name = "Constant_7537-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9144 = "dtu_hlir.div"(%9142, %9143) {node_name = "Div_7538-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9145 = "dtu_hlir.convert"(%9144) {node_name = "Cast_7539-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9146 = "dtu_hlir.convert"(%9145) {node_name = "Cast_7540-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9147 = dtu_hlir.constant  {node_name = "Constant_7541-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9148 = "dtu_hlir.unsqueeze"(%9136, %9147) {node_name = "Unsqueeze_7542-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9149 = dtu_hlir.constant  {node_name = "Constant_7543-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9150 = "dtu_hlir.unsqueeze"(%9139, %9149) {node_name = "Unsqueeze_7544-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9151 = dtu_hlir.constant  {node_name = "Constant_7545-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9152 = "dtu_hlir.unsqueeze"(%9146, %9151) {node_name = "Unsqueeze_7546-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9153 = "dtu_hlir.concatenate"(%9148, %9150, %713, %9152) {dimension = 0 : i64, node_name = "Concat_7547-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9154 = "dtu_hlir.dynamic_reshape"(%9061, %9153) {allowzero = 0 : i64, node_name = "Reshape_7548-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9155 = "dtu_hlir.transpose"(%9154) {node_name = "Transpose_7549-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9156 = dtu_hlir.constant  {node_name = "Constant_7550-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9157 = "dtu_hlir.mul"(%9136, %9156) {node_name = "Mul_7551-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9158 = dtu_hlir.constant  {node_name = "Constant_7552-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9159 = "dtu_hlir.div"(%9142, %9158) {node_name = "Div_7553-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9160 = "dtu_hlir.convert"(%9159) {node_name = "Cast_7554-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9161 = "dtu_hlir.convert"(%9160) {node_name = "Cast_7555-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9162 = dtu_hlir.constant  {node_name = "Constant_7556-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9163 = "dtu_hlir.unsqueeze"(%9157, %9162) {node_name = "Unsqueeze_7557-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9164 = dtu_hlir.constant  {node_name = "Constant_7558-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9165 = "dtu_hlir.unsqueeze"(%9139, %9164) {node_name = "Unsqueeze_7559-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9166 = dtu_hlir.constant  {node_name = "Constant_7560-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9167 = "dtu_hlir.unsqueeze"(%9161, %9166) {node_name = "Unsqueeze_7561-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9168 = "dtu_hlir.concatenate"(%9163, %9165, %9167) {dimension = 0 : i64, node_name = "Concat_7562-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9169 = "dtu_hlir.dynamic_reshape"(%9155, %9168) {allowzero = 0 : i64, node_name = "Reshape_7563-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9170 = "dtu_hlir.shape"(%9097) {end = 2147483647 : i64, node_name = "Shape_7564-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9171 = dtu_hlir.constant  {node_name = "Constant_7565-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9172 = "dtu_hlir.gather"(%9170, %9171) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7566-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9173 = "dtu_hlir.shape"(%9097) {end = 2147483647 : i64, node_name = "Shape_7567-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9174 = dtu_hlir.constant  {node_name = "Constant_7568-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9175 = "dtu_hlir.gather"(%9173, %9174) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7569-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9176 = "dtu_hlir.shape"(%9133) {end = 2147483647 : i64, node_name = "Shape_7570-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9177 = dtu_hlir.constant  {node_name = "Constant_7571-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9178 = "dtu_hlir.gather"(%9176, %9177) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7572-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9179 = dtu_hlir.constant  {node_name = "Constant_7573-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9180 = "dtu_hlir.unsqueeze"(%9172, %9179) {node_name = "Unsqueeze_7574-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9181 = dtu_hlir.constant  {node_name = "Constant_7575-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9182 = "dtu_hlir.unsqueeze"(%9175, %9181) {node_name = "Unsqueeze_7576-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9183 = dtu_hlir.constant  {node_name = "Constant_7577-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9184 = "dtu_hlir.unsqueeze"(%9178, %9183) {node_name = "Unsqueeze_7578-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9185 = "dtu_hlir.concatenate"(%9180, %9182, %9184) {dimension = 0 : i64, node_name = "Concat_7579-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9186 = dtu_hlir.constant  {node_name = "ConstantOfShape_7580-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %9187 = "dtu_hlir.dynamic_broadcast_in_dim"(%9186, %9185) {node_name = "ConstantOfShape_7580-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x4096xf32>
    %9188 = "dtu_hlir.transpose"(%9133) {node_name = "Transpose_7581-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<10x64x4096xf32>
    %9189 = "dtu_hlir.dot_general"(%9097, %9188) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7582-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x4096xf32>) -> tensor<10x4096x4096xf32>
    %9190 = "dtu_hlir.broadcast_in_dim"(%712) {node_name = "Mul_7583-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %9191 = "dtu_hlir.mul"(%9189, %9190) {node_name = "Mul_7583-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9192 = "dtu_hlir.broadcast_in_dim"(%711) {node_name = "Mul_7584-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %9193 = "dtu_hlir.mul"(%9187, %9192) {node_name = "Mul_7584-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9194 = "dtu_hlir.add"(%9191, %9193) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7585-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9195 = "dtu_hlir.softmax"(%9194) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7586-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9196 = "dtu_hlir.convert"(%9195) {node_name = "Cast_7587-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9197 = "dtu_hlir.dot_general"(%9196, %9169) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7588-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x64xf32>) -> tensor<10x4096x64xf32>
    %9198 = "dtu_hlir.shape"(%9197) {end = 2147483647 : i64, node_name = "Shape_7589-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9199 = dtu_hlir.constant  {node_name = "Constant_7590-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9200 = "dtu_hlir.gather"(%9198, %9199) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7591-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9201 = "dtu_hlir.shape"(%9197) {end = 2147483647 : i64, node_name = "Shape_7592-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9202 = dtu_hlir.constant  {node_name = "Constant_7593-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9203 = "dtu_hlir.gather"(%9201, %9202) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7594-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9204 = "dtu_hlir.shape"(%9197) {end = 2147483647 : i64, node_name = "Shape_7595-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9205 = dtu_hlir.constant  {node_name = "Constant_7596-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9206 = "dtu_hlir.gather"(%9204, %9205) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7597-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9207 = dtu_hlir.constant  {node_name = "Constant_7598-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9208 = "dtu_hlir.div"(%9200, %9207) {node_name = "Div_7599-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9209 = "dtu_hlir.convert"(%9208) {node_name = "Cast_7600-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9210 = "dtu_hlir.convert"(%9209) {node_name = "Cast_7601-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9211 = dtu_hlir.constant  {node_name = "Constant_7602-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9212 = "dtu_hlir.unsqueeze"(%9210, %9211) {node_name = "Unsqueeze_7603-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9213 = dtu_hlir.constant  {node_name = "Constant_7604-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9214 = "dtu_hlir.unsqueeze"(%9203, %9213) {node_name = "Unsqueeze_7605-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9215 = dtu_hlir.constant  {node_name = "Constant_7606-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9216 = "dtu_hlir.unsqueeze"(%9206, %9215) {node_name = "Unsqueeze_7607-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9217 = "dtu_hlir.concatenate"(%9212, %710, %9214, %9216) {dimension = 0 : i64, node_name = "Concat_7608-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9218 = "dtu_hlir.dynamic_reshape"(%9197, %9217) {allowzero = 0 : i64, node_name = "Reshape_7609-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %9219 = "dtu_hlir.transpose"(%9218) {node_name = "Transpose_7610-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %9220 = dtu_hlir.constant  {node_name = "Constant_7611-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9221 = "dtu_hlir.div"(%9200, %9220) {node_name = "Div_7612-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9222 = "dtu_hlir.convert"(%9221) {node_name = "Cast_7613-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9223 = "dtu_hlir.convert"(%9222) {node_name = "Cast_7614-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9224 = dtu_hlir.constant  {node_name = "Constant_7615-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9225 = "dtu_hlir.mul"(%9206, %9224) {node_name = "Mul_7616-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9226 = dtu_hlir.constant  {node_name = "Constant_7617-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9227 = "dtu_hlir.unsqueeze"(%9223, %9226) {node_name = "Unsqueeze_7618-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9228 = dtu_hlir.constant  {node_name = "Constant_7619-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9229 = "dtu_hlir.unsqueeze"(%9203, %9228) {node_name = "Unsqueeze_7620-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9230 = dtu_hlir.constant  {node_name = "Constant_7621-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9231 = "dtu_hlir.unsqueeze"(%9225, %9230) {node_name = "Unsqueeze_7622-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9232 = "dtu_hlir.concatenate"(%9227, %9229, %9231) {dimension = 0 : i64, node_name = "Concat_7623-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9233 = "dtu_hlir.dynamic_reshape"(%9219, %9232) {allowzero = 0 : i64, node_name = "Reshape_7624-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %9234 = "dtu_hlir.dot_general"(%9233, %664) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7625-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9235 = "dtu_hlir.add"(%299, %9234) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7626-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9236 = "dtu_hlir.add"(%9235, %9031) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7627-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9237 = dtu_hlir.constant  {node_name = "ReduceMean_7628-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9238 = "dtu_hlir.reshape"(%9237) {node_name = "ReduceMean_7628-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9239 = "dtu_hlir.reduce"(%9236, %9238) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7628-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7628-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9240 = dtu_hlir.constant  {node_name = "ReduceMean_7628-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9241 = "dtu_hlir.unsqueeze"(%9239, %9240) {node_name = "ReduceMean_7628-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9242 = dtu_hlir.constant  {node_name = "ReduceMean_7628-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9243 = "dtu_hlir.broadcast_in_dim"(%9242) {node_name = "ReduceMean_7628-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9244 = "dtu_hlir.div"(%9241, %9243) {node_name = "ReduceMean_7628-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9245 = "dtu_hlir.sub"(%9236, %9244) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_7629-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9246 = dtu_hlir.constant  {node_name = "Constant_7630-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %9247 = "dtu_hlir.broadcast_in_dim"(%9246) {node_name = "Pow_7631-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %9248 = "dtu_hlir.pow"(%9245, %9247) {node_name = "Pow_7631-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9249 = dtu_hlir.constant  {node_name = "ReduceMean_7632-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9250 = "dtu_hlir.reshape"(%9249) {node_name = "ReduceMean_7632-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9251 = "dtu_hlir.reduce"(%9248, %9250) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7632-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7632-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9252 = dtu_hlir.constant  {node_name = "ReduceMean_7632-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9253 = "dtu_hlir.unsqueeze"(%9251, %9252) {node_name = "ReduceMean_7632-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9254 = dtu_hlir.constant  {node_name = "ReduceMean_7632-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9255 = "dtu_hlir.broadcast_in_dim"(%9254) {node_name = "ReduceMean_7632-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9256 = "dtu_hlir.div"(%9253, %9255) {node_name = "ReduceMean_7632-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9257 = dtu_hlir.constant  {node_name = "Constant_7633-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %9258 = "dtu_hlir.broadcast_in_dim"(%9257) {node_name = "Add_7634-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %9259 = "dtu_hlir.add"(%9256, %9258) {node_name = "Add_7634-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9260 = "dtu_hlir.sqrt"(%9259) {node_name = "Sqrt_7635-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9261 = "dtu_hlir.div"(%9245, %9260) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_7636-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9262 = "dtu_hlir.mul"(%9261, %305) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_7637-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9263 = "dtu_hlir.add"(%9262, %306) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7638-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9264 = "dtu_hlir.dot_general"(%9263, %665) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7639-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9265 = "dtu_hlir.dot_general"(%arg2, %666) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7640-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %9266 = "dtu_hlir.dot_general"(%arg2, %667) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7641-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %9267 = "dtu_hlir.shape"(%9264) {end = 2147483647 : i64, node_name = "Shape_7642-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9268 = dtu_hlir.constant  {node_name = "Constant_7643-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9269 = "dtu_hlir.gather"(%9267, %9268) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7644-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9270 = "dtu_hlir.shape"(%9264) {end = 2147483647 : i64, node_name = "Shape_7645-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9271 = dtu_hlir.constant  {node_name = "Constant_7646-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9272 = "dtu_hlir.gather"(%9270, %9271) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7647-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9273 = "dtu_hlir.shape"(%9264) {end = 2147483647 : i64, node_name = "Shape_7648-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9274 = dtu_hlir.constant  {node_name = "Constant_7649-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9275 = "dtu_hlir.gather"(%9273, %9274) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7650-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9276 = dtu_hlir.constant  {node_name = "Constant_7651-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9277 = "dtu_hlir.div"(%9275, %9276) {node_name = "Div_7652-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9278 = "dtu_hlir.convert"(%9277) {node_name = "Cast_7653-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9279 = "dtu_hlir.convert"(%9278) {node_name = "Cast_7654-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9280 = dtu_hlir.constant  {node_name = "Constant_7655-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9281 = "dtu_hlir.unsqueeze"(%9269, %9280) {node_name = "Unsqueeze_7656-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9282 = dtu_hlir.constant  {node_name = "Constant_7657-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9283 = "dtu_hlir.unsqueeze"(%9272, %9282) {node_name = "Unsqueeze_7658-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9284 = dtu_hlir.constant  {node_name = "Constant_7659-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9285 = "dtu_hlir.unsqueeze"(%9279, %9284) {node_name = "Unsqueeze_7660-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9286 = "dtu_hlir.concatenate"(%9281, %9283, %709, %9285) {dimension = 0 : i64, node_name = "Concat_7661-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9287 = "dtu_hlir.dynamic_reshape"(%9264, %9286) {allowzero = 0 : i64, node_name = "Reshape_7662-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9288 = "dtu_hlir.transpose"(%9287) {node_name = "Transpose_7663-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9289 = dtu_hlir.constant  {node_name = "Constant_7664-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9290 = "dtu_hlir.mul"(%9269, %9289) {node_name = "Mul_7665-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9291 = dtu_hlir.constant  {node_name = "Constant_7666-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9292 = "dtu_hlir.div"(%9275, %9291) {node_name = "Div_7667-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9293 = "dtu_hlir.convert"(%9292) {node_name = "Cast_7668-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9294 = "dtu_hlir.convert"(%9293) {node_name = "Cast_7669-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9295 = dtu_hlir.constant  {node_name = "Constant_7670-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9296 = "dtu_hlir.unsqueeze"(%9290, %9295) {node_name = "Unsqueeze_7671-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9297 = dtu_hlir.constant  {node_name = "Constant_7672-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9298 = "dtu_hlir.unsqueeze"(%9272, %9297) {node_name = "Unsqueeze_7673-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9299 = dtu_hlir.constant  {node_name = "Constant_7674-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9300 = "dtu_hlir.unsqueeze"(%9294, %9299) {node_name = "Unsqueeze_7675-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9301 = "dtu_hlir.concatenate"(%9296, %9298, %9300) {dimension = 0 : i64, node_name = "Concat_7676-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9302 = "dtu_hlir.dynamic_reshape"(%9288, %9301) {allowzero = 0 : i64, node_name = "Reshape_7677-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9303 = "dtu_hlir.shape"(%9265) {end = 2147483647 : i64, node_name = "Shape_7678-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9304 = dtu_hlir.constant  {node_name = "Constant_7679-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9305 = "dtu_hlir.gather"(%9303, %9304) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7680-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9306 = "dtu_hlir.shape"(%9265) {end = 2147483647 : i64, node_name = "Shape_7681-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9307 = dtu_hlir.constant  {node_name = "Constant_7682-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9308 = "dtu_hlir.gather"(%9306, %9307) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7683-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9309 = "dtu_hlir.shape"(%9265) {end = 2147483647 : i64, node_name = "Shape_7684-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9310 = dtu_hlir.constant  {node_name = "Constant_7685-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9311 = "dtu_hlir.gather"(%9309, %9310) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7686-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9312 = dtu_hlir.constant  {node_name = "Constant_7687-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9313 = "dtu_hlir.div"(%9311, %9312) {node_name = "Div_7688-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9314 = "dtu_hlir.convert"(%9313) {node_name = "Cast_7689-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9315 = "dtu_hlir.convert"(%9314) {node_name = "Cast_7690-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9316 = dtu_hlir.constant  {node_name = "Constant_7691-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9317 = "dtu_hlir.unsqueeze"(%9305, %9316) {node_name = "Unsqueeze_7692-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9318 = dtu_hlir.constant  {node_name = "Constant_7693-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9319 = "dtu_hlir.unsqueeze"(%9308, %9318) {node_name = "Unsqueeze_7694-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9320 = dtu_hlir.constant  {node_name = "Constant_7695-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9321 = "dtu_hlir.unsqueeze"(%9315, %9320) {node_name = "Unsqueeze_7696-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9322 = "dtu_hlir.concatenate"(%9317, %9319, %708, %9321) {dimension = 0 : i64, node_name = "Concat_7697-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9323 = "dtu_hlir.dynamic_reshape"(%9265, %9322) {allowzero = 0 : i64, node_name = "Reshape_7698-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %9324 = "dtu_hlir.transpose"(%9323) {node_name = "Transpose_7699-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %9325 = dtu_hlir.constant  {node_name = "Constant_7700-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9326 = "dtu_hlir.mul"(%9305, %9325) {node_name = "Mul_7701-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9327 = dtu_hlir.constant  {node_name = "Constant_7702-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9328 = "dtu_hlir.div"(%9311, %9327) {node_name = "Div_7703-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9329 = "dtu_hlir.convert"(%9328) {node_name = "Cast_7704-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9330 = "dtu_hlir.convert"(%9329) {node_name = "Cast_7705-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9331 = dtu_hlir.constant  {node_name = "Constant_7706-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9332 = "dtu_hlir.unsqueeze"(%9326, %9331) {node_name = "Unsqueeze_7707-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9333 = dtu_hlir.constant  {node_name = "Constant_7708-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9334 = "dtu_hlir.unsqueeze"(%9308, %9333) {node_name = "Unsqueeze_7709-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9335 = dtu_hlir.constant  {node_name = "Constant_7710-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9336 = "dtu_hlir.unsqueeze"(%9330, %9335) {node_name = "Unsqueeze_7711-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9337 = "dtu_hlir.concatenate"(%9332, %9334, %9336) {dimension = 0 : i64, node_name = "Concat_7712-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9338 = "dtu_hlir.dynamic_reshape"(%9324, %9337) {allowzero = 0 : i64, node_name = "Reshape_7713-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %9339 = "dtu_hlir.shape"(%9266) {end = 2147483647 : i64, node_name = "Shape_7714-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9340 = dtu_hlir.constant  {node_name = "Constant_7715-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9341 = "dtu_hlir.gather"(%9339, %9340) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7716-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9342 = "dtu_hlir.shape"(%9266) {end = 2147483647 : i64, node_name = "Shape_7717-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9343 = dtu_hlir.constant  {node_name = "Constant_7718-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9344 = "dtu_hlir.gather"(%9342, %9343) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7719-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9345 = "dtu_hlir.shape"(%9266) {end = 2147483647 : i64, node_name = "Shape_7720-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9346 = dtu_hlir.constant  {node_name = "Constant_7721-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9347 = "dtu_hlir.gather"(%9345, %9346) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7722-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9348 = dtu_hlir.constant  {node_name = "Constant_7723-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9349 = "dtu_hlir.div"(%9347, %9348) {node_name = "Div_7724-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9350 = "dtu_hlir.convert"(%9349) {node_name = "Cast_7725-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9351 = "dtu_hlir.convert"(%9350) {node_name = "Cast_7726-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9352 = dtu_hlir.constant  {node_name = "Constant_7727-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9353 = "dtu_hlir.unsqueeze"(%9341, %9352) {node_name = "Unsqueeze_7728-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9354 = dtu_hlir.constant  {node_name = "Constant_7729-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9355 = "dtu_hlir.unsqueeze"(%9344, %9354) {node_name = "Unsqueeze_7730-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9356 = dtu_hlir.constant  {node_name = "Constant_7731-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9357 = "dtu_hlir.unsqueeze"(%9351, %9356) {node_name = "Unsqueeze_7732-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9358 = "dtu_hlir.concatenate"(%9353, %9355, %707, %9357) {dimension = 0 : i64, node_name = "Concat_7733-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9359 = "dtu_hlir.dynamic_reshape"(%9266, %9358) {allowzero = 0 : i64, node_name = "Reshape_7734-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %9360 = "dtu_hlir.transpose"(%9359) {node_name = "Transpose_7735-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %9361 = dtu_hlir.constant  {node_name = "Constant_7736-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9362 = "dtu_hlir.mul"(%9341, %9361) {node_name = "Mul_7737-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9363 = dtu_hlir.constant  {node_name = "Constant_7738-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9364 = "dtu_hlir.div"(%9347, %9363) {node_name = "Div_7739-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9365 = "dtu_hlir.convert"(%9364) {node_name = "Cast_7740-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9366 = "dtu_hlir.convert"(%9365) {node_name = "Cast_7741-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9367 = dtu_hlir.constant  {node_name = "Constant_7742-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9368 = "dtu_hlir.unsqueeze"(%9362, %9367) {node_name = "Unsqueeze_7743-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9369 = dtu_hlir.constant  {node_name = "Constant_7744-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9370 = "dtu_hlir.unsqueeze"(%9344, %9369) {node_name = "Unsqueeze_7745-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9371 = dtu_hlir.constant  {node_name = "Constant_7746-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9372 = "dtu_hlir.unsqueeze"(%9366, %9371) {node_name = "Unsqueeze_7747-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9373 = "dtu_hlir.concatenate"(%9368, %9370, %9372) {dimension = 0 : i64, node_name = "Concat_7748-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9374 = "dtu_hlir.dynamic_reshape"(%9360, %9373) {allowzero = 0 : i64, node_name = "Reshape_7749-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %9375 = "dtu_hlir.shape"(%9302) {end = 2147483647 : i64, node_name = "Shape_7750-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9376 = dtu_hlir.constant  {node_name = "Constant_7751-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9377 = "dtu_hlir.gather"(%9375, %9376) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7752-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9378 = "dtu_hlir.shape"(%9302) {end = 2147483647 : i64, node_name = "Shape_7753-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9379 = dtu_hlir.constant  {node_name = "Constant_7754-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9380 = "dtu_hlir.gather"(%9378, %9379) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7755-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9381 = "dtu_hlir.shape"(%9338) {end = 2147483647 : i64, node_name = "Shape_7756-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<3xi64>
    %9382 = dtu_hlir.constant  {node_name = "Constant_7757-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9383 = "dtu_hlir.gather"(%9381, %9382) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7758-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9384 = dtu_hlir.constant  {node_name = "Constant_7759-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9385 = "dtu_hlir.unsqueeze"(%9377, %9384) {node_name = "Unsqueeze_7760-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9386 = dtu_hlir.constant  {node_name = "Constant_7761-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9387 = "dtu_hlir.unsqueeze"(%9380, %9386) {node_name = "Unsqueeze_7762-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9388 = dtu_hlir.constant  {node_name = "Constant_7763-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9389 = "dtu_hlir.unsqueeze"(%9383, %9388) {node_name = "Unsqueeze_7764-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9390 = "dtu_hlir.concatenate"(%9385, %9387, %9389) {dimension = 0 : i64, node_name = "Concat_7765-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9391 = dtu_hlir.constant  {node_name = "ConstantOfShape_7766-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %9392 = "dtu_hlir.dynamic_broadcast_in_dim"(%9391, %9390) {node_name = "ConstantOfShape_7766-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x77xf32>
    %9393 = "dtu_hlir.transpose"(%9338) {node_name = "Transpose_7767-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<10x64x77xf32>
    %9394 = "dtu_hlir.dot_general"(%9302, %9393) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7768-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x77xf32>) -> tensor<10x4096x77xf32>
    %9395 = "dtu_hlir.broadcast_in_dim"(%706) {node_name = "Mul_7769-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %9396 = "dtu_hlir.mul"(%9394, %9395) {node_name = "Mul_7769-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9397 = "dtu_hlir.broadcast_in_dim"(%705) {node_name = "Mul_7770-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %9398 = "dtu_hlir.mul"(%9392, %9397) {node_name = "Mul_7770-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9399 = "dtu_hlir.add"(%9396, %9398) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7771-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9400 = "dtu_hlir.softmax"(%9399) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7772-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9401 = "dtu_hlir.convert"(%9400) {node_name = "Cast_7773-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9402 = "dtu_hlir.dot_general"(%9401, %9374) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7774-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x77x64xf32>) -> tensor<10x4096x64xf32>
    %9403 = "dtu_hlir.shape"(%9402) {end = 2147483647 : i64, node_name = "Shape_7775-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9404 = dtu_hlir.constant  {node_name = "Constant_7776-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9405 = "dtu_hlir.gather"(%9403, %9404) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7777-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9406 = "dtu_hlir.shape"(%9402) {end = 2147483647 : i64, node_name = "Shape_7778-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9407 = dtu_hlir.constant  {node_name = "Constant_7779-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9408 = "dtu_hlir.gather"(%9406, %9407) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7780-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9409 = "dtu_hlir.shape"(%9402) {end = 2147483647 : i64, node_name = "Shape_7781-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9410 = dtu_hlir.constant  {node_name = "Constant_7782-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9411 = "dtu_hlir.gather"(%9409, %9410) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7783-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9412 = dtu_hlir.constant  {node_name = "Constant_7784-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9413 = "dtu_hlir.div"(%9405, %9412) {node_name = "Div_7785-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9414 = "dtu_hlir.convert"(%9413) {node_name = "Cast_7786-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9415 = "dtu_hlir.convert"(%9414) {node_name = "Cast_7787-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9416 = dtu_hlir.constant  {node_name = "Constant_7788-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9417 = "dtu_hlir.unsqueeze"(%9415, %9416) {node_name = "Unsqueeze_7789-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9418 = dtu_hlir.constant  {node_name = "Constant_7790-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9419 = "dtu_hlir.unsqueeze"(%9408, %9418) {node_name = "Unsqueeze_7791-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9420 = dtu_hlir.constant  {node_name = "Constant_7792-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9421 = "dtu_hlir.unsqueeze"(%9411, %9420) {node_name = "Unsqueeze_7793-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9422 = "dtu_hlir.concatenate"(%9417, %704, %9419, %9421) {dimension = 0 : i64, node_name = "Concat_7794-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9423 = "dtu_hlir.dynamic_reshape"(%9402, %9422) {allowzero = 0 : i64, node_name = "Reshape_7795-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %9424 = "dtu_hlir.transpose"(%9423) {node_name = "Transpose_7796-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %9425 = dtu_hlir.constant  {node_name = "Constant_7797-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9426 = "dtu_hlir.div"(%9405, %9425) {node_name = "Div_7798-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9427 = "dtu_hlir.convert"(%9426) {node_name = "Cast_7799-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9428 = "dtu_hlir.convert"(%9427) {node_name = "Cast_7800-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9429 = dtu_hlir.constant  {node_name = "Constant_7801-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9430 = "dtu_hlir.mul"(%9411, %9429) {node_name = "Mul_7802-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9431 = dtu_hlir.constant  {node_name = "Constant_7803-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9432 = "dtu_hlir.unsqueeze"(%9428, %9431) {node_name = "Unsqueeze_7804-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9433 = dtu_hlir.constant  {node_name = "Constant_7805-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9434 = "dtu_hlir.unsqueeze"(%9408, %9433) {node_name = "Unsqueeze_7806-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9435 = dtu_hlir.constant  {node_name = "Constant_7807-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9436 = "dtu_hlir.unsqueeze"(%9430, %9435) {node_name = "Unsqueeze_7808-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9437 = "dtu_hlir.concatenate"(%9432, %9434, %9436) {dimension = 0 : i64, node_name = "Concat_7809-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9438 = "dtu_hlir.dynamic_reshape"(%9424, %9437) {allowzero = 0 : i64, node_name = "Reshape_7810-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %9439 = "dtu_hlir.dot_general"(%9438, %668) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7811-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9440 = "dtu_hlir.add"(%302, %9439) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7812-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9441 = "dtu_hlir.add"(%9440, %9236) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7813-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9442 = dtu_hlir.constant  {node_name = "ReduceMean_7814-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9443 = "dtu_hlir.reshape"(%9442) {node_name = "ReduceMean_7814-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9444 = "dtu_hlir.reduce"(%9441, %9443) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7814-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7814-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9445 = dtu_hlir.constant  {node_name = "ReduceMean_7814-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9446 = "dtu_hlir.unsqueeze"(%9444, %9445) {node_name = "ReduceMean_7814-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9447 = dtu_hlir.constant  {node_name = "ReduceMean_7814-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9448 = "dtu_hlir.broadcast_in_dim"(%9447) {node_name = "ReduceMean_7814-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9449 = "dtu_hlir.div"(%9446, %9448) {node_name = "ReduceMean_7814-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9450 = "dtu_hlir.sub"(%9441, %9449) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_7815-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9451 = dtu_hlir.constant  {node_name = "Constant_7816-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %9452 = "dtu_hlir.broadcast_in_dim"(%9451) {node_name = "Pow_7817-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %9453 = "dtu_hlir.pow"(%9450, %9452) {node_name = "Pow_7817-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9454 = dtu_hlir.constant  {node_name = "ReduceMean_7818-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9455 = "dtu_hlir.reshape"(%9454) {node_name = "ReduceMean_7818-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9456 = "dtu_hlir.reduce"(%9453, %9455) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7818-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7818-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9457 = dtu_hlir.constant  {node_name = "ReduceMean_7818-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9458 = "dtu_hlir.unsqueeze"(%9456, %9457) {node_name = "ReduceMean_7818-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9459 = dtu_hlir.constant  {node_name = "ReduceMean_7818-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9460 = "dtu_hlir.broadcast_in_dim"(%9459) {node_name = "ReduceMean_7818-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9461 = "dtu_hlir.div"(%9458, %9460) {node_name = "ReduceMean_7818-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9462 = dtu_hlir.constant  {node_name = "Constant_7819-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %9463 = "dtu_hlir.broadcast_in_dim"(%9462) {node_name = "Add_7820-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %9464 = "dtu_hlir.add"(%9461, %9463) {node_name = "Add_7820-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9465 = "dtu_hlir.sqrt"(%9464) {node_name = "Sqrt_7821-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9466 = "dtu_hlir.div"(%9450, %9465) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_7822-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9467 = "dtu_hlir.mul"(%9466, %307) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_7823-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9468 = "dtu_hlir.add"(%9467, %308) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7824-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9469 = "dtu_hlir.dot_general"(%9468, %669) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7825-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x2560xf32>) -> tensor<2x4096x2560xf32>
    %9470 = "dtu_hlir.add"(%300, %9469) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7826-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2560xf32>, tensor<2x4096x2560xf32>) -> tensor<2x4096x2560xf32>
    %9471 = "dtu_hlir.shape"(%9470) {end = 2147483647 : i64, node_name = "Shape_7827-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>) -> tensor<3xi64>
    %9472 = dtu_hlir.constant  {node_name = "Constant_7828-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %9473 = "dtu_hlir.gather"(%9471, %9472) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7829-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %9474 = dtu_hlir.constant  {node_name = "Constant_7830-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9475 = dtu_hlir.constant  {node_name = "Constant_7831-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %9476 = "dtu_hlir.add"(%9473, %9475) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_7832-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %9477 = dtu_hlir.constant  {node_name = "Constant_7833-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %9478 = "dtu_hlir.div"(%9476, %9477) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_7834-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %9479 = dtu_hlir.constant  {node_name = "Constant_7835-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %9480 = "dtu_hlir.mul"(%9478, %9479) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_7836-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %9481 = dtu_hlir.constant  {node_name = "Slice_7837-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %9482 = "dtu_hlir.real_dynamic_slice"(%9470, %9474, %9480, %9481, %9472) {node_name = "Slice_7837-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %9483 = dtu_hlir.constant  {node_name = "Constant_7838-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %9484 = "dtu_hlir.mul"(%9478, %9483) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_7839-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %9485 = "dtu_hlir.shape"(%9480) {end = 2147483647 : i64, node_name = "Slice_7840-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %9486 = dtu_hlir.constant  {node_name = "Slice_7840-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %9487 = "dtu_hlir.dynamic_broadcast_in_dim"(%9486, %9485) {node_name = "Slice_7840-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9488 = "dtu_hlir.real_dynamic_slice"(%9470, %9480, %9484, %9487, %9472) {node_name = "Slice_7840-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %9489 = dtu_hlir.constant  {node_name = "Constant_7841-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %9490 = "dtu_hlir.broadcast_in_dim"(%9489) {node_name = "Div_7842-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %9491 = "dtu_hlir.div"(%9488, %9490) {node_name = "Div_7842-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %9492 = "dtu_hlir.erf"(%9491) {node_name = "Erf_7843-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %9493 = dtu_hlir.constant  {node_name = "Constant_7844-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %9494 = "dtu_hlir.broadcast_in_dim"(%9493) {node_name = "Add_7845-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %9495 = "dtu_hlir.add"(%9492, %9494) {node_name = "Add_7845-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %9496 = "dtu_hlir.mul"(%9488, %9495) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_7846-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %9497 = dtu_hlir.constant  {node_name = "Constant_7847-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %9498 = "dtu_hlir.broadcast_in_dim"(%9497) {node_name = "Mul_7848-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %9499 = "dtu_hlir.mul"(%9496, %9498) {node_name = "Mul_7848-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %9500 = "dtu_hlir.mul"(%9482, %9499) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_7849-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %9501 = "dtu_hlir.dot_general"(%9500, %670) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7850-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<1280x320xf32>) -> tensor<2x4096x320xf32>
    %9502 = "dtu_hlir.add"(%301, %9501) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7851-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9503 = "dtu_hlir.add"(%9502, %9441) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_7852-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9504 = "dtu_hlir.dot_general"(%9503, %671) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7853-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9505 = "dtu_hlir.add"(%309, %9504) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7854-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9506 = dtu_hlir.constant  {node_name = "Constant_7855-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9507 = "dtu_hlir.unsqueeze"(%9001, %9506) {node_name = "Unsqueeze_7856-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9508 = dtu_hlir.constant  {node_name = "Constant_7857-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9509 = "dtu_hlir.unsqueeze"(%9004, %9508) {node_name = "Unsqueeze_7858-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9510 = dtu_hlir.constant  {node_name = "Constant_7859-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9511 = "dtu_hlir.unsqueeze"(%9007, %9510) {node_name = "Unsqueeze_7860-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9512 = dtu_hlir.constant  {node_name = "Constant_7861-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9513 = "dtu_hlir.unsqueeze"(%9019, %9512) {node_name = "Unsqueeze_7862-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9514 = "dtu_hlir.concatenate"(%9507, %9509, %9511, %9513) {dimension = 0 : i64, node_name = "Concat_7863-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9515 = "dtu_hlir.dynamic_reshape"(%9505, %9514) {allowzero = 0 : i64, node_name = "Reshape_7864-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x64x64x320xf32>
    %9516 = "dtu_hlir.transpose"(%9515) {node_name = "Transpose_7865-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>) -> tensor<2x320x64x64xf32>
    %9517 = "dtu_hlir.add"(%9516, %8998) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_7866-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %9518 = "dtu_hlir.concatenate"(%9517, %916) {dimension = 1 : i64, node_name = "Concat_7867-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x640x64x64xf32>
    %9519 = dtu_hlir.constant  {node_name = "Constant_7868-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %9520 = "dtu_hlir.dynamic_reshape"(%9518, %9519) {allowzero = 0 : i64, node_name = "Reshape_7869-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<3xi64>) -> tensor<2x32x81920xf32>
    %9521 = dtu_hlir.constant  {node_name = "Constant_7870-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %9522 = dtu_hlir.constant  {node_name = "Constant_7871-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %9523 = "dtu_hlir.instance_norm"(%9520, %9521, %9522) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_7872-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x81920xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x81920xf32>
    %9524 = "dtu_hlir.shape"(%9518) {end = 2147483647 : i64, node_name = "Shape_7873-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>) -> tensor<4xi64>
    %9525 = "dtu_hlir.dynamic_reshape"(%9523, %9524) {allowzero = 0 : i64, node_name = "Reshape_7874-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x81920xf32>, tensor<4xi64>) -> tensor<2x640x64x64xf32>
    %9526 = "dtu_hlir.mul"(%9525, %672) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_7875-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<640x1x1xf32>) -> tensor<2x640x64x64xf32>
    %9527 = "dtu_hlir.add"(%9526, %673) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_7876-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<640x1x1xf32>) -> tensor<2x640x64x64xf32>
    %9528 = "dtu_hlir.sigmoid"(%9527) {node_name = "Sigmoid_7877-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>) -> tensor<2x640x64x64xf32>
    %9529 = "dtu_hlir.mul"(%9527, %9528) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_7878-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x640x64x64xf32>, tensor<2x640x64x64xf32>) -> tensor<2x640x64x64xf32>
    %9530 = "dtu_hlir.conv_bias"(%9529, %338, %339) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7879-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x64x64xf32>, tensor<320x640x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %9531 = "dtu_hlir.sigmoid"(%915) {node_name = "Sigmoid_7880-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %9532 = "dtu_hlir.mul"(%915, %9531) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7881-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x1280xf32>, tensor<2x1280xf32>) -> tensor<2x1280xf32>
    %9533 = "dtu_hlir.transpose"(%340) {node_name = "Gemm_7882-Transpose-0", node_type = "Gemm", permutation = dense<[1, 0]> : tensor<2xi64>, tif_quantize_type = "f16"} : (tensor<320x1280xf32>) -> tensor<1280x320xf32>
    %9534 = "dtu_hlir.gemm"(%9532, %9533, %341) {alpha = 1.000000e+00 : f32, beta = 1.000000e+00 : f32, node_name = "Gemm_7882-1", node_type = "Gemm", tif_quantize_type = "f16", transA = 0 : i64, transB = 0 : i64} : (tensor<2x1280xf32>, tensor<1280x320xf32>, tensor<320xf32>) -> tensor<2x320xf32>
    %9535 = dtu_hlir.constant  {node_name = "Constant_7883-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %9536 = "dtu_hlir.unsqueeze"(%9534, %9535) {node_name = "Unsqueeze_7884-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320xf32>, tensor<1xi64>) -> tensor<2x320x1xf32>
    %9537 = dtu_hlir.constant  {node_name = "Constant_7885-0", node_type = "Constant"} dense<3> : tensor<1xi64>
    %9538 = "dtu_hlir.unsqueeze"(%9536, %9537) {node_name = "Unsqueeze_7886-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<2x320x1xf32>, tensor<1xi64>) -> tensor<2x320x1x1xf32>
    %9539 = "dtu_hlir.add"(%9530, %9538) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_7887-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %9540 = dtu_hlir.constant  {node_name = "Constant_7888-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %9541 = "dtu_hlir.dynamic_reshape"(%9539, %9540) {allowzero = 0 : i64, node_name = "Reshape_7889-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %9542 = dtu_hlir.constant  {node_name = "Constant_7890-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %9543 = dtu_hlir.constant  {node_name = "Constant_7891-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %9544 = "dtu_hlir.instance_norm"(%9541, %9542, %9543) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_7892-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %9545 = "dtu_hlir.shape"(%9539) {end = 2147483647 : i64, node_name = "Shape_7893-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9546 = "dtu_hlir.dynamic_reshape"(%9544, %9545) {allowzero = 0 : i64, node_name = "Reshape_7894-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %9547 = "dtu_hlir.mul"(%9546, %674) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_7895-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %9548 = "dtu_hlir.add"(%9547, %675) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_7896-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %9549 = "dtu_hlir.sigmoid"(%9548) {node_name = "Sigmoid_7897-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %9550 = "dtu_hlir.mul"(%9548, %9549) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_7898-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %9551 = "dtu_hlir.conv_bias"(%9550, %342, %343) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7899-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<320x320x3x3xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %9552 = "dtu_hlir.conv_bias"(%9518, %344, %345) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7900-0", node_type = "Conv", padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x640x64x64xf32>, tensor<320x640x1x1xf32>, tensor<320xf32>) -> tensor<2x320x64x64xf32>
    %9553 = "dtu_hlir.add"(%9552, %9551) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_7901-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %9554 = dtu_hlir.constant  {node_name = "Constant_7902-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %9555 = "dtu_hlir.broadcast_in_dim"(%9554) {node_name = "Div_7903-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x320x64x64xf32>
    %9556 = "dtu_hlir.div"(%9553, %9555) {node_name = "Div_7903-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %9557 = "dtu_hlir.shape"(%9556) {end = 2147483647 : i64, node_name = "Shape_7904-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9558 = dtu_hlir.constant  {node_name = "Constant_7905-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9559 = "dtu_hlir.gather"(%9557, %9558) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7906-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9560 = "dtu_hlir.shape"(%9556) {end = 2147483647 : i64, node_name = "Shape_7907-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9561 = dtu_hlir.constant  {node_name = "Constant_7908-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9562 = "dtu_hlir.gather"(%9560, %9561) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7909-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9563 = "dtu_hlir.shape"(%9556) {end = 2147483647 : i64, node_name = "Shape_7910-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9564 = dtu_hlir.constant  {node_name = "Constant_7911-0", node_type = "Constant"} dense<3> : tensor<i64>
    %9565 = "dtu_hlir.gather"(%9563, %9564) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7912-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9566 = dtu_hlir.constant  {node_name = "Constant_7913-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %9567 = "dtu_hlir.dynamic_reshape"(%9556, %9566) {allowzero = 0 : i64, node_name = "Reshape_7914-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %9568 = dtu_hlir.constant  {node_name = "Constant_7915-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %9569 = dtu_hlir.constant  {node_name = "Constant_7916-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %9570 = "dtu_hlir.instance_norm"(%9567, %9568, %9569) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_7917-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %9571 = "dtu_hlir.shape"(%9556) {end = 2147483647 : i64, node_name = "Shape_7918-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9572 = "dtu_hlir.dynamic_reshape"(%9570, %9571) {allowzero = 0 : i64, node_name = "Reshape_7919-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %9573 = "dtu_hlir.mul"(%9572, %676) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_7920-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %9574 = "dtu_hlir.add"(%9573, %677) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_7921-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %9575 = "dtu_hlir.shape"(%9574) {end = 2147483647 : i64, node_name = "Shape_7922-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %9576 = dtu_hlir.constant  {node_name = "Constant_7923-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9577 = "dtu_hlir.gather"(%9575, %9576) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7924-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
    %9578 = "dtu_hlir.transpose"(%9574) {node_name = "Transpose_7925-0", node_type = "Transpose", permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x64x64x320xf32>
    %9579 = "dtu_hlir.mul"(%9562, %9565) {node_name = "Mul_7926-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9580 = dtu_hlir.constant  {node_name = "Constant_7927-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9581 = "dtu_hlir.unsqueeze"(%9559, %9580) {node_name = "Unsqueeze_7928-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9582 = dtu_hlir.constant  {node_name = "Constant_7929-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9583 = "dtu_hlir.unsqueeze"(%9579, %9582) {node_name = "Unsqueeze_7930-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9584 = dtu_hlir.constant  {node_name = "Constant_7931-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9585 = "dtu_hlir.unsqueeze"(%9577, %9584) {node_name = "Unsqueeze_7932-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9586 = "dtu_hlir.concatenate"(%9581, %9583, %9585) {dimension = 0 : i64, node_name = "Concat_7933-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9587 = "dtu_hlir.dynamic_reshape"(%9578, %9586) {allowzero = 0 : i64, node_name = "Reshape_7934-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %9588 = "dtu_hlir.dot_general"(%9587, %678) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7935-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9589 = "dtu_hlir.add"(%310, %9588) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7936-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9590 = dtu_hlir.constant  {node_name = "ReduceMean_7937-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9591 = "dtu_hlir.reshape"(%9590) {node_name = "ReduceMean_7937-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9592 = "dtu_hlir.reduce"(%9589, %9591) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7937-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7937-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9593 = dtu_hlir.constant  {node_name = "ReduceMean_7937-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9594 = "dtu_hlir.unsqueeze"(%9592, %9593) {node_name = "ReduceMean_7937-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9595 = dtu_hlir.constant  {node_name = "ReduceMean_7937-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9596 = "dtu_hlir.broadcast_in_dim"(%9595) {node_name = "ReduceMean_7937-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9597 = "dtu_hlir.div"(%9594, %9596) {node_name = "ReduceMean_7937-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9598 = "dtu_hlir.sub"(%9589, %9597) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_7938-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9599 = dtu_hlir.constant  {node_name = "Constant_7939-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %9600 = "dtu_hlir.broadcast_in_dim"(%9599) {node_name = "Pow_7940-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %9601 = "dtu_hlir.pow"(%9598, %9600) {node_name = "Pow_7940-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9602 = dtu_hlir.constant  {node_name = "ReduceMean_7941-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9603 = "dtu_hlir.reshape"(%9602) {node_name = "ReduceMean_7941-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9604 = "dtu_hlir.reduce"(%9601, %9603) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_7941-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_7941-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9605 = dtu_hlir.constant  {node_name = "ReduceMean_7941-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9606 = "dtu_hlir.unsqueeze"(%9604, %9605) {node_name = "ReduceMean_7941-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9607 = dtu_hlir.constant  {node_name = "ReduceMean_7941-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9608 = "dtu_hlir.broadcast_in_dim"(%9607) {node_name = "ReduceMean_7941-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9609 = "dtu_hlir.div"(%9606, %9608) {node_name = "ReduceMean_7941-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9610 = dtu_hlir.constant  {node_name = "Constant_7942-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %9611 = "dtu_hlir.broadcast_in_dim"(%9610) {node_name = "Add_7943-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %9612 = "dtu_hlir.add"(%9609, %9611) {node_name = "Add_7943-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9613 = "dtu_hlir.sqrt"(%9612) {node_name = "Sqrt_7944-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9614 = "dtu_hlir.div"(%9598, %9613) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_7945-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9615 = "dtu_hlir.mul"(%9614, %315) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_7946-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9616 = "dtu_hlir.add"(%9615, %316) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_7947-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9617 = "dtu_hlir.dot_general"(%9616, %679) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7948-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9618 = "dtu_hlir.dot_general"(%9616, %680) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7949-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9619 = "dtu_hlir.dot_general"(%9616, %681) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_7950-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9620 = "dtu_hlir.shape"(%9617) {end = 2147483647 : i64, node_name = "Shape_7951-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9621 = dtu_hlir.constant  {node_name = "Constant_7952-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9622 = "dtu_hlir.gather"(%9620, %9621) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7953-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9623 = "dtu_hlir.shape"(%9617) {end = 2147483647 : i64, node_name = "Shape_7954-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9624 = dtu_hlir.constant  {node_name = "Constant_7955-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9625 = "dtu_hlir.gather"(%9623, %9624) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7956-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9626 = "dtu_hlir.shape"(%9617) {end = 2147483647 : i64, node_name = "Shape_7957-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9627 = dtu_hlir.constant  {node_name = "Constant_7958-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9628 = "dtu_hlir.gather"(%9626, %9627) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7959-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9629 = dtu_hlir.constant  {node_name = "Constant_7960-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9630 = "dtu_hlir.div"(%9628, %9629) {node_name = "Div_7961-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9631 = "dtu_hlir.convert"(%9630) {node_name = "Cast_7962-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9632 = "dtu_hlir.convert"(%9631) {node_name = "Cast_7963-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9633 = dtu_hlir.constant  {node_name = "Constant_7964-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9634 = "dtu_hlir.unsqueeze"(%9622, %9633) {node_name = "Unsqueeze_7965-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9635 = dtu_hlir.constant  {node_name = "Constant_7966-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9636 = "dtu_hlir.unsqueeze"(%9625, %9635) {node_name = "Unsqueeze_7967-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9637 = dtu_hlir.constant  {node_name = "Constant_7968-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9638 = "dtu_hlir.unsqueeze"(%9632, %9637) {node_name = "Unsqueeze_7969-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9639 = "dtu_hlir.concatenate"(%9634, %9636, %703, %9638) {dimension = 0 : i64, node_name = "Concat_7970-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9640 = "dtu_hlir.dynamic_reshape"(%9617, %9639) {allowzero = 0 : i64, node_name = "Reshape_7971-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9641 = "dtu_hlir.transpose"(%9640) {node_name = "Transpose_7972-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9642 = dtu_hlir.constant  {node_name = "Constant_7973-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9643 = "dtu_hlir.mul"(%9622, %9642) {node_name = "Mul_7974-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9644 = dtu_hlir.constant  {node_name = "Constant_7975-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9645 = "dtu_hlir.div"(%9628, %9644) {node_name = "Div_7976-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9646 = "dtu_hlir.convert"(%9645) {node_name = "Cast_7977-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9647 = "dtu_hlir.convert"(%9646) {node_name = "Cast_7978-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9648 = dtu_hlir.constant  {node_name = "Constant_7979-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9649 = "dtu_hlir.unsqueeze"(%9643, %9648) {node_name = "Unsqueeze_7980-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9650 = dtu_hlir.constant  {node_name = "Constant_7981-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9651 = "dtu_hlir.unsqueeze"(%9625, %9650) {node_name = "Unsqueeze_7982-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9652 = dtu_hlir.constant  {node_name = "Constant_7983-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9653 = "dtu_hlir.unsqueeze"(%9647, %9652) {node_name = "Unsqueeze_7984-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9654 = "dtu_hlir.concatenate"(%9649, %9651, %9653) {dimension = 0 : i64, node_name = "Concat_7985-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9655 = "dtu_hlir.dynamic_reshape"(%9641, %9654) {allowzero = 0 : i64, node_name = "Reshape_7986-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9656 = "dtu_hlir.shape"(%9618) {end = 2147483647 : i64, node_name = "Shape_7987-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9657 = dtu_hlir.constant  {node_name = "Constant_7988-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9658 = "dtu_hlir.gather"(%9656, %9657) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7989-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9659 = "dtu_hlir.shape"(%9618) {end = 2147483647 : i64, node_name = "Shape_7990-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9660 = dtu_hlir.constant  {node_name = "Constant_7991-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9661 = "dtu_hlir.gather"(%9659, %9660) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7992-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9662 = "dtu_hlir.shape"(%9618) {end = 2147483647 : i64, node_name = "Shape_7993-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9663 = dtu_hlir.constant  {node_name = "Constant_7994-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9664 = "dtu_hlir.gather"(%9662, %9663) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_7995-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9665 = dtu_hlir.constant  {node_name = "Constant_7996-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9666 = "dtu_hlir.div"(%9664, %9665) {node_name = "Div_7997-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9667 = "dtu_hlir.convert"(%9666) {node_name = "Cast_7998-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9668 = "dtu_hlir.convert"(%9667) {node_name = "Cast_7999-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9669 = dtu_hlir.constant  {node_name = "Constant_8000-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9670 = "dtu_hlir.unsqueeze"(%9658, %9669) {node_name = "Unsqueeze_8001-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9671 = dtu_hlir.constant  {node_name = "Constant_8002-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9672 = "dtu_hlir.unsqueeze"(%9661, %9671) {node_name = "Unsqueeze_8003-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9673 = dtu_hlir.constant  {node_name = "Constant_8004-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9674 = "dtu_hlir.unsqueeze"(%9668, %9673) {node_name = "Unsqueeze_8005-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9675 = "dtu_hlir.concatenate"(%9670, %9672, %702, %9674) {dimension = 0 : i64, node_name = "Concat_8006-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9676 = "dtu_hlir.dynamic_reshape"(%9618, %9675) {allowzero = 0 : i64, node_name = "Reshape_8007-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9677 = "dtu_hlir.transpose"(%9676) {node_name = "Transpose_8008-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9678 = dtu_hlir.constant  {node_name = "Constant_8009-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9679 = "dtu_hlir.mul"(%9658, %9678) {node_name = "Mul_8010-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9680 = dtu_hlir.constant  {node_name = "Constant_8011-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9681 = "dtu_hlir.div"(%9664, %9680) {node_name = "Div_8012-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9682 = "dtu_hlir.convert"(%9681) {node_name = "Cast_8013-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9683 = "dtu_hlir.convert"(%9682) {node_name = "Cast_8014-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9684 = dtu_hlir.constant  {node_name = "Constant_8015-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9685 = "dtu_hlir.unsqueeze"(%9679, %9684) {node_name = "Unsqueeze_8016-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9686 = dtu_hlir.constant  {node_name = "Constant_8017-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9687 = "dtu_hlir.unsqueeze"(%9661, %9686) {node_name = "Unsqueeze_8018-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9688 = dtu_hlir.constant  {node_name = "Constant_8019-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9689 = "dtu_hlir.unsqueeze"(%9683, %9688) {node_name = "Unsqueeze_8020-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9690 = "dtu_hlir.concatenate"(%9685, %9687, %9689) {dimension = 0 : i64, node_name = "Concat_8021-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9691 = "dtu_hlir.dynamic_reshape"(%9677, %9690) {allowzero = 0 : i64, node_name = "Reshape_8022-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9692 = "dtu_hlir.shape"(%9619) {end = 2147483647 : i64, node_name = "Shape_8023-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9693 = dtu_hlir.constant  {node_name = "Constant_8024-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9694 = "dtu_hlir.gather"(%9692, %9693) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8025-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9695 = "dtu_hlir.shape"(%9619) {end = 2147483647 : i64, node_name = "Shape_8026-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9696 = dtu_hlir.constant  {node_name = "Constant_8027-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9697 = "dtu_hlir.gather"(%9695, %9696) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8028-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9698 = "dtu_hlir.shape"(%9619) {end = 2147483647 : i64, node_name = "Shape_8029-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9699 = dtu_hlir.constant  {node_name = "Constant_8030-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9700 = "dtu_hlir.gather"(%9698, %9699) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8031-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9701 = dtu_hlir.constant  {node_name = "Constant_8032-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9702 = "dtu_hlir.div"(%9700, %9701) {node_name = "Div_8033-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9703 = "dtu_hlir.convert"(%9702) {node_name = "Cast_8034-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9704 = "dtu_hlir.convert"(%9703) {node_name = "Cast_8035-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9705 = dtu_hlir.constant  {node_name = "Constant_8036-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9706 = "dtu_hlir.unsqueeze"(%9694, %9705) {node_name = "Unsqueeze_8037-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9707 = dtu_hlir.constant  {node_name = "Constant_8038-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9708 = "dtu_hlir.unsqueeze"(%9697, %9707) {node_name = "Unsqueeze_8039-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9709 = dtu_hlir.constant  {node_name = "Constant_8040-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9710 = "dtu_hlir.unsqueeze"(%9704, %9709) {node_name = "Unsqueeze_8041-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9711 = "dtu_hlir.concatenate"(%9706, %9708, %701, %9710) {dimension = 0 : i64, node_name = "Concat_8042-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9712 = "dtu_hlir.dynamic_reshape"(%9619, %9711) {allowzero = 0 : i64, node_name = "Reshape_8043-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9713 = "dtu_hlir.transpose"(%9712) {node_name = "Transpose_8044-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9714 = dtu_hlir.constant  {node_name = "Constant_8045-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9715 = "dtu_hlir.mul"(%9694, %9714) {node_name = "Mul_8046-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9716 = dtu_hlir.constant  {node_name = "Constant_8047-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9717 = "dtu_hlir.div"(%9700, %9716) {node_name = "Div_8048-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9718 = "dtu_hlir.convert"(%9717) {node_name = "Cast_8049-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9719 = "dtu_hlir.convert"(%9718) {node_name = "Cast_8050-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9720 = dtu_hlir.constant  {node_name = "Constant_8051-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9721 = "dtu_hlir.unsqueeze"(%9715, %9720) {node_name = "Unsqueeze_8052-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9722 = dtu_hlir.constant  {node_name = "Constant_8053-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9723 = "dtu_hlir.unsqueeze"(%9697, %9722) {node_name = "Unsqueeze_8054-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9724 = dtu_hlir.constant  {node_name = "Constant_8055-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9725 = "dtu_hlir.unsqueeze"(%9719, %9724) {node_name = "Unsqueeze_8056-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9726 = "dtu_hlir.concatenate"(%9721, %9723, %9725) {dimension = 0 : i64, node_name = "Concat_8057-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9727 = "dtu_hlir.dynamic_reshape"(%9713, %9726) {allowzero = 0 : i64, node_name = "Reshape_8058-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9728 = "dtu_hlir.shape"(%9655) {end = 2147483647 : i64, node_name = "Shape_8059-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9729 = dtu_hlir.constant  {node_name = "Constant_8060-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9730 = "dtu_hlir.gather"(%9728, %9729) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8061-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9731 = "dtu_hlir.shape"(%9655) {end = 2147483647 : i64, node_name = "Shape_8062-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9732 = dtu_hlir.constant  {node_name = "Constant_8063-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9733 = "dtu_hlir.gather"(%9731, %9732) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8064-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9734 = "dtu_hlir.shape"(%9691) {end = 2147483647 : i64, node_name = "Shape_8065-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9735 = dtu_hlir.constant  {node_name = "Constant_8066-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9736 = "dtu_hlir.gather"(%9734, %9735) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8067-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9737 = dtu_hlir.constant  {node_name = "Constant_8068-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9738 = "dtu_hlir.unsqueeze"(%9730, %9737) {node_name = "Unsqueeze_8069-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9739 = dtu_hlir.constant  {node_name = "Constant_8070-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9740 = "dtu_hlir.unsqueeze"(%9733, %9739) {node_name = "Unsqueeze_8071-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9741 = dtu_hlir.constant  {node_name = "Constant_8072-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9742 = "dtu_hlir.unsqueeze"(%9736, %9741) {node_name = "Unsqueeze_8073-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9743 = "dtu_hlir.concatenate"(%9738, %9740, %9742) {dimension = 0 : i64, node_name = "Concat_8074-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9744 = dtu_hlir.constant  {node_name = "ConstantOfShape_8075-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %9745 = "dtu_hlir.dynamic_broadcast_in_dim"(%9744, %9743) {node_name = "ConstantOfShape_8075-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x4096xf32>
    %9746 = "dtu_hlir.transpose"(%9691) {node_name = "Transpose_8076-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<10x64x4096xf32>
    %9747 = "dtu_hlir.dot_general"(%9655, %9746) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8077-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x4096xf32>) -> tensor<10x4096x4096xf32>
    %9748 = "dtu_hlir.broadcast_in_dim"(%700) {node_name = "Mul_8078-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %9749 = "dtu_hlir.mul"(%9747, %9748) {node_name = "Mul_8078-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9750 = "dtu_hlir.broadcast_in_dim"(%699) {node_name = "Mul_8079-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x4096xf32>
    %9751 = "dtu_hlir.mul"(%9745, %9750) {node_name = "Mul_8079-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9752 = "dtu_hlir.add"(%9749, %9751) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_8080-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9753 = "dtu_hlir.softmax"(%9752) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_8081-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9754 = "dtu_hlir.convert"(%9753) {node_name = "Cast_8082-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>) -> tensor<10x4096x4096xf32>
    %9755 = "dtu_hlir.dot_general"(%9754, %9727) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8083-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x4096xf32>, tensor<10x4096x64xf32>) -> tensor<10x4096x64xf32>
    %9756 = "dtu_hlir.shape"(%9755) {end = 2147483647 : i64, node_name = "Shape_8084-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9757 = dtu_hlir.constant  {node_name = "Constant_8085-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9758 = "dtu_hlir.gather"(%9756, %9757) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8086-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9759 = "dtu_hlir.shape"(%9755) {end = 2147483647 : i64, node_name = "Shape_8087-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9760 = dtu_hlir.constant  {node_name = "Constant_8088-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9761 = "dtu_hlir.gather"(%9759, %9760) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8089-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9762 = "dtu_hlir.shape"(%9755) {end = 2147483647 : i64, node_name = "Shape_8090-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9763 = dtu_hlir.constant  {node_name = "Constant_8091-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9764 = "dtu_hlir.gather"(%9762, %9763) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8092-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9765 = dtu_hlir.constant  {node_name = "Constant_8093-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9766 = "dtu_hlir.div"(%9758, %9765) {node_name = "Div_8094-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9767 = "dtu_hlir.convert"(%9766) {node_name = "Cast_8095-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9768 = "dtu_hlir.convert"(%9767) {node_name = "Cast_8096-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9769 = dtu_hlir.constant  {node_name = "Constant_8097-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9770 = "dtu_hlir.unsqueeze"(%9768, %9769) {node_name = "Unsqueeze_8098-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9771 = dtu_hlir.constant  {node_name = "Constant_8099-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9772 = "dtu_hlir.unsqueeze"(%9761, %9771) {node_name = "Unsqueeze_8100-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9773 = dtu_hlir.constant  {node_name = "Constant_8101-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9774 = "dtu_hlir.unsqueeze"(%9764, %9773) {node_name = "Unsqueeze_8102-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9775 = "dtu_hlir.concatenate"(%9770, %698, %9772, %9774) {dimension = 0 : i64, node_name = "Concat_8103-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9776 = "dtu_hlir.dynamic_reshape"(%9755, %9775) {allowzero = 0 : i64, node_name = "Reshape_8104-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %9777 = "dtu_hlir.transpose"(%9776) {node_name = "Transpose_8105-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %9778 = dtu_hlir.constant  {node_name = "Constant_8106-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9779 = "dtu_hlir.div"(%9758, %9778) {node_name = "Div_8107-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9780 = "dtu_hlir.convert"(%9779) {node_name = "Cast_8108-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9781 = "dtu_hlir.convert"(%9780) {node_name = "Cast_8109-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9782 = dtu_hlir.constant  {node_name = "Constant_8110-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9783 = "dtu_hlir.mul"(%9764, %9782) {node_name = "Mul_8111-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9784 = dtu_hlir.constant  {node_name = "Constant_8112-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9785 = "dtu_hlir.unsqueeze"(%9781, %9784) {node_name = "Unsqueeze_8113-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9786 = dtu_hlir.constant  {node_name = "Constant_8114-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9787 = "dtu_hlir.unsqueeze"(%9761, %9786) {node_name = "Unsqueeze_8115-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9788 = dtu_hlir.constant  {node_name = "Constant_8116-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9789 = "dtu_hlir.unsqueeze"(%9783, %9788) {node_name = "Unsqueeze_8117-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9790 = "dtu_hlir.concatenate"(%9785, %9787, %9789) {dimension = 0 : i64, node_name = "Concat_8118-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9791 = "dtu_hlir.dynamic_reshape"(%9777, %9790) {allowzero = 0 : i64, node_name = "Reshape_8119-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %9792 = "dtu_hlir.dot_general"(%9791, %682) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8120-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9793 = "dtu_hlir.add"(%311, %9792) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_8121-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9794 = "dtu_hlir.add"(%9793, %9589) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_8122-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9795 = dtu_hlir.constant  {node_name = "ReduceMean_8123-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9796 = "dtu_hlir.reshape"(%9795) {node_name = "ReduceMean_8123-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9797 = "dtu_hlir.reduce"(%9794, %9796) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_8123-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_8123-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9798 = dtu_hlir.constant  {node_name = "ReduceMean_8123-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9799 = "dtu_hlir.unsqueeze"(%9797, %9798) {node_name = "ReduceMean_8123-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9800 = dtu_hlir.constant  {node_name = "ReduceMean_8123-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9801 = "dtu_hlir.broadcast_in_dim"(%9800) {node_name = "ReduceMean_8123-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9802 = "dtu_hlir.div"(%9799, %9801) {node_name = "ReduceMean_8123-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9803 = "dtu_hlir.sub"(%9794, %9802) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_8124-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9804 = dtu_hlir.constant  {node_name = "Constant_8125-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %9805 = "dtu_hlir.broadcast_in_dim"(%9804) {node_name = "Pow_8126-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %9806 = "dtu_hlir.pow"(%9803, %9805) {node_name = "Pow_8126-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9807 = dtu_hlir.constant  {node_name = "ReduceMean_8127-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %9808 = "dtu_hlir.reshape"(%9807) {node_name = "ReduceMean_8127-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %9809 = "dtu_hlir.reduce"(%9806, %9808) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_8127-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_8127-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %9810 = dtu_hlir.constant  {node_name = "ReduceMean_8127-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %9811 = "dtu_hlir.unsqueeze"(%9809, %9810) {node_name = "ReduceMean_8127-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %9812 = dtu_hlir.constant  {node_name = "ReduceMean_8127-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %9813 = "dtu_hlir.broadcast_in_dim"(%9812) {node_name = "ReduceMean_8127-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %9814 = "dtu_hlir.div"(%9811, %9813) {node_name = "ReduceMean_8127-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9815 = dtu_hlir.constant  {node_name = "Constant_8128-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %9816 = "dtu_hlir.broadcast_in_dim"(%9815) {node_name = "Add_8129-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %9817 = "dtu_hlir.add"(%9814, %9816) {node_name = "Add_8129-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9818 = "dtu_hlir.sqrt"(%9817) {node_name = "Sqrt_8130-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %9819 = "dtu_hlir.div"(%9803, %9818) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_8131-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %9820 = "dtu_hlir.mul"(%9819, %317) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_8132-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9821 = "dtu_hlir.add"(%9820, %318) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_8133-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %9822 = "dtu_hlir.dot_general"(%9821, %683) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8134-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9823 = "dtu_hlir.dot_general"(%arg2, %684) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8135-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %9824 = "dtu_hlir.dot_general"(%arg2, %685) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8136-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x77x1024xf32>, tensor<1024x320xf32>) -> tensor<2x77x320xf32>
    %9825 = "dtu_hlir.shape"(%9822) {end = 2147483647 : i64, node_name = "Shape_8137-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9826 = dtu_hlir.constant  {node_name = "Constant_8138-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9827 = "dtu_hlir.gather"(%9825, %9826) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8139-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9828 = "dtu_hlir.shape"(%9822) {end = 2147483647 : i64, node_name = "Shape_8140-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9829 = dtu_hlir.constant  {node_name = "Constant_8141-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9830 = "dtu_hlir.gather"(%9828, %9829) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8142-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9831 = "dtu_hlir.shape"(%9822) {end = 2147483647 : i64, node_name = "Shape_8143-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>) -> tensor<3xi64>
    %9832 = dtu_hlir.constant  {node_name = "Constant_8144-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9833 = "dtu_hlir.gather"(%9831, %9832) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8145-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9834 = dtu_hlir.constant  {node_name = "Constant_8146-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9835 = "dtu_hlir.div"(%9833, %9834) {node_name = "Div_8147-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9836 = "dtu_hlir.convert"(%9835) {node_name = "Cast_8148-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9837 = "dtu_hlir.convert"(%9836) {node_name = "Cast_8149-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9838 = dtu_hlir.constant  {node_name = "Constant_8150-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9839 = "dtu_hlir.unsqueeze"(%9827, %9838) {node_name = "Unsqueeze_8151-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9840 = dtu_hlir.constant  {node_name = "Constant_8152-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9841 = "dtu_hlir.unsqueeze"(%9830, %9840) {node_name = "Unsqueeze_8153-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9842 = dtu_hlir.constant  {node_name = "Constant_8154-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9843 = "dtu_hlir.unsqueeze"(%9837, %9842) {node_name = "Unsqueeze_8155-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9844 = "dtu_hlir.concatenate"(%9839, %9841, %697, %9843) {dimension = 0 : i64, node_name = "Concat_8156-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9845 = "dtu_hlir.dynamic_reshape"(%9822, %9844) {allowzero = 0 : i64, node_name = "Reshape_8157-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x4096x5x64xf32>
    %9846 = "dtu_hlir.transpose"(%9845) {node_name = "Transpose_8158-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>) -> tensor<2x5x4096x64xf32>
    %9847 = dtu_hlir.constant  {node_name = "Constant_8159-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9848 = "dtu_hlir.mul"(%9827, %9847) {node_name = "Mul_8160-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9849 = dtu_hlir.constant  {node_name = "Constant_8161-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9850 = "dtu_hlir.div"(%9833, %9849) {node_name = "Div_8162-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9851 = "dtu_hlir.convert"(%9850) {node_name = "Cast_8163-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9852 = "dtu_hlir.convert"(%9851) {node_name = "Cast_8164-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9853 = dtu_hlir.constant  {node_name = "Constant_8165-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9854 = "dtu_hlir.unsqueeze"(%9848, %9853) {node_name = "Unsqueeze_8166-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9855 = dtu_hlir.constant  {node_name = "Constant_8167-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9856 = "dtu_hlir.unsqueeze"(%9830, %9855) {node_name = "Unsqueeze_8168-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9857 = dtu_hlir.constant  {node_name = "Constant_8169-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9858 = "dtu_hlir.unsqueeze"(%9852, %9857) {node_name = "Unsqueeze_8170-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9859 = "dtu_hlir.concatenate"(%9854, %9856, %9858) {dimension = 0 : i64, node_name = "Concat_8171-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9860 = "dtu_hlir.dynamic_reshape"(%9846, %9859) {allowzero = 0 : i64, node_name = "Reshape_8172-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>, tensor<3xi64>) -> tensor<10x4096x64xf32>
    %9861 = "dtu_hlir.shape"(%9823) {end = 2147483647 : i64, node_name = "Shape_8173-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9862 = dtu_hlir.constant  {node_name = "Constant_8174-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9863 = "dtu_hlir.gather"(%9861, %9862) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8175-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9864 = "dtu_hlir.shape"(%9823) {end = 2147483647 : i64, node_name = "Shape_8176-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9865 = dtu_hlir.constant  {node_name = "Constant_8177-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9866 = "dtu_hlir.gather"(%9864, %9865) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8178-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9867 = "dtu_hlir.shape"(%9823) {end = 2147483647 : i64, node_name = "Shape_8179-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9868 = dtu_hlir.constant  {node_name = "Constant_8180-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9869 = "dtu_hlir.gather"(%9867, %9868) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8181-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9870 = dtu_hlir.constant  {node_name = "Constant_8182-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9871 = "dtu_hlir.div"(%9869, %9870) {node_name = "Div_8183-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9872 = "dtu_hlir.convert"(%9871) {node_name = "Cast_8184-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9873 = "dtu_hlir.convert"(%9872) {node_name = "Cast_8185-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9874 = dtu_hlir.constant  {node_name = "Constant_8186-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9875 = "dtu_hlir.unsqueeze"(%9863, %9874) {node_name = "Unsqueeze_8187-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9876 = dtu_hlir.constant  {node_name = "Constant_8188-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9877 = "dtu_hlir.unsqueeze"(%9866, %9876) {node_name = "Unsqueeze_8189-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9878 = dtu_hlir.constant  {node_name = "Constant_8190-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9879 = "dtu_hlir.unsqueeze"(%9873, %9878) {node_name = "Unsqueeze_8191-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9880 = "dtu_hlir.concatenate"(%9875, %9877, %696, %9879) {dimension = 0 : i64, node_name = "Concat_8192-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9881 = "dtu_hlir.dynamic_reshape"(%9823, %9880) {allowzero = 0 : i64, node_name = "Reshape_8193-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %9882 = "dtu_hlir.transpose"(%9881) {node_name = "Transpose_8194-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %9883 = dtu_hlir.constant  {node_name = "Constant_8195-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9884 = "dtu_hlir.mul"(%9863, %9883) {node_name = "Mul_8196-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9885 = dtu_hlir.constant  {node_name = "Constant_8197-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9886 = "dtu_hlir.div"(%9869, %9885) {node_name = "Div_8198-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9887 = "dtu_hlir.convert"(%9886) {node_name = "Cast_8199-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9888 = "dtu_hlir.convert"(%9887) {node_name = "Cast_8200-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9889 = dtu_hlir.constant  {node_name = "Constant_8201-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9890 = "dtu_hlir.unsqueeze"(%9884, %9889) {node_name = "Unsqueeze_8202-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9891 = dtu_hlir.constant  {node_name = "Constant_8203-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9892 = "dtu_hlir.unsqueeze"(%9866, %9891) {node_name = "Unsqueeze_8204-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9893 = dtu_hlir.constant  {node_name = "Constant_8205-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9894 = "dtu_hlir.unsqueeze"(%9888, %9893) {node_name = "Unsqueeze_8206-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9895 = "dtu_hlir.concatenate"(%9890, %9892, %9894) {dimension = 0 : i64, node_name = "Concat_8207-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9896 = "dtu_hlir.dynamic_reshape"(%9882, %9895) {allowzero = 0 : i64, node_name = "Reshape_8208-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %9897 = "dtu_hlir.shape"(%9824) {end = 2147483647 : i64, node_name = "Shape_8209-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9898 = dtu_hlir.constant  {node_name = "Constant_8210-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9899 = "dtu_hlir.gather"(%9897, %9898) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8211-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9900 = "dtu_hlir.shape"(%9824) {end = 2147483647 : i64, node_name = "Shape_8212-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9901 = dtu_hlir.constant  {node_name = "Constant_8213-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9902 = "dtu_hlir.gather"(%9900, %9901) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8214-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9903 = "dtu_hlir.shape"(%9824) {end = 2147483647 : i64, node_name = "Shape_8215-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x77x320xf32>) -> tensor<3xi64>
    %9904 = dtu_hlir.constant  {node_name = "Constant_8216-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9905 = "dtu_hlir.gather"(%9903, %9904) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8217-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9906 = dtu_hlir.constant  {node_name = "Constant_8218-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9907 = "dtu_hlir.div"(%9905, %9906) {node_name = "Div_8219-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9908 = "dtu_hlir.convert"(%9907) {node_name = "Cast_8220-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9909 = "dtu_hlir.convert"(%9908) {node_name = "Cast_8221-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9910 = dtu_hlir.constant  {node_name = "Constant_8222-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9911 = "dtu_hlir.unsqueeze"(%9899, %9910) {node_name = "Unsqueeze_8223-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9912 = dtu_hlir.constant  {node_name = "Constant_8224-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9913 = "dtu_hlir.unsqueeze"(%9902, %9912) {node_name = "Unsqueeze_8225-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9914 = dtu_hlir.constant  {node_name = "Constant_8226-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9915 = "dtu_hlir.unsqueeze"(%9909, %9914) {node_name = "Unsqueeze_8227-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9916 = "dtu_hlir.concatenate"(%9911, %9913, %695, %9915) {dimension = 0 : i64, node_name = "Concat_8228-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9917 = "dtu_hlir.dynamic_reshape"(%9824, %9916) {allowzero = 0 : i64, node_name = "Reshape_8229-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x77x320xf32>, tensor<4xi64>) -> tensor<2x77x5x64xf32>
    %9918 = "dtu_hlir.transpose"(%9917) {node_name = "Transpose_8230-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x77x5x64xf32>) -> tensor<2x5x77x64xf32>
    %9919 = dtu_hlir.constant  {node_name = "Constant_8231-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9920 = "dtu_hlir.mul"(%9899, %9919) {node_name = "Mul_8232-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9921 = dtu_hlir.constant  {node_name = "Constant_8233-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9922 = "dtu_hlir.div"(%9905, %9921) {node_name = "Div_8234-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9923 = "dtu_hlir.convert"(%9922) {node_name = "Cast_8235-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9924 = "dtu_hlir.convert"(%9923) {node_name = "Cast_8236-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9925 = dtu_hlir.constant  {node_name = "Constant_8237-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9926 = "dtu_hlir.unsqueeze"(%9920, %9925) {node_name = "Unsqueeze_8238-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9927 = dtu_hlir.constant  {node_name = "Constant_8239-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9928 = "dtu_hlir.unsqueeze"(%9902, %9927) {node_name = "Unsqueeze_8240-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9929 = dtu_hlir.constant  {node_name = "Constant_8241-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9930 = "dtu_hlir.unsqueeze"(%9924, %9929) {node_name = "Unsqueeze_8242-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9931 = "dtu_hlir.concatenate"(%9926, %9928, %9930) {dimension = 0 : i64, node_name = "Concat_8243-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9932 = "dtu_hlir.dynamic_reshape"(%9918, %9931) {allowzero = 0 : i64, node_name = "Reshape_8244-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x5x77x64xf32>, tensor<3xi64>) -> tensor<10x77x64xf32>
    %9933 = "dtu_hlir.shape"(%9860) {end = 2147483647 : i64, node_name = "Shape_8245-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9934 = dtu_hlir.constant  {node_name = "Constant_8246-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9935 = "dtu_hlir.gather"(%9933, %9934) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8247-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9936 = "dtu_hlir.shape"(%9860) {end = 2147483647 : i64, node_name = "Shape_8248-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9937 = dtu_hlir.constant  {node_name = "Constant_8249-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9938 = "dtu_hlir.gather"(%9936, %9937) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8250-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9939 = "dtu_hlir.shape"(%9896) {end = 2147483647 : i64, node_name = "Shape_8251-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<3xi64>
    %9940 = dtu_hlir.constant  {node_name = "Constant_8252-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9941 = "dtu_hlir.gather"(%9939, %9940) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8253-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9942 = dtu_hlir.constant  {node_name = "Constant_8254-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9943 = "dtu_hlir.unsqueeze"(%9935, %9942) {node_name = "Unsqueeze_8255-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9944 = dtu_hlir.constant  {node_name = "Constant_8256-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9945 = "dtu_hlir.unsqueeze"(%9938, %9944) {node_name = "Unsqueeze_8257-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9946 = dtu_hlir.constant  {node_name = "Constant_8258-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9947 = "dtu_hlir.unsqueeze"(%9941, %9946) {node_name = "Unsqueeze_8259-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9948 = "dtu_hlir.concatenate"(%9943, %9945, %9947) {dimension = 0 : i64, node_name = "Concat_8260-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9949 = dtu_hlir.constant  {node_name = "ConstantOfShape_8261-Const-0", node_type = "ConstantOfShape"} dense<0.000000e+00> : tensor<1xf32>
    %9950 = "dtu_hlir.dynamic_broadcast_in_dim"(%9949, %9948) {node_name = "ConstantOfShape_8261-1", node_type = "ConstantOfShape", tif_quantize_type = "f16"} : (tensor<1xf32>, tensor<3xi64>) -> tensor<10x4096x77xf32>
    %9951 = "dtu_hlir.transpose"(%9896) {node_name = "Transpose_8262-0", node_type = "Transpose", permutation = dense<[0, 2, 1]> : tensor<3xi64>, tif_quantize_type = "f16"} : (tensor<10x77x64xf32>) -> tensor<10x64x77xf32>
    %9952 = "dtu_hlir.dot_general"(%9860, %9951) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8263-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<10x64x77xf32>) -> tensor<10x4096x77xf32>
    %9953 = "dtu_hlir.broadcast_in_dim"(%694) {node_name = "Mul_8264-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %9954 = "dtu_hlir.mul"(%9952, %9953) {node_name = "Mul_8264-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9955 = "dtu_hlir.broadcast_in_dim"(%693) {node_name = "Mul_8265-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<10x4096x77xf32>
    %9956 = "dtu_hlir.mul"(%9950, %9955) {node_name = "Mul_8265-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9957 = "dtu_hlir.add"(%9954, %9956) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_8266-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9958 = "dtu_hlir.softmax"(%9957) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_8267-0", node_type = "Softmax", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9959 = "dtu_hlir.convert"(%9958) {node_name = "Cast_8268-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>) -> tensor<10x4096x77xf32>
    %9960 = "dtu_hlir.dot_general"(%9959, %9932) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8269-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<10x4096x77xf32>, tensor<10x77x64xf32>) -> tensor<10x4096x64xf32>
    %9961 = "dtu_hlir.shape"(%9960) {end = 2147483647 : i64, node_name = "Shape_8270-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9962 = dtu_hlir.constant  {node_name = "Constant_8271-0", node_type = "Constant"} dense<0> : tensor<i64>
    %9963 = "dtu_hlir.gather"(%9961, %9962) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8272-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9964 = "dtu_hlir.shape"(%9960) {end = 2147483647 : i64, node_name = "Shape_8273-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9965 = dtu_hlir.constant  {node_name = "Constant_8274-0", node_type = "Constant"} dense<1> : tensor<i64>
    %9966 = "dtu_hlir.gather"(%9964, %9965) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8275-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9967 = "dtu_hlir.shape"(%9960) {end = 2147483647 : i64, node_name = "Shape_8276-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>) -> tensor<3xi64>
    %9968 = dtu_hlir.constant  {node_name = "Constant_8277-0", node_type = "Constant"} dense<2> : tensor<i64>
    %9969 = "dtu_hlir.gather"(%9967, %9968) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 0 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8278-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<i64>) -> tensor<i64>
    %9970 = dtu_hlir.constant  {node_name = "Constant_8279-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9971 = "dtu_hlir.div"(%9963, %9970) {node_name = "Div_8280-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9972 = "dtu_hlir.convert"(%9971) {node_name = "Cast_8281-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9973 = "dtu_hlir.convert"(%9972) {node_name = "Cast_8282-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9974 = dtu_hlir.constant  {node_name = "Constant_8283-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9975 = "dtu_hlir.unsqueeze"(%9973, %9974) {node_name = "Unsqueeze_8284-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9976 = dtu_hlir.constant  {node_name = "Constant_8285-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9977 = "dtu_hlir.unsqueeze"(%9966, %9976) {node_name = "Unsqueeze_8286-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9978 = dtu_hlir.constant  {node_name = "Constant_8287-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9979 = "dtu_hlir.unsqueeze"(%9969, %9978) {node_name = "Unsqueeze_8288-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9980 = "dtu_hlir.concatenate"(%9975, %692, %9977, %9979) {dimension = 0 : i64, node_name = "Concat_8289-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %9981 = "dtu_hlir.dynamic_reshape"(%9960, %9980) {allowzero = 0 : i64, node_name = "Reshape_8290-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<10x4096x64xf32>, tensor<4xi64>) -> tensor<2x5x4096x64xf32>
    %9982 = "dtu_hlir.transpose"(%9981) {node_name = "Transpose_8291-0", node_type = "Transpose", permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x5x4096x64xf32>) -> tensor<2x4096x5x64xf32>
    %9983 = dtu_hlir.constant  {node_name = "Constant_8292-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9984 = "dtu_hlir.div"(%9963, %9983) {node_name = "Div_8293-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9985 = "dtu_hlir.convert"(%9984) {node_name = "Cast_8294-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9986 = "dtu_hlir.convert"(%9985) {node_name = "Cast_8295-0", node_type = "Cast", tif_quantize_type = "f16"} : (tensor<i64>) -> tensor<i64>
    %9987 = dtu_hlir.constant  {node_name = "Constant_8296-0", node_type = "Constant"} dense<5> : tensor<i64>
    %9988 = "dtu_hlir.mul"(%9969, %9987) {node_name = "Mul_8297-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %9989 = dtu_hlir.constant  {node_name = "Constant_8298-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9990 = "dtu_hlir.unsqueeze"(%9986, %9989) {node_name = "Unsqueeze_8299-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9991 = dtu_hlir.constant  {node_name = "Constant_8300-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9992 = "dtu_hlir.unsqueeze"(%9966, %9991) {node_name = "Unsqueeze_8301-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9993 = dtu_hlir.constant  {node_name = "Constant_8302-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %9994 = "dtu_hlir.unsqueeze"(%9988, %9993) {node_name = "Unsqueeze_8303-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %9995 = "dtu_hlir.concatenate"(%9990, %9992, %9994) {dimension = 0 : i64, node_name = "Concat_8304-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
    %9996 = "dtu_hlir.dynamic_reshape"(%9982, %9995) {allowzero = 0 : i64, node_name = "Reshape_8305-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x5x64xf32>, tensor<3xi64>) -> tensor<2x4096x320xf32>
    %9997 = "dtu_hlir.dot_general"(%9996, %686) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8306-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %9998 = "dtu_hlir.add"(%314, %9997) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_8307-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %9999 = "dtu_hlir.add"(%9998, %9794) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_8308-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %10000 = dtu_hlir.constant  {node_name = "ReduceMean_8309-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %10001 = "dtu_hlir.reshape"(%10000) {node_name = "ReduceMean_8309-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %10002 = "dtu_hlir.reduce"(%9999, %10001) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_8309-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_8309-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %10003 = dtu_hlir.constant  {node_name = "ReduceMean_8309-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %10004 = "dtu_hlir.unsqueeze"(%10002, %10003) {node_name = "ReduceMean_8309-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %10005 = dtu_hlir.constant  {node_name = "ReduceMean_8309-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %10006 = "dtu_hlir.broadcast_in_dim"(%10005) {node_name = "ReduceMean_8309-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %10007 = "dtu_hlir.div"(%10004, %10006) {node_name = "ReduceMean_8309-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %10008 = "dtu_hlir.sub"(%9999, %10007) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Sub_8310-0", node_type = "Sub", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %10009 = dtu_hlir.constant  {node_name = "Constant_8311-0", node_type = "Constant"} dense<2.000000e+00> : tensor<f32>
    %10010 = "dtu_hlir.broadcast_in_dim"(%10009) {node_name = "Pow_8312-0_hlir_0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x320xf32>
    %10011 = "dtu_hlir.pow"(%10008, %10010) {node_name = "Pow_8312-0", node_type = "Pow", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %10012 = dtu_hlir.constant  {node_name = "ReduceMean_8313-Const-0", node_type = "ReduceMean"} dense<0.000000e+00> : tensor<1xf32>
    %10013 = "dtu_hlir.reshape"(%10012) {node_name = "ReduceMean_8313-Reshape-1", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<f32>
    %10014 = "dtu_hlir.reduce"(%10011, %10013) ( {
    ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):	// no predecessors
      %10088 = "dtu_hlir.add"(%arg3, %arg4) {node_name = "ReduceMean_8313-Add-2", node_type = "ReduceMean"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "dtu_hlir.return"(%10088) : (tensor<f32>) -> ()
    }) {dimensions = dense<2> : tensor<1xi64>, node_name = "ReduceMean_8313-Reduce-3", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<f32>) -> tensor<2x4096xf32>
    %10015 = dtu_hlir.constant  {node_name = "ReduceMean_8313-Const-4", node_type = "ReduceMean"} dense<2> : tensor<1xi64>
    %10016 = "dtu_hlir.unsqueeze"(%10014, %10015) {node_name = "ReduceMean_8313-Unsqueeze-5", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096xf32>, tensor<1xi64>) -> tensor<2x4096x1xf32>
    %10017 = dtu_hlir.constant  {node_name = "ReduceMean_8313-Const-6", node_type = "ReduceMean"} dense<3.200000e+02> : tensor<1xf32>
    %10018 = "dtu_hlir.broadcast_in_dim"(%10017) {node_name = "ReduceMean_8313-BroadcastInDim-7", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<1xf32>) -> tensor<2x4096x1xf32>
    %10019 = "dtu_hlir.div"(%10016, %10018) {node_name = "ReduceMean_8313-Div-8", node_type = "ReduceMean", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %10020 = dtu_hlir.constant  {node_name = "Constant_8314-0", node_type = "Constant"} dense<9.99999974E-6> : tensor<f32>
    %10021 = "dtu_hlir.broadcast_in_dim"(%10020) {node_name = "Add_8315-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1xf32>
    %10022 = "dtu_hlir.add"(%10019, %10021) {node_name = "Add_8315-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %10023 = "dtu_hlir.sqrt"(%10022) {node_name = "Sqrt_8316-0", node_type = "Sqrt", tif_quantize_type = "f16"} : (tensor<2x4096x1xf32>) -> tensor<2x4096x1xf32>
    %10024 = "dtu_hlir.div"(%10008, %10023) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Div_8317-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x1xf32>) -> tensor<2x4096x320xf32>
    %10025 = "dtu_hlir.mul"(%10024, %319) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Mul_8318-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %10026 = "dtu_hlir.add"(%10025, %320) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_8319-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf32>
    %10027 = "dtu_hlir.dot_general"(%10026, %687) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8320-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x2560xf32>) -> tensor<2x4096x2560xf32>
    %10028 = "dtu_hlir.add"(%312, %10027) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_8321-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2560xf32>, tensor<2x4096x2560xf32>) -> tensor<2x4096x2560xf32>
    %10029 = "dtu_hlir.shape"(%10028) {end = 2147483647 : i64, node_name = "Shape_8322-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>) -> tensor<3xi64>
    %10030 = dtu_hlir.constant  {node_name = "Constant_8323-0", node_type = "Constant"} dense<-1> : tensor<1xi64>
    %10031 = "dtu_hlir.gather"(%10029, %10030) {dimension_numbers = {collapsed_slice_dims = dense<0> : tensor<1xi64>, index_vector_dim = 1 : i64, offset_dims = dense<[]> : tensor<0xi64>, start_index_map = dense<0> : tensor<1xi64>}, indices_are_sorted = false, node_name = "Gather_8324-0", node_type = "Gather", slice_sizes = dense<1> : tensor<1xi64>, tif_quantize_type = "f16"} : (tensor<3xi64>, tensor<1xi64>) -> tensor<1xi64>
    %10032 = dtu_hlir.constant  {node_name = "Constant_8325-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %10033 = dtu_hlir.constant  {node_name = "Constant_8326-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %10034 = "dtu_hlir.add"(%10031, %10033) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Add_8327-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %10035 = dtu_hlir.constant  {node_name = "Constant_8328-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %10036 = "dtu_hlir.div"(%10034, %10035) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Div_8329-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %10037 = dtu_hlir.constant  {node_name = "Constant_8330-0", node_type = "Constant"} dense<1> : tensor<1xi64>
    %10038 = "dtu_hlir.mul"(%10036, %10037) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_8331-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %10039 = dtu_hlir.constant  {node_name = "Slice_8332-Const-0", node_type = "Slice"} dense<1> : tensor<1xi64>
    %10040 = "dtu_hlir.real_dynamic_slice"(%10028, %10032, %10038, %10039, %10030) {node_name = "Slice_8332-1", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %10041 = dtu_hlir.constant  {node_name = "Constant_8333-0", node_type = "Constant"} dense<2> : tensor<1xi64>
    %10042 = "dtu_hlir.mul"(%10036, %10041) {broadcast_dimensions = dense<0> : tensor<1xi64>, node_name = "Mul_8334-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %10043 = "dtu_hlir.shape"(%10038) {end = 2147483647 : i64, node_name = "Slice_8335-Shape-0", node_type = "Slice", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<1xi64>) -> tensor<1xi64>
    %10044 = dtu_hlir.constant  {node_name = "Slice_8335-Const-1", node_type = "Slice"} dense<1> : tensor<i64>
    %10045 = "dtu_hlir.dynamic_broadcast_in_dim"(%10044, %10043) {node_name = "Slice_8335-DynamicBroadcastInDim-2", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %10046 = "dtu_hlir.real_dynamic_slice"(%10028, %10038, %10042, %10045, %10030) {node_name = "Slice_8335-3", node_type = "Slice", tif_quantize_type = "f16"} : (tensor<2x4096x2560xf32>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<2x4096x1280xf32>
    %10047 = dtu_hlir.constant  {node_name = "Constant_8336-0", node_type = "Constant"} dense<1.41421354> : tensor<f32>
    %10048 = "dtu_hlir.broadcast_in_dim"(%10047) {node_name = "Div_8337-0_hlir_0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %10049 = "dtu_hlir.div"(%10046, %10048) {node_name = "Div_8337-0", node_type = "Div", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %10050 = "dtu_hlir.erf"(%10049) {node_name = "Erf_8338-0", node_type = "Erf", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %10051 = dtu_hlir.constant  {node_name = "Constant_8339-0", node_type = "Constant"} dense<1.000000e+00> : tensor<f32>
    %10052 = "dtu_hlir.broadcast_in_dim"(%10051) {node_name = "Add_8340-0_hlir_0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %10053 = "dtu_hlir.add"(%10050, %10052) {node_name = "Add_8340-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %10054 = "dtu_hlir.mul"(%10046, %10053) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_8341-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %10055 = dtu_hlir.constant  {node_name = "Constant_8342-0", node_type = "Constant"} dense<5.000000e-01> : tensor<f32>
    %10056 = "dtu_hlir.broadcast_in_dim"(%10055) {node_name = "Mul_8343-0_hlir_0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<f32>) -> tensor<2x4096x1280xf32>
    %10057 = "dtu_hlir.mul"(%10054, %10056) {node_name = "Mul_8343-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %10058 = "dtu_hlir.mul"(%10040, %10057) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Mul_8344-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<2x4096x1280xf32>) -> tensor<2x4096x1280xf32>
    %10059 = "dtu_hlir.dot_general"(%10058, %688) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8345-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x1280xf32>, tensor<1280x320xf32>) -> tensor<2x4096x320xf32>
    %10060 = "dtu_hlir.add"(%313, %10059) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_8346-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %10061 = "dtu_hlir.add"(%10060, %9999) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>, node_name = "Add_8347-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %10062 = "dtu_hlir.dot_general"(%10061, %689) {dot_dimension_numbers = {lhs_batching_dimensions = dense<-2> : tensor<1xi64>, lhs_contracting_dimensions = dense<-1> : tensor<1xi64>, rhs_batching_dimensions = dense<-2> : tensor<1xi64>, rhs_contracting_dimensions = dense<-2> : tensor<1xi64>}, node_name = "MatMul_8348-0", node_type = "MatMul", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<320x320xf32>) -> tensor<2x4096x320xf32>
    %10063 = "dtu_hlir.add"(%321, %10062) {broadcast_dimensions = dense<2> : tensor<1xi64>, node_name = "Add_8349-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<320xf32>, tensor<2x4096x320xf32>) -> tensor<2x4096x320xf32>
    %10064 = dtu_hlir.constant  {node_name = "Constant_8350-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %10065 = "dtu_hlir.unsqueeze"(%9559, %10064) {node_name = "Unsqueeze_8351-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %10066 = dtu_hlir.constant  {node_name = "Constant_8352-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %10067 = "dtu_hlir.unsqueeze"(%9562, %10066) {node_name = "Unsqueeze_8353-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %10068 = dtu_hlir.constant  {node_name = "Constant_8354-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %10069 = "dtu_hlir.unsqueeze"(%9565, %10068) {node_name = "Unsqueeze_8355-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %10070 = dtu_hlir.constant  {node_name = "Constant_8356-0", node_type = "Constant"} dense<0> : tensor<1xi64>
    %10071 = "dtu_hlir.unsqueeze"(%9577, %10070) {node_name = "Unsqueeze_8357-0", node_type = "Unsqueeze", tif_quantize_type = "f16"} : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %10072 = "dtu_hlir.concatenate"(%10065, %10067, %10069, %10071) {dimension = 0 : i64, node_name = "Concat_8358-0", node_type = "Concat", tif_quantize_type = "f16"} : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<4xi64>
    %10073 = "dtu_hlir.dynamic_reshape"(%10063, %10072) {allowzero = 0 : i64, node_name = "Reshape_8359-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x4096x320xf32>, tensor<4xi64>) -> tensor<2x64x64x320xf32>
    %10074 = "dtu_hlir.transpose"(%10073) {node_name = "Transpose_8360-0", node_type = "Transpose", permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, tif_quantize_type = "f16"} : (tensor<2x64x64x320xf32>) -> tensor<2x320x64x64xf32>
    %10075 = "dtu_hlir.add"(%10074, %9556) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Add_8361-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %10076 = dtu_hlir.constant  {node_name = "Constant_8362-0", node_type = "Constant"} dense<[0, 32, -1]> : tensor<3xi64>
    %10077 = "dtu_hlir.dynamic_reshape"(%10075, %10076) {allowzero = 0 : i64, node_name = "Reshape_8363-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<3xi64>) -> tensor<2x32x40960xf32>
    %10078 = dtu_hlir.constant  {node_name = "Constant_8364-0", node_type = "Constant"} dense<1.000000e+00> : tensor<32xf32>
    %10079 = dtu_hlir.constant  {node_name = "Constant_8365-0", node_type = "Constant"} dense<0.000000e+00> : tensor<32xf32>
    %10080 = "dtu_hlir.instance_norm"(%10077, %10078, %10079) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 1 : i64, spatial_dimensions = dense<2> : tensor<1xi64>}, node_name = "InstanceNormalization_8366-0", node_type = "InstanceNormalization", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x32x40960xf32>
    %10081 = "dtu_hlir.shape"(%10075) {end = 2147483647 : i64, node_name = "Shape_8367-0", node_type = "Shape", start = 0 : i64, tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<4xi64>
    %10082 = "dtu_hlir.dynamic_reshape"(%10080, %10081) {allowzero = 0 : i64, node_name = "Reshape_8368-0", node_type = "Reshape", tif_quantize_type = "f16"} : (tensor<2x32x40960xf32>, tensor<4xi64>) -> tensor<2x320x64x64xf32>
    %10083 = "dtu_hlir.mul"(%10082, %690) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Mul_8369-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %10084 = "dtu_hlir.add"(%10083, %691) {broadcast_dimensions = dense<[1, 2, 3]> : tensor<3xi64>, node_name = "Add_8370-0", node_type = "Add", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<320x1x1xf32>) -> tensor<2x320x64x64xf32>
    %10085 = "dtu_hlir.sigmoid"(%10084) {node_name = "Sigmoid_8371-0", node_type = "Sigmoid", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %10086 = "dtu_hlir.mul"(%10084, %10085) {broadcast_dimensions = dense<[0, 1, 2, 3]> : tensor<4xi64>, node_name = "Mul_8372-0", node_type = "Mul", tif_quantize_type = "f16"} : (tensor<2x320x64x64xf32>, tensor<2x320x64x64xf32>) -> tensor<2x320x64x64xf32>
    %10087 = "dtu_hlir.conv_bias"(%10086, %370, %371) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 1 : i64, input_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, kernel_input_feature_dimension = 1 : i64, kernel_output_feature_dimension = 0 : i64, kernel_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 1 : i64, output_spatial_dimensions = dense<[2, 3]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_8373-0", node_type = "Conv", padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, tif_quantize_type = "f16", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x320x64x64xf32>, tensor<4x320x3x3xf32>, tensor<4xf32>) -> tensor<2x4x64x64xf32>
    return %10087 : tensor<2x4x64x64xf32>
  }
}

/// IR Dump After hlir::HlirLastPass ///


module attributes {dtu_hlir.executable_id = 1 : i64, dtu_hlir.ktype = 2 : i64, dtu_hlir.layout = "NCHW", dtu_hlir.overflow_clamp = true, dtu_hlir.target = "dorado_3pg"} {
  func @main(%arg0: tensor<2x4x64x64xf32>, %arg1: tensor<1xf32>, %arg2: tensor<2x77x1024xf32>) -> tensor<2x4x64x64xf32> {
    "dtu_hlir.tensor_attr"(%arg0) {dynamic_flag = "positive", max_shape = dense<[2, 4, 64, 64]> : tensor<4xi64>, min_shape = dense<[2, 4, 64, 64]> : tensor<4xi64>, table_index = dense<0> : tensor<1xi64>} : (tensor<2x4x64x64xf32>) -> ()
    "dtu_hlir.tensor_attr"(%arg1) {dynamic_flag = "positive", max_shape = dense<1> : tensor<1xi64>, min_shape = dense<1> : tensor<1xi64>, table_index = dense<1> : tensor<1xi64>} : (tensor<1xf32>) -> ()
    "dtu_hlir.tensor_attr"(%arg2) {dynamic_flag = "positive", max_shape = dense<[2, 77, 1024]> : tensor<3xi64>, min_shape = dense<[2, 77, 1024]> : tensor<3xi64>, table_index = dense<2> : tensor<1xi64>} : (tensor<2x77x1024xf32>) -> ()
    %0 = "dtu_hlir.transpose"(%arg0) {node_name = "Conv_228-0", op_id = 0 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4x64x64xf32>) -> tensor<2x64x64x4xf32>
    %1 = "dtu_hlir.convert"(%0) {gen_loc = "hlir_quantization", op_id = 1 : i64, unique_name = "common20_convert"} : (tensor<2x64x64x4xf32>) -> tensor<2x64x64x4xf16>
    %2 = "dtu_hlir.reshape"(%arg1) : (tensor<1xf32>) -> tensor<1xf32>
    %3 = "dtu_hlir.convert"(%2) {gen_loc = "hlir_quantization", op_id = 2 : i64, unique_name = "common20_convert"} : (tensor<1xf32>) -> tensor<1xf16>
    %4 = "dtu_hlir.convert"(%arg2) {gen_loc = "hlir_quantization", op_id = 3 : i64, unique_name = "common20_convert"} : (tensor<2x77x1024xf32>) -> tensor<2x77x1024xf16>
    %5 = dtu_hlir.constant  {node_name = "conv_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %6 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %7 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %8 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %9 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %10 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %11 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %12 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %13 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %14 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %15 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %16 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %17 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %18 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %19 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %20 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %21 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %22 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %23 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %24 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %25 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %26 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %27 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %28 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %29 = dtu_hlir.constant  {node_name = "down_blocks.0.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %30 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %31 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %32 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %33 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %34 = dtu_hlir.constant  {node_name = "down_blocks.0.downsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %35 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %36 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %37 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %38 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %39 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %40 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %41 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %42 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %43 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %44 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %45 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %46 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %47 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %48 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %49 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %50 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %51 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %52 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %53 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %54 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %55 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %56 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %57 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %58 = dtu_hlir.constant  {node_name = "down_blocks.1.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %59 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %60 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %61 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %62 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %63 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %64 = dtu_hlir.constant  {node_name = "down_blocks.1.downsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %65 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %66 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %67 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %68 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %69 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %70 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %71 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %72 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %73 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %74 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %75 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %76 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %77 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %78 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %79 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %80 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %81 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %82 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %83 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %84 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %85 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %86 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %87 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %88 = dtu_hlir.constant  {node_name = "down_blocks.2.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %89 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %90 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %91 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %92 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %93 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %94 = dtu_hlir.constant  {node_name = "down_blocks.2.downsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %95 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %96 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %97 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %98 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %99 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %100 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %101 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %102 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %103 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %104 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %105 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %106 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %107 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %108 = dtu_hlir.constant  {node_name = "up_blocks.0.upsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %109 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %110 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %111 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %112 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %113 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %114 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %115 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %116 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %117 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %118 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %119 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %120 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %121 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %122 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %123 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %124 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %125 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %126 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %127 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %128 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %129 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %130 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %131 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %132 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %133 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %134 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %135 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %136 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %137 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %138 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %139 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %140 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %141 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %142 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %143 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %144 = dtu_hlir.constant  {node_name = "up_blocks.1.attentions.2.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %145 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %146 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %147 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %148 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %149 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %150 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %151 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %152 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %153 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %154 = dtu_hlir.constant  {node_name = "up_blocks.1.upsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %155 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %156 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %157 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %158 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %159 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %160 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %161 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %162 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %163 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %164 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %165 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %166 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %167 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %168 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %169 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %170 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %171 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %172 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %173 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %174 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %175 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %176 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %177 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %178 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %179 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %180 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %181 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<5120xf32>
    %182 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %183 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %184 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %185 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %186 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %187 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %188 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %189 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %190 = dtu_hlir.constant  {node_name = "up_blocks.2.attentions.2.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %191 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %192 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %193 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %194 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %195 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %196 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %197 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %198 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %199 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %200 = dtu_hlir.constant  {node_name = "up_blocks.2.upsamplers.0.conv.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %201 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %202 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %203 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %204 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %205 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %206 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %207 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %208 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %209 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %210 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %211 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %212 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %213 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %214 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %215 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %216 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %217 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %218 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %219 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %220 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %221 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %222 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %223 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %224 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.1.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %225 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %226 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %227 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<2560xf32>
    %228 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %229 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %230 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %231 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %232 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %233 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %234 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %235 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %236 = dtu_hlir.constant  {node_name = "up_blocks.3.attentions.2.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %237 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %238 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %239 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %240 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %241 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %242 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %243 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %244 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %245 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv_shortcut.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %246 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.proj_in.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %247 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %248 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<10240xf32>
    %249 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %250 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %251 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm1.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %252 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %253 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm2.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %254 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %255 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm3.weight", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %256 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.transformer_blocks.0.norm3.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %257 = dtu_hlir.constant  {node_name = "mid_block.attentions.0.proj_out.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %258 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %259 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %260 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv1.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %261 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv2.bias", node_type = "Constant"} opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %262 = dtu_hlir.constant  {node_name = "conv_out.bias", node_type = "Constant"} dense<[-0.0025138855, 0.00184345245, 3.44514847E-4, 0.00167942047]> : tensor<4xf32>
    %263 = dtu_hlir.constant  {node_name = "onnx::MatMul_9842"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %264 = dtu_hlir.constant  {node_name = "onnx::MatMul_9843"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %265 = dtu_hlir.constant  {node_name = "onnx::MatMul_9844"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %266 = dtu_hlir.constant  {node_name = "onnx::MatMul_9845"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %267 = dtu_hlir.constant  {node_name = "onnx::Mul_9849"} dense<1.250000e-01> : tensor<1xf16>
    %268 = dtu_hlir.constant  {node_name = "onnx::MatMul_9852"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %269 = dtu_hlir.constant  {node_name = "onnx::MatMul_9853"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %270 = dtu_hlir.constant  {node_name = "onnx::MatMul_9854"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %271 = dtu_hlir.constant  {node_name = "onnx::MatMul_9855"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %272 = dtu_hlir.constant  {node_name = "onnx::MatMul_9862"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %273 = dtu_hlir.constant  {node_name = "onnx::MatMul_9863"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf16>
    %274 = dtu_hlir.constant  {node_name = "onnx::MatMul_9864"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %275 = dtu_hlir.constant  {node_name = "onnx::MatMul_9865"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %276 = dtu_hlir.constant  {node_name = "onnx::MatMul_9872"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %277 = dtu_hlir.constant  {node_name = "onnx::MatMul_9873"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %278 = dtu_hlir.constant  {node_name = "onnx::MatMul_9874"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %279 = dtu_hlir.constant  {node_name = "onnx::MatMul_9875"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %280 = dtu_hlir.constant  {node_name = "onnx::MatMul_9882"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %281 = dtu_hlir.constant  {node_name = "onnx::MatMul_9883"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %282 = dtu_hlir.constant  {node_name = "onnx::MatMul_9884"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %283 = dtu_hlir.constant  {node_name = "onnx::MatMul_9885"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %284 = dtu_hlir.constant  {node_name = "onnx::MatMul_9892"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %285 = dtu_hlir.constant  {node_name = "onnx::MatMul_9893"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf16>
    %286 = dtu_hlir.constant  {node_name = "onnx::MatMul_9894"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %287 = dtu_hlir.constant  {node_name = "onnx::MatMul_9895"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %288 = dtu_hlir.constant  {node_name = "onnx::MatMul_9902"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %289 = dtu_hlir.constant  {node_name = "onnx::MatMul_9903"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %290 = dtu_hlir.constant  {node_name = "onnx::MatMul_9904"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %291 = dtu_hlir.constant  {node_name = "onnx::MatMul_9905"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %292 = dtu_hlir.constant  {node_name = "onnx::MatMul_9912"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %293 = dtu_hlir.constant  {node_name = "onnx::MatMul_9913"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %294 = dtu_hlir.constant  {node_name = "onnx::MatMul_9914"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %295 = dtu_hlir.constant  {node_name = "onnx::MatMul_9915"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %296 = dtu_hlir.constant  {node_name = "onnx::MatMul_9922"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %297 = dtu_hlir.constant  {node_name = "onnx::MatMul_9923"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf16>
    %298 = dtu_hlir.constant  {node_name = "onnx::MatMul_9924"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf16>
    %299 = dtu_hlir.constant  {node_name = "onnx::MatMul_9925"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %300 = dtu_hlir.constant  {node_name = "onnx::MatMul_9932"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %301 = dtu_hlir.constant  {node_name = "onnx::MatMul_9933"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %302 = dtu_hlir.constant  {node_name = "onnx::MatMul_9934"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %303 = dtu_hlir.constant  {node_name = "onnx::MatMul_9935"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %304 = dtu_hlir.constant  {node_name = "onnx::MatMul_9942"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %305 = dtu_hlir.constant  {node_name = "onnx::MatMul_9943"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %306 = dtu_hlir.constant  {node_name = "onnx::MatMul_9944"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %307 = dtu_hlir.constant  {node_name = "onnx::MatMul_9945"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %308 = dtu_hlir.constant  {node_name = "onnx::MatMul_9952"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %309 = dtu_hlir.constant  {node_name = "onnx::MatMul_9953"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf16>
    %310 = dtu_hlir.constant  {node_name = "onnx::MatMul_9954"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf16>
    %311 = dtu_hlir.constant  {node_name = "onnx::MatMul_9955"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %312 = dtu_hlir.constant  {node_name = "onnx::MatMul_9962"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %313 = dtu_hlir.constant  {node_name = "onnx::MatMul_9963"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %314 = dtu_hlir.constant  {node_name = "onnx::MatMul_9964"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %315 = dtu_hlir.constant  {node_name = "onnx::MatMul_9965"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %316 = dtu_hlir.constant  {node_name = "onnx::MatMul_9972"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %317 = dtu_hlir.constant  {node_name = "onnx::MatMul_9973"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %318 = dtu_hlir.constant  {node_name = "onnx::MatMul_9974"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %319 = dtu_hlir.constant  {node_name = "onnx::MatMul_9975"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %320 = dtu_hlir.constant  {node_name = "onnx::MatMul_9982"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %321 = dtu_hlir.constant  {node_name = "onnx::MatMul_9983"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf16>
    %322 = dtu_hlir.constant  {node_name = "onnx::MatMul_9984"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf16>
    %323 = dtu_hlir.constant  {node_name = "onnx::MatMul_9985"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %324 = dtu_hlir.constant  {node_name = "onnx::MatMul_9992"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %325 = dtu_hlir.constant  {node_name = "onnx::MatMul_9993"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %326 = dtu_hlir.constant  {node_name = "onnx::MatMul_9994"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %327 = dtu_hlir.constant  {node_name = "onnx::MatMul_9995"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %328 = dtu_hlir.constant  {node_name = "onnx::MatMul_10002"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %329 = dtu_hlir.constant  {node_name = "onnx::MatMul_10003"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %330 = dtu_hlir.constant  {node_name = "onnx::MatMul_10004"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %331 = dtu_hlir.constant  {node_name = "onnx::MatMul_10005"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %332 = dtu_hlir.constant  {node_name = "onnx::MatMul_10012"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %333 = dtu_hlir.constant  {node_name = "onnx::MatMul_10013"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf16>
    %334 = dtu_hlir.constant  {node_name = "onnx::MatMul_10014"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf16>
    %335 = dtu_hlir.constant  {node_name = "onnx::MatMul_10015"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %336 = dtu_hlir.constant  {node_name = "onnx::MatMul_10030"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %337 = dtu_hlir.constant  {node_name = "onnx::MatMul_10031"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %338 = dtu_hlir.constant  {node_name = "onnx::MatMul_10032"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %339 = dtu_hlir.constant  {node_name = "onnx::MatMul_10033"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %340 = dtu_hlir.constant  {node_name = "onnx::MatMul_10040"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %341 = dtu_hlir.constant  {node_name = "onnx::MatMul_10041"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %342 = dtu_hlir.constant  {node_name = "onnx::MatMul_10042"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %343 = dtu_hlir.constant  {node_name = "onnx::MatMul_10043"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %344 = dtu_hlir.constant  {node_name = "onnx::MatMul_10050"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %345 = dtu_hlir.constant  {node_name = "onnx::MatMul_10051"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf16>
    %346 = dtu_hlir.constant  {node_name = "onnx::MatMul_10052"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf16>
    %347 = dtu_hlir.constant  {node_name = "onnx::MatMul_10053"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %348 = dtu_hlir.constant  {node_name = "onnx::MatMul_10077"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %349 = dtu_hlir.constant  {node_name = "onnx::MatMul_10078"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %350 = dtu_hlir.constant  {node_name = "onnx::MatMul_10079"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %351 = dtu_hlir.constant  {node_name = "onnx::MatMul_10080"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %352 = dtu_hlir.constant  {node_name = "onnx::MatMul_10087"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %353 = dtu_hlir.constant  {node_name = "onnx::MatMul_10088"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %354 = dtu_hlir.constant  {node_name = "onnx::MatMul_10089"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %355 = dtu_hlir.constant  {node_name = "onnx::MatMul_10090"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %356 = dtu_hlir.constant  {node_name = "onnx::MatMul_10097"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %357 = dtu_hlir.constant  {node_name = "onnx::MatMul_10098"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf16>
    %358 = dtu_hlir.constant  {node_name = "onnx::MatMul_10099"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf16>
    %359 = dtu_hlir.constant  {node_name = "onnx::MatMul_10100"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %360 = dtu_hlir.constant  {node_name = "onnx::MatMul_10107"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %361 = dtu_hlir.constant  {node_name = "onnx::MatMul_10108"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %362 = dtu_hlir.constant  {node_name = "onnx::MatMul_10109"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %363 = dtu_hlir.constant  {node_name = "onnx::MatMul_10110"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %364 = dtu_hlir.constant  {node_name = "onnx::MatMul_10117"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %365 = dtu_hlir.constant  {node_name = "onnx::MatMul_10118"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %366 = dtu_hlir.constant  {node_name = "onnx::MatMul_10119"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %367 = dtu_hlir.constant  {node_name = "onnx::MatMul_10120"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %368 = dtu_hlir.constant  {node_name = "onnx::MatMul_10127"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %369 = dtu_hlir.constant  {node_name = "onnx::MatMul_10128"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf16>
    %370 = dtu_hlir.constant  {node_name = "onnx::MatMul_10129"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf16>
    %371 = dtu_hlir.constant  {node_name = "onnx::MatMul_10130"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %372 = dtu_hlir.constant  {node_name = "onnx::MatMul_10137"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %373 = dtu_hlir.constant  {node_name = "onnx::MatMul_10138"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %374 = dtu_hlir.constant  {node_name = "onnx::MatMul_10139"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %375 = dtu_hlir.constant  {node_name = "onnx::MatMul_10140"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %376 = dtu_hlir.constant  {node_name = "onnx::MatMul_10147"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %377 = dtu_hlir.constant  {node_name = "onnx::MatMul_10148"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %378 = dtu_hlir.constant  {node_name = "onnx::MatMul_10149"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %379 = dtu_hlir.constant  {node_name = "onnx::MatMul_10150"} opaque<"", "0xDEADBEEF"> : tensor<1024x1280xf16>
    %380 = dtu_hlir.constant  {node_name = "onnx::MatMul_10157"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %381 = dtu_hlir.constant  {node_name = "onnx::MatMul_10158"} opaque<"", "0xDEADBEEF"> : tensor<1280x10240xf16>
    %382 = dtu_hlir.constant  {node_name = "onnx::MatMul_10159"} opaque<"", "0xDEADBEEF"> : tensor<5120x1280xf16>
    %383 = dtu_hlir.constant  {node_name = "onnx::MatMul_10160"} opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %384 = dtu_hlir.constant  {node_name = "onnx::MatMul_10168"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %385 = dtu_hlir.constant  {node_name = "onnx::MatMul_10169"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %386 = dtu_hlir.constant  {node_name = "onnx::MatMul_10170"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %387 = dtu_hlir.constant  {node_name = "onnx::MatMul_10171"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %388 = dtu_hlir.constant  {node_name = "onnx::MatMul_10178"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %389 = dtu_hlir.constant  {node_name = "onnx::MatMul_10179"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %390 = dtu_hlir.constant  {node_name = "onnx::MatMul_10180"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %391 = dtu_hlir.constant  {node_name = "onnx::MatMul_10181"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %392 = dtu_hlir.constant  {node_name = "onnx::MatMul_10188"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %393 = dtu_hlir.constant  {node_name = "onnx::MatMul_10189"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf16>
    %394 = dtu_hlir.constant  {node_name = "onnx::MatMul_10190"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf16>
    %395 = dtu_hlir.constant  {node_name = "onnx::MatMul_10191"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %396 = dtu_hlir.constant  {node_name = "onnx::MatMul_10198"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %397 = dtu_hlir.constant  {node_name = "onnx::MatMul_10199"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %398 = dtu_hlir.constant  {node_name = "onnx::MatMul_10200"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %399 = dtu_hlir.constant  {node_name = "onnx::MatMul_10201"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %400 = dtu_hlir.constant  {node_name = "onnx::MatMul_10208"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %401 = dtu_hlir.constant  {node_name = "onnx::MatMul_10209"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %402 = dtu_hlir.constant  {node_name = "onnx::MatMul_10210"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %403 = dtu_hlir.constant  {node_name = "onnx::MatMul_10211"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %404 = dtu_hlir.constant  {node_name = "onnx::MatMul_10218"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %405 = dtu_hlir.constant  {node_name = "onnx::MatMul_10219"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf16>
    %406 = dtu_hlir.constant  {node_name = "onnx::MatMul_10220"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf16>
    %407 = dtu_hlir.constant  {node_name = "onnx::MatMul_10221"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %408 = dtu_hlir.constant  {node_name = "onnx::MatMul_10228"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %409 = dtu_hlir.constant  {node_name = "onnx::MatMul_10229"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %410 = dtu_hlir.constant  {node_name = "onnx::MatMul_10230"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %411 = dtu_hlir.constant  {node_name = "onnx::MatMul_10231"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %412 = dtu_hlir.constant  {node_name = "onnx::MatMul_10238"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %413 = dtu_hlir.constant  {node_name = "onnx::MatMul_10239"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %414 = dtu_hlir.constant  {node_name = "onnx::MatMul_10240"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %415 = dtu_hlir.constant  {node_name = "onnx::MatMul_10241"} opaque<"", "0xDEADBEEF"> : tensor<1024x640xf16>
    %416 = dtu_hlir.constant  {node_name = "onnx::MatMul_10248"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %417 = dtu_hlir.constant  {node_name = "onnx::MatMul_10249"} opaque<"", "0xDEADBEEF"> : tensor<640x5120xf16>
    %418 = dtu_hlir.constant  {node_name = "onnx::MatMul_10250"} opaque<"", "0xDEADBEEF"> : tensor<2560x640xf16>
    %419 = dtu_hlir.constant  {node_name = "onnx::MatMul_10251"} opaque<"", "0xDEADBEEF"> : tensor<640x640xf16>
    %420 = dtu_hlir.constant  {node_name = "onnx::MatMul_10259"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %421 = dtu_hlir.constant  {node_name = "onnx::MatMul_10260"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %422 = dtu_hlir.constant  {node_name = "onnx::MatMul_10261"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %423 = dtu_hlir.constant  {node_name = "onnx::MatMul_10262"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %424 = dtu_hlir.constant  {node_name = "onnx::MatMul_10269"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %425 = dtu_hlir.constant  {node_name = "onnx::MatMul_10270"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %426 = dtu_hlir.constant  {node_name = "onnx::MatMul_10271"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %427 = dtu_hlir.constant  {node_name = "onnx::MatMul_10272"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %428 = dtu_hlir.constant  {node_name = "onnx::MatMul_10279"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %429 = dtu_hlir.constant  {node_name = "onnx::MatMul_10280"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf16>
    %430 = dtu_hlir.constant  {node_name = "onnx::MatMul_10281"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %431 = dtu_hlir.constant  {node_name = "onnx::MatMul_10282"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %432 = dtu_hlir.constant  {node_name = "onnx::MatMul_10289"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %433 = dtu_hlir.constant  {node_name = "onnx::MatMul_10290"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %434 = dtu_hlir.constant  {node_name = "onnx::MatMul_10291"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %435 = dtu_hlir.constant  {node_name = "onnx::MatMul_10292"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %436 = dtu_hlir.constant  {node_name = "onnx::MatMul_10299"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %437 = dtu_hlir.constant  {node_name = "onnx::MatMul_10300"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %438 = dtu_hlir.constant  {node_name = "onnx::MatMul_10301"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %439 = dtu_hlir.constant  {node_name = "onnx::MatMul_10302"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %440 = dtu_hlir.constant  {node_name = "onnx::MatMul_10309"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %441 = dtu_hlir.constant  {node_name = "onnx::MatMul_10310"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf16>
    %442 = dtu_hlir.constant  {node_name = "onnx::MatMul_10311"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %443 = dtu_hlir.constant  {node_name = "onnx::MatMul_10312"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %444 = dtu_hlir.constant  {node_name = "onnx::MatMul_10319"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %445 = dtu_hlir.constant  {node_name = "onnx::MatMul_10320"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %446 = dtu_hlir.constant  {node_name = "onnx::MatMul_10321"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %447 = dtu_hlir.constant  {node_name = "onnx::MatMul_10322"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %448 = dtu_hlir.constant  {node_name = "onnx::MatMul_10329"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %449 = dtu_hlir.constant  {node_name = "onnx::MatMul_10330"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %450 = dtu_hlir.constant  {node_name = "onnx::MatMul_10331"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %451 = dtu_hlir.constant  {node_name = "onnx::MatMul_10332"} opaque<"", "0xDEADBEEF"> : tensor<1024x320xf16>
    %452 = dtu_hlir.constant  {node_name = "onnx::MatMul_10339"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %453 = dtu_hlir.constant  {node_name = "onnx::MatMul_10340"} opaque<"", "0xDEADBEEF"> : tensor<320x2560xf16>
    %454 = dtu_hlir.constant  {node_name = "onnx::MatMul_10341"} opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %455 = dtu_hlir.constant  {node_name = "onnx::MatMul_10342"} opaque<"", "0xDEADBEEF"> : tensor<320x320xf16>
    %456 = dtu_hlir.constant  {broadcast_dimensions = dense<1> : tensor<1xi64>} opaque<"", "0xDEADBEEF"> : tensor<2x160xf16>
    %457 = "dtu_hlir.fusion"(%3, %456) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<2x160xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>} : (tensor<1xf16>) -> tensor<2x160xf16>
      %2330 = "dtu_hlir.mul"(%2329, %arg4) {node_name = "Mul_208-0", node_type = "Mul"} : (tensor<2x160xf16>, tensor<2x160xf16>) -> tensor<2x160xf16>
      %2331 = "dtu_hlir.cos"(%2330) {node_name = "Cos_210-0", node_type = "Cos"} : (tensor<2x160xf16>) -> tensor<2x160xf16>
      %2332 = "dtu_hlir.tuple"(%2330, %2331) : (tensor<2x160xf16>, tensor<2x160xf16>) -> tuple<tensor<2x160xf16>, tensor<2x160xf16>>
      "dtu_hlir.return"(%2332) : (tuple<tensor<2x160xf16>, tensor<2x160xf16>>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 4 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<2x160xf16>) -> tuple<tensor<2x160xf16>, tensor<2x160xf16>>
    %458 = "dtu_hlir.get_tuple_element"(%457) {index = 0 : i32} : (tuple<tensor<2x160xf16>, tensor<2x160xf16>>) -> tensor<2x160xf16>
    %459 = "dtu_hlir.get_tuple_element"(%457) {index = 1 : i32} : (tuple<tensor<2x160xf16>, tensor<2x160xf16>>) -> tensor<2x160xf16>
    %460 = "dtu_hlir.sine"(%458) {node_name = "Sin_209-0", node_type = "Sin", op_id = 5 : i64, unique_name = "elementwise_sine_dtu"} : (tensor<2x160xf16>) -> tensor<2x160xf16>
    %461 = "dtu_hlir.concatenate"(%459, %460) {dimension = 1 : i64, node_name = "Concat_222-0", node_type = "Concat", op_id = 6 : i64, unique_name = "concatenate_dtu"} : (tensor<2x160xf16>, tensor<2x160xf16>) -> tensor<2x320xf16>
    %462 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<320x1280xf16>
    %463 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %464 = "dtu_hlir.fusion"(%461, %462, %463) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<320x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x320xf16>, tensor<320x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_226-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 7 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x320xf16>, tensor<320x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %465 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %466 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %467 = dtu_hlir.constant  {node_name = "conv_in.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x4x320xf16>
    %468 = "dtu_hlir.conv_bias"(%1, %467, %5) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_228-0", op_id = 8 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x4xf16>, tensor<3x3x4x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %469 = "dtu_hlir.transpose"(%468) {node_name = "Conv_228-0", op_id = 9 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %470 = "dtu_hlir.reshape"(%469) {node_name = "Reshape_230-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %471 = dtu_hlir.constant  {broadcast_dimensions = dense<[]> : tensor<0xi64>} dense<1.000000e+00> : tensor<32xf32>
    %472 = dtu_hlir.constant  {broadcast_dimensions = dense<[]> : tensor<0xi64>} dense<0.000000e+00> : tensor<32xf32>
    %473 = "dtu_hlir.transpose"(%470) {node_name = "InstanceNormalization_233-0", op_id = 10 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %474 = "dtu_hlir.instance_norm"(%473, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 11 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %475 = "dtu_hlir.transpose"(%474) {node_name = "InstanceNormalization_233-0", op_id = 12 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %476 = "dtu_hlir.reshape"(%475) {node_name = "Reshape_235-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %477 = dtu_hlir.constant  {node_name = "Mul_236-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %478 = dtu_hlir.constant  {node_name = "Add_237-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %479 = "dtu_hlir.fusion"(%477, %476, %478) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_236-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_236-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_237-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_237-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_239-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 13 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %480 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %481 = "dtu_hlir.transpose"(%479) {node_name = "Conv_240-0", op_id = 14 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %482 = "dtu_hlir.conv_bias"(%481, %480, %30) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_240-0", op_id = 15 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %483 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %484 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %485 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 16 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %486 = "dtu_hlir.dot_general_bias"(%485, %483, %484) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 17 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x320xf16>
    %487 = "dtu_hlir.broadcast_in_dim"(%486) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 18 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x320xf16>) -> tensor<2x64x64x320xf16>
    %488 = "dtu_hlir.add"(%482, %487) {node_name = "Add_248-0", node_type = "Add", op_id = 19 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %489 = "dtu_hlir.transpose"(%488) {op_id = 20 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %490 = "dtu_hlir.reshape"(%489) {node_name = "Reshape_250-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %491 = "dtu_hlir.transpose"(%490) {node_name = "InstanceNormalization_253-0", op_id = 21 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %492 = "dtu_hlir.instance_norm"(%491, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 22 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %493 = "dtu_hlir.transpose"(%492) {node_name = "InstanceNormalization_253-0", op_id = 23 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %494 = "dtu_hlir.reshape"(%493) {node_name = "Reshape_255-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %495 = dtu_hlir.constant  {node_name = "Mul_256-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %496 = dtu_hlir.constant  {node_name = "Add_257-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %497 = "dtu_hlir.fusion"(%495, %494, %496) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_256-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_256-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_257-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_257-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_259-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 24 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %498 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %499 = "dtu_hlir.transpose"(%497) {node_name = "Conv_260-0", op_id = 25 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %500 = "dtu_hlir.conv_bias"(%499, %498, %31) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_260-0", op_id = 26 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %501 = "dtu_hlir.add"(%468, %500) {node_name = "Add_261-0", node_type = "Add", op_id = 27 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %502 = "dtu_hlir.transpose"(%501) {node_name = "Conv_228-0", op_id = 28 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %503 = "dtu_hlir.reshape"(%502) {node_name = "Reshape_274-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %504 = "dtu_hlir.transpose"(%503) {node_name = "InstanceNormalization_277-0", op_id = 29 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %505 = "dtu_hlir.instance_norm"(%504, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 30 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %506 = "dtu_hlir.transpose"(%505) {node_name = "InstanceNormalization_277-0", op_id = 31 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %507 = "dtu_hlir.reshape"(%506) {node_name = "Reshape_279-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %508 = dtu_hlir.constant  {node_name = "Mul_280-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %509 = dtu_hlir.constant  {node_name = "Add_281-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %510 = "dtu_hlir.fusion"(%508, %507, %509) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_280-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_280-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_281-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_281-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 32 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %511 = "dtu_hlir.transpose"(%510) {node_name = "Transpose_285-0", node_type = "Transpose", op_id = 33 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %512 = "dtu_hlir.reshape"(%511) {node_name = "Reshape_294-0"} : (tensor<2x64x64x320xf16>) -> tensor<2x4096x320xf16>
    %513 = "dtu_hlir.dot_general_bias"(%512, %263, %6) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_296-0", op_id = 34 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %514 = "dtu_hlir.layer_norm_inference"(%513, %11, %12) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_307-0", op_id = 35 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %515 = "dtu_hlir.dot_general"(%514, %264) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_308-0", node_type = "MatMul", op_id = 36 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %516 = "dtu_hlir.dot_general"(%514, %265) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_309-0", node_type = "MatMul", op_id = 37 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %517 = "dtu_hlir.dot_general"(%514, %266) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_310-0", node_type = "MatMul", op_id = 38 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %518 = "dtu_hlir.reshape"(%515) {node_name = "Reshape_331-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %519 = "dtu_hlir.transpose"(%518) {node_name = "Transpose_332-0", node_type = "Transpose", op_id = 39 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %520 = "dtu_hlir.reshape"(%519) {node_name = "Reshape_346-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %521 = "dtu_hlir.reshape"(%516) {node_name = "Reshape_367-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %522 = "dtu_hlir.transpose"(%521) {node_name = "Transpose_368-0", node_type = "Transpose", op_id = 40 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %523 = "dtu_hlir.reshape"(%522) {node_name = "Reshape_382-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %524 = "dtu_hlir.reshape"(%517) {node_name = "Reshape_403-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %525 = "dtu_hlir.transpose"(%524) {node_name = "Transpose_404-0", node_type = "Transpose", op_id = 41 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %526 = "dtu_hlir.reshape"(%525) {node_name = "Reshape_418-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %527 = "dtu_hlir.transpose"(%523) {node_name = "Transpose_436-0", node_type = "Transpose", op_id = 42 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x4096x64xf16>) -> tensor<10x64x4096xf16>
    %528 = "dtu_hlir.dot_general"(%520, %527) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_437-0", node_type = "MatMul", op_id = 43 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x4096xf16>) -> tensor<10x4096x4096xf16>
    %529 = "dtu_hlir.fusion"(%267, %528) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x4096xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_438-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x4096xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_438-0", node_type = "Mul"} : (tensor<10x4096x4096xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x4096xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 44 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %530 = "dtu_hlir.softmax"(%529) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_441-0", node_type = "Softmax", op_id = 45 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %531 = "dtu_hlir.dot_general"(%530, %526) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_443-0", node_type = "MatMul", op_id = 46 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x4096xf16>, tensor<10x4096x64xf16>) -> tensor<10x4096x64xf16>
    %532 = "dtu_hlir.reshape"(%531) {node_name = "Reshape_464-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %533 = "dtu_hlir.transpose"(%532) {node_name = "Transpose_465-0", node_type = "Transpose", op_id = 47 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %534 = "dtu_hlir.reshape"(%533) {node_name = "Reshape_479-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %535 = "dtu_hlir.dot_general_bias"(%534, %268, %7) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_481-0", op_id = 48 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %536 = "dtu_hlir.add"(%535, %513) {node_name = "Add_482-0", node_type = "Add", op_id = 49 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %537 = "dtu_hlir.layer_norm_inference"(%536, %13, %14) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_493-0", op_id = 50 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %538 = "dtu_hlir.dot_general"(%537, %269) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_494-0", node_type = "MatMul", op_id = 51 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %539 = "dtu_hlir.dot_general"(%4, %270) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_495-0", node_type = "MatMul", op_id = 52 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %540 = "dtu_hlir.dot_general"(%4, %271) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_496-0", node_type = "MatMul", op_id = 53 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %541 = "dtu_hlir.reshape"(%538) {node_name = "Reshape_517-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %542 = "dtu_hlir.transpose"(%541) {node_name = "Transpose_518-0", node_type = "Transpose", op_id = 54 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %543 = "dtu_hlir.reshape"(%542) {node_name = "Reshape_532-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %544 = "dtu_hlir.reshape"(%539) {node_name = "Reshape_553-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %545 = "dtu_hlir.transpose"(%544) {node_name = "Transpose_554-0", node_type = "Transpose", op_id = 55 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %546 = "dtu_hlir.reshape"(%545) {node_name = "Reshape_568-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %547 = "dtu_hlir.reshape"(%540) {node_name = "Reshape_589-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %548 = "dtu_hlir.transpose"(%547) {node_name = "Transpose_590-0", node_type = "Transpose", op_id = 56 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %549 = "dtu_hlir.reshape"(%548) {node_name = "Reshape_604-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %550 = "dtu_hlir.transpose"(%546) {node_name = "Transpose_622-0", node_type = "Transpose", op_id = 57 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x77x64xf16>) -> tensor<10x64x77xf16>
    %551 = "dtu_hlir.dot_general"(%543, %550) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_623-0", node_type = "MatMul", op_id = 58 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x77xf16>) -> tensor<10x4096x77xf16>
    %552 = "dtu_hlir.fusion"(%267, %551) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_624-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_624-0", node_type = "Mul"} : (tensor<10x4096x77xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 59 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %553 = "dtu_hlir.softmax"(%552) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_627-0", node_type = "Softmax", op_id = 60 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %554 = "dtu_hlir.dot_general"(%553, %549) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_629-0", node_type = "MatMul", op_id = 61 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x77xf16>, tensor<10x77x64xf16>) -> tensor<10x4096x64xf16>
    %555 = "dtu_hlir.reshape"(%554) {node_name = "Reshape_650-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %556 = "dtu_hlir.transpose"(%555) {node_name = "Transpose_651-0", node_type = "Transpose", op_id = 62 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %557 = "dtu_hlir.reshape"(%556) {node_name = "Reshape_665-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %558 = "dtu_hlir.dot_general_bias"(%557, %272, %10) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_667-0", op_id = 63 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %559 = "dtu_hlir.add"(%558, %536) {node_name = "Add_668-0", node_type = "Add", op_id = 64 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %560 = "dtu_hlir.layer_norm_inference"(%559, %15, %16) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_679-0", op_id = 65 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %561 = "dtu_hlir.dot_general_bias"(%560, %273, %8) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_681-0", op_id = 66 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x2560xf16>, tensor<2560xf32>) -> tensor<2x4096x2560xf16>
    %562 = "dtu_hlir.slice"(%561) {limit_indices = dense<[2, 4096, 1280]> : tensor<3xi64>, node_name = "Slice_692-1", op_id = 67 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %563 = "dtu_hlir.slice"(%561) {limit_indices = dense<[2, 4096, 2560]> : tensor<3xi64>, node_name = "Slice_695-3", op_id = 68 : i64, start_indices = dense<[0, 0, 1280]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %564 = "dtu_hlir.gelu"(%563) {approximate = false, node_name = "Mul_703-0", op_id = 69 : i64, unique_name = "common20_gelu"} : (tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %565 = "dtu_hlir.mul"(%562, %564) {node_name = "Mul_704-0", node_type = "Mul", op_id = 70 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x4096x1280xf16>, tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %566 = "dtu_hlir.dot_general_bias"(%565, %274, %9) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_706-0", op_id = 71 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %567 = "dtu_hlir.add"(%566, %559) {node_name = "Add_707-0", node_type = "Add", op_id = 72 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %568 = "dtu_hlir.dot_general_bias"(%567, %275, %17) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_709-0", op_id = 73 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %569 = "dtu_hlir.reshape"(%568) {node_name = "Reshape_719-0"} : (tensor<2x4096x320xf16>) -> tensor<2x64x64x320xf16>
    %570 = "dtu_hlir.add"(%569, %501) {node_name = "Add_721-0", node_type = "Add", op_id = 74 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %571 = "dtu_hlir.transpose"(%570) {node_name = "Transpose_720-0", op_id = 75 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %572 = "dtu_hlir.reshape"(%571) {node_name = "Reshape_723-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %573 = "dtu_hlir.transpose"(%572) {node_name = "InstanceNormalization_726-0", op_id = 76 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %574 = "dtu_hlir.instance_norm"(%573, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 77 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %575 = "dtu_hlir.transpose"(%574) {node_name = "InstanceNormalization_726-0", op_id = 78 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %576 = "dtu_hlir.reshape"(%575) {node_name = "Reshape_728-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %577 = dtu_hlir.constant  {node_name = "Mul_729-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %578 = dtu_hlir.constant  {node_name = "Add_730-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %579 = "dtu_hlir.fusion"(%577, %576, %578) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_729-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_729-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_730-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_730-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_732-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 79 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %580 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %581 = "dtu_hlir.transpose"(%579) {node_name = "Conv_733-0", op_id = 80 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %582 = "dtu_hlir.conv_bias"(%581, %580, %32) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_733-0", op_id = 81 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %583 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %584 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %585 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 82 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %586 = "dtu_hlir.dot_general_bias"(%585, %583, %584) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 83 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x320xf16>
    %587 = "dtu_hlir.broadcast_in_dim"(%586) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 84 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x320xf16>) -> tensor<2x64x64x320xf16>
    %588 = "dtu_hlir.add"(%582, %587) {node_name = "Add_741-0", node_type = "Add", op_id = 85 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %589 = "dtu_hlir.transpose"(%588) {op_id = 86 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %590 = "dtu_hlir.reshape"(%589) {node_name = "Reshape_743-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %591 = "dtu_hlir.transpose"(%590) {node_name = "InstanceNormalization_746-0", op_id = 87 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %592 = "dtu_hlir.instance_norm"(%591, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 88 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %593 = "dtu_hlir.transpose"(%592) {node_name = "InstanceNormalization_746-0", op_id = 89 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %594 = "dtu_hlir.reshape"(%593) {node_name = "Reshape_748-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %595 = dtu_hlir.constant  {node_name = "Mul_749-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %596 = dtu_hlir.constant  {node_name = "Add_750-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %597 = "dtu_hlir.fusion"(%595, %594, %596) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_749-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_749-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_750-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_750-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_752-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 90 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %598 = dtu_hlir.constant  {node_name = "down_blocks.0.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %599 = "dtu_hlir.transpose"(%597) {node_name = "Conv_753-0", op_id = 91 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %600 = "dtu_hlir.conv_bias"(%599, %598, %33) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_753-0", op_id = 92 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %601 = "dtu_hlir.add"(%570, %600) {node_name = "Add_754-0", node_type = "Add", op_id = 93 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %602 = "dtu_hlir.transpose"(%601) {node_name = "Transpose_720-0", op_id = 94 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %603 = "dtu_hlir.reshape"(%602) {node_name = "Reshape_767-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %604 = "dtu_hlir.transpose"(%603) {node_name = "InstanceNormalization_770-0", op_id = 95 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %605 = "dtu_hlir.instance_norm"(%604, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 96 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %606 = "dtu_hlir.transpose"(%605) {node_name = "InstanceNormalization_770-0", op_id = 97 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %607 = "dtu_hlir.reshape"(%606) {node_name = "Reshape_772-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %608 = dtu_hlir.constant  {node_name = "Mul_773-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %609 = dtu_hlir.constant  {node_name = "Add_774-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %610 = "dtu_hlir.fusion"(%608, %607, %609) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_773-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_773-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_774-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_774-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 98 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %611 = "dtu_hlir.transpose"(%610) {node_name = "Transpose_778-0", node_type = "Transpose", op_id = 99 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %612 = "dtu_hlir.reshape"(%611) {node_name = "Reshape_787-0"} : (tensor<2x64x64x320xf16>) -> tensor<2x4096x320xf16>
    %613 = "dtu_hlir.dot_general_bias"(%612, %276, %18) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_789-0", op_id = 100 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %614 = "dtu_hlir.layer_norm_inference"(%613, %23, %24) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_800-0", op_id = 101 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %615 = "dtu_hlir.dot_general"(%614, %277) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_801-0", node_type = "MatMul", op_id = 102 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %616 = "dtu_hlir.dot_general"(%614, %278) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_802-0", node_type = "MatMul", op_id = 103 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %617 = "dtu_hlir.dot_general"(%614, %279) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_803-0", node_type = "MatMul", op_id = 104 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %618 = "dtu_hlir.reshape"(%615) {node_name = "Reshape_824-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %619 = "dtu_hlir.transpose"(%618) {node_name = "Transpose_825-0", node_type = "Transpose", op_id = 105 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %620 = "dtu_hlir.reshape"(%619) {node_name = "Reshape_839-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %621 = "dtu_hlir.reshape"(%616) {node_name = "Reshape_860-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %622 = "dtu_hlir.transpose"(%621) {node_name = "Transpose_861-0", node_type = "Transpose", op_id = 106 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %623 = "dtu_hlir.reshape"(%622) {node_name = "Reshape_875-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %624 = "dtu_hlir.reshape"(%617) {node_name = "Reshape_896-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %625 = "dtu_hlir.transpose"(%624) {node_name = "Transpose_897-0", node_type = "Transpose", op_id = 107 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %626 = "dtu_hlir.reshape"(%625) {node_name = "Reshape_911-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %627 = "dtu_hlir.transpose"(%623) {node_name = "Transpose_929-0", node_type = "Transpose", op_id = 108 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x4096x64xf16>) -> tensor<10x64x4096xf16>
    %628 = "dtu_hlir.dot_general"(%620, %627) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_930-0", node_type = "MatMul", op_id = 109 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x4096xf16>) -> tensor<10x4096x4096xf16>
    %629 = "dtu_hlir.fusion"(%267, %628) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x4096xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_438-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x4096xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_931-0", node_type = "Mul"} : (tensor<10x4096x4096xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x4096xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 110 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %630 = "dtu_hlir.softmax"(%629) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_934-0", node_type = "Softmax", op_id = 111 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %631 = "dtu_hlir.dot_general"(%630, %626) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_936-0", node_type = "MatMul", op_id = 112 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x4096xf16>, tensor<10x4096x64xf16>) -> tensor<10x4096x64xf16>
    %632 = "dtu_hlir.reshape"(%631) {node_name = "Reshape_957-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %633 = "dtu_hlir.transpose"(%632) {node_name = "Transpose_958-0", node_type = "Transpose", op_id = 113 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %634 = "dtu_hlir.reshape"(%633) {node_name = "Reshape_972-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %635 = "dtu_hlir.dot_general_bias"(%634, %280, %19) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_974-0", op_id = 114 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %636 = "dtu_hlir.add"(%635, %613) {node_name = "Add_975-0", node_type = "Add", op_id = 115 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %637 = "dtu_hlir.layer_norm_inference"(%636, %25, %26) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_986-0", op_id = 116 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %638 = "dtu_hlir.dot_general"(%637, %281) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_987-0", node_type = "MatMul", op_id = 117 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %639 = "dtu_hlir.dot_general"(%4, %282) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_988-0", node_type = "MatMul", op_id = 118 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %640 = "dtu_hlir.dot_general"(%4, %283) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_989-0", node_type = "MatMul", op_id = 119 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %641 = "dtu_hlir.reshape"(%638) {node_name = "Reshape_1010-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %642 = "dtu_hlir.transpose"(%641) {node_name = "Transpose_1011-0", node_type = "Transpose", op_id = 120 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %643 = "dtu_hlir.reshape"(%642) {node_name = "Reshape_1025-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %644 = "dtu_hlir.reshape"(%639) {node_name = "Reshape_1046-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %645 = "dtu_hlir.transpose"(%644) {node_name = "Transpose_1047-0", node_type = "Transpose", op_id = 121 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %646 = "dtu_hlir.reshape"(%645) {node_name = "Reshape_1061-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %647 = "dtu_hlir.reshape"(%640) {node_name = "Reshape_1082-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %648 = "dtu_hlir.transpose"(%647) {node_name = "Transpose_1083-0", node_type = "Transpose", op_id = 122 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %649 = "dtu_hlir.reshape"(%648) {node_name = "Reshape_1097-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %650 = "dtu_hlir.transpose"(%646) {node_name = "Transpose_1115-0", node_type = "Transpose", op_id = 123 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x77x64xf16>) -> tensor<10x64x77xf16>
    %651 = "dtu_hlir.dot_general"(%643, %650) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1116-0", node_type = "MatMul", op_id = 124 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x77xf16>) -> tensor<10x4096x77xf16>
    %652 = "dtu_hlir.fusion"(%267, %651) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_624-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1117-0", node_type = "Mul"} : (tensor<10x4096x77xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 125 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %653 = "dtu_hlir.softmax"(%652) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1120-0", node_type = "Softmax", op_id = 126 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %654 = "dtu_hlir.dot_general"(%653, %649) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1122-0", node_type = "MatMul", op_id = 127 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x77xf16>, tensor<10x77x64xf16>) -> tensor<10x4096x64xf16>
    %655 = "dtu_hlir.reshape"(%654) {node_name = "Reshape_1143-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %656 = "dtu_hlir.transpose"(%655) {node_name = "Transpose_1144-0", node_type = "Transpose", op_id = 128 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %657 = "dtu_hlir.reshape"(%656) {node_name = "Reshape_1158-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %658 = "dtu_hlir.dot_general_bias"(%657, %284, %22) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1160-0", op_id = 129 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %659 = "dtu_hlir.add"(%658, %636) {node_name = "Add_1161-0", node_type = "Add", op_id = 130 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %660 = "dtu_hlir.layer_norm_inference"(%659, %27, %28) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_1172-0", op_id = 131 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %661 = "dtu_hlir.dot_general_bias"(%660, %285, %20) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1174-0", op_id = 132 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x2560xf16>, tensor<2560xf32>) -> tensor<2x4096x2560xf16>
    %662 = "dtu_hlir.slice"(%661) {limit_indices = dense<[2, 4096, 1280]> : tensor<3xi64>, node_name = "Slice_1185-1", op_id = 133 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %663 = "dtu_hlir.slice"(%661) {limit_indices = dense<[2, 4096, 2560]> : tensor<3xi64>, node_name = "Slice_1188-3", op_id = 134 : i64, start_indices = dense<[0, 0, 1280]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %664 = "dtu_hlir.gelu"(%663) {approximate = false, node_name = "Mul_1196-0", op_id = 135 : i64, unique_name = "common20_gelu"} : (tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %665 = "dtu_hlir.mul"(%662, %664) {node_name = "Mul_1197-0", node_type = "Mul", op_id = 136 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x4096x1280xf16>, tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %666 = "dtu_hlir.dot_general_bias"(%665, %286, %21) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1199-0", op_id = 137 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %667 = "dtu_hlir.add"(%666, %659) {node_name = "Add_1200-0", node_type = "Add", op_id = 138 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %668 = "dtu_hlir.dot_general_bias"(%667, %287, %29) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1202-0", op_id = 139 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %669 = "dtu_hlir.reshape"(%668) {node_name = "Reshape_1212-0"} : (tensor<2x4096x320xf16>) -> tensor<2x64x64x320xf16>
    %670 = "dtu_hlir.add"(%669, %601) {node_name = "Add_1214-0", node_type = "Add", op_id = 140 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %671 = dtu_hlir.constant  {node_name = "down_blocks.0.downsamplers.0.conv.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %672 = "dtu_hlir.conv_bias"(%670, %671, %34) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1215-0", op_id = 141 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<2> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x32x32x320xf16>
    %673 = "dtu_hlir.transpose"(%672) {node_name = "Conv_1215-0", op_id = 142 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x320xf16>) -> tensor<2x320x32x32xf16>
    %674 = "dtu_hlir.reshape"(%673) {node_name = "Reshape_1217-0"} : (tensor<2x320x32x32xf16>) -> tensor<2x32x10240xf16>
    %675 = "dtu_hlir.transpose"(%674) {node_name = "InstanceNormalization_1220-0", op_id = 143 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %676 = "dtu_hlir.instance_norm"(%675, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 144 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %677 = "dtu_hlir.transpose"(%676) {node_name = "InstanceNormalization_1220-0", op_id = 145 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %678 = "dtu_hlir.reshape"(%677) {node_name = "Reshape_1222-0"} : (tensor<2x32x10240xf16>) -> tensor<2x320x32x32xf16>
    %679 = dtu_hlir.constant  {node_name = "Mul_1223-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %680 = dtu_hlir.constant  {node_name = "Add_1224-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %681 = "dtu_hlir.fusion"(%679, %678, %680) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x32x32xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1223-0"} : (tensor<2x320xf16>) -> tensor<2x320x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1223-0", node_type = "Mul"} : (tensor<2x320x32x32xf16>, tensor<2x320x32x32xf16>) -> tensor<2x320x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_1224-0"} : (tensor<2x320xf16>) -> tensor<2x320x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_1224-0", node_type = "Add"} : (tensor<2x320x32x32xf16>, tensor<2x320x32x32xf16>) -> tensor<2x320x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_1226-0"} : (tensor<2x320x32x32xf16>) -> tensor<2x320x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 146 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x32x32xf16>, tensor<2x320xf16>) -> tensor<2x320x32x32xf16>
    %682 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x640xf16>
    %683 = "dtu_hlir.transpose"(%681) {node_name = "Conv_1227-0", op_id = 147 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x32x32xf16>) -> tensor<2x32x32x320xf16>
    %684 = "dtu_hlir.conv_bias"(%683, %682, %59) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1227-0", op_id = 148 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x320xf16>, tensor<3x3x320x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %685 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x640xf16>
    %686 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %687 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 149 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %688 = "dtu_hlir.dot_general_bias"(%687, %685, %686) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 150 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x640xf16>, tensor<640xf32>) -> tensor<2x640xf16>
    %689 = "dtu_hlir.broadcast_in_dim"(%688) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 151 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x640xf16>) -> tensor<2x32x32x640xf16>
    %690 = "dtu_hlir.add"(%684, %689) {node_name = "Add_1235-0", node_type = "Add", op_id = 152 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %691 = "dtu_hlir.transpose"(%690) {op_id = 153 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %692 = "dtu_hlir.reshape"(%691) {node_name = "Reshape_1237-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %693 = "dtu_hlir.transpose"(%692) {node_name = "InstanceNormalization_1240-0", op_id = 154 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %694 = "dtu_hlir.instance_norm"(%693, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 155 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %695 = "dtu_hlir.transpose"(%694) {node_name = "InstanceNormalization_1240-0", op_id = 156 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %696 = "dtu_hlir.reshape"(%695) {node_name = "Reshape_1242-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %697 = dtu_hlir.constant  {node_name = "Mul_1243-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %698 = dtu_hlir.constant  {node_name = "Add_1244-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %699 = "dtu_hlir.fusion"(%697, %696, %698) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1243-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1243-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_1244-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_1244-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_1246-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 157 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %700 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %701 = "dtu_hlir.transpose"(%699) {node_name = "Conv_1247-0", op_id = 158 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %702 = "dtu_hlir.conv_bias"(%701, %700, %60) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1247-0", op_id = 159 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %703 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.0.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x320x640xf16>
    %704 = "dtu_hlir.conv_bias"(%672, %703, %61) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1248-0", op_id = 160 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x320xf16>, tensor<1x1x320x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %705 = "dtu_hlir.add"(%704, %702) {node_name = "Add_1249-0", node_type = "Add", op_id = 161 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %706 = "dtu_hlir.transpose"(%705) {node_name = "Conv_1248-0", op_id = 162 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %707 = "dtu_hlir.reshape"(%706) {node_name = "Reshape_1262-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %708 = "dtu_hlir.transpose"(%707) {node_name = "InstanceNormalization_1265-0", op_id = 163 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %709 = "dtu_hlir.instance_norm"(%708, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 164 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %710 = "dtu_hlir.transpose"(%709) {node_name = "InstanceNormalization_1265-0", op_id = 165 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %711 = "dtu_hlir.reshape"(%710) {node_name = "Reshape_1267-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %712 = dtu_hlir.constant  {node_name = "Mul_1268-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %713 = dtu_hlir.constant  {node_name = "Add_1269-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %714 = "dtu_hlir.fusion"(%712, %711, %713) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1268-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1268-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_1269-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_1269-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 166 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %715 = "dtu_hlir.transpose"(%714) {node_name = "Transpose_1273-0", node_type = "Transpose", op_id = 167 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %716 = "dtu_hlir.reshape"(%715) {node_name = "Reshape_1282-0"} : (tensor<2x32x32x640xf16>) -> tensor<2x1024x640xf16>
    %717 = "dtu_hlir.dot_general_bias"(%716, %288, %35) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1284-0", op_id = 168 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %718 = "dtu_hlir.layer_norm_inference"(%717, %40, %41) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_1295-0", op_id = 169 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %719 = "dtu_hlir.dot_general"(%718, %289) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1296-0", node_type = "MatMul", op_id = 170 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %720 = "dtu_hlir.dot_general"(%718, %290) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1297-0", node_type = "MatMul", op_id = 171 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %721 = "dtu_hlir.dot_general"(%718, %291) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1298-0", node_type = "MatMul", op_id = 172 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %722 = "dtu_hlir.reshape"(%719) {node_name = "Reshape_1319-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %723 = "dtu_hlir.transpose"(%722) {node_name = "Transpose_1320-0", node_type = "Transpose", op_id = 173 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %724 = "dtu_hlir.reshape"(%723) {node_name = "Reshape_1334-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %725 = "dtu_hlir.reshape"(%720) {node_name = "Reshape_1355-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %726 = "dtu_hlir.transpose"(%725) {node_name = "Transpose_1356-0", node_type = "Transpose", op_id = 174 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %727 = "dtu_hlir.reshape"(%726) {node_name = "Reshape_1370-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %728 = "dtu_hlir.reshape"(%721) {node_name = "Reshape_1391-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %729 = "dtu_hlir.transpose"(%728) {node_name = "Transpose_1392-0", node_type = "Transpose", op_id = 175 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %730 = "dtu_hlir.reshape"(%729) {node_name = "Reshape_1406-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %731 = "dtu_hlir.transpose"(%727) {node_name = "Transpose_1424-0", node_type = "Transpose", op_id = 176 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x1024x64xf16>) -> tensor<20x64x1024xf16>
    %732 = "dtu_hlir.dot_general"(%724, %731) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1425-0", node_type = "MatMul", op_id = 177 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x1024xf16>) -> tensor<20x1024x1024xf16>
    %733 = "dtu_hlir.fusion"(%267, %732) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x1024xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1426-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x1024xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1426-0", node_type = "Mul"} : (tensor<20x1024x1024xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x1024xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 178 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %734 = "dtu_hlir.softmax"(%733) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1429-0", node_type = "Softmax", op_id = 179 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %735 = "dtu_hlir.dot_general"(%734, %730) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1431-0", node_type = "MatMul", op_id = 180 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x1024xf16>, tensor<20x1024x64xf16>) -> tensor<20x1024x64xf16>
    %736 = "dtu_hlir.reshape"(%735) {node_name = "Reshape_1452-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %737 = "dtu_hlir.transpose"(%736) {node_name = "Transpose_1453-0", node_type = "Transpose", op_id = 181 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %738 = "dtu_hlir.reshape"(%737) {node_name = "Reshape_1467-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %739 = "dtu_hlir.dot_general_bias"(%738, %292, %36) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1469-0", op_id = 182 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %740 = "dtu_hlir.add"(%739, %717) {node_name = "Add_1470-0", node_type = "Add", op_id = 183 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %741 = "dtu_hlir.layer_norm_inference"(%740, %42, %43) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_1481-0", op_id = 184 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %742 = "dtu_hlir.dot_general"(%741, %293) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1482-0", node_type = "MatMul", op_id = 185 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %743 = "dtu_hlir.dot_general"(%4, %294) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1483-0", node_type = "MatMul", op_id = 186 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %744 = "dtu_hlir.dot_general"(%4, %295) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1484-0", node_type = "MatMul", op_id = 187 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %745 = "dtu_hlir.reshape"(%742) {node_name = "Reshape_1505-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %746 = "dtu_hlir.transpose"(%745) {node_name = "Transpose_1506-0", node_type = "Transpose", op_id = 188 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %747 = "dtu_hlir.reshape"(%746) {node_name = "Reshape_1520-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %748 = "dtu_hlir.reshape"(%743) {node_name = "Reshape_1541-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %749 = "dtu_hlir.transpose"(%748) {node_name = "Transpose_1542-0", node_type = "Transpose", op_id = 189 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %750 = "dtu_hlir.reshape"(%749) {node_name = "Reshape_1556-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %751 = "dtu_hlir.reshape"(%744) {node_name = "Reshape_1577-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %752 = "dtu_hlir.transpose"(%751) {node_name = "Transpose_1578-0", node_type = "Transpose", op_id = 190 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %753 = "dtu_hlir.reshape"(%752) {node_name = "Reshape_1592-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %754 = "dtu_hlir.transpose"(%750) {node_name = "Transpose_1610-0", node_type = "Transpose", op_id = 191 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x77x64xf16>) -> tensor<20x64x77xf16>
    %755 = "dtu_hlir.dot_general"(%747, %754) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1611-0", node_type = "MatMul", op_id = 192 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x77xf16>) -> tensor<20x1024x77xf16>
    %756 = "dtu_hlir.fusion"(%267, %755) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1612-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1612-0", node_type = "Mul"} : (tensor<20x1024x77xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 193 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %757 = "dtu_hlir.softmax"(%756) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1615-0", node_type = "Softmax", op_id = 194 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %758 = "dtu_hlir.dot_general"(%757, %753) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1617-0", node_type = "MatMul", op_id = 195 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x77xf16>, tensor<20x77x64xf16>) -> tensor<20x1024x64xf16>
    %759 = "dtu_hlir.reshape"(%758) {node_name = "Reshape_1638-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %760 = "dtu_hlir.transpose"(%759) {node_name = "Transpose_1639-0", node_type = "Transpose", op_id = 196 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %761 = "dtu_hlir.reshape"(%760) {node_name = "Reshape_1653-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %762 = "dtu_hlir.dot_general_bias"(%761, %296, %39) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1655-0", op_id = 197 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %763 = "dtu_hlir.add"(%762, %740) {node_name = "Add_1656-0", node_type = "Add", op_id = 198 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %764 = "dtu_hlir.layer_norm_inference"(%763, %44, %45) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_1667-0", op_id = 199 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %765 = "dtu_hlir.dot_general_bias"(%764, %297, %37) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1669-0", op_id = 200 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x5120xf16>, tensor<5120xf32>) -> tensor<2x1024x5120xf16>
    %766 = "dtu_hlir.slice"(%765) {limit_indices = dense<[2, 1024, 2560]> : tensor<3xi64>, node_name = "Slice_1680-1", op_id = 201 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %767 = "dtu_hlir.slice"(%765) {limit_indices = dense<[2, 1024, 5120]> : tensor<3xi64>, node_name = "Slice_1683-3", op_id = 202 : i64, start_indices = dense<[0, 0, 2560]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %768 = "dtu_hlir.gelu"(%767) {approximate = false, node_name = "Mul_1691-0", op_id = 203 : i64, unique_name = "common20_gelu"} : (tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %769 = "dtu_hlir.mul"(%766, %768) {node_name = "Mul_1692-0", node_type = "Mul", op_id = 204 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x1024x2560xf16>, tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %770 = "dtu_hlir.dot_general_bias"(%769, %298, %38) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1694-0", op_id = 205 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x2560xf16>, tensor<2560x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %771 = "dtu_hlir.add"(%770, %763) {node_name = "Add_1695-0", node_type = "Add", op_id = 206 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %772 = "dtu_hlir.dot_general_bias"(%771, %299, %46) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1697-0", op_id = 207 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %773 = "dtu_hlir.reshape"(%772) {node_name = "Reshape_1707-0"} : (tensor<2x1024x640xf16>) -> tensor<2x32x32x640xf16>
    %774 = "dtu_hlir.add"(%773, %705) {node_name = "Add_1709-0", node_type = "Add", op_id = 208 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %775 = "dtu_hlir.transpose"(%774) {node_name = "Transpose_1708-0", op_id = 209 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %776 = "dtu_hlir.reshape"(%775) {node_name = "Reshape_1711-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %777 = "dtu_hlir.transpose"(%776) {node_name = "InstanceNormalization_1714-0", op_id = 210 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %778 = "dtu_hlir.instance_norm"(%777, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 211 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %779 = "dtu_hlir.transpose"(%778) {node_name = "InstanceNormalization_1714-0", op_id = 212 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %780 = "dtu_hlir.reshape"(%779) {node_name = "Reshape_1716-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %781 = dtu_hlir.constant  {node_name = "Mul_1717-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %782 = dtu_hlir.constant  {node_name = "Add_1718-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %783 = "dtu_hlir.fusion"(%781, %780, %782) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1717-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1717-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_1718-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_1718-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_1720-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 213 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %784 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %785 = "dtu_hlir.transpose"(%783) {node_name = "Conv_1721-0", op_id = 214 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %786 = "dtu_hlir.conv_bias"(%785, %784, %62) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1721-0", op_id = 215 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %787 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x640xf16>
    %788 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %789 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 216 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %790 = "dtu_hlir.dot_general_bias"(%789, %787, %788) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 217 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x640xf16>, tensor<640xf32>) -> tensor<2x640xf16>
    %791 = "dtu_hlir.broadcast_in_dim"(%790) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 218 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x640xf16>) -> tensor<2x32x32x640xf16>
    %792 = "dtu_hlir.add"(%786, %791) {node_name = "Add_1729-0", node_type = "Add", op_id = 219 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %793 = "dtu_hlir.transpose"(%792) {op_id = 220 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %794 = "dtu_hlir.reshape"(%793) {node_name = "Reshape_1731-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %795 = "dtu_hlir.transpose"(%794) {node_name = "InstanceNormalization_1734-0", op_id = 221 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %796 = "dtu_hlir.instance_norm"(%795, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 222 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %797 = "dtu_hlir.transpose"(%796) {node_name = "InstanceNormalization_1734-0", op_id = 223 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %798 = "dtu_hlir.reshape"(%797) {node_name = "Reshape_1736-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %799 = dtu_hlir.constant  {node_name = "Mul_1737-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %800 = dtu_hlir.constant  {node_name = "Add_1738-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %801 = "dtu_hlir.fusion"(%799, %798, %800) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1737-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1737-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_1738-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_1738-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_1740-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 224 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %802 = dtu_hlir.constant  {node_name = "down_blocks.1.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %803 = "dtu_hlir.transpose"(%801) {node_name = "Conv_1741-0", op_id = 225 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %804 = "dtu_hlir.conv_bias"(%803, %802, %63) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_1741-0", op_id = 226 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %805 = "dtu_hlir.add"(%774, %804) {node_name = "Add_1742-0", node_type = "Add", op_id = 227 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %806 = "dtu_hlir.transpose"(%805) {node_name = "Transpose_1708-0", op_id = 228 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %807 = "dtu_hlir.reshape"(%806) {node_name = "Reshape_1755-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %808 = "dtu_hlir.transpose"(%807) {node_name = "InstanceNormalization_1758-0", op_id = 229 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %809 = "dtu_hlir.instance_norm"(%808, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 230 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %810 = "dtu_hlir.transpose"(%809) {node_name = "InstanceNormalization_1758-0", op_id = 231 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %811 = "dtu_hlir.reshape"(%810) {node_name = "Reshape_1760-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %812 = dtu_hlir.constant  {node_name = "Mul_1761-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %813 = dtu_hlir.constant  {node_name = "Add_1762-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %814 = "dtu_hlir.fusion"(%812, %811, %813) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_1761-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1761-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_1762-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_1762-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 232 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %815 = "dtu_hlir.transpose"(%814) {node_name = "Transpose_1766-0", node_type = "Transpose", op_id = 233 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %816 = "dtu_hlir.reshape"(%815) {node_name = "Reshape_1775-0"} : (tensor<2x32x32x640xf16>) -> tensor<2x1024x640xf16>
    %817 = "dtu_hlir.dot_general_bias"(%816, %300, %47) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1777-0", op_id = 234 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %818 = "dtu_hlir.layer_norm_inference"(%817, %52, %53) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_1788-0", op_id = 235 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %819 = "dtu_hlir.dot_general"(%818, %301) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1789-0", node_type = "MatMul", op_id = 236 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %820 = "dtu_hlir.dot_general"(%818, %302) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1790-0", node_type = "MatMul", op_id = 237 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %821 = "dtu_hlir.dot_general"(%818, %303) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1791-0", node_type = "MatMul", op_id = 238 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %822 = "dtu_hlir.reshape"(%819) {node_name = "Reshape_1812-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %823 = "dtu_hlir.transpose"(%822) {node_name = "Transpose_1813-0", node_type = "Transpose", op_id = 239 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %824 = "dtu_hlir.reshape"(%823) {node_name = "Reshape_1827-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %825 = "dtu_hlir.reshape"(%820) {node_name = "Reshape_1848-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %826 = "dtu_hlir.transpose"(%825) {node_name = "Transpose_1849-0", node_type = "Transpose", op_id = 240 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %827 = "dtu_hlir.reshape"(%826) {node_name = "Reshape_1863-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %828 = "dtu_hlir.reshape"(%821) {node_name = "Reshape_1884-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %829 = "dtu_hlir.transpose"(%828) {node_name = "Transpose_1885-0", node_type = "Transpose", op_id = 241 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %830 = "dtu_hlir.reshape"(%829) {node_name = "Reshape_1899-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %831 = "dtu_hlir.transpose"(%827) {node_name = "Transpose_1917-0", node_type = "Transpose", op_id = 242 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x1024x64xf16>) -> tensor<20x64x1024xf16>
    %832 = "dtu_hlir.dot_general"(%824, %831) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1918-0", node_type = "MatMul", op_id = 243 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x1024xf16>) -> tensor<20x1024x1024xf16>
    %833 = "dtu_hlir.fusion"(%267, %832) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x1024xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1426-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x1024xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_1919-0", node_type = "Mul"} : (tensor<20x1024x1024xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x1024xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 244 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %834 = "dtu_hlir.softmax"(%833) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_1922-0", node_type = "Softmax", op_id = 245 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %835 = "dtu_hlir.dot_general"(%834, %830) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_1924-0", node_type = "MatMul", op_id = 246 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x1024xf16>, tensor<20x1024x64xf16>) -> tensor<20x1024x64xf16>
    %836 = "dtu_hlir.reshape"(%835) {node_name = "Reshape_1945-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %837 = "dtu_hlir.transpose"(%836) {node_name = "Transpose_1946-0", node_type = "Transpose", op_id = 247 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %838 = "dtu_hlir.reshape"(%837) {node_name = "Reshape_1960-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %839 = "dtu_hlir.dot_general_bias"(%838, %304, %48) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_1962-0", op_id = 248 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %840 = "dtu_hlir.add"(%839, %817) {node_name = "Add_1963-0", node_type = "Add", op_id = 249 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %841 = "dtu_hlir.layer_norm_inference"(%840, %54, %55) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_1974-0", op_id = 250 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %842 = "dtu_hlir.dot_general"(%841, %305) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1975-0", node_type = "MatMul", op_id = 251 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %843 = "dtu_hlir.dot_general"(%4, %306) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1976-0", node_type = "MatMul", op_id = 252 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %844 = "dtu_hlir.dot_general"(%4, %307) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_1977-0", node_type = "MatMul", op_id = 253 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %845 = "dtu_hlir.reshape"(%842) {node_name = "Reshape_1998-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %846 = "dtu_hlir.transpose"(%845) {node_name = "Transpose_1999-0", node_type = "Transpose", op_id = 254 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %847 = "dtu_hlir.reshape"(%846) {node_name = "Reshape_2013-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %848 = "dtu_hlir.reshape"(%843) {node_name = "Reshape_2034-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %849 = "dtu_hlir.transpose"(%848) {node_name = "Transpose_2035-0", node_type = "Transpose", op_id = 255 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %850 = "dtu_hlir.reshape"(%849) {node_name = "Reshape_2049-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %851 = "dtu_hlir.reshape"(%844) {node_name = "Reshape_2070-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %852 = "dtu_hlir.transpose"(%851) {node_name = "Transpose_2071-0", node_type = "Transpose", op_id = 256 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %853 = "dtu_hlir.reshape"(%852) {node_name = "Reshape_2085-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %854 = "dtu_hlir.transpose"(%850) {node_name = "Transpose_2103-0", node_type = "Transpose", op_id = 257 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x77x64xf16>) -> tensor<20x64x77xf16>
    %855 = "dtu_hlir.dot_general"(%847, %854) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2104-0", node_type = "MatMul", op_id = 258 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x77xf16>) -> tensor<20x1024x77xf16>
    %856 = "dtu_hlir.fusion"(%267, %855) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1612-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2105-0", node_type = "Mul"} : (tensor<20x1024x77xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 259 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %857 = "dtu_hlir.softmax"(%856) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2108-0", node_type = "Softmax", op_id = 260 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %858 = "dtu_hlir.dot_general"(%857, %853) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2110-0", node_type = "MatMul", op_id = 261 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x77xf16>, tensor<20x77x64xf16>) -> tensor<20x1024x64xf16>
    %859 = "dtu_hlir.reshape"(%858) {node_name = "Reshape_2131-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %860 = "dtu_hlir.transpose"(%859) {node_name = "Transpose_2132-0", node_type = "Transpose", op_id = 262 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %861 = "dtu_hlir.reshape"(%860) {node_name = "Reshape_2146-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %862 = "dtu_hlir.dot_general_bias"(%861, %308, %51) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2148-0", op_id = 263 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %863 = "dtu_hlir.add"(%862, %840) {node_name = "Add_2149-0", node_type = "Add", op_id = 264 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %864 = "dtu_hlir.layer_norm_inference"(%863, %56, %57) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_2160-0", op_id = 265 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %865 = "dtu_hlir.dot_general_bias"(%864, %309, %49) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2162-0", op_id = 266 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x5120xf16>, tensor<5120xf32>) -> tensor<2x1024x5120xf16>
    %866 = "dtu_hlir.slice"(%865) {limit_indices = dense<[2, 1024, 2560]> : tensor<3xi64>, node_name = "Slice_2173-1", op_id = 267 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %867 = "dtu_hlir.slice"(%865) {limit_indices = dense<[2, 1024, 5120]> : tensor<3xi64>, node_name = "Slice_2176-3", op_id = 268 : i64, start_indices = dense<[0, 0, 2560]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %868 = "dtu_hlir.gelu"(%867) {approximate = false, node_name = "Mul_2184-0", op_id = 269 : i64, unique_name = "common20_gelu"} : (tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %869 = "dtu_hlir.mul"(%866, %868) {node_name = "Mul_2185-0", node_type = "Mul", op_id = 270 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x1024x2560xf16>, tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %870 = "dtu_hlir.dot_general_bias"(%869, %310, %50) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2187-0", op_id = 271 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x2560xf16>, tensor<2560x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %871 = "dtu_hlir.add"(%870, %863) {node_name = "Add_2188-0", node_type = "Add", op_id = 272 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %872 = "dtu_hlir.dot_general_bias"(%871, %311, %58) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2190-0", op_id = 273 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %873 = "dtu_hlir.reshape"(%872) {node_name = "Reshape_2200-0"} : (tensor<2x1024x640xf16>) -> tensor<2x32x32x640xf16>
    %874 = "dtu_hlir.add"(%873, %805) {node_name = "Add_2202-0", node_type = "Add", op_id = 274 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %875 = dtu_hlir.constant  {node_name = "down_blocks.1.downsamplers.0.conv.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %876 = "dtu_hlir.conv_bias"(%874, %875, %64) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2203-0", op_id = 275 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<2> : tensor<2xi64>} : (tensor<2x32x32x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x16x16x640xf16>
    %877 = "dtu_hlir.transpose"(%876) {node_name = "Conv_2203-0", op_id = 276 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x640xf16>) -> tensor<2x640x16x16xf16>
    %878 = "dtu_hlir.reshape"(%877) {node_name = "Reshape_2205-0"} : (tensor<2x640x16x16xf16>) -> tensor<2x32x5120xf16>
    %879 = "dtu_hlir.transpose"(%878) {node_name = "InstanceNormalization_2208-0", op_id = 277 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x5120xf16>) -> tensor<2x5120x32xf16>
    %880 = "dtu_hlir.instance_norm"(%879, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 278 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x5120x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x5120x32xf16>
    %881 = "dtu_hlir.transpose"(%880) {node_name = "InstanceNormalization_2208-0", op_id = 279 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x5120x32xf16>) -> tensor<2x32x5120xf16>
    %882 = "dtu_hlir.reshape"(%881) {node_name = "Reshape_2210-0"} : (tensor<2x32x5120xf16>) -> tensor<2x640x16x16xf16>
    %883 = dtu_hlir.constant  {node_name = "Mul_2211-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %884 = dtu_hlir.constant  {node_name = "Add_2212-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %885 = "dtu_hlir.fusion"(%883, %882, %884) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x16x16xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2211-0"} : (tensor<2x640xf16>) -> tensor<2x640x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2211-0", node_type = "Mul"} : (tensor<2x640x16x16xf16>, tensor<2x640x16x16xf16>) -> tensor<2x640x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_2212-0"} : (tensor<2x640xf16>) -> tensor<2x640x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_2212-0", node_type = "Add"} : (tensor<2x640x16x16xf16>, tensor<2x640x16x16xf16>) -> tensor<2x640x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_2214-0"} : (tensor<2x640x16x16xf16>) -> tensor<2x640x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 280 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x16x16xf16>, tensor<2x640xf16>) -> tensor<2x640x16x16xf16>
    %886 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x1280xf16>
    %887 = "dtu_hlir.transpose"(%885) {node_name = "Conv_2215-0", op_id = 281 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x16x16xf16>) -> tensor<2x16x16x640xf16>
    %888 = "dtu_hlir.conv_bias"(%887, %886, %89) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2215-0", op_id = 282 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x640xf16>, tensor<3x3x640x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %889 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %890 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %891 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 283 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %892 = "dtu_hlir.dot_general_bias"(%891, %889, %890) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 284 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %893 = "dtu_hlir.broadcast_in_dim"(%892) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 285 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x16x16x1280xf16>
    %894 = "dtu_hlir.add"(%888, %893) {node_name = "Add_2223-0", node_type = "Add", op_id = 286 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %895 = "dtu_hlir.transpose"(%894) {op_id = 287 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %896 = "dtu_hlir.reshape"(%895) {node_name = "Reshape_2225-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %897 = "dtu_hlir.transpose"(%896) {node_name = "InstanceNormalization_2228-0", op_id = 288 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %898 = "dtu_hlir.instance_norm"(%897, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 289 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %899 = "dtu_hlir.transpose"(%898) {node_name = "InstanceNormalization_2228-0", op_id = 290 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %900 = "dtu_hlir.reshape"(%899) {node_name = "Reshape_2230-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %901 = dtu_hlir.constant  {node_name = "Mul_2231-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %902 = dtu_hlir.constant  {node_name = "Add_2232-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %903 = "dtu_hlir.fusion"(%901, %900, %902) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2231-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2231-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_2232-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_2232-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_2234-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 291 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %904 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %905 = "dtu_hlir.transpose"(%903) {node_name = "Conv_2235-0", op_id = 292 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %906 = "dtu_hlir.conv_bias"(%905, %904, %90) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2235-0", op_id = 293 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %907 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.0.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x640x1280xf16>
    %908 = "dtu_hlir.conv_bias"(%876, %907, %91) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2236-0", op_id = 294 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x640xf16>, tensor<1x1x640x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %909 = "dtu_hlir.add"(%908, %906) {node_name = "Add_2237-0", node_type = "Add", op_id = 295 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %910 = "dtu_hlir.transpose"(%909) {node_name = "Conv_2236-0", op_id = 296 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %911 = "dtu_hlir.reshape"(%910) {node_name = "Reshape_2250-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %912 = "dtu_hlir.transpose"(%911) {node_name = "InstanceNormalization_2253-0", op_id = 297 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %913 = "dtu_hlir.instance_norm"(%912, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 298 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %914 = "dtu_hlir.transpose"(%913) {node_name = "InstanceNormalization_2253-0", op_id = 299 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %915 = "dtu_hlir.reshape"(%914) {node_name = "Reshape_2255-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %916 = dtu_hlir.constant  {node_name = "Mul_2256-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %917 = dtu_hlir.constant  {node_name = "Add_2257-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %918 = "dtu_hlir.fusion"(%916, %915, %917) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2256-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2256-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_2257-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_2257-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 300 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %919 = "dtu_hlir.transpose"(%918) {node_name = "Transpose_2261-0", node_type = "Transpose", op_id = 301 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %920 = "dtu_hlir.reshape"(%919) {node_name = "Reshape_2270-0"} : (tensor<2x16x16x1280xf16>) -> tensor<2x256x1280xf16>
    %921 = "dtu_hlir.dot_general_bias"(%920, %312, %65) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2272-0", op_id = 302 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %922 = "dtu_hlir.layer_norm_inference"(%921, %70, %71) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_2283-0", op_id = 303 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %923 = "dtu_hlir.dot_general"(%922, %313) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2284-0", node_type = "MatMul", op_id = 304 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %924 = "dtu_hlir.dot_general"(%922, %314) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2285-0", node_type = "MatMul", op_id = 305 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %925 = "dtu_hlir.dot_general"(%922, %315) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2286-0", node_type = "MatMul", op_id = 306 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %926 = "dtu_hlir.reshape"(%923) {node_name = "Reshape_2307-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %927 = "dtu_hlir.transpose"(%926) {node_name = "Transpose_2308-0", node_type = "Transpose", op_id = 307 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %928 = "dtu_hlir.reshape"(%927) {node_name = "Reshape_2322-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %929 = "dtu_hlir.reshape"(%924) {node_name = "Reshape_2343-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %930 = "dtu_hlir.transpose"(%929) {node_name = "Transpose_2344-0", node_type = "Transpose", op_id = 308 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %931 = "dtu_hlir.reshape"(%930) {node_name = "Reshape_2358-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %932 = "dtu_hlir.reshape"(%925) {node_name = "Reshape_2379-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %933 = "dtu_hlir.transpose"(%932) {node_name = "Transpose_2380-0", node_type = "Transpose", op_id = 309 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %934 = "dtu_hlir.reshape"(%933) {node_name = "Reshape_2394-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %935 = "dtu_hlir.transpose"(%931) {node_name = "Transpose_2412-0", node_type = "Transpose", op_id = 310 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x256x64xf16>) -> tensor<40x64x256xf16>
    %936 = "dtu_hlir.dot_general"(%928, %935) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2413-0", node_type = "MatMul", op_id = 311 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x256xf16>) -> tensor<40x256x256xf16>
    %937 = "dtu_hlir.fusion"(%267, %936) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x256xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2414-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x256xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2414-0", node_type = "Mul"} : (tensor<40x256x256xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x256xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 312 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %938 = "dtu_hlir.softmax"(%937) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2417-0", node_type = "Softmax", op_id = 313 : i64, unique_name = "common20_softmax"} : (tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %939 = "dtu_hlir.dot_general"(%938, %934) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2419-0", node_type = "MatMul", op_id = 314 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x256xf16>, tensor<40x256x64xf16>) -> tensor<40x256x64xf16>
    %940 = "dtu_hlir.reshape"(%939) {node_name = "Reshape_2440-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %941 = "dtu_hlir.transpose"(%940) {node_name = "Transpose_2441-0", node_type = "Transpose", op_id = 315 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %942 = "dtu_hlir.reshape"(%941) {node_name = "Reshape_2455-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %943 = "dtu_hlir.dot_general_bias"(%942, %316, %66) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2457-0", op_id = 316 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %944 = "dtu_hlir.add"(%943, %921) {node_name = "Add_2458-0", node_type = "Add", op_id = 317 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %945 = "dtu_hlir.layer_norm_inference"(%944, %72, %73) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_2469-0", op_id = 318 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %946 = "dtu_hlir.dot_general"(%945, %317) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2470-0", node_type = "MatMul", op_id = 319 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %947 = "dtu_hlir.dot_general"(%4, %318) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2471-0", node_type = "MatMul", op_id = 320 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %948 = "dtu_hlir.dot_general"(%4, %319) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2472-0", node_type = "MatMul", op_id = 321 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %949 = "dtu_hlir.reshape"(%946) {node_name = "Reshape_2493-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %950 = "dtu_hlir.transpose"(%949) {node_name = "Transpose_2494-0", node_type = "Transpose", op_id = 322 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %951 = "dtu_hlir.reshape"(%950) {node_name = "Reshape_2508-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %952 = "dtu_hlir.reshape"(%947) {node_name = "Reshape_2529-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %953 = "dtu_hlir.transpose"(%952) {node_name = "Transpose_2530-0", node_type = "Transpose", op_id = 323 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %954 = "dtu_hlir.reshape"(%953) {node_name = "Reshape_2544-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %955 = "dtu_hlir.reshape"(%948) {node_name = "Reshape_2565-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %956 = "dtu_hlir.transpose"(%955) {node_name = "Transpose_2566-0", node_type = "Transpose", op_id = 324 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %957 = "dtu_hlir.reshape"(%956) {node_name = "Reshape_2580-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %958 = "dtu_hlir.transpose"(%954) {node_name = "Transpose_2598-0", node_type = "Transpose", op_id = 325 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x77x64xf16>) -> tensor<40x64x77xf16>
    %959 = "dtu_hlir.dot_general"(%951, %958) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2599-0", node_type = "MatMul", op_id = 326 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x77xf16>) -> tensor<40x256x77xf16>
    %960 = "dtu_hlir.fusion"(%267, %959) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2600-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2600-0", node_type = "Mul"} : (tensor<40x256x77xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 327 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %961 = "dtu_hlir.softmax"(%960) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2603-0", node_type = "Softmax", op_id = 328 : i64, unique_name = "common20_softmax"} : (tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %962 = "dtu_hlir.dot_general"(%961, %957) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2605-0", node_type = "MatMul", op_id = 329 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x77xf16>, tensor<40x77x64xf16>) -> tensor<40x256x64xf16>
    %963 = "dtu_hlir.reshape"(%962) {node_name = "Reshape_2626-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %964 = "dtu_hlir.transpose"(%963) {node_name = "Transpose_2627-0", node_type = "Transpose", op_id = 330 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %965 = "dtu_hlir.reshape"(%964) {node_name = "Reshape_2641-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %966 = "dtu_hlir.dot_general_bias"(%965, %320, %69) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2643-0", op_id = 331 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %967 = "dtu_hlir.add"(%966, %944) {node_name = "Add_2644-0", node_type = "Add", op_id = 332 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %968 = "dtu_hlir.layer_norm_inference"(%967, %74, %75) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_2655-0", op_id = 333 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %969 = "dtu_hlir.dot_general_bias"(%968, %321, %67) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2657-0", op_id = 334 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x10240xf16>, tensor<10240xf32>) -> tensor<2x256x10240xf16>
    %970 = "dtu_hlir.slice"(%969) {limit_indices = dense<[2, 256, 5120]> : tensor<3xi64>, node_name = "Slice_2668-1", op_id = 335 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %971 = "dtu_hlir.slice"(%969) {limit_indices = dense<[2, 256, 10240]> : tensor<3xi64>, node_name = "Slice_2671-3", op_id = 336 : i64, start_indices = dense<[0, 0, 5120]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %972 = "dtu_hlir.gelu"(%971) {approximate = false, node_name = "Mul_2679-0", op_id = 337 : i64, unique_name = "common20_gelu"} : (tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %973 = "dtu_hlir.mul"(%970, %972) {node_name = "Mul_2680-0", node_type = "Mul", op_id = 338 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x256x5120xf16>, tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %974 = "dtu_hlir.dot_general_bias"(%973, %322, %68) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2682-0", op_id = 339 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x5120xf16>, tensor<5120x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %975 = "dtu_hlir.add"(%974, %967) {node_name = "Add_2683-0", node_type = "Add", op_id = 340 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %976 = "dtu_hlir.dot_general_bias"(%975, %323, %76) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2685-0", op_id = 341 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %977 = "dtu_hlir.reshape"(%976) {node_name = "Reshape_2695-0"} : (tensor<2x256x1280xf16>) -> tensor<2x16x16x1280xf16>
    %978 = "dtu_hlir.add"(%977, %909) {node_name = "Add_2697-0", node_type = "Add", op_id = 342 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %979 = "dtu_hlir.transpose"(%978) {node_name = "Transpose_2696-0", op_id = 343 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %980 = "dtu_hlir.reshape"(%979) {node_name = "Reshape_2699-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %981 = "dtu_hlir.transpose"(%980) {node_name = "InstanceNormalization_2702-0", op_id = 344 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %982 = "dtu_hlir.instance_norm"(%981, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 345 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %983 = "dtu_hlir.transpose"(%982) {node_name = "InstanceNormalization_2702-0", op_id = 346 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %984 = "dtu_hlir.reshape"(%983) {node_name = "Reshape_2704-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %985 = dtu_hlir.constant  {node_name = "Mul_2705-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %986 = dtu_hlir.constant  {node_name = "Add_2706-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %987 = "dtu_hlir.fusion"(%985, %984, %986) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2705-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2705-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_2706-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_2706-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_2708-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 347 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %988 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %989 = "dtu_hlir.transpose"(%987) {node_name = "Conv_2709-0", op_id = 348 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %990 = "dtu_hlir.conv_bias"(%989, %988, %92) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2709-0", op_id = 349 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %991 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %992 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %993 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 350 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %994 = "dtu_hlir.dot_general_bias"(%993, %991, %992) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 351 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %995 = "dtu_hlir.broadcast_in_dim"(%994) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 352 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x16x16x1280xf16>
    %996 = "dtu_hlir.add"(%990, %995) {node_name = "Add_2717-0", node_type = "Add", op_id = 353 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %997 = "dtu_hlir.transpose"(%996) {op_id = 354 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %998 = "dtu_hlir.reshape"(%997) {node_name = "Reshape_2719-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %999 = "dtu_hlir.transpose"(%998) {node_name = "InstanceNormalization_2722-0", op_id = 355 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1000 = "dtu_hlir.instance_norm"(%999, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 356 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1001 = "dtu_hlir.transpose"(%1000) {node_name = "InstanceNormalization_2722-0", op_id = 357 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1002 = "dtu_hlir.reshape"(%1001) {node_name = "Reshape_2724-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1003 = dtu_hlir.constant  {node_name = "Mul_2725-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1004 = dtu_hlir.constant  {node_name = "Add_2726-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1005 = "dtu_hlir.fusion"(%1003, %1002, %1004) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2725-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2725-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_2726-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_2726-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_2728-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 358 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1006 = dtu_hlir.constant  {node_name = "down_blocks.2.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1007 = "dtu_hlir.transpose"(%1005) {node_name = "Conv_2729-0", op_id = 359 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1008 = "dtu_hlir.conv_bias"(%1007, %1006, %93) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_2729-0", op_id = 360 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1009 = "dtu_hlir.add"(%978, %1008) {node_name = "Add_2730-0", node_type = "Add", op_id = 361 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1010 = "dtu_hlir.transpose"(%1009) {node_name = "Transpose_2696-0", op_id = 362 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1011 = "dtu_hlir.reshape"(%1010) {node_name = "Reshape_2743-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %1012 = "dtu_hlir.transpose"(%1011) {node_name = "InstanceNormalization_2746-0", op_id = 363 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1013 = "dtu_hlir.instance_norm"(%1012, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 364 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1014 = "dtu_hlir.transpose"(%1013) {node_name = "InstanceNormalization_2746-0", op_id = 365 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1015 = "dtu_hlir.reshape"(%1014) {node_name = "Reshape_2748-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1016 = dtu_hlir.constant  {node_name = "Mul_2749-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1017 = dtu_hlir.constant  {node_name = "Add_2750-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1018 = "dtu_hlir.fusion"(%1016, %1015, %1017) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_2749-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2749-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_2750-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_2750-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 366 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1019 = "dtu_hlir.transpose"(%1018) {node_name = "Transpose_2754-0", node_type = "Transpose", op_id = 367 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1020 = "dtu_hlir.reshape"(%1019) {node_name = "Reshape_2763-0"} : (tensor<2x16x16x1280xf16>) -> tensor<2x256x1280xf16>
    %1021 = "dtu_hlir.dot_general_bias"(%1020, %324, %77) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2765-0", op_id = 368 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1022 = "dtu_hlir.layer_norm_inference"(%1021, %82, %83) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_2776-0", op_id = 369 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1023 = "dtu_hlir.dot_general"(%1022, %325) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2777-0", node_type = "MatMul", op_id = 370 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1024 = "dtu_hlir.dot_general"(%1022, %326) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2778-0", node_type = "MatMul", op_id = 371 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1025 = "dtu_hlir.dot_general"(%1022, %327) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2779-0", node_type = "MatMul", op_id = 372 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1026 = "dtu_hlir.reshape"(%1023) {node_name = "Reshape_2800-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1027 = "dtu_hlir.transpose"(%1026) {node_name = "Transpose_2801-0", node_type = "Transpose", op_id = 373 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1028 = "dtu_hlir.reshape"(%1027) {node_name = "Reshape_2815-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1029 = "dtu_hlir.reshape"(%1024) {node_name = "Reshape_2836-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1030 = "dtu_hlir.transpose"(%1029) {node_name = "Transpose_2837-0", node_type = "Transpose", op_id = 374 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1031 = "dtu_hlir.reshape"(%1030) {node_name = "Reshape_2851-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1032 = "dtu_hlir.reshape"(%1025) {node_name = "Reshape_2872-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1033 = "dtu_hlir.transpose"(%1032) {node_name = "Transpose_2873-0", node_type = "Transpose", op_id = 375 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1034 = "dtu_hlir.reshape"(%1033) {node_name = "Reshape_2887-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1035 = "dtu_hlir.transpose"(%1031) {node_name = "Transpose_2905-0", node_type = "Transpose", op_id = 376 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x256x64xf16>) -> tensor<40x64x256xf16>
    %1036 = "dtu_hlir.dot_general"(%1028, %1035) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2906-0", node_type = "MatMul", op_id = 377 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x256xf16>) -> tensor<40x256x256xf16>
    %1037 = "dtu_hlir.fusion"(%267, %1036) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x256xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2414-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x256xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_2907-0", node_type = "Mul"} : (tensor<40x256x256xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x256xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 378 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1038 = "dtu_hlir.softmax"(%1037) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_2910-0", node_type = "Softmax", op_id = 379 : i64, unique_name = "common20_softmax"} : (tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1039 = "dtu_hlir.dot_general"(%1038, %1034) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_2912-0", node_type = "MatMul", op_id = 380 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x256xf16>, tensor<40x256x64xf16>) -> tensor<40x256x64xf16>
    %1040 = "dtu_hlir.reshape"(%1039) {node_name = "Reshape_2933-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1041 = "dtu_hlir.transpose"(%1040) {node_name = "Transpose_2934-0", node_type = "Transpose", op_id = 381 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1042 = "dtu_hlir.reshape"(%1041) {node_name = "Reshape_2948-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1043 = "dtu_hlir.dot_general_bias"(%1042, %328, %78) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_2950-0", op_id = 382 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1044 = "dtu_hlir.add"(%1043, %1021) {node_name = "Add_2951-0", node_type = "Add", op_id = 383 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1045 = "dtu_hlir.layer_norm_inference"(%1044, %84, %85) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_2962-0", op_id = 384 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1046 = "dtu_hlir.dot_general"(%1045, %329) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2963-0", node_type = "MatMul", op_id = 385 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1047 = "dtu_hlir.dot_general"(%4, %330) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2964-0", node_type = "MatMul", op_id = 386 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1048 = "dtu_hlir.dot_general"(%4, %331) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_2965-0", node_type = "MatMul", op_id = 387 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1049 = "dtu_hlir.reshape"(%1046) {node_name = "Reshape_2986-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1050 = "dtu_hlir.transpose"(%1049) {node_name = "Transpose_2987-0", node_type = "Transpose", op_id = 388 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1051 = "dtu_hlir.reshape"(%1050) {node_name = "Reshape_3001-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1052 = "dtu_hlir.reshape"(%1047) {node_name = "Reshape_3022-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1053 = "dtu_hlir.transpose"(%1052) {node_name = "Transpose_3023-0", node_type = "Transpose", op_id = 389 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1054 = "dtu_hlir.reshape"(%1053) {node_name = "Reshape_3037-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1055 = "dtu_hlir.reshape"(%1048) {node_name = "Reshape_3058-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1056 = "dtu_hlir.transpose"(%1055) {node_name = "Transpose_3059-0", node_type = "Transpose", op_id = 390 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1057 = "dtu_hlir.reshape"(%1056) {node_name = "Reshape_3073-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1058 = "dtu_hlir.transpose"(%1054) {node_name = "Transpose_3091-0", node_type = "Transpose", op_id = 391 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x77x64xf16>) -> tensor<40x64x77xf16>
    %1059 = "dtu_hlir.dot_general"(%1051, %1058) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_3092-0", node_type = "MatMul", op_id = 392 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x77xf16>) -> tensor<40x256x77xf16>
    %1060 = "dtu_hlir.fusion"(%267, %1059) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2600-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3093-0", node_type = "Mul"} : (tensor<40x256x77xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 393 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1061 = "dtu_hlir.softmax"(%1060) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_3096-0", node_type = "Softmax", op_id = 394 : i64, unique_name = "common20_softmax"} : (tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1062 = "dtu_hlir.dot_general"(%1061, %1057) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_3098-0", node_type = "MatMul", op_id = 395 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x77xf16>, tensor<40x77x64xf16>) -> tensor<40x256x64xf16>
    %1063 = "dtu_hlir.reshape"(%1062) {node_name = "Reshape_3119-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1064 = "dtu_hlir.transpose"(%1063) {node_name = "Transpose_3120-0", node_type = "Transpose", op_id = 396 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1065 = "dtu_hlir.reshape"(%1064) {node_name = "Reshape_3134-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1066 = "dtu_hlir.dot_general_bias"(%1065, %332, %81) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3136-0", op_id = 397 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1067 = "dtu_hlir.add"(%1066, %1044) {node_name = "Add_3137-0", node_type = "Add", op_id = 398 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1068 = "dtu_hlir.layer_norm_inference"(%1067, %86, %87) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_3148-0", op_id = 399 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1069 = "dtu_hlir.dot_general_bias"(%1068, %333, %79) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3150-0", op_id = 400 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x10240xf16>, tensor<10240xf32>) -> tensor<2x256x10240xf16>
    %1070 = "dtu_hlir.slice"(%1069) {limit_indices = dense<[2, 256, 5120]> : tensor<3xi64>, node_name = "Slice_3161-1", op_id = 401 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1071 = "dtu_hlir.slice"(%1069) {limit_indices = dense<[2, 256, 10240]> : tensor<3xi64>, node_name = "Slice_3164-3", op_id = 402 : i64, start_indices = dense<[0, 0, 5120]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1072 = "dtu_hlir.gelu"(%1071) {approximate = false, node_name = "Mul_3172-0", op_id = 403 : i64, unique_name = "common20_gelu"} : (tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1073 = "dtu_hlir.mul"(%1070, %1072) {node_name = "Mul_3173-0", node_type = "Mul", op_id = 404 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x256x5120xf16>, tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1074 = "dtu_hlir.dot_general_bias"(%1073, %334, %80) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3175-0", op_id = 405 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x5120xf16>, tensor<5120x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1075 = "dtu_hlir.add"(%1074, %1067) {node_name = "Add_3176-0", node_type = "Add", op_id = 406 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1076 = "dtu_hlir.dot_general_bias"(%1075, %335, %88) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3178-0", op_id = 407 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1077 = "dtu_hlir.reshape"(%1076) {node_name = "Reshape_3188-0"} : (tensor<2x256x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1078 = "dtu_hlir.add"(%1077, %1009) {node_name = "Add_3190-0", node_type = "Add", op_id = 408 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1079 = dtu_hlir.constant  {node_name = "down_blocks.2.downsamplers.0.conv.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1080 = "dtu_hlir.conv_bias"(%1078, %1079, %94) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3191-0", op_id = 409 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<2> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1081 = "dtu_hlir.transpose"(%1080) {node_name = "Conv_3191-0", op_id = 410 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1082 = "dtu_hlir.reshape"(%1081) {node_name = "Reshape_3193-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1083 = "dtu_hlir.transpose"(%1082) {node_name = "InstanceNormalization_3196-0", op_id = 411 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1084 = "dtu_hlir.instance_norm"(%1083, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 412 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1085 = "dtu_hlir.transpose"(%1084) {node_name = "InstanceNormalization_3196-0", op_id = 413 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1086 = "dtu_hlir.reshape"(%1085) {node_name = "Reshape_3198-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1087 = dtu_hlir.constant  {node_name = "Mul_3199-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1088 = dtu_hlir.constant  {node_name = "Add_3200-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1089 = "dtu_hlir.fusion"(%1087, %1086, %1088) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3199-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3199-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3200-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3200-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3202-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 414 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1090 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1091 = "dtu_hlir.transpose"(%1089) {node_name = "Conv_3203-0", op_id = 415 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1092 = "dtu_hlir.conv_bias"(%1091, %1090, %95) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3203-0", op_id = 416 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1093 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1094 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1095 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 417 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1096 = "dtu_hlir.dot_general_bias"(%1095, %1093, %1094) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 418 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1097 = "dtu_hlir.broadcast_in_dim"(%1096) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 419 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1098 = "dtu_hlir.add"(%1092, %1097) {node_name = "Add_3211-0", node_type = "Add", op_id = 420 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1099 = "dtu_hlir.transpose"(%1098) {op_id = 421 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1100 = "dtu_hlir.reshape"(%1099) {node_name = "Reshape_3213-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1101 = "dtu_hlir.transpose"(%1100) {node_name = "InstanceNormalization_3216-0", op_id = 422 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1102 = "dtu_hlir.instance_norm"(%1101, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 423 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1103 = "dtu_hlir.transpose"(%1102) {node_name = "InstanceNormalization_3216-0", op_id = 424 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1104 = "dtu_hlir.reshape"(%1103) {node_name = "Reshape_3218-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1105 = dtu_hlir.constant  {node_name = "Mul_3219-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1106 = dtu_hlir.constant  {node_name = "Add_3220-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1107 = "dtu_hlir.fusion"(%1105, %1104, %1106) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3219-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3219-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3220-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3220-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3222-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 425 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1108 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1109 = "dtu_hlir.transpose"(%1107) {node_name = "Conv_3223-0", op_id = 426 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1110 = "dtu_hlir.conv_bias"(%1109, %1108, %96) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3223-0", op_id = 427 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1111 = "dtu_hlir.add"(%1080, %1110) {node_name = "Add_3224-0", node_type = "Add", op_id = 428 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1112 = "dtu_hlir.transpose"(%1111) {node_name = "Conv_3191-0", op_id = 429 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1113 = "dtu_hlir.reshape"(%1112) {node_name = "Reshape_3228-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1114 = "dtu_hlir.transpose"(%1113) {node_name = "InstanceNormalization_3231-0", op_id = 430 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1115 = "dtu_hlir.instance_norm"(%1114, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 431 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1116 = "dtu_hlir.transpose"(%1115) {node_name = "InstanceNormalization_3231-0", op_id = 432 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1117 = "dtu_hlir.reshape"(%1116) {node_name = "Reshape_3233-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1118 = dtu_hlir.constant  {node_name = "Mul_3234-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1119 = dtu_hlir.constant  {node_name = "Add_3235-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1120 = "dtu_hlir.fusion"(%1118, %1117, %1119) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3234-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3234-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3235-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3235-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3237-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 433 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1121 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1122 = "dtu_hlir.transpose"(%1120) {node_name = "Conv_3238-0", op_id = 434 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1123 = "dtu_hlir.conv_bias"(%1122, %1121, %97) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3238-0", op_id = 435 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1124 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1125 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1126 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 436 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1127 = "dtu_hlir.dot_general_bias"(%1126, %1124, %1125) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 437 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1128 = "dtu_hlir.broadcast_in_dim"(%1127) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 438 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1129 = "dtu_hlir.add"(%1123, %1128) {node_name = "Add_3246-0", node_type = "Add", op_id = 439 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1130 = "dtu_hlir.transpose"(%1129) {op_id = 440 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1131 = "dtu_hlir.reshape"(%1130) {node_name = "Reshape_3248-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1132 = "dtu_hlir.transpose"(%1131) {node_name = "InstanceNormalization_3251-0", op_id = 441 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1133 = "dtu_hlir.instance_norm"(%1132, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 442 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1134 = "dtu_hlir.transpose"(%1133) {node_name = "InstanceNormalization_3251-0", op_id = 443 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1135 = "dtu_hlir.reshape"(%1134) {node_name = "Reshape_3253-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1136 = dtu_hlir.constant  {node_name = "Mul_3254-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1137 = dtu_hlir.constant  {node_name = "Add_3255-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1138 = "dtu_hlir.fusion"(%1136, %1135, %1137) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3254-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3254-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3255-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3255-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3257-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 444 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1139 = dtu_hlir.constant  {node_name = "down_blocks.3.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1140 = "dtu_hlir.transpose"(%1138) {node_name = "Conv_3258-0", op_id = 445 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1141 = "dtu_hlir.conv_bias"(%1140, %1139, %98) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3258-0", op_id = 446 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1142 = "dtu_hlir.add"(%1111, %1141) {node_name = "Add_3259-0", node_type = "Add", op_id = 447 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1143 = "dtu_hlir.transpose"(%1142) {node_name = "Conv_3191-0", op_id = 448 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1144 = "dtu_hlir.reshape"(%1143) {node_name = "Reshape_3263-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1145 = "dtu_hlir.transpose"(%1144) {node_name = "InstanceNormalization_3266-0", op_id = 449 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1146 = "dtu_hlir.instance_norm"(%1145, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 450 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1147 = "dtu_hlir.transpose"(%1146) {node_name = "InstanceNormalization_3266-0", op_id = 451 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1148 = "dtu_hlir.reshape"(%1147) {node_name = "Reshape_3268-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1149 = dtu_hlir.constant  {node_name = "Mul_3269-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1150 = dtu_hlir.constant  {node_name = "Add_3270-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1151 = "dtu_hlir.fusion"(%1149, %1148, %1150) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3269-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3269-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3270-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3270-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3272-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 452 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1152 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1153 = "dtu_hlir.transpose"(%1151) {node_name = "Conv_3273-0", op_id = 453 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1154 = "dtu_hlir.conv_bias"(%1153, %1152, %258) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3273-0", op_id = 454 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1155 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1156 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1157 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 455 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1158 = "dtu_hlir.dot_general_bias"(%1157, %1155, %1156) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 456 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1159 = "dtu_hlir.broadcast_in_dim"(%1158) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 457 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1160 = "dtu_hlir.add"(%1154, %1159) {node_name = "Add_3281-0", node_type = "Add", op_id = 458 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1161 = "dtu_hlir.transpose"(%1160) {op_id = 459 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1162 = "dtu_hlir.reshape"(%1161) {node_name = "Reshape_3283-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1163 = "dtu_hlir.transpose"(%1162) {node_name = "InstanceNormalization_3286-0", op_id = 460 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1164 = "dtu_hlir.instance_norm"(%1163, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 461 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1165 = "dtu_hlir.transpose"(%1164) {node_name = "InstanceNormalization_3286-0", op_id = 462 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1166 = "dtu_hlir.reshape"(%1165) {node_name = "Reshape_3288-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1167 = dtu_hlir.constant  {node_name = "Mul_3289-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1168 = dtu_hlir.constant  {node_name = "Add_3290-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1169 = "dtu_hlir.fusion"(%1167, %1166, %1168) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3289-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3289-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3290-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3290-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3292-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 463 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1170 = dtu_hlir.constant  {node_name = "mid_block.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1171 = "dtu_hlir.transpose"(%1169) {node_name = "Conv_3293-0", op_id = 464 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1172 = "dtu_hlir.conv_bias"(%1171, %1170, %259) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3293-0", op_id = 465 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1173 = "dtu_hlir.add"(%1142, %1172) {node_name = "Add_3294-0", node_type = "Add", op_id = 466 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1174 = "dtu_hlir.transpose"(%1173) {node_name = "Conv_3191-0", op_id = 467 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1175 = "dtu_hlir.reshape"(%1174) {node_name = "Reshape_3307-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1176 = "dtu_hlir.transpose"(%1175) {node_name = "InstanceNormalization_3310-0", op_id = 468 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1177 = "dtu_hlir.instance_norm"(%1176, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 469 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1178 = "dtu_hlir.transpose"(%1177) {node_name = "InstanceNormalization_3310-0", op_id = 470 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1179 = "dtu_hlir.reshape"(%1178) {node_name = "Reshape_3312-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1180 = dtu_hlir.constant  {node_name = "Mul_3313-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1181 = dtu_hlir.constant  {node_name = "Add_3314-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1182 = "dtu_hlir.fusion"(%1180, %1179, %1181) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3313-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3313-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3314-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3314-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 471 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1183 = "dtu_hlir.transpose"(%1182) {node_name = "Transpose_3318-0", node_type = "Transpose", op_id = 472 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1184 = "dtu_hlir.reshape"(%1183) {node_name = "Reshape_3327-0"} : (tensor<2x8x8x1280xf16>) -> tensor<2x64x1280xf16>
    %1185 = "dtu_hlir.dot_general_bias"(%1184, %336, %246) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3329-0", op_id = 473 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1186 = "dtu_hlir.layer_norm_inference"(%1185, %251, %252) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_3340-0", op_id = 474 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x64x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1187 = "dtu_hlir.dot_general"(%1186, %337) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3341-0", node_type = "MatMul", op_id = 475 : i64, unique_name = "dot_general_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x64x1280xf16>
    %1188 = "dtu_hlir.dot_general"(%1186, %338) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3342-0", node_type = "MatMul", op_id = 476 : i64, unique_name = "dot_general_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x64x1280xf16>
    %1189 = "dtu_hlir.dot_general"(%1186, %339) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3343-0", node_type = "MatMul", op_id = 477 : i64, unique_name = "dot_general_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x64x1280xf16>
    %1190 = "dtu_hlir.reshape"(%1187) {node_name = "Reshape_3364-0"} : (tensor<2x64x1280xf16>) -> tensor<2x64x20x64xf16>
    %1191 = "dtu_hlir.transpose"(%1190) {node_name = "Transpose_3365-0", node_type = "Transpose", op_id = 478 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x20x64xf16>) -> tensor<2x20x64x64xf16>
    %1192 = "dtu_hlir.reshape"(%1191) {node_name = "Reshape_3379-0"} : (tensor<2x20x64x64xf16>) -> tensor<40x64x64xf16>
    %1193 = "dtu_hlir.reshape"(%1188) {node_name = "Reshape_3400-0"} : (tensor<2x64x1280xf16>) -> tensor<2x64x20x64xf16>
    %1194 = "dtu_hlir.transpose"(%1193) {node_name = "Transpose_3401-0", node_type = "Transpose", op_id = 479 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x20x64xf16>) -> tensor<2x20x64x64xf16>
    %1195 = "dtu_hlir.reshape"(%1194) {node_name = "Reshape_3415-0"} : (tensor<2x20x64x64xf16>) -> tensor<40x64x64xf16>
    %1196 = "dtu_hlir.reshape"(%1189) {node_name = "Reshape_3436-0"} : (tensor<2x64x1280xf16>) -> tensor<2x64x20x64xf16>
    %1197 = "dtu_hlir.transpose"(%1196) {node_name = "Transpose_3437-0", node_type = "Transpose", op_id = 480 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x20x64xf16>) -> tensor<2x20x64x64xf16>
    %1198 = "dtu_hlir.reshape"(%1197) {node_name = "Reshape_3451-0"} : (tensor<2x20x64x64xf16>) -> tensor<40x64x64xf16>
    %1199 = "dtu_hlir.transpose"(%1195) {node_name = "Transpose_3469-0", node_type = "Transpose", op_id = 481 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x64x64xf16>) -> tensor<40x64x64xf16>
    %1200 = "dtu_hlir.dot_general"(%1192, %1199) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_3470-0", node_type = "MatMul", op_id = 482 : i64, unique_name = "dot_general_dtu"} : (tensor<40x64x64xf16>, tensor<40x64x64xf16>) -> tensor<40x64x64xf16>
    %1201 = "dtu_hlir.fusion"(%267, %1200) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x64x64xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_3471-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3471-0", node_type = "Mul"} : (tensor<40x64x64xf16>, tensor<40x64x64xf16>) -> tensor<40x64x64xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 483 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x64x64xf16>) -> tensor<40x64x64xf16>
    %1202 = "dtu_hlir.softmax"(%1201) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_3474-0", node_type = "Softmax", op_id = 484 : i64, unique_name = "common20_softmax"} : (tensor<40x64x64xf16>) -> tensor<40x64x64xf16>
    %1203 = "dtu_hlir.dot_general"(%1202, %1198) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_3476-0", node_type = "MatMul", op_id = 485 : i64, unique_name = "dot_general_dtu"} : (tensor<40x64x64xf16>, tensor<40x64x64xf16>) -> tensor<40x64x64xf16>
    %1204 = "dtu_hlir.reshape"(%1203) {node_name = "Reshape_3497-0"} : (tensor<40x64x64xf16>) -> tensor<2x20x64x64xf16>
    %1205 = "dtu_hlir.transpose"(%1204) {node_name = "Transpose_3498-0", node_type = "Transpose", op_id = 486 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x64x64xf16>) -> tensor<2x64x20x64xf16>
    %1206 = "dtu_hlir.reshape"(%1205) {node_name = "Reshape_3512-0"} : (tensor<2x64x20x64xf16>) -> tensor<2x64x1280xf16>
    %1207 = "dtu_hlir.dot_general_bias"(%1206, %340, %247) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3514-0", op_id = 487 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1208 = "dtu_hlir.add"(%1207, %1185) {node_name = "Add_3515-0", node_type = "Add", op_id = 488 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x1280xf16>, tensor<2x64x1280xf16>) -> tensor<2x64x1280xf16>
    %1209 = "dtu_hlir.layer_norm_inference"(%1208, %253, %254) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_3526-0", op_id = 489 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x64x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1210 = "dtu_hlir.dot_general"(%1209, %341) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3527-0", node_type = "MatMul", op_id = 490 : i64, unique_name = "dot_general_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x64x1280xf16>
    %1211 = "dtu_hlir.dot_general"(%4, %342) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3528-0", node_type = "MatMul", op_id = 491 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1212 = "dtu_hlir.dot_general"(%4, %343) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3529-0", node_type = "MatMul", op_id = 492 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1213 = "dtu_hlir.reshape"(%1210) {node_name = "Reshape_3550-0"} : (tensor<2x64x1280xf16>) -> tensor<2x64x20x64xf16>
    %1214 = "dtu_hlir.transpose"(%1213) {node_name = "Transpose_3551-0", node_type = "Transpose", op_id = 493 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x20x64xf16>) -> tensor<2x20x64x64xf16>
    %1215 = "dtu_hlir.reshape"(%1214) {node_name = "Reshape_3565-0"} : (tensor<2x20x64x64xf16>) -> tensor<40x64x64xf16>
    %1216 = "dtu_hlir.reshape"(%1211) {node_name = "Reshape_3586-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1217 = "dtu_hlir.transpose"(%1216) {node_name = "Transpose_3587-0", node_type = "Transpose", op_id = 494 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1218 = "dtu_hlir.reshape"(%1217) {node_name = "Reshape_3601-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1219 = "dtu_hlir.reshape"(%1212) {node_name = "Reshape_3622-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1220 = "dtu_hlir.transpose"(%1219) {node_name = "Transpose_3623-0", node_type = "Transpose", op_id = 495 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1221 = "dtu_hlir.reshape"(%1220) {node_name = "Reshape_3637-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1222 = "dtu_hlir.transpose"(%1218) {node_name = "Transpose_3655-0", node_type = "Transpose", op_id = 496 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x77x64xf16>) -> tensor<40x64x77xf16>
    %1223 = "dtu_hlir.dot_general"(%1215, %1222) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_3656-0", node_type = "MatMul", op_id = 497 : i64, unique_name = "dot_general_dtu"} : (tensor<40x64x64xf16>, tensor<40x64x77xf16>) -> tensor<40x64x77xf16>
    %1224 = "dtu_hlir.fusion"(%267, %1223) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x64x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_3657-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x64x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3657-0", node_type = "Mul"} : (tensor<40x64x77xf16>, tensor<40x64x77xf16>) -> tensor<40x64x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x64x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 498 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x64x77xf16>) -> tensor<40x64x77xf16>
    %1225 = "dtu_hlir.softmax"(%1224) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_3660-0", node_type = "Softmax", op_id = 499 : i64, unique_name = "common20_softmax"} : (tensor<40x64x77xf16>) -> tensor<40x64x77xf16>
    %1226 = "dtu_hlir.dot_general"(%1225, %1221) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_3662-0", node_type = "MatMul", op_id = 500 : i64, unique_name = "dot_general_dtu"} : (tensor<40x64x77xf16>, tensor<40x77x64xf16>) -> tensor<40x64x64xf16>
    %1227 = "dtu_hlir.reshape"(%1226) {node_name = "Reshape_3683-0"} : (tensor<40x64x64xf16>) -> tensor<2x20x64x64xf16>
    %1228 = "dtu_hlir.transpose"(%1227) {node_name = "Transpose_3684-0", node_type = "Transpose", op_id = 501 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x64x64xf16>) -> tensor<2x64x20x64xf16>
    %1229 = "dtu_hlir.reshape"(%1228) {node_name = "Reshape_3698-0"} : (tensor<2x64x20x64xf16>) -> tensor<2x64x1280xf16>
    %1230 = "dtu_hlir.dot_general_bias"(%1229, %344, %250) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3700-0", op_id = 502 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1231 = "dtu_hlir.add"(%1230, %1208) {node_name = "Add_3701-0", node_type = "Add", op_id = 503 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x1280xf16>, tensor<2x64x1280xf16>) -> tensor<2x64x1280xf16>
    %1232 = "dtu_hlir.layer_norm_inference"(%1231, %255, %256) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_3712-0", op_id = 504 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x64x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1233 = "dtu_hlir.dot_general_bias"(%1232, %345, %248) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3714-0", op_id = 505 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x10240xf16>, tensor<10240xf32>) -> tensor<2x64x10240xf16>
    %1234 = "dtu_hlir.slice"(%1233) {limit_indices = dense<[2, 64, 5120]> : tensor<3xi64>, node_name = "Slice_3725-1", op_id = 506 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x64x10240xf16>) -> tensor<2x64x5120xf16>
    %1235 = "dtu_hlir.slice"(%1233) {limit_indices = dense<[2, 64, 10240]> : tensor<3xi64>, node_name = "Slice_3728-3", op_id = 507 : i64, start_indices = dense<[0, 0, 5120]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x64x10240xf16>) -> tensor<2x64x5120xf16>
    %1236 = "dtu_hlir.gelu"(%1235) {approximate = false, node_name = "Mul_3736-0", op_id = 508 : i64, unique_name = "common20_gelu"} : (tensor<2x64x5120xf16>) -> tensor<2x64x5120xf16>
    %1237 = "dtu_hlir.mul"(%1234, %1236) {node_name = "Mul_3737-0", node_type = "Mul", op_id = 509 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x64x5120xf16>, tensor<2x64x5120xf16>) -> tensor<2x64x5120xf16>
    %1238 = "dtu_hlir.dot_general_bias"(%1237, %346, %249) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3739-0", op_id = 510 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x64x5120xf16>, tensor<5120x1280xf16>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1239 = "dtu_hlir.add"(%1238, %1231) {node_name = "Add_3740-0", node_type = "Add", op_id = 511 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x1280xf16>, tensor<2x64x1280xf16>) -> tensor<2x64x1280xf16>
    %1240 = "dtu_hlir.dot_general_bias"(%1239, %347, %257) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3742-0", op_id = 512 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x64x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x64x1280xf16>
    %1241 = "dtu_hlir.reshape"(%1240) {node_name = "Reshape_3752-0"} : (tensor<2x64x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1242 = "dtu_hlir.add"(%1241, %1173) {node_name = "Add_3754-0", node_type = "Add", op_id = 513 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1243 = "dtu_hlir.transpose"(%1242) {node_name = "Transpose_3753-0", op_id = 514 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1244 = "dtu_hlir.reshape"(%1243) {node_name = "Reshape_3756-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1245 = "dtu_hlir.transpose"(%1244) {node_name = "InstanceNormalization_3759-0", op_id = 515 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1246 = "dtu_hlir.instance_norm"(%1245, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 516 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1247 = "dtu_hlir.transpose"(%1246) {node_name = "InstanceNormalization_3759-0", op_id = 517 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1248 = "dtu_hlir.reshape"(%1247) {node_name = "Reshape_3761-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1249 = dtu_hlir.constant  {node_name = "Mul_3762-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1250 = dtu_hlir.constant  {node_name = "Add_3763-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1251 = "dtu_hlir.fusion"(%1249, %1248, %1250) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3762-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3762-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3763-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3763-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3765-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 518 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1252 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1253 = "dtu_hlir.transpose"(%1251) {node_name = "Conv_3766-0", op_id = 519 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1254 = "dtu_hlir.conv_bias"(%1253, %1252, %260) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3766-0", op_id = 520 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1255 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1256 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1257 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 521 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1258 = "dtu_hlir.dot_general_bias"(%1257, %1255, %1256) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 522 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1259 = "dtu_hlir.broadcast_in_dim"(%1258) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 523 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1260 = "dtu_hlir.add"(%1254, %1259) {node_name = "Add_3774-0", node_type = "Add", op_id = 524 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1261 = "dtu_hlir.transpose"(%1260) {op_id = 525 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1262 = "dtu_hlir.reshape"(%1261) {node_name = "Reshape_3776-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1263 = "dtu_hlir.transpose"(%1262) {node_name = "InstanceNormalization_3779-0", op_id = 526 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1264 = "dtu_hlir.instance_norm"(%1263, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 527 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1265 = "dtu_hlir.transpose"(%1264) {node_name = "InstanceNormalization_3779-0", op_id = 528 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1266 = "dtu_hlir.reshape"(%1265) {node_name = "Reshape_3781-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1267 = dtu_hlir.constant  {node_name = "Mul_3782-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1268 = dtu_hlir.constant  {node_name = "Add_3783-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1269 = "dtu_hlir.fusion"(%1267, %1266, %1268) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3782-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3782-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3783-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3783-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3785-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 529 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1270 = dtu_hlir.constant  {node_name = "mid_block.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1271 = "dtu_hlir.transpose"(%1269) {node_name = "Conv_3786-0", op_id = 530 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1272 = "dtu_hlir.conv_bias"(%1271, %1270, %261) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3786-0", op_id = 531 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1273 = "dtu_hlir.add"(%1242, %1272) {node_name = "Add_3787-0", node_type = "Add", op_id = 532 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1274 = "dtu_hlir.concatenate"(%1273, %1142) {dimension = 3 : i64, node_name = "Concat_3790-0", op_id = 533 : i64, unique_name = "concatenate_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x2560xf16>
    %1275 = "dtu_hlir.transpose"(%1274) {node_name = "Transpose_3753-0", op_id = 534 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x2560xf16>) -> tensor<2x2560x8x8xf16>
    %1276 = "dtu_hlir.reshape"(%1275) {node_name = "Reshape_3792-0"} : (tensor<2x2560x8x8xf16>) -> tensor<2x32x5120xf16>
    %1277 = "dtu_hlir.transpose"(%1276) {node_name = "InstanceNormalization_3795-0", op_id = 535 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x5120xf16>) -> tensor<2x5120x32xf16>
    %1278 = "dtu_hlir.instance_norm"(%1277, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 536 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x5120x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x5120x32xf16>
    %1279 = "dtu_hlir.transpose"(%1278) {node_name = "InstanceNormalization_3795-0", op_id = 537 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x5120x32xf16>) -> tensor<2x32x5120xf16>
    %1280 = "dtu_hlir.reshape"(%1279) {node_name = "Reshape_3797-0"} : (tensor<2x32x5120xf16>) -> tensor<2x2560x8x8xf16>
    %1281 = dtu_hlir.constant  {node_name = "Mul_3798-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1282 = dtu_hlir.constant  {node_name = "Add_3799-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1283 = "dtu_hlir.fusion"(%1281, %1280, %1282) ( {
    ^bb0(%arg3: tensor<2x2560xf16>, %arg4: tensor<2x2560x8x8xf16>, %arg5: tensor<2x2560xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3798-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3798-0", node_type = "Mul"} : (tensor<2x2560x8x8xf16>, tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3799-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3799-0", node_type = "Add"} : (tensor<2x2560x8x8xf16>, tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3801-0"} : (tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x2560x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 538 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x2560xf16>, tensor<2x2560x8x8xf16>, tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
    %1284 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x2560x1280xf16>
    %1285 = "dtu_hlir.transpose"(%1283) {node_name = "Conv_3802-0", op_id = 539 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x8x8xf16>) -> tensor<2x8x8x2560xf16>
    %1286 = "dtu_hlir.conv_bias"(%1285, %1284, %99) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3802-0", op_id = 540 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x2560xf16>, tensor<3x3x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1287 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1288 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1289 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 541 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1290 = "dtu_hlir.dot_general_bias"(%1289, %1287, %1288) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 542 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1291 = "dtu_hlir.broadcast_in_dim"(%1290) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 543 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1292 = "dtu_hlir.add"(%1286, %1291) {node_name = "Add_3810-0", node_type = "Add", op_id = 544 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1293 = "dtu_hlir.transpose"(%1292) {op_id = 545 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1294 = "dtu_hlir.reshape"(%1293) {node_name = "Reshape_3812-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1295 = "dtu_hlir.transpose"(%1294) {node_name = "InstanceNormalization_3815-0", op_id = 546 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1296 = "dtu_hlir.instance_norm"(%1295, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 547 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1297 = "dtu_hlir.transpose"(%1296) {node_name = "InstanceNormalization_3815-0", op_id = 548 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1298 = "dtu_hlir.reshape"(%1297) {node_name = "Reshape_3817-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1299 = dtu_hlir.constant  {node_name = "Mul_3818-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1300 = dtu_hlir.constant  {node_name = "Add_3819-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1301 = "dtu_hlir.fusion"(%1299, %1298, %1300) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3818-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3818-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3819-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3819-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3821-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 549 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1302 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1303 = "dtu_hlir.transpose"(%1301) {node_name = "Conv_3822-0", op_id = 550 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1304 = "dtu_hlir.conv_bias"(%1303, %1302, %100) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3822-0", op_id = 551 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1305 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.0.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x2560x1280xf16>
    %1306 = "dtu_hlir.conv_bias"(%1274, %1305, %101) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3823-0", op_id = 552 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x2560xf16>, tensor<1x1x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1307 = "dtu_hlir.add"(%1306, %1304) {node_name = "Add_3824-0", node_type = "Add", op_id = 553 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1308 = "dtu_hlir.concatenate"(%1307, %1111) {dimension = 3 : i64, node_name = "Concat_3827-0", op_id = 554 : i64, unique_name = "concatenate_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x2560xf16>
    %1309 = "dtu_hlir.transpose"(%1308) {node_name = "Conv_3823-0", op_id = 555 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x2560xf16>) -> tensor<2x2560x8x8xf16>
    %1310 = "dtu_hlir.reshape"(%1309) {node_name = "Reshape_3829-0"} : (tensor<2x2560x8x8xf16>) -> tensor<2x32x5120xf16>
    %1311 = "dtu_hlir.transpose"(%1310) {node_name = "InstanceNormalization_3832-0", op_id = 556 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x5120xf16>) -> tensor<2x5120x32xf16>
    %1312 = "dtu_hlir.instance_norm"(%1311, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 557 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x5120x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x5120x32xf16>
    %1313 = "dtu_hlir.transpose"(%1312) {node_name = "InstanceNormalization_3832-0", op_id = 558 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x5120x32xf16>) -> tensor<2x32x5120xf16>
    %1314 = "dtu_hlir.reshape"(%1313) {node_name = "Reshape_3834-0"} : (tensor<2x32x5120xf16>) -> tensor<2x2560x8x8xf16>
    %1315 = dtu_hlir.constant  {node_name = "Mul_3835-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1316 = dtu_hlir.constant  {node_name = "Add_3836-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1317 = "dtu_hlir.fusion"(%1315, %1314, %1316) ( {
    ^bb0(%arg3: tensor<2x2560xf16>, %arg4: tensor<2x2560x8x8xf16>, %arg5: tensor<2x2560xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3835-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3835-0", node_type = "Mul"} : (tensor<2x2560x8x8xf16>, tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3836-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3836-0", node_type = "Add"} : (tensor<2x2560x8x8xf16>, tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3838-0"} : (tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x2560x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 559 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x2560xf16>, tensor<2x2560x8x8xf16>, tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
    %1318 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x2560x1280xf16>
    %1319 = "dtu_hlir.transpose"(%1317) {node_name = "Conv_3839-0", op_id = 560 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x8x8xf16>) -> tensor<2x8x8x2560xf16>
    %1320 = "dtu_hlir.conv_bias"(%1319, %1318, %102) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3839-0", op_id = 561 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x2560xf16>, tensor<3x3x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1321 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1322 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1323 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 562 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1324 = "dtu_hlir.dot_general_bias"(%1323, %1321, %1322) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 563 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1325 = "dtu_hlir.broadcast_in_dim"(%1324) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 564 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1326 = "dtu_hlir.add"(%1320, %1325) {node_name = "Add_3847-0", node_type = "Add", op_id = 565 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1327 = "dtu_hlir.transpose"(%1326) {op_id = 566 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1328 = "dtu_hlir.reshape"(%1327) {node_name = "Reshape_3849-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1329 = "dtu_hlir.transpose"(%1328) {node_name = "InstanceNormalization_3852-0", op_id = 567 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1330 = "dtu_hlir.instance_norm"(%1329, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 568 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1331 = "dtu_hlir.transpose"(%1330) {node_name = "InstanceNormalization_3852-0", op_id = 569 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1332 = "dtu_hlir.reshape"(%1331) {node_name = "Reshape_3854-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1333 = dtu_hlir.constant  {node_name = "Mul_3855-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1334 = dtu_hlir.constant  {node_name = "Add_3856-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1335 = "dtu_hlir.fusion"(%1333, %1332, %1334) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3855-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3855-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3856-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3856-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3858-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 570 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1336 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1337 = "dtu_hlir.transpose"(%1335) {node_name = "Conv_3859-0", op_id = 571 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1338 = "dtu_hlir.conv_bias"(%1337, %1336, %103) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3859-0", op_id = 572 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1339 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.1.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x2560x1280xf16>
    %1340 = "dtu_hlir.conv_bias"(%1308, %1339, %104) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3860-0", op_id = 573 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x2560xf16>, tensor<1x1x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1341 = "dtu_hlir.add"(%1340, %1338) {node_name = "Add_3861-0", node_type = "Add", op_id = 574 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1342 = "dtu_hlir.concatenate"(%1341, %1080) {dimension = 3 : i64, node_name = "Concat_3864-0", op_id = 575 : i64, unique_name = "concatenate_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x2560xf16>
    %1343 = "dtu_hlir.transpose"(%1342) {node_name = "Conv_3860-0", op_id = 576 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x2560xf16>) -> tensor<2x2560x8x8xf16>
    %1344 = "dtu_hlir.reshape"(%1343) {node_name = "Reshape_3866-0"} : (tensor<2x2560x8x8xf16>) -> tensor<2x32x5120xf16>
    %1345 = "dtu_hlir.transpose"(%1344) {node_name = "InstanceNormalization_3869-0", op_id = 577 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x5120xf16>) -> tensor<2x5120x32xf16>
    %1346 = "dtu_hlir.instance_norm"(%1345, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 578 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x5120x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x5120x32xf16>
    %1347 = "dtu_hlir.transpose"(%1346) {node_name = "InstanceNormalization_3869-0", op_id = 579 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x5120x32xf16>) -> tensor<2x32x5120xf16>
    %1348 = "dtu_hlir.reshape"(%1347) {node_name = "Reshape_3871-0"} : (tensor<2x32x5120xf16>) -> tensor<2x2560x8x8xf16>
    %1349 = dtu_hlir.constant  {node_name = "Mul_3872-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1350 = dtu_hlir.constant  {node_name = "Add_3873-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1351 = "dtu_hlir.fusion"(%1349, %1348, %1350) ( {
    ^bb0(%arg3: tensor<2x2560xf16>, %arg4: tensor<2x2560x8x8xf16>, %arg5: tensor<2x2560xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3872-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3872-0", node_type = "Mul"} : (tensor<2x2560x8x8xf16>, tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3873-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3873-0", node_type = "Add"} : (tensor<2x2560x8x8xf16>, tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3875-0"} : (tensor<2x2560x8x8xf16>) -> tensor<2x2560x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x2560x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 580 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x2560xf16>, tensor<2x2560x8x8xf16>, tensor<2x2560xf16>) -> tensor<2x2560x8x8xf16>
    %1352 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x2560x1280xf16>
    %1353 = "dtu_hlir.transpose"(%1351) {node_name = "Conv_3876-0", op_id = 581 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x8x8xf16>) -> tensor<2x8x8x2560xf16>
    %1354 = "dtu_hlir.conv_bias"(%1353, %1352, %105) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3876-0", op_id = 582 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x2560xf16>, tensor<3x3x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1355 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1356 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1357 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 583 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1358 = "dtu_hlir.dot_general_bias"(%1357, %1355, %1356) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 584 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1359 = "dtu_hlir.broadcast_in_dim"(%1358) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 585 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1360 = "dtu_hlir.add"(%1354, %1359) {node_name = "Add_3884-0", node_type = "Add", op_id = 586 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1361 = "dtu_hlir.transpose"(%1360) {op_id = 587 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x8x8x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1362 = "dtu_hlir.reshape"(%1361) {node_name = "Reshape_3886-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x32x2560xf16>
    %1363 = "dtu_hlir.transpose"(%1362) {node_name = "InstanceNormalization_3889-0", op_id = 588 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x2560xf16>) -> tensor<2x2560x32xf16>
    %1364 = "dtu_hlir.instance_norm"(%1363, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 589 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x2560x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x2560x32xf16>
    %1365 = "dtu_hlir.transpose"(%1364) {node_name = "InstanceNormalization_3889-0", op_id = 590 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x32xf16>) -> tensor<2x32x2560xf16>
    %1366 = "dtu_hlir.reshape"(%1365) {node_name = "Reshape_3891-0"} : (tensor<2x32x2560xf16>) -> tensor<2x1280x8x8xf16>
    %1367 = dtu_hlir.constant  {node_name = "Mul_3892-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1368 = dtu_hlir.constant  {node_name = "Add_3893-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1369 = "dtu_hlir.fusion"(%1367, %1366, %1368) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x8x8xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3892-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3892-0", node_type = "Mul"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3893-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3893-0", node_type = "Add"} : (tensor<2x1280x8x8xf16>, tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3895-0"} : (tensor<2x1280x8x8xf16>) -> tensor<2x1280x8x8xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x8x8xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 591 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x8x8xf16>, tensor<2x1280xf16>) -> tensor<2x1280x8x8xf16>
    %1370 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1371 = "dtu_hlir.transpose"(%1369) {node_name = "Conv_3896-0", op_id = 592 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x8x8xf16>) -> tensor<2x8x8x1280xf16>
    %1372 = "dtu_hlir.conv_bias"(%1371, %1370, %106) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3896-0", op_id = 593 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1373 = dtu_hlir.constant  {node_name = "up_blocks.0.resnets.2.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x2560x1280xf16>
    %1374 = "dtu_hlir.conv_bias"(%1342, %1373, %107) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3897-0", op_id = 594 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x8x8x2560xf16>, tensor<1x1x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x8x8x1280xf16>
    %1375 = "dtu_hlir.add"(%1374, %1372) {node_name = "Add_3898-0", node_type = "Add", op_id = 595 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x8x8x1280xf16>, tensor<2x8x8x1280xf16>) -> tensor<2x8x8x1280xf16>
    %1376 = dtu_hlir.constant dense<[]> : tensor<0xi32>
    %1377 = dtu_hlir.constant  {node_name = "Resize_3901-Const-3"} dense<[0.000000e+00, 0.00782680511, 0.00782680511, 0.000000e+00, 1.681560e-44, 4.763220e-37, 0.000000e+00, 0.000000e+00]> : tensor<8xf32>
    %1378 = dtu_hlir.constant  {node_name = "onnx::Resize_10070"} dense<[1.000000e+00, 2.000000e+00, 2.000000e+00, 1.000000e+00]> : tensor<4xf32>
    %1379 = "dtu_hlir.resize"(%1375, %1377, %1378, %1376) {coordinate_transformation_mode = 1 : i64, cubic_coeff_a = -7.500000e-01 : f32, exclude_outside = false, extrapolation_value = 0.000000e+00 : f32, mode = 0 : i64, nearest_mode = 3 : i64, node_name = "Resize_3901-4", op_id = 596 : i64, resize_dimensions = dense<[1, 2]> : tensor<2xi64>, unique_name = "resize_dtu"} : (tensor<2x8x8x1280xf16>, tensor<8xf32>, tensor<4xf32>, tensor<0xi32>) -> tensor<2x16x16x1280xf16>
    %1380 = dtu_hlir.constant  {node_name = "up_blocks.0.upsamplers.0.conv.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1381 = "dtu_hlir.conv_bias"(%1379, %1380, %108) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3902-0", op_id = 597 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1382 = "dtu_hlir.concatenate"(%1381, %1078) {dimension = 3 : i64, node_name = "Concat_3903-0", op_id = 598 : i64, unique_name = "concatenate_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x2560xf16>
    %1383 = "dtu_hlir.transpose"(%1382) {node_name = "Conv_3902-0", op_id = 599 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x2560xf16>) -> tensor<2x2560x16x16xf16>
    %1384 = "dtu_hlir.reshape"(%1383) {node_name = "Reshape_3905-0"} : (tensor<2x2560x16x16xf16>) -> tensor<2x32x20480xf16>
    %1385 = "dtu_hlir.transpose"(%1384) {node_name = "InstanceNormalization_3908-0", op_id = 600 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1386 = "dtu_hlir.instance_norm"(%1385, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 601 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1387 = "dtu_hlir.transpose"(%1386) {node_name = "InstanceNormalization_3908-0", op_id = 602 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1388 = "dtu_hlir.reshape"(%1387) {node_name = "Reshape_3910-0"} : (tensor<2x32x20480xf16>) -> tensor<2x2560x16x16xf16>
    %1389 = dtu_hlir.constant  {node_name = "Mul_3911-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1390 = dtu_hlir.constant  {node_name = "Add_3912-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1391 = "dtu_hlir.fusion"(%1389, %1388, %1390) ( {
    ^bb0(%arg3: tensor<2x2560xf16>, %arg4: tensor<2x2560x16x16xf16>, %arg5: tensor<2x2560xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3911-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3911-0", node_type = "Mul"} : (tensor<2x2560x16x16xf16>, tensor<2x2560x16x16xf16>) -> tensor<2x2560x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3912-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3912-0", node_type = "Add"} : (tensor<2x2560x16x16xf16>, tensor<2x2560x16x16xf16>) -> tensor<2x2560x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3914-0"} : (tensor<2x2560x16x16xf16>) -> tensor<2x2560x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x2560x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 603 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x2560xf16>, tensor<2x2560x16x16xf16>, tensor<2x2560xf16>) -> tensor<2x2560x16x16xf16>
    %1392 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x2560x1280xf16>
    %1393 = "dtu_hlir.transpose"(%1391) {node_name = "Conv_3915-0", op_id = 604 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x16x16xf16>) -> tensor<2x16x16x2560xf16>
    %1394 = "dtu_hlir.conv_bias"(%1393, %1392, %145) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3915-0", op_id = 605 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x2560xf16>, tensor<3x3x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1395 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1396 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1397 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 606 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1398 = "dtu_hlir.dot_general_bias"(%1397, %1395, %1396) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 607 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1399 = "dtu_hlir.broadcast_in_dim"(%1398) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 608 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1400 = "dtu_hlir.add"(%1394, %1399) {node_name = "Add_3923-0", node_type = "Add", op_id = 609 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1401 = "dtu_hlir.transpose"(%1400) {op_id = 610 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1402 = "dtu_hlir.reshape"(%1401) {node_name = "Reshape_3925-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %1403 = "dtu_hlir.transpose"(%1402) {node_name = "InstanceNormalization_3928-0", op_id = 611 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1404 = "dtu_hlir.instance_norm"(%1403, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 612 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1405 = "dtu_hlir.transpose"(%1404) {node_name = "InstanceNormalization_3928-0", op_id = 613 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1406 = "dtu_hlir.reshape"(%1405) {node_name = "Reshape_3930-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1407 = dtu_hlir.constant  {node_name = "Mul_3931-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1408 = dtu_hlir.constant  {node_name = "Add_3932-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1409 = "dtu_hlir.fusion"(%1407, %1406, %1408) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3931-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3931-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3932-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3932-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_3934-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 614 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1410 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1411 = "dtu_hlir.transpose"(%1409) {node_name = "Conv_3935-0", op_id = 615 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1412 = "dtu_hlir.conv_bias"(%1411, %1410, %146) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3935-0", op_id = 616 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1413 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.0.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x2560x1280xf16>
    %1414 = "dtu_hlir.conv_bias"(%1382, %1413, %147) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_3936-0", op_id = 617 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x2560xf16>, tensor<1x1x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1415 = "dtu_hlir.add"(%1414, %1412) {node_name = "Add_3937-0", node_type = "Add", op_id = 618 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1416 = "dtu_hlir.transpose"(%1415) {node_name = "Conv_3936-0", op_id = 619 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1417 = "dtu_hlir.reshape"(%1416) {node_name = "Reshape_3950-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %1418 = "dtu_hlir.transpose"(%1417) {node_name = "InstanceNormalization_3953-0", op_id = 620 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1419 = "dtu_hlir.instance_norm"(%1418, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 621 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1420 = "dtu_hlir.transpose"(%1419) {node_name = "InstanceNormalization_3953-0", op_id = 622 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1421 = "dtu_hlir.reshape"(%1420) {node_name = "Reshape_3955-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1422 = dtu_hlir.constant  {node_name = "Mul_3956-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1423 = dtu_hlir.constant  {node_name = "Add_3957-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1424 = "dtu_hlir.fusion"(%1422, %1421, %1423) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_3956-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_3956-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_3957-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_3957-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 623 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1425 = "dtu_hlir.transpose"(%1424) {node_name = "Transpose_3961-0", node_type = "Transpose", op_id = 624 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1426 = "dtu_hlir.reshape"(%1425) {node_name = "Reshape_3970-0"} : (tensor<2x16x16x1280xf16>) -> tensor<2x256x1280xf16>
    %1427 = "dtu_hlir.dot_general_bias"(%1426, %348, %109) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_3972-0", op_id = 625 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1428 = "dtu_hlir.layer_norm_inference"(%1427, %114, %115) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_3983-0", op_id = 626 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1429 = "dtu_hlir.dot_general"(%1428, %349) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3984-0", node_type = "MatMul", op_id = 627 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1430 = "dtu_hlir.dot_general"(%1428, %350) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3985-0", node_type = "MatMul", op_id = 628 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1431 = "dtu_hlir.dot_general"(%1428, %351) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_3986-0", node_type = "MatMul", op_id = 629 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1432 = "dtu_hlir.reshape"(%1429) {node_name = "Reshape_4007-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1433 = "dtu_hlir.transpose"(%1432) {node_name = "Transpose_4008-0", node_type = "Transpose", op_id = 630 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1434 = "dtu_hlir.reshape"(%1433) {node_name = "Reshape_4022-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1435 = "dtu_hlir.reshape"(%1430) {node_name = "Reshape_4043-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1436 = "dtu_hlir.transpose"(%1435) {node_name = "Transpose_4044-0", node_type = "Transpose", op_id = 631 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1437 = "dtu_hlir.reshape"(%1436) {node_name = "Reshape_4058-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1438 = "dtu_hlir.reshape"(%1431) {node_name = "Reshape_4079-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1439 = "dtu_hlir.transpose"(%1438) {node_name = "Transpose_4080-0", node_type = "Transpose", op_id = 632 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1440 = "dtu_hlir.reshape"(%1439) {node_name = "Reshape_4094-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1441 = "dtu_hlir.transpose"(%1437) {node_name = "Transpose_4112-0", node_type = "Transpose", op_id = 633 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x256x64xf16>) -> tensor<40x64x256xf16>
    %1442 = "dtu_hlir.dot_general"(%1434, %1441) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4113-0", node_type = "MatMul", op_id = 634 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x256xf16>) -> tensor<40x256x256xf16>
    %1443 = "dtu_hlir.fusion"(%267, %1442) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x256xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2414-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x256xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4114-0", node_type = "Mul"} : (tensor<40x256x256xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x256xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 635 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1444 = "dtu_hlir.softmax"(%1443) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4117-0", node_type = "Softmax", op_id = 636 : i64, unique_name = "common20_softmax"} : (tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1445 = "dtu_hlir.dot_general"(%1444, %1440) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4119-0", node_type = "MatMul", op_id = 637 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x256xf16>, tensor<40x256x64xf16>) -> tensor<40x256x64xf16>
    %1446 = "dtu_hlir.reshape"(%1445) {node_name = "Reshape_4140-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1447 = "dtu_hlir.transpose"(%1446) {node_name = "Transpose_4141-0", node_type = "Transpose", op_id = 638 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1448 = "dtu_hlir.reshape"(%1447) {node_name = "Reshape_4155-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1449 = "dtu_hlir.dot_general_bias"(%1448, %352, %110) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4157-0", op_id = 639 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1450 = "dtu_hlir.add"(%1449, %1427) {node_name = "Add_4158-0", node_type = "Add", op_id = 640 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1451 = "dtu_hlir.layer_norm_inference"(%1450, %116, %117) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_4169-0", op_id = 641 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1452 = "dtu_hlir.dot_general"(%1451, %353) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4170-0", node_type = "MatMul", op_id = 642 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1453 = "dtu_hlir.dot_general"(%4, %354) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4171-0", node_type = "MatMul", op_id = 643 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1454 = "dtu_hlir.dot_general"(%4, %355) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4172-0", node_type = "MatMul", op_id = 644 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1455 = "dtu_hlir.reshape"(%1452) {node_name = "Reshape_4193-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1456 = "dtu_hlir.transpose"(%1455) {node_name = "Transpose_4194-0", node_type = "Transpose", op_id = 645 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1457 = "dtu_hlir.reshape"(%1456) {node_name = "Reshape_4208-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1458 = "dtu_hlir.reshape"(%1453) {node_name = "Reshape_4229-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1459 = "dtu_hlir.transpose"(%1458) {node_name = "Transpose_4230-0", node_type = "Transpose", op_id = 646 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1460 = "dtu_hlir.reshape"(%1459) {node_name = "Reshape_4244-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1461 = "dtu_hlir.reshape"(%1454) {node_name = "Reshape_4265-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1462 = "dtu_hlir.transpose"(%1461) {node_name = "Transpose_4266-0", node_type = "Transpose", op_id = 647 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1463 = "dtu_hlir.reshape"(%1462) {node_name = "Reshape_4280-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1464 = "dtu_hlir.transpose"(%1460) {node_name = "Transpose_4298-0", node_type = "Transpose", op_id = 648 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x77x64xf16>) -> tensor<40x64x77xf16>
    %1465 = "dtu_hlir.dot_general"(%1457, %1464) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4299-0", node_type = "MatMul", op_id = 649 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x77xf16>) -> tensor<40x256x77xf16>
    %1466 = "dtu_hlir.fusion"(%267, %1465) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2600-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4300-0", node_type = "Mul"} : (tensor<40x256x77xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 650 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1467 = "dtu_hlir.softmax"(%1466) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4303-0", node_type = "Softmax", op_id = 651 : i64, unique_name = "common20_softmax"} : (tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1468 = "dtu_hlir.dot_general"(%1467, %1463) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4305-0", node_type = "MatMul", op_id = 652 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x77xf16>, tensor<40x77x64xf16>) -> tensor<40x256x64xf16>
    %1469 = "dtu_hlir.reshape"(%1468) {node_name = "Reshape_4326-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1470 = "dtu_hlir.transpose"(%1469) {node_name = "Transpose_4327-0", node_type = "Transpose", op_id = 653 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1471 = "dtu_hlir.reshape"(%1470) {node_name = "Reshape_4341-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1472 = "dtu_hlir.dot_general_bias"(%1471, %356, %113) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4343-0", op_id = 654 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1473 = "dtu_hlir.add"(%1472, %1450) {node_name = "Add_4344-0", node_type = "Add", op_id = 655 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1474 = "dtu_hlir.layer_norm_inference"(%1473, %118, %119) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_4355-0", op_id = 656 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1475 = "dtu_hlir.dot_general_bias"(%1474, %357, %111) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4357-0", op_id = 657 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x10240xf16>, tensor<10240xf32>) -> tensor<2x256x10240xf16>
    %1476 = "dtu_hlir.slice"(%1475) {limit_indices = dense<[2, 256, 5120]> : tensor<3xi64>, node_name = "Slice_4368-1", op_id = 658 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1477 = "dtu_hlir.slice"(%1475) {limit_indices = dense<[2, 256, 10240]> : tensor<3xi64>, node_name = "Slice_4371-3", op_id = 659 : i64, start_indices = dense<[0, 0, 5120]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1478 = "dtu_hlir.gelu"(%1477) {approximate = false, node_name = "Mul_4379-0", op_id = 660 : i64, unique_name = "common20_gelu"} : (tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1479 = "dtu_hlir.mul"(%1476, %1478) {node_name = "Mul_4380-0", node_type = "Mul", op_id = 661 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x256x5120xf16>, tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1480 = "dtu_hlir.dot_general_bias"(%1479, %358, %112) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4382-0", op_id = 662 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x5120xf16>, tensor<5120x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1481 = "dtu_hlir.add"(%1480, %1473) {node_name = "Add_4383-0", node_type = "Add", op_id = 663 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1482 = "dtu_hlir.dot_general_bias"(%1481, %359, %120) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4385-0", op_id = 664 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1483 = "dtu_hlir.reshape"(%1482) {node_name = "Reshape_4395-0"} : (tensor<2x256x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1484 = "dtu_hlir.add"(%1483, %1415) {node_name = "Add_4397-0", node_type = "Add", op_id = 665 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1485 = "dtu_hlir.concatenate"(%1484, %978) {dimension = 3 : i64, node_name = "Concat_4398-0", op_id = 666 : i64, unique_name = "concatenate_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x2560xf16>
    %1486 = "dtu_hlir.transpose"(%1485) {node_name = "Transpose_4396-0", op_id = 667 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x2560xf16>) -> tensor<2x2560x16x16xf16>
    %1487 = "dtu_hlir.reshape"(%1486) {node_name = "Reshape_4400-0"} : (tensor<2x2560x16x16xf16>) -> tensor<2x32x20480xf16>
    %1488 = "dtu_hlir.transpose"(%1487) {node_name = "InstanceNormalization_4403-0", op_id = 668 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1489 = "dtu_hlir.instance_norm"(%1488, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 669 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1490 = "dtu_hlir.transpose"(%1489) {node_name = "InstanceNormalization_4403-0", op_id = 670 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1491 = "dtu_hlir.reshape"(%1490) {node_name = "Reshape_4405-0"} : (tensor<2x32x20480xf16>) -> tensor<2x2560x16x16xf16>
    %1492 = dtu_hlir.constant  {node_name = "Mul_4406-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1493 = dtu_hlir.constant  {node_name = "Add_4407-0"} opaque<"", "0xDEADBEEF"> : tensor<2x2560xf16>
    %1494 = "dtu_hlir.fusion"(%1492, %1491, %1493) ( {
    ^bb0(%arg3: tensor<2x2560xf16>, %arg4: tensor<2x2560x16x16xf16>, %arg5: tensor<2x2560xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4406-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4406-0", node_type = "Mul"} : (tensor<2x2560x16x16xf16>, tensor<2x2560x16x16xf16>) -> tensor<2x2560x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_4407-0"} : (tensor<2x2560xf16>) -> tensor<2x2560x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_4407-0", node_type = "Add"} : (tensor<2x2560x16x16xf16>, tensor<2x2560x16x16xf16>) -> tensor<2x2560x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_4409-0"} : (tensor<2x2560x16x16xf16>) -> tensor<2x2560x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x2560x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 671 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x2560xf16>, tensor<2x2560x16x16xf16>, tensor<2x2560xf16>) -> tensor<2x2560x16x16xf16>
    %1495 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x2560x1280xf16>
    %1496 = "dtu_hlir.transpose"(%1494) {node_name = "Conv_4410-0", op_id = 672 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x2560x16x16xf16>) -> tensor<2x16x16x2560xf16>
    %1497 = "dtu_hlir.conv_bias"(%1496, %1495, %148) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4410-0", op_id = 673 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x2560xf16>, tensor<3x3x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1498 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1499 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1500 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 674 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1501 = "dtu_hlir.dot_general_bias"(%1500, %1498, %1499) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 675 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1502 = "dtu_hlir.broadcast_in_dim"(%1501) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 676 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1503 = "dtu_hlir.add"(%1497, %1502) {node_name = "Add_4418-0", node_type = "Add", op_id = 677 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1504 = "dtu_hlir.transpose"(%1503) {op_id = 678 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1505 = "dtu_hlir.reshape"(%1504) {node_name = "Reshape_4420-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %1506 = "dtu_hlir.transpose"(%1505) {node_name = "InstanceNormalization_4423-0", op_id = 679 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1507 = "dtu_hlir.instance_norm"(%1506, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 680 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1508 = "dtu_hlir.transpose"(%1507) {node_name = "InstanceNormalization_4423-0", op_id = 681 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1509 = "dtu_hlir.reshape"(%1508) {node_name = "Reshape_4425-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1510 = dtu_hlir.constant  {node_name = "Mul_4426-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1511 = dtu_hlir.constant  {node_name = "Add_4427-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1512 = "dtu_hlir.fusion"(%1510, %1509, %1511) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4426-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4426-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_4427-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_4427-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_4429-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 682 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1513 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1514 = "dtu_hlir.transpose"(%1512) {node_name = "Conv_4430-0", op_id = 683 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1515 = "dtu_hlir.conv_bias"(%1514, %1513, %149) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4430-0", op_id = 684 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1516 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.1.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x2560x1280xf16>
    %1517 = "dtu_hlir.conv_bias"(%1485, %1516, %150) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4431-0", op_id = 685 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x2560xf16>, tensor<1x1x2560x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1518 = "dtu_hlir.add"(%1517, %1515) {node_name = "Add_4432-0", node_type = "Add", op_id = 686 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1519 = "dtu_hlir.transpose"(%1518) {node_name = "Conv_4431-0", op_id = 687 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1520 = "dtu_hlir.reshape"(%1519) {node_name = "Reshape_4445-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %1521 = "dtu_hlir.transpose"(%1520) {node_name = "InstanceNormalization_4448-0", op_id = 688 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1522 = "dtu_hlir.instance_norm"(%1521, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 689 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1523 = "dtu_hlir.transpose"(%1522) {node_name = "InstanceNormalization_4448-0", op_id = 690 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1524 = "dtu_hlir.reshape"(%1523) {node_name = "Reshape_4450-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1525 = dtu_hlir.constant  {node_name = "Mul_4451-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1526 = dtu_hlir.constant  {node_name = "Add_4452-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1527 = "dtu_hlir.fusion"(%1525, %1524, %1526) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4451-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4451-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_4452-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_4452-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 691 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1528 = "dtu_hlir.transpose"(%1527) {node_name = "Transpose_4456-0", node_type = "Transpose", op_id = 692 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1529 = "dtu_hlir.reshape"(%1528) {node_name = "Reshape_4465-0"} : (tensor<2x16x16x1280xf16>) -> tensor<2x256x1280xf16>
    %1530 = "dtu_hlir.dot_general_bias"(%1529, %360, %121) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4467-0", op_id = 693 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1531 = "dtu_hlir.layer_norm_inference"(%1530, %126, %127) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_4478-0", op_id = 694 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1532 = "dtu_hlir.dot_general"(%1531, %361) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4479-0", node_type = "MatMul", op_id = 695 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1533 = "dtu_hlir.dot_general"(%1531, %362) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4480-0", node_type = "MatMul", op_id = 696 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1534 = "dtu_hlir.dot_general"(%1531, %363) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4481-0", node_type = "MatMul", op_id = 697 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1535 = "dtu_hlir.reshape"(%1532) {node_name = "Reshape_4502-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1536 = "dtu_hlir.transpose"(%1535) {node_name = "Transpose_4503-0", node_type = "Transpose", op_id = 698 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1537 = "dtu_hlir.reshape"(%1536) {node_name = "Reshape_4517-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1538 = "dtu_hlir.reshape"(%1533) {node_name = "Reshape_4538-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1539 = "dtu_hlir.transpose"(%1538) {node_name = "Transpose_4539-0", node_type = "Transpose", op_id = 699 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1540 = "dtu_hlir.reshape"(%1539) {node_name = "Reshape_4553-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1541 = "dtu_hlir.reshape"(%1534) {node_name = "Reshape_4574-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1542 = "dtu_hlir.transpose"(%1541) {node_name = "Transpose_4575-0", node_type = "Transpose", op_id = 700 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1543 = "dtu_hlir.reshape"(%1542) {node_name = "Reshape_4589-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1544 = "dtu_hlir.transpose"(%1540) {node_name = "Transpose_4607-0", node_type = "Transpose", op_id = 701 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x256x64xf16>) -> tensor<40x64x256xf16>
    %1545 = "dtu_hlir.dot_general"(%1537, %1544) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4608-0", node_type = "MatMul", op_id = 702 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x256xf16>) -> tensor<40x256x256xf16>
    %1546 = "dtu_hlir.fusion"(%267, %1545) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x256xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2414-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x256xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4609-0", node_type = "Mul"} : (tensor<40x256x256xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x256xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 703 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1547 = "dtu_hlir.softmax"(%1546) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4612-0", node_type = "Softmax", op_id = 704 : i64, unique_name = "common20_softmax"} : (tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1548 = "dtu_hlir.dot_general"(%1547, %1543) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4614-0", node_type = "MatMul", op_id = 705 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x256xf16>, tensor<40x256x64xf16>) -> tensor<40x256x64xf16>
    %1549 = "dtu_hlir.reshape"(%1548) {node_name = "Reshape_4635-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1550 = "dtu_hlir.transpose"(%1549) {node_name = "Transpose_4636-0", node_type = "Transpose", op_id = 706 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1551 = "dtu_hlir.reshape"(%1550) {node_name = "Reshape_4650-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1552 = "dtu_hlir.dot_general_bias"(%1551, %364, %122) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4652-0", op_id = 707 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1553 = "dtu_hlir.add"(%1552, %1530) {node_name = "Add_4653-0", node_type = "Add", op_id = 708 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1554 = "dtu_hlir.layer_norm_inference"(%1553, %128, %129) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_4664-0", op_id = 709 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1555 = "dtu_hlir.dot_general"(%1554, %365) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4665-0", node_type = "MatMul", op_id = 710 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1556 = "dtu_hlir.dot_general"(%4, %366) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4666-0", node_type = "MatMul", op_id = 711 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1557 = "dtu_hlir.dot_general"(%4, %367) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4667-0", node_type = "MatMul", op_id = 712 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1558 = "dtu_hlir.reshape"(%1555) {node_name = "Reshape_4688-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1559 = "dtu_hlir.transpose"(%1558) {node_name = "Transpose_4689-0", node_type = "Transpose", op_id = 713 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1560 = "dtu_hlir.reshape"(%1559) {node_name = "Reshape_4703-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1561 = "dtu_hlir.reshape"(%1556) {node_name = "Reshape_4724-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1562 = "dtu_hlir.transpose"(%1561) {node_name = "Transpose_4725-0", node_type = "Transpose", op_id = 714 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1563 = "dtu_hlir.reshape"(%1562) {node_name = "Reshape_4739-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1564 = "dtu_hlir.reshape"(%1557) {node_name = "Reshape_4760-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1565 = "dtu_hlir.transpose"(%1564) {node_name = "Transpose_4761-0", node_type = "Transpose", op_id = 715 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1566 = "dtu_hlir.reshape"(%1565) {node_name = "Reshape_4775-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1567 = "dtu_hlir.transpose"(%1563) {node_name = "Transpose_4793-0", node_type = "Transpose", op_id = 716 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x77x64xf16>) -> tensor<40x64x77xf16>
    %1568 = "dtu_hlir.dot_general"(%1560, %1567) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4794-0", node_type = "MatMul", op_id = 717 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x77xf16>) -> tensor<40x256x77xf16>
    %1569 = "dtu_hlir.fusion"(%267, %1568) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2600-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4795-0", node_type = "Mul"} : (tensor<40x256x77xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 718 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1570 = "dtu_hlir.softmax"(%1569) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_4798-0", node_type = "Softmax", op_id = 719 : i64, unique_name = "common20_softmax"} : (tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1571 = "dtu_hlir.dot_general"(%1570, %1566) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_4800-0", node_type = "MatMul", op_id = 720 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x77xf16>, tensor<40x77x64xf16>) -> tensor<40x256x64xf16>
    %1572 = "dtu_hlir.reshape"(%1571) {node_name = "Reshape_4821-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1573 = "dtu_hlir.transpose"(%1572) {node_name = "Transpose_4822-0", node_type = "Transpose", op_id = 721 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1574 = "dtu_hlir.reshape"(%1573) {node_name = "Reshape_4836-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1575 = "dtu_hlir.dot_general_bias"(%1574, %368, %125) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4838-0", op_id = 722 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1576 = "dtu_hlir.add"(%1575, %1553) {node_name = "Add_4839-0", node_type = "Add", op_id = 723 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1577 = "dtu_hlir.layer_norm_inference"(%1576, %130, %131) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_4850-0", op_id = 724 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1578 = "dtu_hlir.dot_general_bias"(%1577, %369, %123) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4852-0", op_id = 725 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x10240xf16>, tensor<10240xf32>) -> tensor<2x256x10240xf16>
    %1579 = "dtu_hlir.slice"(%1578) {limit_indices = dense<[2, 256, 5120]> : tensor<3xi64>, node_name = "Slice_4863-1", op_id = 726 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1580 = "dtu_hlir.slice"(%1578) {limit_indices = dense<[2, 256, 10240]> : tensor<3xi64>, node_name = "Slice_4866-3", op_id = 727 : i64, start_indices = dense<[0, 0, 5120]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1581 = "dtu_hlir.gelu"(%1580) {approximate = false, node_name = "Mul_4874-0", op_id = 728 : i64, unique_name = "common20_gelu"} : (tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1582 = "dtu_hlir.mul"(%1579, %1581) {node_name = "Mul_4875-0", node_type = "Mul", op_id = 729 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x256x5120xf16>, tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1583 = "dtu_hlir.dot_general_bias"(%1582, %370, %124) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4877-0", op_id = 730 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x5120xf16>, tensor<5120x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1584 = "dtu_hlir.add"(%1583, %1576) {node_name = "Add_4878-0", node_type = "Add", op_id = 731 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1585 = "dtu_hlir.dot_general_bias"(%1584, %371, %132) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4880-0", op_id = 732 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1586 = "dtu_hlir.reshape"(%1585) {node_name = "Reshape_4890-0"} : (tensor<2x256x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1587 = "dtu_hlir.add"(%1586, %1518) {node_name = "Add_4892-0", node_type = "Add", op_id = 733 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1588 = "dtu_hlir.concatenate"(%1587, %876) {dimension = 3 : i64, node_name = "Concat_4893-0", op_id = 734 : i64, unique_name = "concatenate_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x640xf16>) -> tensor<2x16x16x1920xf16>
    %1589 = "dtu_hlir.transpose"(%1588) {node_name = "Transpose_4891-0", op_id = 735 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1920xf16>) -> tensor<2x1920x16x16xf16>
    %1590 = "dtu_hlir.reshape"(%1589) {node_name = "Reshape_4895-0"} : (tensor<2x1920x16x16xf16>) -> tensor<2x32x15360xf16>
    %1591 = "dtu_hlir.transpose"(%1590) {node_name = "InstanceNormalization_4898-0", op_id = 736 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x15360xf16>) -> tensor<2x15360x32xf16>
    %1592 = "dtu_hlir.instance_norm"(%1591, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 737 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x15360x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x15360x32xf16>
    %1593 = "dtu_hlir.transpose"(%1592) {node_name = "InstanceNormalization_4898-0", op_id = 738 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x15360x32xf16>) -> tensor<2x32x15360xf16>
    %1594 = "dtu_hlir.reshape"(%1593) {node_name = "Reshape_4900-0"} : (tensor<2x32x15360xf16>) -> tensor<2x1920x16x16xf16>
    %1595 = dtu_hlir.constant  {node_name = "Mul_4901-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1920xf16>
    %1596 = dtu_hlir.constant  {node_name = "Add_4902-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1920xf16>
    %1597 = "dtu_hlir.fusion"(%1595, %1594, %1596) ( {
    ^bb0(%arg3: tensor<2x1920xf16>, %arg4: tensor<2x1920x16x16xf16>, %arg5: tensor<2x1920xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4901-0"} : (tensor<2x1920xf16>) -> tensor<2x1920x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4901-0", node_type = "Mul"} : (tensor<2x1920x16x16xf16>, tensor<2x1920x16x16xf16>) -> tensor<2x1920x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_4902-0"} : (tensor<2x1920xf16>) -> tensor<2x1920x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_4902-0", node_type = "Add"} : (tensor<2x1920x16x16xf16>, tensor<2x1920x16x16xf16>) -> tensor<2x1920x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_4904-0"} : (tensor<2x1920x16x16xf16>) -> tensor<2x1920x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1920x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 739 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1920xf16>, tensor<2x1920x16x16xf16>, tensor<2x1920xf16>) -> tensor<2x1920x16x16xf16>
    %1598 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1920x1280xf16>
    %1599 = "dtu_hlir.transpose"(%1597) {node_name = "Conv_4905-0", op_id = 740 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1920x16x16xf16>) -> tensor<2x16x16x1920xf16>
    %1600 = "dtu_hlir.conv_bias"(%1599, %1598, %151) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4905-0", op_id = 741 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1920xf16>, tensor<3x3x1920x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1601 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x1280xf16>
    %1602 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280xf32>
    %1603 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 742 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1604 = "dtu_hlir.dot_general_bias"(%1603, %1601, %1602) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 743 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1605 = "dtu_hlir.broadcast_in_dim"(%1604) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 744 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1606 = "dtu_hlir.add"(%1600, %1605) {node_name = "Add_4913-0", node_type = "Add", op_id = 745 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1607 = "dtu_hlir.transpose"(%1606) {op_id = 746 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1608 = "dtu_hlir.reshape"(%1607) {node_name = "Reshape_4915-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %1609 = "dtu_hlir.transpose"(%1608) {node_name = "InstanceNormalization_4918-0", op_id = 747 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1610 = "dtu_hlir.instance_norm"(%1609, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 748 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1611 = "dtu_hlir.transpose"(%1610) {node_name = "InstanceNormalization_4918-0", op_id = 749 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1612 = "dtu_hlir.reshape"(%1611) {node_name = "Reshape_4920-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1613 = dtu_hlir.constant  {node_name = "Mul_4921-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1614 = dtu_hlir.constant  {node_name = "Add_4922-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1615 = "dtu_hlir.fusion"(%1613, %1612, %1614) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4921-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4921-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_4922-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_4922-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_4924-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 750 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1616 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1617 = "dtu_hlir.transpose"(%1615) {node_name = "Conv_4925-0", op_id = 751 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1618 = "dtu_hlir.conv_bias"(%1617, %1616, %152) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4925-0", op_id = 752 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1619 = dtu_hlir.constant  {node_name = "up_blocks.1.resnets.2.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x1920x1280xf16>
    %1620 = "dtu_hlir.conv_bias"(%1588, %1619, %153) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_4926-0", op_id = 753 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x16x16x1920xf16>, tensor<1x1x1920x1280xf16>, tensor<1280xf32>) -> tensor<2x16x16x1280xf16>
    %1621 = "dtu_hlir.add"(%1620, %1618) {node_name = "Add_4927-0", node_type = "Add", op_id = 754 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1622 = "dtu_hlir.transpose"(%1621) {node_name = "Conv_4926-0", op_id = 755 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x16x16x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1623 = "dtu_hlir.reshape"(%1622) {node_name = "Reshape_4940-0"} : (tensor<2x1280x16x16xf16>) -> tensor<2x32x10240xf16>
    %1624 = "dtu_hlir.transpose"(%1623) {node_name = "InstanceNormalization_4943-0", op_id = 756 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x10240xf16>) -> tensor<2x10240x32xf16>
    %1625 = "dtu_hlir.instance_norm"(%1624, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 757 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x10240x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x10240x32xf16>
    %1626 = "dtu_hlir.transpose"(%1625) {node_name = "InstanceNormalization_4943-0", op_id = 758 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x10240x32xf16>) -> tensor<2x32x10240xf16>
    %1627 = "dtu_hlir.reshape"(%1626) {node_name = "Reshape_4945-0"} : (tensor<2x32x10240xf16>) -> tensor<2x1280x16x16xf16>
    %1628 = dtu_hlir.constant  {node_name = "Mul_4946-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1629 = dtu_hlir.constant  {node_name = "Add_4947-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1630 = "dtu_hlir.fusion"(%1628, %1627, %1629) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x16x16xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_4946-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_4946-0", node_type = "Mul"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_4947-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_4947-0", node_type = "Add"} : (tensor<2x1280x16x16xf16>, tensor<2x1280x16x16xf16>) -> tensor<2x1280x16x16xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x1280x16x16xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 759 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x16x16xf16>, tensor<2x1280xf16>) -> tensor<2x1280x16x16xf16>
    %1631 = "dtu_hlir.transpose"(%1630) {node_name = "Transpose_4951-0", node_type = "Transpose", op_id = 760 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x16x16xf16>) -> tensor<2x16x16x1280xf16>
    %1632 = "dtu_hlir.reshape"(%1631) {node_name = "Reshape_4960-0"} : (tensor<2x16x16x1280xf16>) -> tensor<2x256x1280xf16>
    %1633 = "dtu_hlir.dot_general_bias"(%1632, %372, %133) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_4962-0", op_id = 761 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1634 = "dtu_hlir.layer_norm_inference"(%1633, %138, %139) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_4973-0", op_id = 762 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1635 = "dtu_hlir.dot_general"(%1634, %373) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4974-0", node_type = "MatMul", op_id = 763 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1636 = "dtu_hlir.dot_general"(%1634, %374) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4975-0", node_type = "MatMul", op_id = 764 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1637 = "dtu_hlir.dot_general"(%1634, %375) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_4976-0", node_type = "MatMul", op_id = 765 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1638 = "dtu_hlir.reshape"(%1635) {node_name = "Reshape_4997-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1639 = "dtu_hlir.transpose"(%1638) {node_name = "Transpose_4998-0", node_type = "Transpose", op_id = 766 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1640 = "dtu_hlir.reshape"(%1639) {node_name = "Reshape_5012-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1641 = "dtu_hlir.reshape"(%1636) {node_name = "Reshape_5033-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1642 = "dtu_hlir.transpose"(%1641) {node_name = "Transpose_5034-0", node_type = "Transpose", op_id = 767 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1643 = "dtu_hlir.reshape"(%1642) {node_name = "Reshape_5048-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1644 = "dtu_hlir.reshape"(%1637) {node_name = "Reshape_5069-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1645 = "dtu_hlir.transpose"(%1644) {node_name = "Transpose_5070-0", node_type = "Transpose", op_id = 768 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1646 = "dtu_hlir.reshape"(%1645) {node_name = "Reshape_5084-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1647 = "dtu_hlir.transpose"(%1643) {node_name = "Transpose_5102-0", node_type = "Transpose", op_id = 769 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x256x64xf16>) -> tensor<40x64x256xf16>
    %1648 = "dtu_hlir.dot_general"(%1640, %1647) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5103-0", node_type = "MatMul", op_id = 770 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x256xf16>) -> tensor<40x256x256xf16>
    %1649 = "dtu_hlir.fusion"(%267, %1648) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x256xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2414-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x256xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5104-0", node_type = "Mul"} : (tensor<40x256x256xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x256xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 771 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1650 = "dtu_hlir.softmax"(%1649) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5107-0", node_type = "Softmax", op_id = 772 : i64, unique_name = "common20_softmax"} : (tensor<40x256x256xf16>) -> tensor<40x256x256xf16>
    %1651 = "dtu_hlir.dot_general"(%1650, %1646) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5109-0", node_type = "MatMul", op_id = 773 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x256xf16>, tensor<40x256x64xf16>) -> tensor<40x256x64xf16>
    %1652 = "dtu_hlir.reshape"(%1651) {node_name = "Reshape_5130-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1653 = "dtu_hlir.transpose"(%1652) {node_name = "Transpose_5131-0", node_type = "Transpose", op_id = 774 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1654 = "dtu_hlir.reshape"(%1653) {node_name = "Reshape_5145-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1655 = "dtu_hlir.dot_general_bias"(%1654, %376, %134) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5147-0", op_id = 775 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1656 = "dtu_hlir.add"(%1655, %1633) {node_name = "Add_5148-0", node_type = "Add", op_id = 776 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1657 = "dtu_hlir.layer_norm_inference"(%1656, %140, %141) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_5159-0", op_id = 777 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1658 = "dtu_hlir.dot_general"(%1657, %377) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5160-0", node_type = "MatMul", op_id = 778 : i64, unique_name = "dot_general_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>) -> tensor<2x256x1280xf16>
    %1659 = "dtu_hlir.dot_general"(%4, %378) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5161-0", node_type = "MatMul", op_id = 779 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1660 = "dtu_hlir.dot_general"(%4, %379) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5162-0", node_type = "MatMul", op_id = 780 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x1280xf16>) -> tensor<2x77x1280xf16>
    %1661 = "dtu_hlir.reshape"(%1658) {node_name = "Reshape_5183-0"} : (tensor<2x256x1280xf16>) -> tensor<2x256x20x64xf16>
    %1662 = "dtu_hlir.transpose"(%1661) {node_name = "Transpose_5184-0", node_type = "Transpose", op_id = 781 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x256x20x64xf16>) -> tensor<2x20x256x64xf16>
    %1663 = "dtu_hlir.reshape"(%1662) {node_name = "Reshape_5198-0"} : (tensor<2x20x256x64xf16>) -> tensor<40x256x64xf16>
    %1664 = "dtu_hlir.reshape"(%1659) {node_name = "Reshape_5219-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1665 = "dtu_hlir.transpose"(%1664) {node_name = "Transpose_5220-0", node_type = "Transpose", op_id = 782 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1666 = "dtu_hlir.reshape"(%1665) {node_name = "Reshape_5234-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1667 = "dtu_hlir.reshape"(%1660) {node_name = "Reshape_5255-0"} : (tensor<2x77x1280xf16>) -> tensor<2x77x20x64xf16>
    %1668 = "dtu_hlir.transpose"(%1667) {node_name = "Transpose_5256-0", node_type = "Transpose", op_id = 783 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x20x64xf16>) -> tensor<2x20x77x64xf16>
    %1669 = "dtu_hlir.reshape"(%1668) {node_name = "Reshape_5270-0"} : (tensor<2x20x77x64xf16>) -> tensor<40x77x64xf16>
    %1670 = "dtu_hlir.transpose"(%1666) {node_name = "Transpose_5288-0", node_type = "Transpose", op_id = 784 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<40x77x64xf16>) -> tensor<40x64x77xf16>
    %1671 = "dtu_hlir.dot_general"(%1663, %1670) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5289-0", node_type = "MatMul", op_id = 785 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x64xf16>, tensor<40x64x77xf16>) -> tensor<40x256x77xf16>
    %1672 = "dtu_hlir.fusion"(%267, %1671) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<40x256x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_2600-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<40x256x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5290-0", node_type = "Mul"} : (tensor<40x256x77xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<40x256x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 786 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1673 = "dtu_hlir.softmax"(%1672) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5293-0", node_type = "Softmax", op_id = 787 : i64, unique_name = "common20_softmax"} : (tensor<40x256x77xf16>) -> tensor<40x256x77xf16>
    %1674 = "dtu_hlir.dot_general"(%1673, %1669) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5295-0", node_type = "MatMul", op_id = 788 : i64, unique_name = "dot_general_dtu"} : (tensor<40x256x77xf16>, tensor<40x77x64xf16>) -> tensor<40x256x64xf16>
    %1675 = "dtu_hlir.reshape"(%1674) {node_name = "Reshape_5316-0"} : (tensor<40x256x64xf16>) -> tensor<2x20x256x64xf16>
    %1676 = "dtu_hlir.transpose"(%1675) {node_name = "Transpose_5317-0", node_type = "Transpose", op_id = 789 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x20x256x64xf16>) -> tensor<2x256x20x64xf16>
    %1677 = "dtu_hlir.reshape"(%1676) {node_name = "Reshape_5331-0"} : (tensor<2x256x20x64xf16>) -> tensor<2x256x1280xf16>
    %1678 = "dtu_hlir.dot_general_bias"(%1677, %380, %137) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5333-0", op_id = 790 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1679 = "dtu_hlir.add"(%1678, %1656) {node_name = "Add_5334-0", node_type = "Add", op_id = 791 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1680 = "dtu_hlir.layer_norm_inference"(%1679, %142, %143) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_5345-0", op_id = 792 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x256x1280xf16>, tensor<1280xf32>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1681 = "dtu_hlir.dot_general_bias"(%1680, %381, %135) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5347-0", op_id = 793 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x10240xf16>, tensor<10240xf32>) -> tensor<2x256x10240xf16>
    %1682 = "dtu_hlir.slice"(%1681) {limit_indices = dense<[2, 256, 5120]> : tensor<3xi64>, node_name = "Slice_5358-1", op_id = 794 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1683 = "dtu_hlir.slice"(%1681) {limit_indices = dense<[2, 256, 10240]> : tensor<3xi64>, node_name = "Slice_5361-3", op_id = 795 : i64, start_indices = dense<[0, 0, 5120]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x256x10240xf16>) -> tensor<2x256x5120xf16>
    %1684 = "dtu_hlir.gelu"(%1683) {approximate = false, node_name = "Mul_5369-0", op_id = 796 : i64, unique_name = "common20_gelu"} : (tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1685 = "dtu_hlir.mul"(%1682, %1684) {node_name = "Mul_5370-0", node_type = "Mul", op_id = 797 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x256x5120xf16>, tensor<2x256x5120xf16>) -> tensor<2x256x5120xf16>
    %1686 = "dtu_hlir.dot_general_bias"(%1685, %382, %136) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5372-0", op_id = 798 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x5120xf16>, tensor<5120x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1687 = "dtu_hlir.add"(%1686, %1679) {node_name = "Add_5373-0", node_type = "Add", op_id = 799 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x256x1280xf16>, tensor<2x256x1280xf16>) -> tensor<2x256x1280xf16>
    %1688 = "dtu_hlir.dot_general_bias"(%1687, %383, %144) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5375-0", op_id = 800 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x256x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x256x1280xf16>
    %1689 = "dtu_hlir.reshape"(%1688) {node_name = "Reshape_5385-0"} : (tensor<2x256x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1690 = "dtu_hlir.add"(%1689, %1621) {node_name = "Add_5387-0", node_type = "Add", op_id = 801 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x16x16x1280xf16>, tensor<2x16x16x1280xf16>) -> tensor<2x16x16x1280xf16>
    %1691 = "dtu_hlir.resize"(%1690, %1377, %1378, %1376) {coordinate_transformation_mode = 1 : i64, cubic_coeff_a = -7.500000e-01 : f32, exclude_outside = false, extrapolation_value = 0.000000e+00 : f32, mode = 0 : i64, nearest_mode = 3 : i64, node_name = "Resize_5388-4", op_id = 802 : i64, resize_dimensions = dense<[1, 2]> : tensor<2xi64>, unique_name = "resize_dtu"} : (tensor<2x16x16x1280xf16>, tensor<8xf32>, tensor<4xf32>, tensor<0xi32>) -> tensor<2x32x32x1280xf16>
    %1692 = dtu_hlir.constant  {node_name = "up_blocks.1.upsamplers.0.conv.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x1280xf16>
    %1693 = "dtu_hlir.conv_bias"(%1691, %1692, %154) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5389-0", op_id = 803 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x1280xf16>, tensor<3x3x1280x1280xf16>, tensor<1280xf32>) -> tensor<2x32x32x1280xf16>
    %1694 = "dtu_hlir.concatenate"(%1693, %874) {dimension = 3 : i64, node_name = "Concat_5390-0", op_id = 804 : i64, unique_name = "concatenate_dtu"} : (tensor<2x32x32x1280xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x1920xf16>
    %1695 = "dtu_hlir.transpose"(%1694) {node_name = "Conv_5389-0", op_id = 805 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x1920xf16>) -> tensor<2x1920x32x32xf16>
    %1696 = "dtu_hlir.reshape"(%1695) {node_name = "Reshape_5392-0"} : (tensor<2x1920x32x32xf16>) -> tensor<2x32x61440xf16>
    %1697 = "dtu_hlir.transpose"(%1696) {node_name = "InstanceNormalization_5395-0", op_id = 806 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x61440xf16>) -> tensor<2x61440x32xf16>
    %1698 = "dtu_hlir.instance_norm"(%1697, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 807 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x61440x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x61440x32xf16>
    %1699 = "dtu_hlir.transpose"(%1698) {node_name = "InstanceNormalization_5395-0", op_id = 808 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x61440x32xf16>) -> tensor<2x32x61440xf16>
    %1700 = "dtu_hlir.reshape"(%1699) {node_name = "Reshape_5397-0"} : (tensor<2x32x61440xf16>) -> tensor<2x1920x32x32xf16>
    %1701 = dtu_hlir.constant  {node_name = "Mul_5398-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1920xf16>
    %1702 = dtu_hlir.constant  {node_name = "Add_5399-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1920xf16>
    %1703 = "dtu_hlir.fusion"(%1701, %1700, %1702) ( {
    ^bb0(%arg3: tensor<2x1920xf16>, %arg4: tensor<2x1920x32x32xf16>, %arg5: tensor<2x1920xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5398-0"} : (tensor<2x1920xf16>) -> tensor<2x1920x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5398-0", node_type = "Mul"} : (tensor<2x1920x32x32xf16>, tensor<2x1920x32x32xf16>) -> tensor<2x1920x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_5399-0"} : (tensor<2x1920xf16>) -> tensor<2x1920x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_5399-0", node_type = "Add"} : (tensor<2x1920x32x32xf16>, tensor<2x1920x32x32xf16>) -> tensor<2x1920x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_5401-0"} : (tensor<2x1920x32x32xf16>) -> tensor<2x1920x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1920x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 809 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1920xf16>, tensor<2x1920x32x32xf16>, tensor<2x1920xf16>) -> tensor<2x1920x32x32xf16>
    %1704 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1920x640xf16>
    %1705 = "dtu_hlir.transpose"(%1703) {node_name = "Conv_5402-0", op_id = 810 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1920x32x32xf16>) -> tensor<2x32x32x1920xf16>
    %1706 = "dtu_hlir.conv_bias"(%1705, %1704, %191) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5402-0", op_id = 811 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x1920xf16>, tensor<3x3x1920x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1707 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x640xf16>
    %1708 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %1709 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 812 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1710 = "dtu_hlir.dot_general_bias"(%1709, %1707, %1708) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 813 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x640xf16>, tensor<640xf32>) -> tensor<2x640xf16>
    %1711 = "dtu_hlir.broadcast_in_dim"(%1710) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 814 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x640xf16>) -> tensor<2x32x32x640xf16>
    %1712 = "dtu_hlir.add"(%1706, %1711) {node_name = "Add_5410-0", node_type = "Add", op_id = 815 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1713 = "dtu_hlir.transpose"(%1712) {op_id = 816 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %1714 = "dtu_hlir.reshape"(%1713) {node_name = "Reshape_5412-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %1715 = "dtu_hlir.transpose"(%1714) {node_name = "InstanceNormalization_5415-0", op_id = 817 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1716 = "dtu_hlir.instance_norm"(%1715, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 818 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1717 = "dtu_hlir.transpose"(%1716) {node_name = "InstanceNormalization_5415-0", op_id = 819 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1718 = "dtu_hlir.reshape"(%1717) {node_name = "Reshape_5417-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %1719 = dtu_hlir.constant  {node_name = "Mul_5418-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1720 = dtu_hlir.constant  {node_name = "Add_5419-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1721 = "dtu_hlir.fusion"(%1719, %1718, %1720) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5418-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5418-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_5419-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_5419-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_5421-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 820 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %1722 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %1723 = "dtu_hlir.transpose"(%1721) {node_name = "Conv_5422-0", op_id = 821 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %1724 = "dtu_hlir.conv_bias"(%1723, %1722, %192) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5422-0", op_id = 822 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1725 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.0.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x1920x640xf16>
    %1726 = "dtu_hlir.conv_bias"(%1694, %1725, %193) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5423-0", op_id = 823 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x1920xf16>, tensor<1x1x1920x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1727 = "dtu_hlir.add"(%1726, %1724) {node_name = "Add_5424-0", node_type = "Add", op_id = 824 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1728 = "dtu_hlir.transpose"(%1727) {node_name = "Conv_5423-0", op_id = 825 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %1729 = "dtu_hlir.reshape"(%1728) {node_name = "Reshape_5437-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %1730 = "dtu_hlir.transpose"(%1729) {node_name = "InstanceNormalization_5440-0", op_id = 826 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1731 = "dtu_hlir.instance_norm"(%1730, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 827 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1732 = "dtu_hlir.transpose"(%1731) {node_name = "InstanceNormalization_5440-0", op_id = 828 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1733 = "dtu_hlir.reshape"(%1732) {node_name = "Reshape_5442-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %1734 = dtu_hlir.constant  {node_name = "Mul_5443-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1735 = dtu_hlir.constant  {node_name = "Add_5444-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1736 = "dtu_hlir.fusion"(%1734, %1733, %1735) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5443-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5443-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_5444-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_5444-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 829 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %1737 = "dtu_hlir.transpose"(%1736) {node_name = "Transpose_5448-0", node_type = "Transpose", op_id = 830 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %1738 = "dtu_hlir.reshape"(%1737) {node_name = "Reshape_5457-0"} : (tensor<2x32x32x640xf16>) -> tensor<2x1024x640xf16>
    %1739 = "dtu_hlir.dot_general_bias"(%1738, %384, %155) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5459-0", op_id = 831 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1740 = "dtu_hlir.layer_norm_inference"(%1739, %160, %161) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_5470-0", op_id = 832 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1741 = "dtu_hlir.dot_general"(%1740, %385) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5471-0", node_type = "MatMul", op_id = 833 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1742 = "dtu_hlir.dot_general"(%1740, %386) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5472-0", node_type = "MatMul", op_id = 834 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1743 = "dtu_hlir.dot_general"(%1740, %387) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5473-0", node_type = "MatMul", op_id = 835 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1744 = "dtu_hlir.reshape"(%1741) {node_name = "Reshape_5494-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1745 = "dtu_hlir.transpose"(%1744) {node_name = "Transpose_5495-0", node_type = "Transpose", op_id = 836 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1746 = "dtu_hlir.reshape"(%1745) {node_name = "Reshape_5509-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1747 = "dtu_hlir.reshape"(%1742) {node_name = "Reshape_5530-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1748 = "dtu_hlir.transpose"(%1747) {node_name = "Transpose_5531-0", node_type = "Transpose", op_id = 837 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1749 = "dtu_hlir.reshape"(%1748) {node_name = "Reshape_5545-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1750 = "dtu_hlir.reshape"(%1743) {node_name = "Reshape_5566-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1751 = "dtu_hlir.transpose"(%1750) {node_name = "Transpose_5567-0", node_type = "Transpose", op_id = 838 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1752 = "dtu_hlir.reshape"(%1751) {node_name = "Reshape_5581-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1753 = "dtu_hlir.transpose"(%1749) {node_name = "Transpose_5599-0", node_type = "Transpose", op_id = 839 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x1024x64xf16>) -> tensor<20x64x1024xf16>
    %1754 = "dtu_hlir.dot_general"(%1746, %1753) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5600-0", node_type = "MatMul", op_id = 840 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x1024xf16>) -> tensor<20x1024x1024xf16>
    %1755 = "dtu_hlir.fusion"(%267, %1754) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x1024xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1426-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x1024xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5601-0", node_type = "Mul"} : (tensor<20x1024x1024xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x1024xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 841 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %1756 = "dtu_hlir.softmax"(%1755) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5604-0", node_type = "Softmax", op_id = 842 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %1757 = "dtu_hlir.dot_general"(%1756, %1752) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5606-0", node_type = "MatMul", op_id = 843 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x1024xf16>, tensor<20x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1758 = "dtu_hlir.reshape"(%1757) {node_name = "Reshape_5627-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %1759 = "dtu_hlir.transpose"(%1758) {node_name = "Transpose_5628-0", node_type = "Transpose", op_id = 844 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %1760 = "dtu_hlir.reshape"(%1759) {node_name = "Reshape_5642-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %1761 = "dtu_hlir.dot_general_bias"(%1760, %388, %156) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5644-0", op_id = 845 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1762 = "dtu_hlir.add"(%1761, %1739) {node_name = "Add_5645-0", node_type = "Add", op_id = 846 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1763 = "dtu_hlir.layer_norm_inference"(%1762, %162, %163) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_5656-0", op_id = 847 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1764 = "dtu_hlir.dot_general"(%1763, %389) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5657-0", node_type = "MatMul", op_id = 848 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1765 = "dtu_hlir.dot_general"(%4, %390) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5658-0", node_type = "MatMul", op_id = 849 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %1766 = "dtu_hlir.dot_general"(%4, %391) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5659-0", node_type = "MatMul", op_id = 850 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %1767 = "dtu_hlir.reshape"(%1764) {node_name = "Reshape_5680-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1768 = "dtu_hlir.transpose"(%1767) {node_name = "Transpose_5681-0", node_type = "Transpose", op_id = 851 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1769 = "dtu_hlir.reshape"(%1768) {node_name = "Reshape_5695-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1770 = "dtu_hlir.reshape"(%1765) {node_name = "Reshape_5716-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %1771 = "dtu_hlir.transpose"(%1770) {node_name = "Transpose_5717-0", node_type = "Transpose", op_id = 852 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %1772 = "dtu_hlir.reshape"(%1771) {node_name = "Reshape_5731-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %1773 = "dtu_hlir.reshape"(%1766) {node_name = "Reshape_5752-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %1774 = "dtu_hlir.transpose"(%1773) {node_name = "Transpose_5753-0", node_type = "Transpose", op_id = 853 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %1775 = "dtu_hlir.reshape"(%1774) {node_name = "Reshape_5767-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %1776 = "dtu_hlir.transpose"(%1772) {node_name = "Transpose_5785-0", node_type = "Transpose", op_id = 854 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x77x64xf16>) -> tensor<20x64x77xf16>
    %1777 = "dtu_hlir.dot_general"(%1769, %1776) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5786-0", node_type = "MatMul", op_id = 855 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x77xf16>) -> tensor<20x1024x77xf16>
    %1778 = "dtu_hlir.fusion"(%267, %1777) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1612-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5787-0", node_type = "Mul"} : (tensor<20x1024x77xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 856 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %1779 = "dtu_hlir.softmax"(%1778) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_5790-0", node_type = "Softmax", op_id = 857 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %1780 = "dtu_hlir.dot_general"(%1779, %1775) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_5792-0", node_type = "MatMul", op_id = 858 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x77xf16>, tensor<20x77x64xf16>) -> tensor<20x1024x64xf16>
    %1781 = "dtu_hlir.reshape"(%1780) {node_name = "Reshape_5813-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %1782 = "dtu_hlir.transpose"(%1781) {node_name = "Transpose_5814-0", node_type = "Transpose", op_id = 859 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %1783 = "dtu_hlir.reshape"(%1782) {node_name = "Reshape_5828-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %1784 = "dtu_hlir.dot_general_bias"(%1783, %392, %159) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5830-0", op_id = 860 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1785 = "dtu_hlir.add"(%1784, %1762) {node_name = "Add_5831-0", node_type = "Add", op_id = 861 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1786 = "dtu_hlir.layer_norm_inference"(%1785, %164, %165) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_5842-0", op_id = 862 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1787 = "dtu_hlir.dot_general_bias"(%1786, %393, %157) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5844-0", op_id = 863 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x5120xf16>, tensor<5120xf32>) -> tensor<2x1024x5120xf16>
    %1788 = "dtu_hlir.slice"(%1787) {limit_indices = dense<[2, 1024, 2560]> : tensor<3xi64>, node_name = "Slice_5855-1", op_id = 864 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %1789 = "dtu_hlir.slice"(%1787) {limit_indices = dense<[2, 1024, 5120]> : tensor<3xi64>, node_name = "Slice_5858-3", op_id = 865 : i64, start_indices = dense<[0, 0, 2560]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %1790 = "dtu_hlir.gelu"(%1789) {approximate = false, node_name = "Mul_5866-0", op_id = 866 : i64, unique_name = "common20_gelu"} : (tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %1791 = "dtu_hlir.mul"(%1788, %1790) {node_name = "Mul_5867-0", node_type = "Mul", op_id = 867 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x1024x2560xf16>, tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %1792 = "dtu_hlir.dot_general_bias"(%1791, %394, %158) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5869-0", op_id = 868 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x2560xf16>, tensor<2560x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1793 = "dtu_hlir.add"(%1792, %1785) {node_name = "Add_5870-0", node_type = "Add", op_id = 869 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1794 = "dtu_hlir.dot_general_bias"(%1793, %395, %166) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5872-0", op_id = 870 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1795 = "dtu_hlir.reshape"(%1794) {node_name = "Reshape_5882-0"} : (tensor<2x1024x640xf16>) -> tensor<2x32x32x640xf16>
    %1796 = "dtu_hlir.add"(%1795, %1727) {node_name = "Add_5884-0", node_type = "Add", op_id = 871 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1797 = "dtu_hlir.concatenate"(%1796, %774) {dimension = 3 : i64, node_name = "Concat_5885-0", op_id = 872 : i64, unique_name = "concatenate_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x1280xf16>
    %1798 = "dtu_hlir.transpose"(%1797) {node_name = "Transpose_5883-0", op_id = 873 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x1280xf16>) -> tensor<2x1280x32x32xf16>
    %1799 = "dtu_hlir.reshape"(%1798) {node_name = "Reshape_5887-0"} : (tensor<2x1280x32x32xf16>) -> tensor<2x32x40960xf16>
    %1800 = "dtu_hlir.transpose"(%1799) {node_name = "InstanceNormalization_5890-0", op_id = 874 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %1801 = "dtu_hlir.instance_norm"(%1800, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 875 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %1802 = "dtu_hlir.transpose"(%1801) {node_name = "InstanceNormalization_5890-0", op_id = 876 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %1803 = "dtu_hlir.reshape"(%1802) {node_name = "Reshape_5892-0"} : (tensor<2x32x40960xf16>) -> tensor<2x1280x32x32xf16>
    %1804 = dtu_hlir.constant  {node_name = "Mul_5893-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1805 = dtu_hlir.constant  {node_name = "Add_5894-0"} opaque<"", "0xDEADBEEF"> : tensor<2x1280xf16>
    %1806 = "dtu_hlir.fusion"(%1804, %1803, %1805) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<2x1280x32x32xf16>, %arg5: tensor<2x1280xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5893-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5893-0", node_type = "Mul"} : (tensor<2x1280x32x32xf16>, tensor<2x1280x32x32xf16>) -> tensor<2x1280x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_5894-0"} : (tensor<2x1280xf16>) -> tensor<2x1280x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_5894-0", node_type = "Add"} : (tensor<2x1280x32x32xf16>, tensor<2x1280x32x32xf16>) -> tensor<2x1280x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_5896-0"} : (tensor<2x1280x32x32xf16>) -> tensor<2x1280x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x1280x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 877 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x1280xf16>, tensor<2x1280x32x32xf16>, tensor<2x1280xf16>) -> tensor<2x1280x32x32xf16>
    %1807 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x1280x640xf16>
    %1808 = "dtu_hlir.transpose"(%1806) {node_name = "Conv_5897-0", op_id = 878 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1280x32x32xf16>) -> tensor<2x32x32x1280xf16>
    %1809 = "dtu_hlir.conv_bias"(%1808, %1807, %194) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5897-0", op_id = 879 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x1280xf16>, tensor<3x3x1280x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1810 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x640xf16>
    %1811 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %1812 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 880 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1813 = "dtu_hlir.dot_general_bias"(%1812, %1810, %1811) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 881 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x640xf16>, tensor<640xf32>) -> tensor<2x640xf16>
    %1814 = "dtu_hlir.broadcast_in_dim"(%1813) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 882 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x640xf16>) -> tensor<2x32x32x640xf16>
    %1815 = "dtu_hlir.add"(%1809, %1814) {node_name = "Add_5905-0", node_type = "Add", op_id = 883 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1816 = "dtu_hlir.transpose"(%1815) {op_id = 884 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %1817 = "dtu_hlir.reshape"(%1816) {node_name = "Reshape_5907-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %1818 = "dtu_hlir.transpose"(%1817) {node_name = "InstanceNormalization_5910-0", op_id = 885 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1819 = "dtu_hlir.instance_norm"(%1818, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 886 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1820 = "dtu_hlir.transpose"(%1819) {node_name = "InstanceNormalization_5910-0", op_id = 887 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1821 = "dtu_hlir.reshape"(%1820) {node_name = "Reshape_5912-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %1822 = dtu_hlir.constant  {node_name = "Mul_5913-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1823 = dtu_hlir.constant  {node_name = "Add_5914-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1824 = "dtu_hlir.fusion"(%1822, %1821, %1823) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5913-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5913-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_5914-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_5914-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_5916-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 888 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %1825 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %1826 = "dtu_hlir.transpose"(%1824) {node_name = "Conv_5917-0", op_id = 889 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %1827 = "dtu_hlir.conv_bias"(%1826, %1825, %195) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5917-0", op_id = 890 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1828 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.1.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x1280x640xf16>
    %1829 = "dtu_hlir.conv_bias"(%1797, %1828, %196) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_5918-0", op_id = 891 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x1280xf16>, tensor<1x1x1280x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1830 = "dtu_hlir.add"(%1829, %1827) {node_name = "Add_5919-0", node_type = "Add", op_id = 892 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1831 = "dtu_hlir.transpose"(%1830) {node_name = "Conv_5918-0", op_id = 893 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %1832 = "dtu_hlir.reshape"(%1831) {node_name = "Reshape_5932-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %1833 = "dtu_hlir.transpose"(%1832) {node_name = "InstanceNormalization_5935-0", op_id = 894 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1834 = "dtu_hlir.instance_norm"(%1833, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 895 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1835 = "dtu_hlir.transpose"(%1834) {node_name = "InstanceNormalization_5935-0", op_id = 896 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1836 = "dtu_hlir.reshape"(%1835) {node_name = "Reshape_5937-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %1837 = dtu_hlir.constant  {node_name = "Mul_5938-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1838 = dtu_hlir.constant  {node_name = "Add_5939-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1839 = "dtu_hlir.fusion"(%1837, %1836, %1838) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_5938-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_5938-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_5939-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_5939-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 897 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %1840 = "dtu_hlir.transpose"(%1839) {node_name = "Transpose_5943-0", node_type = "Transpose", op_id = 898 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %1841 = "dtu_hlir.reshape"(%1840) {node_name = "Reshape_5952-0"} : (tensor<2x32x32x640xf16>) -> tensor<2x1024x640xf16>
    %1842 = "dtu_hlir.dot_general_bias"(%1841, %396, %167) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_5954-0", op_id = 899 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1843 = "dtu_hlir.layer_norm_inference"(%1842, %172, %173) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_5965-0", op_id = 900 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1844 = "dtu_hlir.dot_general"(%1843, %397) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5966-0", node_type = "MatMul", op_id = 901 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1845 = "dtu_hlir.dot_general"(%1843, %398) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5967-0", node_type = "MatMul", op_id = 902 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1846 = "dtu_hlir.dot_general"(%1843, %399) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_5968-0", node_type = "MatMul", op_id = 903 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1847 = "dtu_hlir.reshape"(%1844) {node_name = "Reshape_5989-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1848 = "dtu_hlir.transpose"(%1847) {node_name = "Transpose_5990-0", node_type = "Transpose", op_id = 904 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1849 = "dtu_hlir.reshape"(%1848) {node_name = "Reshape_6004-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1850 = "dtu_hlir.reshape"(%1845) {node_name = "Reshape_6025-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1851 = "dtu_hlir.transpose"(%1850) {node_name = "Transpose_6026-0", node_type = "Transpose", op_id = 905 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1852 = "dtu_hlir.reshape"(%1851) {node_name = "Reshape_6040-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1853 = "dtu_hlir.reshape"(%1846) {node_name = "Reshape_6061-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1854 = "dtu_hlir.transpose"(%1853) {node_name = "Transpose_6062-0", node_type = "Transpose", op_id = 906 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1855 = "dtu_hlir.reshape"(%1854) {node_name = "Reshape_6076-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1856 = "dtu_hlir.transpose"(%1852) {node_name = "Transpose_6094-0", node_type = "Transpose", op_id = 907 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x1024x64xf16>) -> tensor<20x64x1024xf16>
    %1857 = "dtu_hlir.dot_general"(%1849, %1856) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6095-0", node_type = "MatMul", op_id = 908 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x1024xf16>) -> tensor<20x1024x1024xf16>
    %1858 = "dtu_hlir.fusion"(%267, %1857) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x1024xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1426-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x1024xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6096-0", node_type = "Mul"} : (tensor<20x1024x1024xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x1024xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 909 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %1859 = "dtu_hlir.softmax"(%1858) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6099-0", node_type = "Softmax", op_id = 910 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %1860 = "dtu_hlir.dot_general"(%1859, %1855) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6101-0", node_type = "MatMul", op_id = 911 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x1024xf16>, tensor<20x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1861 = "dtu_hlir.reshape"(%1860) {node_name = "Reshape_6122-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %1862 = "dtu_hlir.transpose"(%1861) {node_name = "Transpose_6123-0", node_type = "Transpose", op_id = 912 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %1863 = "dtu_hlir.reshape"(%1862) {node_name = "Reshape_6137-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %1864 = "dtu_hlir.dot_general_bias"(%1863, %400, %168) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6139-0", op_id = 913 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1865 = "dtu_hlir.add"(%1864, %1842) {node_name = "Add_6140-0", node_type = "Add", op_id = 914 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1866 = "dtu_hlir.layer_norm_inference"(%1865, %174, %175) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_6151-0", op_id = 915 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1867 = "dtu_hlir.dot_general"(%1866, %401) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6152-0", node_type = "MatMul", op_id = 916 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1868 = "dtu_hlir.dot_general"(%4, %402) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6153-0", node_type = "MatMul", op_id = 917 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %1869 = "dtu_hlir.dot_general"(%4, %403) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6154-0", node_type = "MatMul", op_id = 918 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %1870 = "dtu_hlir.reshape"(%1867) {node_name = "Reshape_6175-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1871 = "dtu_hlir.transpose"(%1870) {node_name = "Transpose_6176-0", node_type = "Transpose", op_id = 919 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1872 = "dtu_hlir.reshape"(%1871) {node_name = "Reshape_6190-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1873 = "dtu_hlir.reshape"(%1868) {node_name = "Reshape_6211-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %1874 = "dtu_hlir.transpose"(%1873) {node_name = "Transpose_6212-0", node_type = "Transpose", op_id = 920 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %1875 = "dtu_hlir.reshape"(%1874) {node_name = "Reshape_6226-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %1876 = "dtu_hlir.reshape"(%1869) {node_name = "Reshape_6247-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %1877 = "dtu_hlir.transpose"(%1876) {node_name = "Transpose_6248-0", node_type = "Transpose", op_id = 921 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %1878 = "dtu_hlir.reshape"(%1877) {node_name = "Reshape_6262-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %1879 = "dtu_hlir.transpose"(%1875) {node_name = "Transpose_6280-0", node_type = "Transpose", op_id = 922 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x77x64xf16>) -> tensor<20x64x77xf16>
    %1880 = "dtu_hlir.dot_general"(%1872, %1879) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6281-0", node_type = "MatMul", op_id = 923 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x77xf16>) -> tensor<20x1024x77xf16>
    %1881 = "dtu_hlir.fusion"(%267, %1880) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1612-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6282-0", node_type = "Mul"} : (tensor<20x1024x77xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 924 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %1882 = "dtu_hlir.softmax"(%1881) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6285-0", node_type = "Softmax", op_id = 925 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %1883 = "dtu_hlir.dot_general"(%1882, %1878) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6287-0", node_type = "MatMul", op_id = 926 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x77xf16>, tensor<20x77x64xf16>) -> tensor<20x1024x64xf16>
    %1884 = "dtu_hlir.reshape"(%1883) {node_name = "Reshape_6308-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %1885 = "dtu_hlir.transpose"(%1884) {node_name = "Transpose_6309-0", node_type = "Transpose", op_id = 927 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %1886 = "dtu_hlir.reshape"(%1885) {node_name = "Reshape_6323-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %1887 = "dtu_hlir.dot_general_bias"(%1886, %404, %171) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6325-0", op_id = 928 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1888 = "dtu_hlir.add"(%1887, %1865) {node_name = "Add_6326-0", node_type = "Add", op_id = 929 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1889 = "dtu_hlir.layer_norm_inference"(%1888, %176, %177) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_6337-0", op_id = 930 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1890 = "dtu_hlir.dot_general_bias"(%1889, %405, %169) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6339-0", op_id = 931 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x5120xf16>, tensor<5120xf32>) -> tensor<2x1024x5120xf16>
    %1891 = "dtu_hlir.slice"(%1890) {limit_indices = dense<[2, 1024, 2560]> : tensor<3xi64>, node_name = "Slice_6350-1", op_id = 932 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %1892 = "dtu_hlir.slice"(%1890) {limit_indices = dense<[2, 1024, 5120]> : tensor<3xi64>, node_name = "Slice_6353-3", op_id = 933 : i64, start_indices = dense<[0, 0, 2560]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %1893 = "dtu_hlir.gelu"(%1892) {approximate = false, node_name = "Mul_6361-0", op_id = 934 : i64, unique_name = "common20_gelu"} : (tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %1894 = "dtu_hlir.mul"(%1891, %1893) {node_name = "Mul_6362-0", node_type = "Mul", op_id = 935 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x1024x2560xf16>, tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %1895 = "dtu_hlir.dot_general_bias"(%1894, %406, %170) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6364-0", op_id = 936 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x2560xf16>, tensor<2560x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1896 = "dtu_hlir.add"(%1895, %1888) {node_name = "Add_6365-0", node_type = "Add", op_id = 937 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1897 = "dtu_hlir.dot_general_bias"(%1896, %407, %178) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6367-0", op_id = 938 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1898 = "dtu_hlir.reshape"(%1897) {node_name = "Reshape_6377-0"} : (tensor<2x1024x640xf16>) -> tensor<2x32x32x640xf16>
    %1899 = "dtu_hlir.add"(%1898, %1830) {node_name = "Add_6379-0", node_type = "Add", op_id = 939 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1900 = "dtu_hlir.concatenate"(%1899, %672) {dimension = 3 : i64, node_name = "Concat_6380-0", op_id = 940 : i64, unique_name = "concatenate_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x320xf16>) -> tensor<2x32x32x960xf16>
    %1901 = "dtu_hlir.transpose"(%1900) {node_name = "Transpose_6378-0", op_id = 941 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x960xf16>) -> tensor<2x960x32x32xf16>
    %1902 = "dtu_hlir.reshape"(%1901) {node_name = "Reshape_6382-0"} : (tensor<2x960x32x32xf16>) -> tensor<2x32x30720xf16>
    %1903 = "dtu_hlir.transpose"(%1902) {node_name = "InstanceNormalization_6385-0", op_id = 942 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x30720xf16>) -> tensor<2x30720x32xf16>
    %1904 = "dtu_hlir.instance_norm"(%1903, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 943 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x30720x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x30720x32xf16>
    %1905 = "dtu_hlir.transpose"(%1904) {node_name = "InstanceNormalization_6385-0", op_id = 944 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x30720x32xf16>) -> tensor<2x32x30720xf16>
    %1906 = "dtu_hlir.reshape"(%1905) {node_name = "Reshape_6387-0"} : (tensor<2x32x30720xf16>) -> tensor<2x960x32x32xf16>
    %1907 = dtu_hlir.constant  {node_name = "Mul_6388-0"} opaque<"", "0xDEADBEEF"> : tensor<2x960xf16>
    %1908 = dtu_hlir.constant  {node_name = "Add_6389-0"} opaque<"", "0xDEADBEEF"> : tensor<2x960xf16>
    %1909 = "dtu_hlir.fusion"(%1907, %1906, %1908) ( {
    ^bb0(%arg3: tensor<2x960xf16>, %arg4: tensor<2x960x32x32xf16>, %arg5: tensor<2x960xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6388-0"} : (tensor<2x960xf16>) -> tensor<2x960x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6388-0", node_type = "Mul"} : (tensor<2x960x32x32xf16>, tensor<2x960x32x32xf16>) -> tensor<2x960x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_6389-0"} : (tensor<2x960xf16>) -> tensor<2x960x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_6389-0", node_type = "Add"} : (tensor<2x960x32x32xf16>, tensor<2x960x32x32xf16>) -> tensor<2x960x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_6391-0"} : (tensor<2x960x32x32xf16>) -> tensor<2x960x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x960x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 945 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x960xf16>, tensor<2x960x32x32xf16>, tensor<2x960xf16>) -> tensor<2x960x32x32xf16>
    %1910 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x960x640xf16>
    %1911 = "dtu_hlir.transpose"(%1909) {node_name = "Conv_6392-0", op_id = 946 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x960x32x32xf16>) -> tensor<2x32x32x960xf16>
    %1912 = "dtu_hlir.conv_bias"(%1911, %1910, %197) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6392-0", op_id = 947 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x960xf16>, tensor<3x3x960x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1913 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x640xf16>
    %1914 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<640xf32>
    %1915 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 948 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %1916 = "dtu_hlir.dot_general_bias"(%1915, %1913, %1914) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 949 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x640xf16>, tensor<640xf32>) -> tensor<2x640xf16>
    %1917 = "dtu_hlir.broadcast_in_dim"(%1916) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 950 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x640xf16>) -> tensor<2x32x32x640xf16>
    %1918 = "dtu_hlir.add"(%1912, %1917) {node_name = "Add_6400-0", node_type = "Add", op_id = 951 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1919 = "dtu_hlir.transpose"(%1918) {op_id = 952 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %1920 = "dtu_hlir.reshape"(%1919) {node_name = "Reshape_6402-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %1921 = "dtu_hlir.transpose"(%1920) {node_name = "InstanceNormalization_6405-0", op_id = 953 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1922 = "dtu_hlir.instance_norm"(%1921, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 954 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1923 = "dtu_hlir.transpose"(%1922) {node_name = "InstanceNormalization_6405-0", op_id = 955 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1924 = "dtu_hlir.reshape"(%1923) {node_name = "Reshape_6407-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %1925 = dtu_hlir.constant  {node_name = "Mul_6408-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1926 = dtu_hlir.constant  {node_name = "Add_6409-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1927 = "dtu_hlir.fusion"(%1925, %1924, %1926) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6408-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6408-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_6409-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_6409-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_6411-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 956 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %1928 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %1929 = "dtu_hlir.transpose"(%1927) {node_name = "Conv_6412-0", op_id = 957 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %1930 = "dtu_hlir.conv_bias"(%1929, %1928, %198) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6412-0", op_id = 958 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1931 = dtu_hlir.constant  {node_name = "up_blocks.2.resnets.2.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x960x640xf16>
    %1932 = "dtu_hlir.conv_bias"(%1900, %1931, %199) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6413-0", op_id = 959 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x32x32x960xf16>, tensor<1x1x960x640xf16>, tensor<640xf32>) -> tensor<2x32x32x640xf16>
    %1933 = "dtu_hlir.add"(%1932, %1930) {node_name = "Add_6414-0", node_type = "Add", op_id = 960 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %1934 = "dtu_hlir.transpose"(%1933) {node_name = "Conv_6413-0", op_id = 961 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x32x640xf16>) -> tensor<2x640x32x32xf16>
    %1935 = "dtu_hlir.reshape"(%1934) {node_name = "Reshape_6427-0"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x20480xf16>
    %1936 = "dtu_hlir.transpose"(%1935) {node_name = "InstanceNormalization_6430-0", op_id = 962 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x20480xf16>) -> tensor<2x20480x32xf16>
    %1937 = "dtu_hlir.instance_norm"(%1936, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 963 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x20480x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x20480x32xf16>
    %1938 = "dtu_hlir.transpose"(%1937) {node_name = "InstanceNormalization_6430-0", op_id = 964 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x20480x32xf16>) -> tensor<2x32x20480xf16>
    %1939 = "dtu_hlir.reshape"(%1938) {node_name = "Reshape_6432-0"} : (tensor<2x32x20480xf16>) -> tensor<2x640x32x32xf16>
    %1940 = dtu_hlir.constant  {node_name = "Mul_6433-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1941 = dtu_hlir.constant  {node_name = "Add_6434-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %1942 = "dtu_hlir.fusion"(%1940, %1939, %1941) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x32x32xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6433-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6433-0", node_type = "Mul"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_6434-0"} : (tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_6434-0", node_type = "Add"} : (tensor<2x640x32x32xf16>, tensor<2x640x32x32xf16>) -> tensor<2x640x32x32xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x640x32x32xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 965 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x32x32xf16>, tensor<2x640xf16>) -> tensor<2x640x32x32xf16>
    %1943 = "dtu_hlir.transpose"(%1942) {node_name = "Transpose_6438-0", node_type = "Transpose", op_id = 966 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x32x32xf16>) -> tensor<2x32x32x640xf16>
    %1944 = "dtu_hlir.reshape"(%1943) {node_name = "Reshape_6447-0"} : (tensor<2x32x32x640xf16>) -> tensor<2x1024x640xf16>
    %1945 = "dtu_hlir.dot_general_bias"(%1944, %408, %179) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6449-0", op_id = 967 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1946 = "dtu_hlir.layer_norm_inference"(%1945, %184, %185) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_6460-0", op_id = 968 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1947 = "dtu_hlir.dot_general"(%1946, %409) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6461-0", node_type = "MatMul", op_id = 969 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1948 = "dtu_hlir.dot_general"(%1946, %410) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6462-0", node_type = "MatMul", op_id = 970 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1949 = "dtu_hlir.dot_general"(%1946, %411) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6463-0", node_type = "MatMul", op_id = 971 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1950 = "dtu_hlir.reshape"(%1947) {node_name = "Reshape_6484-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1951 = "dtu_hlir.transpose"(%1950) {node_name = "Transpose_6485-0", node_type = "Transpose", op_id = 972 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1952 = "dtu_hlir.reshape"(%1951) {node_name = "Reshape_6499-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1953 = "dtu_hlir.reshape"(%1948) {node_name = "Reshape_6520-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1954 = "dtu_hlir.transpose"(%1953) {node_name = "Transpose_6521-0", node_type = "Transpose", op_id = 973 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1955 = "dtu_hlir.reshape"(%1954) {node_name = "Reshape_6535-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1956 = "dtu_hlir.reshape"(%1949) {node_name = "Reshape_6556-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1957 = "dtu_hlir.transpose"(%1956) {node_name = "Transpose_6557-0", node_type = "Transpose", op_id = 974 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1958 = "dtu_hlir.reshape"(%1957) {node_name = "Reshape_6571-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1959 = "dtu_hlir.transpose"(%1955) {node_name = "Transpose_6589-0", node_type = "Transpose", op_id = 975 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x1024x64xf16>) -> tensor<20x64x1024xf16>
    %1960 = "dtu_hlir.dot_general"(%1952, %1959) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6590-0", node_type = "MatMul", op_id = 976 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x1024xf16>) -> tensor<20x1024x1024xf16>
    %1961 = "dtu_hlir.fusion"(%267, %1960) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x1024xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1426-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x1024xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6591-0", node_type = "Mul"} : (tensor<20x1024x1024xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x1024xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 977 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %1962 = "dtu_hlir.softmax"(%1961) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6594-0", node_type = "Softmax", op_id = 978 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x1024xf16>) -> tensor<20x1024x1024xf16>
    %1963 = "dtu_hlir.dot_general"(%1962, %1958) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6596-0", node_type = "MatMul", op_id = 979 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x1024xf16>, tensor<20x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1964 = "dtu_hlir.reshape"(%1963) {node_name = "Reshape_6617-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %1965 = "dtu_hlir.transpose"(%1964) {node_name = "Transpose_6618-0", node_type = "Transpose", op_id = 980 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %1966 = "dtu_hlir.reshape"(%1965) {node_name = "Reshape_6632-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %1967 = "dtu_hlir.dot_general_bias"(%1966, %412, %180) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6634-0", op_id = 981 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1968 = "dtu_hlir.add"(%1967, %1945) {node_name = "Add_6635-0", node_type = "Add", op_id = 982 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1969 = "dtu_hlir.layer_norm_inference"(%1968, %186, %187) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_6646-0", op_id = 983 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1970 = "dtu_hlir.dot_general"(%1969, %413) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6647-0", node_type = "MatMul", op_id = 984 : i64, unique_name = "dot_general_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>) -> tensor<2x1024x640xf16>
    %1971 = "dtu_hlir.dot_general"(%4, %414) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6648-0", node_type = "MatMul", op_id = 985 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %1972 = "dtu_hlir.dot_general"(%4, %415) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6649-0", node_type = "MatMul", op_id = 986 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x640xf16>) -> tensor<2x77x640xf16>
    %1973 = "dtu_hlir.reshape"(%1970) {node_name = "Reshape_6670-0"} : (tensor<2x1024x640xf16>) -> tensor<2x1024x10x64xf16>
    %1974 = "dtu_hlir.transpose"(%1973) {node_name = "Transpose_6671-0", node_type = "Transpose", op_id = 987 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x1024x10x64xf16>) -> tensor<2x10x1024x64xf16>
    %1975 = "dtu_hlir.reshape"(%1974) {node_name = "Reshape_6685-0"} : (tensor<2x10x1024x64xf16>) -> tensor<20x1024x64xf16>
    %1976 = "dtu_hlir.reshape"(%1971) {node_name = "Reshape_6706-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %1977 = "dtu_hlir.transpose"(%1976) {node_name = "Transpose_6707-0", node_type = "Transpose", op_id = 988 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %1978 = "dtu_hlir.reshape"(%1977) {node_name = "Reshape_6721-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %1979 = "dtu_hlir.reshape"(%1972) {node_name = "Reshape_6742-0"} : (tensor<2x77x640xf16>) -> tensor<2x77x10x64xf16>
    %1980 = "dtu_hlir.transpose"(%1979) {node_name = "Transpose_6743-0", node_type = "Transpose", op_id = 989 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x10x64xf16>) -> tensor<2x10x77x64xf16>
    %1981 = "dtu_hlir.reshape"(%1980) {node_name = "Reshape_6757-0"} : (tensor<2x10x77x64xf16>) -> tensor<20x77x64xf16>
    %1982 = "dtu_hlir.transpose"(%1978) {node_name = "Transpose_6775-0", node_type = "Transpose", op_id = 990 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<20x77x64xf16>) -> tensor<20x64x77xf16>
    %1983 = "dtu_hlir.dot_general"(%1975, %1982) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6776-0", node_type = "MatMul", op_id = 991 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x64xf16>, tensor<20x64x77xf16>) -> tensor<20x1024x77xf16>
    %1984 = "dtu_hlir.fusion"(%267, %1983) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<20x1024x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_1612-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<20x1024x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6777-0", node_type = "Mul"} : (tensor<20x1024x77xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<20x1024x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 992 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %1985 = "dtu_hlir.softmax"(%1984) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_6780-0", node_type = "Softmax", op_id = 993 : i64, unique_name = "common20_softmax"} : (tensor<20x1024x77xf16>) -> tensor<20x1024x77xf16>
    %1986 = "dtu_hlir.dot_general"(%1985, %1981) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_6782-0", node_type = "MatMul", op_id = 994 : i64, unique_name = "dot_general_dtu"} : (tensor<20x1024x77xf16>, tensor<20x77x64xf16>) -> tensor<20x1024x64xf16>
    %1987 = "dtu_hlir.reshape"(%1986) {node_name = "Reshape_6803-0"} : (tensor<20x1024x64xf16>) -> tensor<2x10x1024x64xf16>
    %1988 = "dtu_hlir.transpose"(%1987) {node_name = "Transpose_6804-0", node_type = "Transpose", op_id = 995 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x10x1024x64xf16>) -> tensor<2x1024x10x64xf16>
    %1989 = "dtu_hlir.reshape"(%1988) {node_name = "Reshape_6818-0"} : (tensor<2x1024x10x64xf16>) -> tensor<2x1024x640xf16>
    %1990 = "dtu_hlir.dot_general_bias"(%1989, %416, %183) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6820-0", op_id = 996 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1991 = "dtu_hlir.add"(%1990, %1968) {node_name = "Add_6821-0", node_type = "Add", op_id = 997 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %1992 = "dtu_hlir.layer_norm_inference"(%1991, %188, %189) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_6832-0", op_id = 998 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x1024x640xf16>, tensor<640xf32>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1993 = "dtu_hlir.dot_general_bias"(%1992, %417, %181) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6834-0", op_id = 999 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x5120xf16>, tensor<5120xf32>) -> tensor<2x1024x5120xf16>
    %1994 = "dtu_hlir.slice"(%1993) {limit_indices = dense<[2, 1024, 2560]> : tensor<3xi64>, node_name = "Slice_6845-1", op_id = 1000 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %1995 = "dtu_hlir.slice"(%1993) {limit_indices = dense<[2, 1024, 5120]> : tensor<3xi64>, node_name = "Slice_6848-3", op_id = 1001 : i64, start_indices = dense<[0, 0, 2560]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x1024x5120xf16>) -> tensor<2x1024x2560xf16>
    %1996 = "dtu_hlir.gelu"(%1995) {approximate = false, node_name = "Mul_6856-0", op_id = 1002 : i64, unique_name = "common20_gelu"} : (tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %1997 = "dtu_hlir.mul"(%1994, %1996) {node_name = "Mul_6857-0", node_type = "Mul", op_id = 1003 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x1024x2560xf16>, tensor<2x1024x2560xf16>) -> tensor<2x1024x2560xf16>
    %1998 = "dtu_hlir.dot_general_bias"(%1997, %418, %182) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6859-0", op_id = 1004 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x2560xf16>, tensor<2560x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %1999 = "dtu_hlir.add"(%1998, %1991) {node_name = "Add_6860-0", node_type = "Add", op_id = 1005 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x1024x640xf16>, tensor<2x1024x640xf16>) -> tensor<2x1024x640xf16>
    %2000 = "dtu_hlir.dot_general_bias"(%1999, %419, %190) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6862-0", op_id = 1006 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1024x640xf16>, tensor<640x640xf16>, tensor<640xf32>) -> tensor<2x1024x640xf16>
    %2001 = "dtu_hlir.reshape"(%2000) {node_name = "Reshape_6872-0"} : (tensor<2x1024x640xf16>) -> tensor<2x32x32x640xf16>
    %2002 = "dtu_hlir.add"(%2001, %1933) {node_name = "Add_6874-0", node_type = "Add", op_id = 1007 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x32x32x640xf16>, tensor<2x32x32x640xf16>) -> tensor<2x32x32x640xf16>
    %2003 = "dtu_hlir.resize"(%2002, %1377, %1378, %1376) {coordinate_transformation_mode = 1 : i64, cubic_coeff_a = -7.500000e-01 : f32, exclude_outside = false, extrapolation_value = 0.000000e+00 : f32, mode = 0 : i64, nearest_mode = 3 : i64, node_name = "Resize_6875-4", op_id = 1008 : i64, resize_dimensions = dense<[1, 2]> : tensor<2xi64>, unique_name = "resize_dtu"} : (tensor<2x32x32x640xf16>, tensor<8xf32>, tensor<4xf32>, tensor<0xi32>) -> tensor<2x64x64x640xf16>
    %2004 = dtu_hlir.constant  {node_name = "up_blocks.2.upsamplers.0.conv.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x640xf16>
    %2005 = "dtu_hlir.conv_bias"(%2003, %2004, %200) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6876-0", op_id = 1009 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x640xf16>, tensor<3x3x640x640xf16>, tensor<640xf32>) -> tensor<2x64x64x640xf16>
    %2006 = "dtu_hlir.concatenate"(%2005, %670) {dimension = 3 : i64, node_name = "Concat_6877-0", op_id = 1010 : i64, unique_name = "concatenate_dtu"} : (tensor<2x64x64x640xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x960xf16>
    %2007 = "dtu_hlir.transpose"(%2006) {node_name = "Conv_6876-0", op_id = 1011 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x960xf16>) -> tensor<2x960x64x64xf16>
    %2008 = "dtu_hlir.reshape"(%2007) {node_name = "Reshape_6879-0"} : (tensor<2x960x64x64xf16>) -> tensor<2x32x122880xf16>
    %2009 = "dtu_hlir.transpose"(%2008) {node_name = "InstanceNormalization_6882-0", op_id = 1012 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x122880xf16>) -> tensor<2x122880x32xf16>
    %2010 = "dtu_hlir.instance_norm"(%2009, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1013 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x122880x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x122880x32xf16>
    %2011 = "dtu_hlir.transpose"(%2010) {node_name = "InstanceNormalization_6882-0", op_id = 1014 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x122880x32xf16>) -> tensor<2x32x122880xf16>
    %2012 = "dtu_hlir.reshape"(%2011) {node_name = "Reshape_6884-0"} : (tensor<2x32x122880xf16>) -> tensor<2x960x64x64xf16>
    %2013 = dtu_hlir.constant  {node_name = "Mul_6885-0"} opaque<"", "0xDEADBEEF"> : tensor<2x960xf16>
    %2014 = dtu_hlir.constant  {node_name = "Add_6886-0"} opaque<"", "0xDEADBEEF"> : tensor<2x960xf16>
    %2015 = "dtu_hlir.fusion"(%2013, %2012, %2014) ( {
    ^bb0(%arg3: tensor<2x960xf16>, %arg4: tensor<2x960x64x64xf16>, %arg5: tensor<2x960xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6885-0"} : (tensor<2x960xf16>) -> tensor<2x960x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6885-0", node_type = "Mul"} : (tensor<2x960x64x64xf16>, tensor<2x960x64x64xf16>) -> tensor<2x960x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_6886-0"} : (tensor<2x960xf16>) -> tensor<2x960x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_6886-0", node_type = "Add"} : (tensor<2x960x64x64xf16>, tensor<2x960x64x64xf16>) -> tensor<2x960x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_6888-0"} : (tensor<2x960x64x64xf16>) -> tensor<2x960x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x960x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1015 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x960xf16>, tensor<2x960x64x64xf16>, tensor<2x960xf16>) -> tensor<2x960x64x64xf16>
    %2016 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x960x320xf16>
    %2017 = "dtu_hlir.transpose"(%2015) {node_name = "Conv_6889-0", op_id = 1016 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x960x64x64xf16>) -> tensor<2x64x64x960xf16>
    %2018 = "dtu_hlir.conv_bias"(%2017, %2016, %237) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6889-0", op_id = 1017 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x960xf16>, tensor<3x3x960x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2019 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %2020 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %2021 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 1018 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %2022 = "dtu_hlir.dot_general_bias"(%2021, %2019, %2020) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 1019 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x320xf16>
    %2023 = "dtu_hlir.broadcast_in_dim"(%2022) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 1020 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x320xf16>) -> tensor<2x64x64x320xf16>
    %2024 = "dtu_hlir.add"(%2018, %2023) {node_name = "Add_6897-0", node_type = "Add", op_id = 1021 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2025 = "dtu_hlir.transpose"(%2024) {op_id = 1022 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %2026 = "dtu_hlir.reshape"(%2025) {node_name = "Reshape_6899-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %2027 = "dtu_hlir.transpose"(%2026) {node_name = "InstanceNormalization_6902-0", op_id = 1023 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %2028 = "dtu_hlir.instance_norm"(%2027, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1024 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %2029 = "dtu_hlir.transpose"(%2028) {node_name = "InstanceNormalization_6902-0", op_id = 1025 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %2030 = "dtu_hlir.reshape"(%2029) {node_name = "Reshape_6904-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %2031 = dtu_hlir.constant  {node_name = "Mul_6905-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2032 = dtu_hlir.constant  {node_name = "Add_6906-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2033 = "dtu_hlir.fusion"(%2031, %2030, %2032) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6905-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6905-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_6906-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_6906-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_6908-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1026 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %2034 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %2035 = "dtu_hlir.transpose"(%2033) {node_name = "Conv_6909-0", op_id = 1027 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %2036 = "dtu_hlir.conv_bias"(%2035, %2034, %238) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6909-0", op_id = 1028 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2037 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.0.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x960x320xf16>
    %2038 = "dtu_hlir.conv_bias"(%2006, %2037, %239) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_6910-0", op_id = 1029 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x960xf16>, tensor<1x1x960x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2039 = "dtu_hlir.add"(%2038, %2036) {node_name = "Add_6911-0", node_type = "Add", op_id = 1030 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2040 = "dtu_hlir.transpose"(%2039) {node_name = "Conv_6910-0", op_id = 1031 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %2041 = "dtu_hlir.reshape"(%2040) {node_name = "Reshape_6924-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %2042 = "dtu_hlir.transpose"(%2041) {node_name = "InstanceNormalization_6927-0", op_id = 1032 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %2043 = "dtu_hlir.instance_norm"(%2042, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1033 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %2044 = "dtu_hlir.transpose"(%2043) {node_name = "InstanceNormalization_6927-0", op_id = 1034 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %2045 = "dtu_hlir.reshape"(%2044) {node_name = "Reshape_6929-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %2046 = dtu_hlir.constant  {node_name = "Mul_6930-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2047 = dtu_hlir.constant  {node_name = "Add_6931-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2048 = "dtu_hlir.fusion"(%2046, %2045, %2047) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_6930-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_6930-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_6931-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_6931-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1035 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %2049 = "dtu_hlir.transpose"(%2048) {node_name = "Transpose_6935-0", node_type = "Transpose", op_id = 1036 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %2050 = "dtu_hlir.reshape"(%2049) {node_name = "Reshape_6944-0"} : (tensor<2x64x64x320xf16>) -> tensor<2x4096x320xf16>
    %2051 = "dtu_hlir.dot_general_bias"(%2050, %420, %201) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_6946-0", op_id = 1037 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2052 = "dtu_hlir.layer_norm_inference"(%2051, %206, %207) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_6957-0", op_id = 1038 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2053 = "dtu_hlir.dot_general"(%2052, %421) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6958-0", node_type = "MatMul", op_id = 1039 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2054 = "dtu_hlir.dot_general"(%2052, %422) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6959-0", node_type = "MatMul", op_id = 1040 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2055 = "dtu_hlir.dot_general"(%2052, %423) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_6960-0", node_type = "MatMul", op_id = 1041 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2056 = "dtu_hlir.reshape"(%2053) {node_name = "Reshape_6981-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2057 = "dtu_hlir.transpose"(%2056) {node_name = "Transpose_6982-0", node_type = "Transpose", op_id = 1042 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2058 = "dtu_hlir.reshape"(%2057) {node_name = "Reshape_6996-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2059 = "dtu_hlir.reshape"(%2054) {node_name = "Reshape_7017-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2060 = "dtu_hlir.transpose"(%2059) {node_name = "Transpose_7018-0", node_type = "Transpose", op_id = 1043 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2061 = "dtu_hlir.reshape"(%2060) {node_name = "Reshape_7032-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2062 = "dtu_hlir.reshape"(%2055) {node_name = "Reshape_7053-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2063 = "dtu_hlir.transpose"(%2062) {node_name = "Transpose_7054-0", node_type = "Transpose", op_id = 1044 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2064 = "dtu_hlir.reshape"(%2063) {node_name = "Reshape_7068-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2065 = "dtu_hlir.transpose"(%2061) {node_name = "Transpose_7086-0", node_type = "Transpose", op_id = 1045 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x4096x64xf16>) -> tensor<10x64x4096xf16>
    %2066 = "dtu_hlir.dot_general"(%2058, %2065) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7087-0", node_type = "MatMul", op_id = 1046 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x4096xf16>) -> tensor<10x4096x4096xf16>
    %2067 = "dtu_hlir.fusion"(%267, %2066) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x4096xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_438-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x4096xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7088-0", node_type = "Mul"} : (tensor<10x4096x4096xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x4096xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1047 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %2068 = "dtu_hlir.softmax"(%2067) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7091-0", node_type = "Softmax", op_id = 1048 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %2069 = "dtu_hlir.dot_general"(%2068, %2064) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7093-0", node_type = "MatMul", op_id = 1049 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x4096xf16>, tensor<10x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2070 = "dtu_hlir.reshape"(%2069) {node_name = "Reshape_7114-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %2071 = "dtu_hlir.transpose"(%2070) {node_name = "Transpose_7115-0", node_type = "Transpose", op_id = 1050 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %2072 = "dtu_hlir.reshape"(%2071) {node_name = "Reshape_7129-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %2073 = "dtu_hlir.dot_general_bias"(%2072, %424, %202) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7131-0", op_id = 1051 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2074 = "dtu_hlir.add"(%2073, %2051) {node_name = "Add_7132-0", node_type = "Add", op_id = 1052 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2075 = "dtu_hlir.layer_norm_inference"(%2074, %208, %209) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_7143-0", op_id = 1053 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2076 = "dtu_hlir.dot_general"(%2075, %425) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7144-0", node_type = "MatMul", op_id = 1054 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2077 = "dtu_hlir.dot_general"(%4, %426) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7145-0", node_type = "MatMul", op_id = 1055 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %2078 = "dtu_hlir.dot_general"(%4, %427) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7146-0", node_type = "MatMul", op_id = 1056 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %2079 = "dtu_hlir.reshape"(%2076) {node_name = "Reshape_7167-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2080 = "dtu_hlir.transpose"(%2079) {node_name = "Transpose_7168-0", node_type = "Transpose", op_id = 1057 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2081 = "dtu_hlir.reshape"(%2080) {node_name = "Reshape_7182-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2082 = "dtu_hlir.reshape"(%2077) {node_name = "Reshape_7203-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %2083 = "dtu_hlir.transpose"(%2082) {node_name = "Transpose_7204-0", node_type = "Transpose", op_id = 1058 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %2084 = "dtu_hlir.reshape"(%2083) {node_name = "Reshape_7218-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %2085 = "dtu_hlir.reshape"(%2078) {node_name = "Reshape_7239-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %2086 = "dtu_hlir.transpose"(%2085) {node_name = "Transpose_7240-0", node_type = "Transpose", op_id = 1059 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %2087 = "dtu_hlir.reshape"(%2086) {node_name = "Reshape_7254-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %2088 = "dtu_hlir.transpose"(%2084) {node_name = "Transpose_7272-0", node_type = "Transpose", op_id = 1060 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x77x64xf16>) -> tensor<10x64x77xf16>
    %2089 = "dtu_hlir.dot_general"(%2081, %2088) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7273-0", node_type = "MatMul", op_id = 1061 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x77xf16>) -> tensor<10x4096x77xf16>
    %2090 = "dtu_hlir.fusion"(%267, %2089) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_624-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7274-0", node_type = "Mul"} : (tensor<10x4096x77xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1062 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %2091 = "dtu_hlir.softmax"(%2090) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7277-0", node_type = "Softmax", op_id = 1063 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %2092 = "dtu_hlir.dot_general"(%2091, %2087) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7279-0", node_type = "MatMul", op_id = 1064 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x77xf16>, tensor<10x77x64xf16>) -> tensor<10x4096x64xf16>
    %2093 = "dtu_hlir.reshape"(%2092) {node_name = "Reshape_7300-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %2094 = "dtu_hlir.transpose"(%2093) {node_name = "Transpose_7301-0", node_type = "Transpose", op_id = 1065 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %2095 = "dtu_hlir.reshape"(%2094) {node_name = "Reshape_7315-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %2096 = "dtu_hlir.dot_general_bias"(%2095, %428, %205) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7317-0", op_id = 1066 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2097 = "dtu_hlir.add"(%2096, %2074) {node_name = "Add_7318-0", node_type = "Add", op_id = 1067 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2098 = "dtu_hlir.layer_norm_inference"(%2097, %210, %211) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_7329-0", op_id = 1068 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2099 = "dtu_hlir.dot_general_bias"(%2098, %429, %203) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7331-0", op_id = 1069 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x2560xf16>, tensor<2560xf32>) -> tensor<2x4096x2560xf16>
    %2100 = "dtu_hlir.slice"(%2099) {limit_indices = dense<[2, 4096, 1280]> : tensor<3xi64>, node_name = "Slice_7342-1", op_id = 1070 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %2101 = "dtu_hlir.slice"(%2099) {limit_indices = dense<[2, 4096, 2560]> : tensor<3xi64>, node_name = "Slice_7345-3", op_id = 1071 : i64, start_indices = dense<[0, 0, 1280]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %2102 = "dtu_hlir.gelu"(%2101) {approximate = false, node_name = "Mul_7353-0", op_id = 1072 : i64, unique_name = "common20_gelu"} : (tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %2103 = "dtu_hlir.mul"(%2100, %2102) {node_name = "Mul_7354-0", node_type = "Mul", op_id = 1073 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x4096x1280xf16>, tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %2104 = "dtu_hlir.dot_general_bias"(%2103, %430, %204) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7356-0", op_id = 1074 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2105 = "dtu_hlir.add"(%2104, %2097) {node_name = "Add_7357-0", node_type = "Add", op_id = 1075 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2106 = "dtu_hlir.dot_general_bias"(%2105, %431, %212) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7359-0", op_id = 1076 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2107 = "dtu_hlir.reshape"(%2106) {node_name = "Reshape_7369-0"} : (tensor<2x4096x320xf16>) -> tensor<2x64x64x320xf16>
    %2108 = "dtu_hlir.add"(%2107, %2039) {node_name = "Add_7371-0", node_type = "Add", op_id = 1077 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2109 = "dtu_hlir.concatenate"(%2108, %570) {dimension = 3 : i64, node_name = "Concat_7372-0", op_id = 1078 : i64, unique_name = "concatenate_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x640xf16>
    %2110 = "dtu_hlir.transpose"(%2109) {node_name = "Transpose_7370-0", op_id = 1079 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x640xf16>) -> tensor<2x640x64x64xf16>
    %2111 = "dtu_hlir.reshape"(%2110) {node_name = "Reshape_7374-0"} : (tensor<2x640x64x64xf16>) -> tensor<2x32x81920xf16>
    %2112 = "dtu_hlir.transpose"(%2111) {node_name = "InstanceNormalization_7377-0", op_id = 1080 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x81920xf16>) -> tensor<2x81920x32xf16>
    %2113 = "dtu_hlir.instance_norm"(%2112, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1081 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x81920x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x81920x32xf16>
    %2114 = "dtu_hlir.transpose"(%2113) {node_name = "InstanceNormalization_7377-0", op_id = 1082 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x81920x32xf16>) -> tensor<2x32x81920xf16>
    %2115 = "dtu_hlir.reshape"(%2114) {node_name = "Reshape_7379-0"} : (tensor<2x32x81920xf16>) -> tensor<2x640x64x64xf16>
    %2116 = dtu_hlir.constant  {node_name = "Mul_7380-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %2117 = dtu_hlir.constant  {node_name = "Add_7381-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %2118 = "dtu_hlir.fusion"(%2116, %2115, %2117) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x64x64xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7380-0"} : (tensor<2x640xf16>) -> tensor<2x640x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7380-0", node_type = "Mul"} : (tensor<2x640x64x64xf16>, tensor<2x640x64x64xf16>) -> tensor<2x640x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_7381-0"} : (tensor<2x640xf16>) -> tensor<2x640x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_7381-0", node_type = "Add"} : (tensor<2x640x64x64xf16>, tensor<2x640x64x64xf16>) -> tensor<2x640x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_7383-0"} : (tensor<2x640x64x64xf16>) -> tensor<2x640x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1083 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x64x64xf16>, tensor<2x640xf16>) -> tensor<2x640x64x64xf16>
    %2119 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x320xf16>
    %2120 = "dtu_hlir.transpose"(%2118) {node_name = "Conv_7384-0", op_id = 1084 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x64x64xf16>) -> tensor<2x64x64x640xf16>
    %2121 = "dtu_hlir.conv_bias"(%2120, %2119, %240) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7384-0", op_id = 1085 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x640xf16>, tensor<3x3x640x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2122 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %2123 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %2124 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 1086 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %2125 = "dtu_hlir.dot_general_bias"(%2124, %2122, %2123) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 1087 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x320xf16>
    %2126 = "dtu_hlir.broadcast_in_dim"(%2125) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 1088 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x320xf16>) -> tensor<2x64x64x320xf16>
    %2127 = "dtu_hlir.add"(%2121, %2126) {node_name = "Add_7392-0", node_type = "Add", op_id = 1089 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2128 = "dtu_hlir.transpose"(%2127) {op_id = 1090 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %2129 = "dtu_hlir.reshape"(%2128) {node_name = "Reshape_7394-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %2130 = "dtu_hlir.transpose"(%2129) {node_name = "InstanceNormalization_7397-0", op_id = 1091 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %2131 = "dtu_hlir.instance_norm"(%2130, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1092 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %2132 = "dtu_hlir.transpose"(%2131) {node_name = "InstanceNormalization_7397-0", op_id = 1093 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %2133 = "dtu_hlir.reshape"(%2132) {node_name = "Reshape_7399-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %2134 = dtu_hlir.constant  {node_name = "Mul_7400-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2135 = dtu_hlir.constant  {node_name = "Add_7401-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2136 = "dtu_hlir.fusion"(%2134, %2133, %2135) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7400-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7400-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_7401-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_7401-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_7403-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1094 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %2137 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %2138 = "dtu_hlir.transpose"(%2136) {node_name = "Conv_7404-0", op_id = 1095 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %2139 = "dtu_hlir.conv_bias"(%2138, %2137, %241) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7404-0", op_id = 1096 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2140 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.1.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x640x320xf16>
    %2141 = "dtu_hlir.conv_bias"(%2109, %2140, %242) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7405-0", op_id = 1097 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x640xf16>, tensor<1x1x640x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2142 = "dtu_hlir.add"(%2141, %2139) {node_name = "Add_7406-0", node_type = "Add", op_id = 1098 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2143 = "dtu_hlir.transpose"(%2142) {node_name = "Conv_7405-0", op_id = 1099 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %2144 = "dtu_hlir.reshape"(%2143) {node_name = "Reshape_7419-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %2145 = "dtu_hlir.transpose"(%2144) {node_name = "InstanceNormalization_7422-0", op_id = 1100 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %2146 = "dtu_hlir.instance_norm"(%2145, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1101 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %2147 = "dtu_hlir.transpose"(%2146) {node_name = "InstanceNormalization_7422-0", op_id = 1102 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %2148 = "dtu_hlir.reshape"(%2147) {node_name = "Reshape_7424-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %2149 = dtu_hlir.constant  {node_name = "Mul_7425-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2150 = dtu_hlir.constant  {node_name = "Add_7426-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2151 = "dtu_hlir.fusion"(%2149, %2148, %2150) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7425-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7425-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_7426-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_7426-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1103 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %2152 = "dtu_hlir.transpose"(%2151) {node_name = "Transpose_7430-0", node_type = "Transpose", op_id = 1104 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %2153 = "dtu_hlir.reshape"(%2152) {node_name = "Reshape_7439-0"} : (tensor<2x64x64x320xf16>) -> tensor<2x4096x320xf16>
    %2154 = "dtu_hlir.dot_general_bias"(%2153, %432, %213) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7441-0", op_id = 1105 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2155 = "dtu_hlir.layer_norm_inference"(%2154, %218, %219) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_7452-0", op_id = 1106 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2156 = "dtu_hlir.dot_general"(%2155, %433) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7453-0", node_type = "MatMul", op_id = 1107 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2157 = "dtu_hlir.dot_general"(%2155, %434) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7454-0", node_type = "MatMul", op_id = 1108 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2158 = "dtu_hlir.dot_general"(%2155, %435) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7455-0", node_type = "MatMul", op_id = 1109 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2159 = "dtu_hlir.reshape"(%2156) {node_name = "Reshape_7476-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2160 = "dtu_hlir.transpose"(%2159) {node_name = "Transpose_7477-0", node_type = "Transpose", op_id = 1110 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2161 = "dtu_hlir.reshape"(%2160) {node_name = "Reshape_7491-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2162 = "dtu_hlir.reshape"(%2157) {node_name = "Reshape_7512-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2163 = "dtu_hlir.transpose"(%2162) {node_name = "Transpose_7513-0", node_type = "Transpose", op_id = 1111 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2164 = "dtu_hlir.reshape"(%2163) {node_name = "Reshape_7527-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2165 = "dtu_hlir.reshape"(%2158) {node_name = "Reshape_7548-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2166 = "dtu_hlir.transpose"(%2165) {node_name = "Transpose_7549-0", node_type = "Transpose", op_id = 1112 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2167 = "dtu_hlir.reshape"(%2166) {node_name = "Reshape_7563-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2168 = "dtu_hlir.transpose"(%2164) {node_name = "Transpose_7581-0", node_type = "Transpose", op_id = 1113 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x4096x64xf16>) -> tensor<10x64x4096xf16>
    %2169 = "dtu_hlir.dot_general"(%2161, %2168) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7582-0", node_type = "MatMul", op_id = 1114 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x4096xf16>) -> tensor<10x4096x4096xf16>
    %2170 = "dtu_hlir.fusion"(%267, %2169) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x4096xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_438-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x4096xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7583-0", node_type = "Mul"} : (tensor<10x4096x4096xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x4096xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1115 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %2171 = "dtu_hlir.softmax"(%2170) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7586-0", node_type = "Softmax", op_id = 1116 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %2172 = "dtu_hlir.dot_general"(%2171, %2167) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7588-0", node_type = "MatMul", op_id = 1117 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x4096xf16>, tensor<10x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2173 = "dtu_hlir.reshape"(%2172) {node_name = "Reshape_7609-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %2174 = "dtu_hlir.transpose"(%2173) {node_name = "Transpose_7610-0", node_type = "Transpose", op_id = 1118 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %2175 = "dtu_hlir.reshape"(%2174) {node_name = "Reshape_7624-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %2176 = "dtu_hlir.dot_general_bias"(%2175, %436, %214) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7626-0", op_id = 1119 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2177 = "dtu_hlir.add"(%2176, %2154) {node_name = "Add_7627-0", node_type = "Add", op_id = 1120 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2178 = "dtu_hlir.layer_norm_inference"(%2177, %220, %221) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_7638-0", op_id = 1121 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2179 = "dtu_hlir.dot_general"(%2178, %437) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7639-0", node_type = "MatMul", op_id = 1122 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2180 = "dtu_hlir.dot_general"(%4, %438) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7640-0", node_type = "MatMul", op_id = 1123 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %2181 = "dtu_hlir.dot_general"(%4, %439) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7641-0", node_type = "MatMul", op_id = 1124 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %2182 = "dtu_hlir.reshape"(%2179) {node_name = "Reshape_7662-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2183 = "dtu_hlir.transpose"(%2182) {node_name = "Transpose_7663-0", node_type = "Transpose", op_id = 1125 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2184 = "dtu_hlir.reshape"(%2183) {node_name = "Reshape_7677-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2185 = "dtu_hlir.reshape"(%2180) {node_name = "Reshape_7698-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %2186 = "dtu_hlir.transpose"(%2185) {node_name = "Transpose_7699-0", node_type = "Transpose", op_id = 1126 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %2187 = "dtu_hlir.reshape"(%2186) {node_name = "Reshape_7713-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %2188 = "dtu_hlir.reshape"(%2181) {node_name = "Reshape_7734-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %2189 = "dtu_hlir.transpose"(%2188) {node_name = "Transpose_7735-0", node_type = "Transpose", op_id = 1127 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %2190 = "dtu_hlir.reshape"(%2189) {node_name = "Reshape_7749-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %2191 = "dtu_hlir.transpose"(%2187) {node_name = "Transpose_7767-0", node_type = "Transpose", op_id = 1128 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x77x64xf16>) -> tensor<10x64x77xf16>
    %2192 = "dtu_hlir.dot_general"(%2184, %2191) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7768-0", node_type = "MatMul", op_id = 1129 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x77xf16>) -> tensor<10x4096x77xf16>
    %2193 = "dtu_hlir.fusion"(%267, %2192) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_624-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7769-0", node_type = "Mul"} : (tensor<10x4096x77xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1130 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %2194 = "dtu_hlir.softmax"(%2193) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_7772-0", node_type = "Softmax", op_id = 1131 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %2195 = "dtu_hlir.dot_general"(%2194, %2190) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_7774-0", node_type = "MatMul", op_id = 1132 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x77xf16>, tensor<10x77x64xf16>) -> tensor<10x4096x64xf16>
    %2196 = "dtu_hlir.reshape"(%2195) {node_name = "Reshape_7795-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %2197 = "dtu_hlir.transpose"(%2196) {node_name = "Transpose_7796-0", node_type = "Transpose", op_id = 1133 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %2198 = "dtu_hlir.reshape"(%2197) {node_name = "Reshape_7810-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %2199 = "dtu_hlir.dot_general_bias"(%2198, %440, %217) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7812-0", op_id = 1134 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2200 = "dtu_hlir.add"(%2199, %2177) {node_name = "Add_7813-0", node_type = "Add", op_id = 1135 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2201 = "dtu_hlir.layer_norm_inference"(%2200, %222, %223) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_7824-0", op_id = 1136 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2202 = "dtu_hlir.dot_general_bias"(%2201, %441, %215) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7826-0", op_id = 1137 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x2560xf16>, tensor<2560xf32>) -> tensor<2x4096x2560xf16>
    %2203 = "dtu_hlir.slice"(%2202) {limit_indices = dense<[2, 4096, 1280]> : tensor<3xi64>, node_name = "Slice_7837-1", op_id = 1138 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %2204 = "dtu_hlir.slice"(%2202) {limit_indices = dense<[2, 4096, 2560]> : tensor<3xi64>, node_name = "Slice_7840-3", op_id = 1139 : i64, start_indices = dense<[0, 0, 1280]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %2205 = "dtu_hlir.gelu"(%2204) {approximate = false, node_name = "Mul_7848-0", op_id = 1140 : i64, unique_name = "common20_gelu"} : (tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %2206 = "dtu_hlir.mul"(%2203, %2205) {node_name = "Mul_7849-0", node_type = "Mul", op_id = 1141 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x4096x1280xf16>, tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %2207 = "dtu_hlir.dot_general_bias"(%2206, %442, %216) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7851-0", op_id = 1142 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2208 = "dtu_hlir.add"(%2207, %2200) {node_name = "Add_7852-0", node_type = "Add", op_id = 1143 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2209 = "dtu_hlir.dot_general_bias"(%2208, %443, %224) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7854-0", op_id = 1144 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2210 = "dtu_hlir.reshape"(%2209) {node_name = "Reshape_7864-0"} : (tensor<2x4096x320xf16>) -> tensor<2x64x64x320xf16>
    %2211 = "dtu_hlir.add"(%2210, %2142) {node_name = "Add_7866-0", node_type = "Add", op_id = 1145 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2212 = "dtu_hlir.concatenate"(%2211, %468) {dimension = 3 : i64, node_name = "Concat_7867-0", op_id = 1146 : i64, unique_name = "concatenate_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x640xf16>
    %2213 = "dtu_hlir.transpose"(%2212) {node_name = "Transpose_7865-0", op_id = 1147 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x640xf16>) -> tensor<2x640x64x64xf16>
    %2214 = "dtu_hlir.reshape"(%2213) {node_name = "Reshape_7869-0"} : (tensor<2x640x64x64xf16>) -> tensor<2x32x81920xf16>
    %2215 = "dtu_hlir.transpose"(%2214) {node_name = "InstanceNormalization_7872-0", op_id = 1148 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x81920xf16>) -> tensor<2x81920x32xf16>
    %2216 = "dtu_hlir.instance_norm"(%2215, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1149 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x81920x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x81920x32xf16>
    %2217 = "dtu_hlir.transpose"(%2216) {node_name = "InstanceNormalization_7872-0", op_id = 1150 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x81920x32xf16>) -> tensor<2x32x81920xf16>
    %2218 = "dtu_hlir.reshape"(%2217) {node_name = "Reshape_7874-0"} : (tensor<2x32x81920xf16>) -> tensor<2x640x64x64xf16>
    %2219 = dtu_hlir.constant  {node_name = "Mul_7875-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %2220 = dtu_hlir.constant  {node_name = "Add_7876-0"} opaque<"", "0xDEADBEEF"> : tensor<2x640xf16>
    %2221 = "dtu_hlir.fusion"(%2219, %2218, %2220) ( {
    ^bb0(%arg3: tensor<2x640xf16>, %arg4: tensor<2x640x64x64xf16>, %arg5: tensor<2x640xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7875-0"} : (tensor<2x640xf16>) -> tensor<2x640x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7875-0", node_type = "Mul"} : (tensor<2x640x64x64xf16>, tensor<2x640x64x64xf16>) -> tensor<2x640x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_7876-0"} : (tensor<2x640xf16>) -> tensor<2x640x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_7876-0", node_type = "Add"} : (tensor<2x640x64x64xf16>, tensor<2x640x64x64xf16>) -> tensor<2x640x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_7878-0"} : (tensor<2x640x64x64xf16>) -> tensor<2x640x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x640x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1151 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x640xf16>, tensor<2x640x64x64xf16>, tensor<2x640xf16>) -> tensor<2x640x64x64xf16>
    %2222 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv1.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x640x320xf16>
    %2223 = "dtu_hlir.transpose"(%2221) {node_name = "Conv_7879-0", op_id = 1152 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x640x64x64xf16>) -> tensor<2x64x64x640xf16>
    %2224 = "dtu_hlir.conv_bias"(%2223, %2222, %243) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7879-0", op_id = 1153 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x640xf16>, tensor<3x3x640x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2225 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<1280x320xf16>
    %2226 = dtu_hlir.constant opaque<"", "0xDEADBEEF"> : tensor<320xf32>
    %2227 = "dtu_hlir.fusion"(%464, %465, %466) ( {
    ^bb0(%arg3: tensor<2x1280xf16>, %arg4: tensor<1280x1280xf16>, %arg5: tensor<1280xf32>):	// no predecessors
      %2329 = "dtu_hlir.dot_general_bias"(%arg3, %arg4, %arg5) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, precision_config = ["DEFAULT", "DEFAULT"]} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
      %2330 = "dtu_hlir.swish"(%2329) {node_name = "Mul_242-0"} : (tensor<2x1280xf16>) -> tensor<2x1280xf16>
      "dtu_hlir.return"(%2330) : (tensor<2x1280xf16>) -> ()
    }) {activate_func = "swish", fusion_kind = 38 : i32, fusion_name = "DotgBiasActivation", op_id = 1154 : i64, unique_name = "dorado_dot_general_bias_activation_dtu"} : (tensor<2x1280xf16>, tensor<1280x1280xf16>, tensor<1280xf32>) -> tensor<2x1280xf16>
    %2228 = "dtu_hlir.dot_general_bias"(%2227, %2225, %2226) {dot_dimension_numbers = {lhs_batching_dimensions = dense<[]> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, op_id = 1155 : i64, precision_config = ["DEFAULT", "DEFAULT"], unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x320xf16>
    %2229 = "dtu_hlir.broadcast_in_dim"(%2228) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, op_id = 1156 : i64, unique_name = "common20_broadcast_in_dim"} : (tensor<2x320xf16>) -> tensor<2x64x64x320xf16>
    %2230 = "dtu_hlir.add"(%2224, %2229) {node_name = "Add_7887-0", node_type = "Add", op_id = 1157 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2231 = "dtu_hlir.transpose"(%2230) {op_id = 1158 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %2232 = "dtu_hlir.reshape"(%2231) {node_name = "Reshape_7889-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %2233 = "dtu_hlir.transpose"(%2232) {node_name = "InstanceNormalization_7892-0", op_id = 1159 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %2234 = "dtu_hlir.instance_norm"(%2233, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1160 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %2235 = "dtu_hlir.transpose"(%2234) {node_name = "InstanceNormalization_7892-0", op_id = 1161 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %2236 = "dtu_hlir.reshape"(%2235) {node_name = "Reshape_7894-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %2237 = dtu_hlir.constant  {node_name = "Mul_7895-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2238 = dtu_hlir.constant  {node_name = "Add_7896-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2239 = "dtu_hlir.fusion"(%2237, %2236, %2238) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7895-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7895-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_7896-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_7896-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_7898-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1162 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %2240 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv2.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x320xf16>
    %2241 = "dtu_hlir.transpose"(%2239) {node_name = "Conv_7899-0", op_id = 1163 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %2242 = "dtu_hlir.conv_bias"(%2241, %2240, %244) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7899-0", op_id = 1164 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2243 = dtu_hlir.constant  {node_name = "up_blocks.3.resnets.2.conv_shortcut.weight"} opaque<"", "0xDEADBEEF"> : tensor<1x1x640x320xf16>
    %2244 = "dtu_hlir.conv_bias"(%2212, %2243, %245) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_7900-0", op_id = 1165 : i64, padding = dense<0> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x640xf16>, tensor<1x1x640x320xf16>, tensor<320xf32>) -> tensor<2x64x64x320xf16>
    %2245 = "dtu_hlir.add"(%2244, %2242) {node_name = "Add_7901-0", node_type = "Add", op_id = 1166 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2246 = "dtu_hlir.transpose"(%2245) {node_name = "Conv_7900-0", op_id = 1167 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %2247 = "dtu_hlir.reshape"(%2246) {node_name = "Reshape_7914-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %2248 = "dtu_hlir.transpose"(%2247) {node_name = "InstanceNormalization_7917-0", op_id = 1168 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %2249 = "dtu_hlir.instance_norm"(%2248, %471, %472) {epsilon = 9.99999997E-7 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1169 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %2250 = "dtu_hlir.transpose"(%2249) {node_name = "InstanceNormalization_7917-0", op_id = 1170 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %2251 = "dtu_hlir.reshape"(%2250) {node_name = "Reshape_7919-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %2252 = dtu_hlir.constant  {node_name = "Mul_7920-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2253 = dtu_hlir.constant  {node_name = "Add_7921-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2254 = "dtu_hlir.fusion"(%2252, %2251, %2253) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_7920-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_7920-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_7921-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_7921-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2332) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1171 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %2255 = "dtu_hlir.transpose"(%2254) {node_name = "Transpose_7925-0", node_type = "Transpose", op_id = 1172 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %2256 = "dtu_hlir.reshape"(%2255) {node_name = "Reshape_7934-0"} : (tensor<2x64x64x320xf16>) -> tensor<2x4096x320xf16>
    %2257 = "dtu_hlir.dot_general_bias"(%2256, %444, %225) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_7936-0", op_id = 1173 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2258 = "dtu_hlir.layer_norm_inference"(%2257, %230, %231) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_7947-0", op_id = 1174 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2259 = "dtu_hlir.dot_general"(%2258, %445) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7948-0", node_type = "MatMul", op_id = 1175 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2260 = "dtu_hlir.dot_general"(%2258, %446) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7949-0", node_type = "MatMul", op_id = 1176 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2261 = "dtu_hlir.dot_general"(%2258, %447) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_7950-0", node_type = "MatMul", op_id = 1177 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2262 = "dtu_hlir.reshape"(%2259) {node_name = "Reshape_7971-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2263 = "dtu_hlir.transpose"(%2262) {node_name = "Transpose_7972-0", node_type = "Transpose", op_id = 1178 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2264 = "dtu_hlir.reshape"(%2263) {node_name = "Reshape_7986-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2265 = "dtu_hlir.reshape"(%2260) {node_name = "Reshape_8007-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2266 = "dtu_hlir.transpose"(%2265) {node_name = "Transpose_8008-0", node_type = "Transpose", op_id = 1179 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2267 = "dtu_hlir.reshape"(%2266) {node_name = "Reshape_8022-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2268 = "dtu_hlir.reshape"(%2261) {node_name = "Reshape_8043-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2269 = "dtu_hlir.transpose"(%2268) {node_name = "Transpose_8044-0", node_type = "Transpose", op_id = 1180 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2270 = "dtu_hlir.reshape"(%2269) {node_name = "Reshape_8058-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2271 = "dtu_hlir.transpose"(%2267) {node_name = "Transpose_8076-0", node_type = "Transpose", op_id = 1181 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x4096x64xf16>) -> tensor<10x64x4096xf16>
    %2272 = "dtu_hlir.dot_general"(%2264, %2271) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_8077-0", node_type = "MatMul", op_id = 1182 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x4096xf16>) -> tensor<10x4096x4096xf16>
    %2273 = "dtu_hlir.fusion"(%267, %2272) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x4096xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_438-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x4096xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_8078-0", node_type = "Mul"} : (tensor<10x4096x4096xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x4096xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1183 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %2274 = "dtu_hlir.softmax"(%2273) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_8081-0", node_type = "Softmax", op_id = 1184 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x4096xf16>) -> tensor<10x4096x4096xf16>
    %2275 = "dtu_hlir.dot_general"(%2274, %2270) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_8083-0", node_type = "MatMul", op_id = 1185 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x4096xf16>, tensor<10x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2276 = "dtu_hlir.reshape"(%2275) {node_name = "Reshape_8104-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %2277 = "dtu_hlir.transpose"(%2276) {node_name = "Transpose_8105-0", node_type = "Transpose", op_id = 1186 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %2278 = "dtu_hlir.reshape"(%2277) {node_name = "Reshape_8119-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %2279 = "dtu_hlir.dot_general_bias"(%2278, %448, %226) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_8121-0", op_id = 1187 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2280 = "dtu_hlir.add"(%2279, %2257) {node_name = "Add_8122-0", node_type = "Add", op_id = 1188 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2281 = "dtu_hlir.layer_norm_inference"(%2280, %232, %233) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_8133-0", op_id = 1189 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2282 = "dtu_hlir.dot_general"(%2281, %449) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_8134-0", node_type = "MatMul", op_id = 1190 : i64, unique_name = "dot_general_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>) -> tensor<2x4096x320xf16>
    %2283 = "dtu_hlir.dot_general"(%4, %450) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_8135-0", node_type = "MatMul", op_id = 1191 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %2284 = "dtu_hlir.dot_general"(%4, %451) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "MatMul_8136-0", node_type = "MatMul", op_id = 1192 : i64, unique_name = "dot_general_dtu"} : (tensor<2x77x1024xf16>, tensor<1024x320xf16>) -> tensor<2x77x320xf16>
    %2285 = "dtu_hlir.reshape"(%2282) {node_name = "Reshape_8157-0"} : (tensor<2x4096x320xf16>) -> tensor<2x4096x5x64xf16>
    %2286 = "dtu_hlir.transpose"(%2285) {node_name = "Transpose_8158-0", node_type = "Transpose", op_id = 1193 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x4096x5x64xf16>) -> tensor<2x5x4096x64xf16>
    %2287 = "dtu_hlir.reshape"(%2286) {node_name = "Reshape_8172-0"} : (tensor<2x5x4096x64xf16>) -> tensor<10x4096x64xf16>
    %2288 = "dtu_hlir.reshape"(%2283) {node_name = "Reshape_8193-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %2289 = "dtu_hlir.transpose"(%2288) {node_name = "Transpose_8194-0", node_type = "Transpose", op_id = 1194 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %2290 = "dtu_hlir.reshape"(%2289) {node_name = "Reshape_8208-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %2291 = "dtu_hlir.reshape"(%2284) {node_name = "Reshape_8229-0"} : (tensor<2x77x320xf16>) -> tensor<2x77x5x64xf16>
    %2292 = "dtu_hlir.transpose"(%2291) {node_name = "Transpose_8230-0", node_type = "Transpose", op_id = 1195 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x77x5x64xf16>) -> tensor<2x5x77x64xf16>
    %2293 = "dtu_hlir.reshape"(%2292) {node_name = "Reshape_8244-0"} : (tensor<2x5x77x64xf16>) -> tensor<10x77x64xf16>
    %2294 = "dtu_hlir.transpose"(%2290) {node_name = "Transpose_8262-0", node_type = "Transpose", op_id = 1196 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<10x77x64xf16>) -> tensor<10x64x77xf16>
    %2295 = "dtu_hlir.dot_general"(%2287, %2294) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_8263-0", node_type = "MatMul", op_id = 1197 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x64xf16>, tensor<10x64x77xf16>) -> tensor<10x4096x77xf16>
    %2296 = "dtu_hlir.fusion"(%267, %2295) ( {
    ^bb0(%arg3: tensor<1xf16>, %arg4: tensor<10x4096x77xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[]> : tensor<0xi64>, node_name = "Mul_624-0_hlir_0", node_type = "Mul"} : (tensor<1xf16>) -> tensor<10x4096x77xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_8264-0", node_type = "Mul"} : (tensor<10x4096x77xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
      "dtu_hlir.return"(%2330) : (tensor<10x4096x77xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1198 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<1xf16>, tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %2297 = "dtu_hlir.softmax"(%2296) {accurate = true, axis = -1 : i64, epsilon = 0.000000e+00 : f32, logarithmic = false, node_name = "Softmax_8267-0", node_type = "Softmax", op_id = 1199 : i64, unique_name = "common20_softmax"} : (tensor<10x4096x77xf16>) -> tensor<10x4096x77xf16>
    %2298 = "dtu_hlir.dot_general"(%2297, %2293) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<0> : tensor<1xi64>, rhs_contracting_dimensions = dense<1> : tensor<1xi64>}, node_name = "MatMul_8269-0", node_type = "MatMul", op_id = 1200 : i64, unique_name = "dot_general_dtu"} : (tensor<10x4096x77xf16>, tensor<10x77x64xf16>) -> tensor<10x4096x64xf16>
    %2299 = "dtu_hlir.reshape"(%2298) {node_name = "Reshape_8290-0"} : (tensor<10x4096x64xf16>) -> tensor<2x5x4096x64xf16>
    %2300 = "dtu_hlir.transpose"(%2299) {node_name = "Transpose_8291-0", node_type = "Transpose", op_id = 1201 : i64, permutation = dense<[0, 2, 1, 3]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x5x4096x64xf16>) -> tensor<2x4096x5x64xf16>
    %2301 = "dtu_hlir.reshape"(%2300) {node_name = "Reshape_8305-0"} : (tensor<2x4096x5x64xf16>) -> tensor<2x4096x320xf16>
    %2302 = "dtu_hlir.dot_general_bias"(%2301, %452, %229) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_8307-0", op_id = 1202 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2303 = "dtu_hlir.add"(%2302, %2280) {node_name = "Add_8308-0", node_type = "Add", op_id = 1203 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2304 = "dtu_hlir.layer_norm_inference"(%2303, %234, %235) {axis = 2 : i64, epsilon = 9.99999974E-6 : f32, ln_type = 0 : i64, node_name = "Add_8319-0", op_id = 1204 : i64, unique_name = "layer_norm_inference_dtu"} : (tensor<2x4096x320xf16>, tensor<320xf32>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2305 = "dtu_hlir.dot_general_bias"(%2304, %453, %227) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_8321-0", op_id = 1205 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x2560xf16>, tensor<2560xf32>) -> tensor<2x4096x2560xf16>
    %2306 = "dtu_hlir.slice"(%2305) {limit_indices = dense<[2, 4096, 1280]> : tensor<3xi64>, node_name = "Slice_8332-1", op_id = 1206 : i64, start_indices = dense<0> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %2307 = "dtu_hlir.slice"(%2305) {limit_indices = dense<[2, 4096, 2560]> : tensor<3xi64>, node_name = "Slice_8335-3", op_id = 1207 : i64, start_indices = dense<[0, 0, 1280]> : tensor<3xi64>, strides = dense<1> : tensor<3xi64>, unique_name = "slice_dtu"} : (tensor<2x4096x2560xf16>) -> tensor<2x4096x1280xf16>
    %2308 = "dtu_hlir.gelu"(%2307) {approximate = false, node_name = "Mul_8343-0", op_id = 1208 : i64, unique_name = "common20_gelu"} : (tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %2309 = "dtu_hlir.mul"(%2306, %2308) {node_name = "Mul_8344-0", node_type = "Mul", op_id = 1209 : i64, unique_name = "elementwise_mul_dtu"} : (tensor<2x4096x1280xf16>, tensor<2x4096x1280xf16>) -> tensor<2x4096x1280xf16>
    %2310 = "dtu_hlir.dot_general_bias"(%2309, %454, %228) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_8346-0", op_id = 1210 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x1280xf16>, tensor<1280x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2311 = "dtu_hlir.add"(%2310, %2303) {node_name = "Add_8347-0", node_type = "Add", op_id = 1211 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x4096x320xf16>, tensor<2x4096x320xf16>) -> tensor<2x4096x320xf16>
    %2312 = "dtu_hlir.dot_general_bias"(%2311, %455, %236) {dot_dimension_numbers = {lhs_batching_dimensions = dense<0> : tensor<1xi64>, lhs_contracting_dimensions = dense<2> : tensor<1xi64>, rhs_batching_dimensions = dense<[]> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}, node_name = "Add_8349-0", op_id = 1212 : i64, unique_name = "dorado_dot_general_bias_dtu"} : (tensor<2x4096x320xf16>, tensor<320x320xf16>, tensor<320xf32>) -> tensor<2x4096x320xf16>
    %2313 = "dtu_hlir.reshape"(%2312) {node_name = "Reshape_8359-0"} : (tensor<2x4096x320xf16>) -> tensor<2x64x64x320xf16>
    %2314 = "dtu_hlir.add"(%2313, %2245) {node_name = "Add_8361-0", node_type = "Add", op_id = 1213 : i64, unique_name = "elementwise_add_dtu"} : (tensor<2x64x64x320xf16>, tensor<2x64x64x320xf16>) -> tensor<2x64x64x320xf16>
    %2315 = "dtu_hlir.transpose"(%2314) {node_name = "Transpose_8360-0", op_id = 1214 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x320xf16>) -> tensor<2x320x64x64xf16>
    %2316 = "dtu_hlir.reshape"(%2315) {node_name = "Reshape_8363-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x32x40960xf16>
    %2317 = "dtu_hlir.transpose"(%2316) {node_name = "InstanceNormalization_8366-0", op_id = 1215 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x32x40960xf16>) -> tensor<2x40960x32xf16>
    %2318 = "dtu_hlir.instance_norm"(%2317, %471, %472) {epsilon = 9.99999974E-6 : f32, layout = {batch_dimension = 0 : i64, feature_dimension = 2 : i64, spatial_dimensions = dense<1> : tensor<1xi64>}, op_id = 1216 : i64, unique_name = "instance_norm_dtu"} : (tensor<2x40960x32xf16>, tensor<32xf32>, tensor<32xf32>) -> tensor<2x40960x32xf16>
    %2319 = "dtu_hlir.transpose"(%2318) {node_name = "InstanceNormalization_8366-0", op_id = 1217 : i64, permutation = dense<[0, 2, 1]> : tensor<3xi64>, unique_name = "transpose_dtu"} : (tensor<2x40960x32xf16>) -> tensor<2x32x40960xf16>
    %2320 = "dtu_hlir.reshape"(%2319) {node_name = "Reshape_8368-0"} : (tensor<2x32x40960xf16>) -> tensor<2x320x64x64xf16>
    %2321 = dtu_hlir.constant  {node_name = "Mul_8369-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2322 = dtu_hlir.constant  {node_name = "Add_8370-0"} opaque<"", "0xDEADBEEF"> : tensor<2x320xf16>
    %2323 = "dtu_hlir.fusion"(%2321, %2320, %2322) ( {
    ^bb0(%arg3: tensor<2x320xf16>, %arg4: tensor<2x320x64x64xf16>, %arg5: tensor<2x320xf16>):	// no predecessors
      %2329 = "dtu_hlir.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Mul_8369-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2330 = "dtu_hlir.mul"(%arg4, %2329) {node_name = "Mul_8369-0", node_type = "Mul"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2331 = "dtu_hlir.broadcast_in_dim"(%arg5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>, node_name = "Add_8370-0"} : (tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
      %2332 = "dtu_hlir.add"(%2330, %2331) {node_name = "Add_8370-0", node_type = "Add"} : (tensor<2x320x64x64xf16>, tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      %2333 = "dtu_hlir.swish"(%2332) {node_name = "Mul_8372-0"} : (tensor<2x320x64x64xf16>) -> tensor<2x320x64x64xf16>
      "dtu_hlir.return"(%2333) : (tensor<2x320x64x64xf16>) -> ()
    }) {fusion_kind = 1 : i32, fusion_name = "ElementWise", op_id = 1218 : i64, unique_name = "elementwise_fusion_dtu"} : (tensor<2x320xf16>, tensor<2x320x64x64xf16>, tensor<2x320xf16>) -> tensor<2x320x64x64xf16>
    %2324 = "dtu_hlir.transpose"(%2323) {node_name = "Conv_8373-0", op_id = 1219 : i64, permutation = dense<[0, 2, 3, 1]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x320x64x64xf16>) -> tensor<2x64x64x320xf16>
    %2325 = dtu_hlir.constant  {node_name = "conv_out.weight"} opaque<"", "0xDEADBEEF"> : tensor<3x3x320x4xf16>
    %2326 = "dtu_hlir.conv_bias"(%2324, %2325, %262) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : vector<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : vector<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : vector<2xi64>}, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, node_name = "Conv_8373-0", op_id = 1220 : i64, padding = dense<1> : tensor<4xi64>, rhs_dilation = dense<1> : tensor<2xi64>, unique_name = "dorado_dtu_conv_bias_dtu", window_strides = dense<1> : tensor<2xi64>} : (tensor<2x64x64x320xf16>, tensor<3x3x320x4xf16>, tensor<4xf32>) -> tensor<2x64x64x4xf16>
    %2327 = "dtu_hlir.transpose"(%2326) {node_name = "Conv_8373-0", op_id = 1221 : i64, permutation = dense<[0, 3, 1, 2]> : tensor<4xi64>, unique_name = "transpose_dtu"} : (tensor<2x64x64x4xf16>) -> tensor<2x4x64x64xf16>
    %2328 = "dtu_hlir.convert"(%2327) {gen_loc = "hlir_quantization", op_id = 1222 : i64, unique_name = "common20_convert"} : (tensor<2x4x64x64xf16>) -> tensor<2x4x64x64xf32>
    "dtu_hlir.tensor_attr"(%2328) {dynamic_flag = "positive", max_shape = dense<[2, 4, 64, 64]> : tensor<4xi64>, min_shape = dense<[2, 4, 64, 64]> : tensor<4xi64>, table_index = dense<3> : tensor<1xi64>} : (tensor<2x4x64x64xf32>) -> ()
    return %2328 : tensor<2x4x64x64xf32>
  }
}

