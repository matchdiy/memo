{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Schedule Primitives in TVM](https://tvm.apache.org/docs/how_to/work_with_schedules/schedule_primitives.html#sphx-glr-how-to-work-with-schedules-schedule-primitives-py)\n",
    "\n",
    "## Create Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm import relay\n",
    "import numpy as np\n",
    "\n",
    "# declare some variables for use later\n",
    "n = te.var(\"n\")\n",
    "m = te.var(\"m\")\n",
    "\n",
    "def test_elewise_mul():\n",
    "  # declare a matrix element-wise multiply\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.placeholder((m, n), name=\"B\")\n",
    "  C = te.compute((m, n), lambda i, j: A[i, j] * B[i, j], name=\"C\")\n",
    "\n",
    "  s = te.create_schedule([C.op])\n",
    "  # lower will transform the computation from definition to the real\n",
    "  # callable function. With argument `simple_mode=True`, it will\n",
    "  # return you a readable C like statement, we use it here to print the\n",
    "  # schedule result.\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_elewise_mul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Split\n",
    "\n",
    "SplitFactor：将指定维度按照指定长度进行切分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_factor(axis, factor=32):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  xo, xi = s[B].split(B.op.axis[axis], factor=factor)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_split_factor(axis=0)\n",
    "test_split_factor(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SplitParts：将指定维度按照指定份数进行切分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_nparts(nparts=8):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  bx, tx = s[B].split(B.op.axis[0], nparts=nparts)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_split_nparts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Tile\n",
    "\n",
    "分块操作，注意Tile和Split是有区别的，我们无法用两个Split完成一个Tile的功能，但是可以通过Tile覆盖Split功能（Split中不切的axis-factor设为1）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tile(x, y, simple_mode=True):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=x, y_factor=y)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=simple_mode))\n",
    "\n",
    "test_tile(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较一下 ___Split___ 和 ___Tile___ 的行为： ___Tile___ 需要 ___Split___ 和 ___Reorder___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_x2(x_factor, y_factor):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  xo, xi = s[B].split(B.op.axis[0], factor=x_factor)\n",
    "  yo, yi = s[B].split(B.op.axis[1], factor=y_factor)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "print('*'*64)\n",
    "print('test_split_x2(10, 5)')\n",
    "print('*'*64)\n",
    "test_split_x2(10, 5)\n",
    "print('*'*64)\n",
    "print('test_tile(10, 5)')\n",
    "print('*'*64)\n",
    "test_tile(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较一下 ___Split___ 和 ___Tile___ 的行为：用 ___Tile___ 来实现 ___Split___：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_factor(axis=1, factor=5)\n",
    "test_tile(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Fuse\n",
    "\n",
    "合并连续的 ___N___ 个维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fuse(axis0, axis1, simple_mode=True):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  # tile to four axes first: (i.outer, j.outer, i.inner, j.inner)\n",
    "  axes4 = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)\n",
    "  # then fuse (i.inner, j.inner) into one axis: (i.inner.j.inner.fused)\n",
    "  fused = s[B].fuse(axes4[axis0], axes4[axis1])\n",
    "  ### fused = s[B].fuse(xo, yo)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=simple_mode))\n",
    "\n",
    "test_fuse(1, 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Reorder\n",
    "\n",
    "维度调换，相当于Transpose功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reorder(axis0, axis1, axis2, axis3):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  # tile to four axes first: (i.outer, j.outer, i.inner, j.inner)\n",
    "  axes4 = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)\n",
    "  s[B].reorder(axes4[axis0], axes4[axis1], axes4[axis2], axes4[axis3])\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_reorder(0, 1, 2, 3) # no change\n",
    "test_reorder(2, 1, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Gpu::Bind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bind():\n",
    "  A = te.placeholder((n,), name=\"A\")\n",
    "  B = te.compute(A.shape, lambda i: A[i], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  bx, tx = s[B].split(B.op.axis[0], factor=64)\n",
    "  s[B].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
    "  s[B].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "test_bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::ComputeAt\n",
    "\n",
    "移动一个Compute Stage 到指定的计算指定维度中，看起来可以用在 Fusion 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compute_at(axis):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j] + 1, name=\"B\")\n",
    "  C = te.compute((m, n), lambda i, j: B[i, j] * 2, name=\"C\")\n",
    "  s = te.create_schedule(C.op)\n",
    "  # move computation of B into the first axis of computation of C\n",
    "  s[B].compute_at(s[C], C.op.axis[axis]) \n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_compute_at(-1)\n",
    "test_compute_at(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Compute_Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compute_inline():\n",
    "  A = te.placeholder((m,), name=\"A\")\n",
    "  B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "  C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "  s = te.create_schedule(C.op)\n",
    "  # mark one stage as inline\n",
    "  s[B].compute_inline()\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_compute_inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Compute_Root\n",
    "\n",
    "将一个计算移动到根上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compute_root():\n",
    "  A = te.placeholder((m,), name=\"A\")\n",
    "  B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "  C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "  s = te.create_schedule(C.op)\n",
    "  s[B].compute_at(s[C], C.op.axis[0])\n",
    "  # move computation of one stage to the root\n",
    "  s[B].compute_root()\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_compute_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Parallel\n",
    "\n",
    "为了支持CPU上类似 _openmp_ 方式的并行计算， GPU上还是要用 ___bind___。 GCU上可以用来划分 4xCluster 并行，以及 6xSip 并行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = te.placeholder((n, m), name='A')\n",
    "l = te.reduce_axis((0, m), name = 'l')\n",
    "B = te.compute((n,), lambda i: te.sum(A[i, l], axis=l), name='B')\n",
    "s = te.create_schedule(B.op)\n",
    "s[B].parallel(B.op.reduce_axis[0])\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Unroll\n",
    "\n",
    "Unroll 的 axis 要求是 constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_unroll():\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  B = te.placeholder((m, n), name='B')\n",
    "  C = te.compute((m, n), lambda i, j: A[i, j] + B[i, j], name='C')\n",
    "  s = te.create_schedule(C.op)\n",
    "  xo, xi = s[C].split(s[C].op.axis[0], factor=4)\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  s[C].unroll(xi)\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_unroll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::CacheRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cache_read():\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  k = te.reduce_axis((0, n), name='k')\n",
    "  B = te.compute((m,), lambda i: te.sum(A[i, k], axis=k), name='B')\n",
    "  s = te.create_schedule(B.op)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  AA = s.cache_read(A, \"shared\", [B])\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_cache_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::CacheWrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cache_write():\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  k = te.reduce_axis((0, n), name='k')\n",
    "  B = te.compute((m,), lambda i: te.sum(A[i, k], axis=k), name='B')\n",
    "  s = te.create_schedule(B.op)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  BW = s.cache_write(B, \"local\")\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_cache_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::StorageAlign\n",
    "\n",
    "Set alignment requirement for specific axis\n",
    "\n",
    "This ensures that stride[axis] == k * factor + offset for some k. This is useful to set memory layout to for more friendly memory access pattern. For example, we can set alignment to be factor=2, offset=1 to avoid bank conflict for thread access on higher dimension in GPU shared memory.\n",
    "\n",
    "Parameters:\n",
    " * axis (IterVar) – The axis dimension to be aligned.\n",
    " * factor (int) – The factor in alignment specification.\n",
    " * offset (int) – The offset in the alignment specification.\n",
    "\n",
    "计算公式：`stride=size + floormod(offset - floormod(size, factor), factor)`。StorageAlign看起来完全是为了优化 GPU 的 _shared memory_ 访问时的 _bank conflict_ 而引入的定制优化。现代 GPU 的 _shared memory_ 一般是 32bits interleaving， 有32个 bank，由此可以计算出如果我们让每个 thread 处理连续 128bytes 的数据，会导致所有的 thread 都会同时访问相同的 bank。这个时候就需要改变数据存储的格式，比如说申请一个buffer，它的 row 为128+4bytes，其中前128bytes写入有效数据，后面的4bytes为 padding 的 dummy 数据， 那么 t0 和 t1 ... t15 在第一个访问时间点上会分别访问 bank0, bank1 ... bank15，来解决访问冲突。 StorageAlign 里边的 factor 看起来对应与每个 thread 访问的数据量，这种方式可以解决不同的 row 之间的访问冲突，并不是最极限的优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tset_storage_align():\n",
    "  # m = 100\n",
    "  # n = 128\n",
    "  factor_val = 97\n",
    "  offset = 16\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  k = te.reduce_axis((0, n), name='k')\n",
    "  B = te.compute((m,), lambda i: te.sum(A[i, k], axis=k), name='B')\n",
    "  s = te.create_schedule(B.op)\n",
    "  ## cache read will create a buffer, buffer is only 1 axis\n",
    "  AA = s.cache_read(A, \"shared\", [B])\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "  print(\"---------cutting line---------\")\n",
    "  s[AA].storage_align(AA.op.axis[0], factor_val, offset)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "tset_storage_align()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Pragma\n",
    "\n",
    "Pragma 将生成 pragma_scope， 这回有助于一些实验中的功能以及外部的扩展。\n",
    "\n",
    "Most pragmas are advanced/experimental features and may subject to change. List of supported pragmas:\n",
    "\n",
    "* debug_skip_region\n",
    "\n",
    "  Force skip the region marked by the axis and turn it into no-op. This is useful for debug purposes.\n",
    "\n",
    "* parallel_launch_point\n",
    "\n",
    "  Specify to launch parallel threads outside the specified iteration loop. By default the threads launch at the point of parallel construct. This pragma moves the launching point to even outer scope. The threads are launched once and reused across multiple parallel constructs as BSP style program.\n",
    "\n",
    "* parallel_barrier_when_finish\n",
    "\n",
    "  Insert a synchronization barrier between working threads after the specified loop iteration finishes.\n",
    "\n",
    "* parallel_stride_pattern\n",
    "\n",
    "  Hint parallel loop to execute in strided pattern. for (int i = task_id; i < end; i += num_task)\n",
    "\n",
    "这个功能可以穿透式的带一些信息给到底层，虽然提供了便利性，但应该最小限度使用，以免在程序变得庞大后难以维护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pragma():\n",
    "  A = te.placeholder((n, m), name='A')\n",
    "  k = te.reduce_axis((0, n), name='k')\n",
    "  l = te.reduce_axis((0, m), name = 'l')\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, l], axis=l), name='B')\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=4)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  s[B].pragma(ki, \"unroll\")\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_pragma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::CreateGroup\n",
    "\n",
    "create_group 对从inputs到outputs的所有stage创建group，group本质上是一个虚拟stage，可以通过操作这个虚拟stage来一起操作这个group里的所有stage。\n",
    "\n",
    "本例中，通过compute_at使这个group中的D和E，一起附加到F的reduce维度操作中。这样临时Buffer D 变成了一个Scalar。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_create_group():\n",
    "  k = te.reduce_axis((0, n), name='k')\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  B = te.placeholder((m, n), name='B')\n",
    "\n",
    "  D = te.compute((m, n), lambda i, j: A[i, j] + B[i, j], name='D')\n",
    "  E = te.compute((m, n), lambda i, j: D[i, j] + B[i, j], name='E')\n",
    "  F = te.compute((m,), lambda i: te.sum(E[i, k], axis=k), name='F')\n",
    "\n",
    "  s = te.create_schedule(F.op)\n",
    "\n",
    "  print(tvm.lower(s, [A, B, E], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "\n",
    "  g = s.create_group(outputs = E, inputs = [A, B], include_inputs=True)\n",
    "  g.compute_at(s[F], F.op.reduce_axis[0])\n",
    "\n",
    "  print(tvm.lower(s, [A, B, E], simple_mode=True))\n",
    "\n",
    "test_create_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::SetScope\n",
    "\n",
    "指定当前 _stage_ 的计算结果保存的位置，在没有指定的情况下默认 `storage_scope=global`，可以通过 `set_stage` 指定成 _shared_, 通常用于用于 _thread_ 之间的数据共享。\n",
    "\n",
    "`set_scope`比`cache_read`以及`cache_write`提供更灵活的操作，后两者实现中使用了这个功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_scope():\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  k = te.reduce_axis((0, n), name='k')\n",
    "  B = te.compute((m,), lambda i: te.sum(A[i, k], axis=k), name='B')\n",
    "  C = te.compute((m,), lambda i: B[i] + 10, name='C')\n",
    "  s = te.create_schedule(C.op)\n",
    "  print(tvm.lower(s, [A, C], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  s[B].set_scope('shared')\n",
    "  print(tvm.lower(s, [A, C], simple_mode=True))\n",
    "\n",
    "test_set_scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Vectorize\n",
    "\n",
    "___注意:___ `m` 和 `n` 是 `var`的时候Vectorize无法使能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vectorize():\n",
    "  m = 1024\n",
    "  n = 1024\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  B = te.placeholder((m, n), name='B')\n",
    "  C = te.compute(\n",
    "            (m, n),\n",
    "            lambda x, y: A[x, y] + B[x, y],\n",
    "            name='C')\n",
    "  s = te.create_schedule(C.op)\n",
    "  xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], 32, 32)\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  \n",
    "  s[C].vectorize(yi)\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Normalize\n",
    "\n",
    "`normalize` 与 `create_group`，`rfactor`，`cache_read`，`cache_write` 一样，作用域是全部的 stages。\n",
    "\n",
    "下面的例子看起来在TVM框架中会自动调用 `normalize` 无需用户手动调用，也不希望用户手动调用，同样的 `rebase` 也是 。\n",
    "[tqChen](https://github.com/apache/tvm/issues/733#issuecomment-355420100)\n",
    "I think a good way is always avoid calling normalize manually as it will be called right before we do lowering, it might be good to add this to the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normalize():\n",
    "  A = te.placeholder((n,), name='A')\n",
    "  B = te.placeholder((n,), name='B')\n",
    "  k = te.reduce_axis((10, n), 'k')\n",
    "  C = te.compute((1,), lambda _: te.sum(A[k] * B[k], axis=k), name='C')\n",
    "  s = te.create_schedule(C.op)\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  s = s.normalize()\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Prefetch\n",
    "\n",
    "Prefetch the specified variable\n",
    "\n",
    "* Parameters\n",
    "  * _tensor_ (Tensor) – The tensor to be prefetched\n",
    "  * _var_ (IterVar) – The loop point at which the prefetching is applied\n",
    "  * _offset_ (Expr) – The number of iterations to be prefetched before actual execution\n",
    "\n",
    "___FIXME___：_tir.prefetch_ 的行为我没有理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prefetch():\n",
    "  k = te.reduce_axis((0, n), name='k')\n",
    "  A = te.placeholder((m, n), name='A')\n",
    "  B = te.compute((m,), lambda i: te.sum(A[i, k], axis=k), name='B')\n",
    "  s = te.create_schedule(B.op)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "  s[B].prefetch(A, s[B].op.reduce_axis[0], 11)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_prefetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Tensorize\n",
    "\n",
    "Note that intrin_func now returns a triplet: (body, reduce_reset, reduce_update). If tensorization includes all the reduce axes, function body() will be invoked, otherwise reduce_reset() and reduce_update() together will be used. In our example body() and reduce_update() share the same implementation, while in other cases, hardware may have different instructions for these two functions. Moreover, we can see now bb.strides[0] is different from l due to the tiling.\n",
    "\n",
    "`intrin_func`返回 (body, reduce_reset, reduce_update)，在 tensorization 包含了完成的 reduce axes 的话那么只调用 body()；否则的话需要调用 reduce_reset() 和 reduce_update()来组合完成 partail sum 的初期化以及更新：\n",
    "* test_tensorize(): 不切分 reduce axis 时的实现\n",
    "* test_tensorize2(): 切分 reduce axis 时的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tensorize():\n",
    "  N, M, L = 1024, 512, 64\n",
    "  A = te.placeholder((N, L), name='A')\n",
    "  B = te.placeholder((M, L), name='B')\n",
    "  k = te.reduce_axis((0, L), name='k')\n",
    "  C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[j, k], axis=k), name='C')\n",
    "  s = te.create_schedule(C.op)\n",
    "\n",
    "  def intrin_gemv(m, l):\n",
    "      a = te.placeholder((l,), name='a')\n",
    "      b = te.placeholder((m, l), name='b')\n",
    "      k = te.reduce_axis((0, l), name='k')\n",
    "      c =  te.compute((m,), lambda i: te.sum(a[k] * b[i, k], axis=k), name='c')\n",
    "      Abuf = tvm.tir.decl_buffer(a.shape, a.dtype, name='A', offset_factor=1, strides=[1])\n",
    "      Bbuf = tvm.tir.decl_buffer(b.shape, b.dtype, name='B', offset_factor=1, strides=[te.var(\"s1\"), 1])\n",
    "      Cbuf = tvm.tir.decl_buffer(c.shape, c.dtype, name='C', offset_factor=1, strides=[1])\n",
    "      \n",
    "      def intrin_func(ins, outs):\n",
    "          ib = tvm.tir.ir_builder.create()\n",
    "          aa, bb = ins\n",
    "          cc = outs[0]\n",
    "          ib.emit(tvm.tir.call_extern(\"int32\", \"gemv_update\", cc.access_ptr(\"w\"), aa.access_ptr(\"r\"), bb.access_ptr(\"r\"), m, l, bb.strides[0]))\n",
    "          return ib.get()\n",
    "      #with tvm.build_config(offset_factor=1):\n",
    "      with relay.build_config(opt_level=0):\n",
    "          return te.decl_tensor_intrin(c.op, intrin_func, binds={a: Abuf, b: Bbuf, c: Cbuf})\n",
    "\n",
    "  factor = 16\n",
    "  x, y = C.op.axis\n",
    "  z, = C.op.reduce_axis\n",
    "  yo, yi = s[C].split(y, factor=factor)\n",
    "  s[C].reorder(x, yo, yi, z)\n",
    "\n",
    "  gemv = intrin_gemv(factor, L)\n",
    "\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "  print(\"---------cutting line---------\")\n",
    "\n",
    "  s[C].tensorize(yi, gemv)\n",
    "\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_tensorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemv_impl():\n",
    "    cc_code = \"\"\"\n",
    "      extern \"C\" int gemv_update(float *cc, float *aa, float *bb, int m, int l, int stride) {\n",
    "        for (int i = 0; i < m; ++i) {\n",
    "            for (int j = 0; j < l; ++j) {\n",
    "                cc[i] += aa[j] * bb[i * stride + j];\n",
    "            }\n",
    "        }\n",
    "        return 0;\n",
    "      }\n",
    "      extern \"C\" int gemv_reset(float *cc, int m) {\n",
    "        for (int i = 0; i < m; ++i) {\n",
    "            cc[i] = 0.0;\n",
    "        }\n",
    "        return 0;\n",
    "      }\n",
    "    \"\"\"\n",
    "    from tvm.contrib import utils, clang\n",
    "\n",
    "    temp = utils.tempdir()\n",
    "    ll_path = temp.relpath(\"temp.ll\")\n",
    "    # Create LLVM ir from c source code\n",
    "    ll_code = clang.create_llvm(cc_code, output=ll_path)\n",
    "    return ll_code\n",
    "\n",
    "\n",
    "def intrin_gemv(m, l):\n",
    "    a = te.placeholder((l,), name=\"a\")\n",
    "    b = te.placeholder((m, l), name=\"b\")\n",
    "    k = te.reduce_axis((0, l), name=\"k\")\n",
    "    c = te.compute((m,), lambda i: te.sum(a[k] * b[i, k], axis=k), name=\"c\")\n",
    "    Ab = tvm.tir.decl_buffer(a.shape, a.dtype, name=\"A\", offset_factor=1, strides=[1])\n",
    "    Bb = tvm.tir.decl_buffer(b.shape, b.dtype, name=\"B\", offset_factor=1, strides=[te.var(\"s1\"), 1])\n",
    "    Cb = tvm.tir.decl_buffer(c.shape, c.dtype, name=\"C\", offset_factor=1, strides=[1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        aa, bb = ins\n",
    "        cc = outs[0]\n",
    "\n",
    "        def _body():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(\n",
    "                tvm.tir.call_extern(\n",
    "                    \"int32\",\n",
    "                    \"gemv_update\",\n",
    "                    cc.access_ptr(\"w\"),\n",
    "                    aa.access_ptr(\"r\"),\n",
    "                    bb.access_ptr(\"r\"),\n",
    "                    m,\n",
    "                    l,\n",
    "                    bb.strides[0],\n",
    "                )\n",
    "            )\n",
    "            return ib.get()\n",
    "\n",
    "        def _reduce_reset():\n",
    "            ib = tvm.tir.ir_builder.create()\n",
    "            ib.emit(tvm.tir.call_extern(\"int32\", \"gemv_reset\", cc.access_ptr(\"w\"), m))\n",
    "            return ib.get()\n",
    "\n",
    "        def _reduce_update():\n",
    "            return _body()\n",
    "\n",
    "        return _body(), _reduce_reset(), _reduce_update()\n",
    "\n",
    "    return te.decl_tensor_intrin(c.op, intrin_func, binds={a: Ab, b: Bb, c: Cb})\n",
    "\n",
    "def test_tensorize2():\n",
    "  N, M, L = 1024, 512, 64\n",
    "  A = te.placeholder((N, L), name='A')\n",
    "  B = te.placeholder((M, L), name='B')\n",
    "  k = te.reduce_axis((0, L), name='k')\n",
    "  C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[j, k], axis=k), name='C')\n",
    "  s = te.create_schedule(C.op)\n",
    "\n",
    "  factor = 16\n",
    "  x, y = C.op.axis\n",
    "  z, = C.op.reduce_axis\n",
    "  yo, yi = s[C].split(y, factor=factor)\n",
    "  s[C].reorder(x, yo, yi, z)\n",
    "  zo, zi = s[C].split(z, factor=factor)\n",
    "  s[C].reorder(x, yo, zo, yi, zi)\n",
    "\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "  gemv = intrin_gemv(factor, factor)\n",
    "  s[C].tensorize(yi, gemv)\n",
    "  s[C].pragma(yo, \"import_llvm\", gemv_impl())\n",
    "  \n",
    "  print(\"---------cutting line---------\")\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_tensorize2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "221bae69870c1675c4cfc152c4d60c3a5cfbb0e1cf1a5072332d9dded6c10f66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
