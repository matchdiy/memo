{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Schedule Primitives in TVM](https://tvm.apache.org/docs/how_to/work_with_schedules/schedule_primitives.html#sphx-glr-how-to-work-with-schedules-schedule-primitives-py)\n",
    "\n",
    "## Create Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_3: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_4: int32], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m, n], [stride_2, stride_5: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    for (j: int32, 0, n) {\n",
      "      C[((i*stride_2) + (j*stride_5))] = (A[((i*stride) + (j*stride_3))]*B[((i*stride_1) + (j*stride_4))])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "# declare some variables for use later\n",
    "n = te.var(\"n\")\n",
    "m = te.var(\"m\")\n",
    "\n",
    "def test_elewise_mul():\n",
    "  # declare a matrix element-wise multiply\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.placeholder((m, n), name=\"B\")\n",
    "  C = te.compute((m, n), lambda i, j: A[i, j] * B[i, j], name=\"C\")\n",
    "\n",
    "  s = te.create_schedule([C.op])\n",
    "  # lower will transform the computation from definition to the real\n",
    "  # callable function. With argument `simple_mode=True`, it will\n",
    "  # return you a readable C like statement, we use it here to print the\n",
    "  # schedule result.\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_elewise_mul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Split\n",
    "\n",
    "SplitFactor：将指定维度按照指定长度进行切分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 31), 32)) {\n",
      "    for (i.inner: int32, 0, 32) {\n",
      "      if @tir.likely((((i.outer*32) + i.inner) < m), dtype=bool) {\n",
      "        for (j: int32, 0, n) {\n",
      "          let cse_var_1: int32 = ((i.outer*32) + i.inner)\n",
      "          B[((cse_var_1*stride_1) + (j*stride_3))] = A[((cse_var_1*stride) + (j*stride_2))]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 31), 32)) {\n",
      "      for (j.inner: int32, 0, 32) {\n",
      "        if @tir.likely((((j.outer*32) + j.inner) < n), dtype=bool) {\n",
      "          let cse_var_1: int32 = ((j.outer*32) + j.inner)\n",
      "          B[((i*stride_1) + (cse_var_1*stride_3))] = A[((i*stride) + (cse_var_1*stride_2))]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_split_factor(axis, factor=32):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  xo, xi = s[B].split(B.op.axis[axis], factor=factor)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_split_factor(axis=0)\n",
    "test_split_factor(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SplitParts：将指定维度按照指定份数进行切分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, 8) {\n",
      "    for (i.inner: int32, 0, floordiv((m + 7), 8)) {\n",
      "      if @tir.likely(((i.inner + (i.outer*floordiv((m + 7), 8))) < m), dtype=bool) {\n",
      "        B[((i.inner + (i.outer*floordiv((m + 7), 8)))*stride_1)] = A[((i.inner + (i.outer*floordiv((m + 7), 8)))*stride)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_split_nparts(nparts=8):\n",
    "  A = te.placeholder((m,), name=\"A\")\n",
    "  B = te.compute((m,), lambda i: A[i], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  bx, tx = s[B].split(B.op.axis[0], nparts=nparts)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_split_nparts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Tile\n",
    "\n",
    "分块操作，注意Tile和Split是有区别的，我们无法用两个Split完成一个Tile的功能，但是可以通过Tile覆盖Split功能（Split中不切的axis-factor设为1）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (i.inner: int32, 0, 10) {\n",
      "        if @tir.likely((((i.outer*10) + i.inner) < m), dtype=bool) {\n",
      "          for (j.inner: int32, 0, 5) {\n",
      "            if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "              let cse_var_2: int32 = ((j.outer*5) + j.inner)\n",
      "              let cse_var_1: int32 = ((i.outer*10) + i.inner)\n",
      "              B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_tile(x, y, simple_mode=True):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=x, y_factor=y)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=simple_mode))\n",
    "\n",
    "test_tile(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较一下 ___Split___ 和 ___Tile___ 的行为： ___Tile___ 需要 ___Split___ 和 ___Reorder___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************\n",
      "test_split_x2(10, 5)\n",
      "****************************************************************\n",
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "    for (i.inner: int32, 0, 10) {\n",
      "      if @tir.likely((((i.outer*10) + i.inner) < m), dtype=bool) {\n",
      "        for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "          for (j.inner: int32, 0, 5) {\n",
      "            if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "              let cse_var_2: int32 = ((j.outer*5) + j.inner)\n",
      "              let cse_var_1: int32 = ((i.outer*10) + i.inner)\n",
      "              B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "****************************************************************\n",
      "test_tile(10, 5)\n",
      "****************************************************************\n",
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (i.inner: int32, 0, 10) {\n",
      "        if @tir.likely((((i.outer*10) + i.inner) < m), dtype=bool) {\n",
      "          for (j.inner: int32, 0, 5) {\n",
      "            if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "              let cse_var_2: int32 = ((j.outer*5) + j.inner)\n",
      "              let cse_var_1: int32 = ((i.outer*10) + i.inner)\n",
      "              B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_split_x2(x_factor, y_factor):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  xo, xi = s[B].split(B.op.axis[0], factor=x_factor)\n",
    "  yo, yi = s[B].split(B.op.axis[1], factor=y_factor)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "print('*'*64)\n",
    "print('test_split_x2(10, 5)')\n",
    "print('*'*64)\n",
    "test_split_x2(10, 5)\n",
    "print('*'*64)\n",
    "print('test_tile(10, 5)')\n",
    "print('*'*64)\n",
    "test_tile(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较一下 ___Split___ 和 ___Tile___ 的行为：用 ___Tile___ 来实现 ___Split___：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (j.inner: int32, 0, 5) {\n",
      "        if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "          let cse_var_1: int32 = ((j.outer*5) + j.inner)\n",
      "          B[((i*stride_1) + (cse_var_1*stride_3))] = A[((i*stride) + (cse_var_1*stride_2))]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, m) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (j.inner: int32, 0, 5) {\n",
      "        if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "          let cse_var_1: int32 = ((j.outer*5) + j.inner)\n",
      "          B[((i.outer*stride_1) + (cse_var_1*stride_3))] = A[((i.outer*stride) + (cse_var_1*stride_2))]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_split_factor(axis=1, factor=5)\n",
    "test_tile(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Fuse\n",
    "\n",
    "合并连续的 ___N___ 个维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "    for (j.outer.i.inner.fused: int32, 0, (floordiv((n + 4), 5)*10)) {\n",
      "      if @tir.likely((((i.outer*10) + floormod(j.outer.i.inner.fused, 10)) < m), dtype=bool) {\n",
      "        for (j.inner: int32, 0, 5) {\n",
      "          if @tir.likely((((floordiv(j.outer.i.inner.fused, 10)*5) + j.inner) < n), dtype=bool) {\n",
      "            let cse_var_2: int32 = ((floordiv(j.outer.i.inner.fused, 10)*5) + j.inner)\n",
      "            let cse_var_1: int32 = ((i.outer*10) + floormod(j.outer.i.inner.fused, 10))\n",
      "            B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_fuse(axis0, axis1, simple_mode=True):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  # tile to four axes first: (i.outer, j.outer, i.inner, j.inner)\n",
    "  axes4 = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)\n",
    "  # then fuse (i.inner, j.inner) into one axis: (i.inner.j.inner.fused)\n",
    "  fused = s[B].fuse(axes4[axis0], axes4[axis1])\n",
    "  ### fused = s[B].fuse(xo, yo)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=simple_mode))\n",
    "\n",
    "test_fuse(1, 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Reorder\n",
    "\n",
    "维度调换，相当于Transpose功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (i.inner: int32, 0, 10) {\n",
      "        if @tir.likely((((i.outer*10) + i.inner) < m), dtype=bool) {\n",
      "          for (j.inner: int32, 0, 5) {\n",
      "            if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "              let cse_var_2: int32 = ((j.outer*5) + j.inner)\n",
      "              let cse_var_1: int32 = ((i.outer*10) + i.inner)\n",
      "              B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_3: int32], type=\"auto\")} {\n",
      "  for (i.inner: int32, 0, 10) {\n",
      "    for (j.outer: int32, 0, floordiv((n + 4), 5)) {\n",
      "      for (i.outer: int32, 0, floordiv((m + 9), 10)) {\n",
      "        if @tir.likely((((i.outer*10) + i.inner) < m), dtype=bool) {\n",
      "          for (j.inner: int32, 0, 5) {\n",
      "            if @tir.likely((((j.outer*5) + j.inner) < n), dtype=bool) {\n",
      "              let cse_var_2: int32 = ((j.outer*5) + j.inner)\n",
      "              let cse_var_1: int32 = ((i.outer*10) + i.inner)\n",
      "              B[((cse_var_1*stride_1) + (cse_var_2*stride_3))] = A[((cse_var_1*stride) + (cse_var_2*stride_2))]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_reorder(axis0, axis1, axis2, axis3):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  # tile to four axes first: (i.outer, j.outer, i.inner, j.inner)\n",
    "  axes4 = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)\n",
    "  s[B].reorder(axes4[axis0], axes4[axis1], axes4[axis2], axes4[axis3])\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_reorder(0, 1, 2, 3) # no change\n",
    "test_reorder(2, 1, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Gpu::Bind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n], [stride_1], type=\"auto\")} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = floordiv((n + 63), 64);\n",
      "  attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 64;\n",
      "  if @tir.likely((((blockIdx.x*64) + threadIdx.x) < n), dtype=bool) {\n",
      "    B[(((blockIdx.x*64) + threadIdx.x)*stride_1)] = A[(((blockIdx.x*64) + threadIdx.x)*stride)]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_bind():\n",
    "  A = te.placeholder((n,), name=\"A\")\n",
    "  B = te.compute(A.shape, lambda i: A[i], name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  bx, tx = s[B].split(B.op.axis[0], factor=64)\n",
    "  s[B].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
    "  s[B].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "test_bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Compute_At\n",
    "\n",
    "移动一个Compute Stage 到指定的计算指定维度中，看起来可以用在 Fusion 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_3: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_4: int32], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m, n], [stride_2, stride_5: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    for (j: int32, 0, n) {\n",
      "      B[((i*stride_1) + (j*stride_4))] = (A[((i*stride) + (j*stride_3))] + 1f32)\n",
      "      C[((i*stride_2) + (j*stride_5))] = (B[((i*stride_1) + (j*stride_4))]*2f32)\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m, n: int32], [stride, stride_3: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m, n], [stride_1, stride_4: int32], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m, n], [stride_2, stride_5: int32], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    for (j: int32, 0, n) {\n",
      "      B[((i*stride_1) + (j*stride_4))] = (A[((i*stride) + (j*stride_3))] + 1f32)\n",
      "    }\n",
      "    for (j_1: int32, 0, n) {\n",
      "      C[((i*stride_2) + (j_1*stride_5))] = (B[((i*stride_1) + (j_1*stride_4))]*2f32)\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_compute_at(axis):\n",
    "  A = te.placeholder((m, n), name=\"A\")\n",
    "  B = te.compute((m, n), lambda i, j: A[i, j] + 1, name=\"B\")\n",
    "  C = te.compute((m, n), lambda i, j: B[i, j] * 2, name=\"C\")\n",
    "  s = te.create_schedule(C.op)\n",
    "  # move computation of B into the first axis of computation of C\n",
    "  s[B].compute_at(s[C], C.op.axis[axis]) \n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_compute_at(-1)\n",
    "test_compute_at(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Compute_Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m], [stride_2], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    C[(i*stride_2)] = ((A[(i*stride)] + 1f32)*2f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_compute_inline():\n",
    "  A = te.placeholder((m,), name=\"A\")\n",
    "  B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "  C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "  s = te.create_schedule(C.op)\n",
    "  # mark one stage as inline\n",
    "  s[B].compute_inline()\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_compute_inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Compute_Root\n",
    "\n",
    "将一个计算移动到根上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*m: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*m)], [], type=\"auto\"),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [(stride_2: int32*m)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [m], [stride], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [m], [stride_1], type=\"auto\"), C_1: C_3: Buffer(C_2, float32, [m], [stride_2], type=\"auto\")} {\n",
      "  for (i: int32, 0, m) {\n",
      "    B[(i*stride_1)] = (A[(i*stride)] + 1f32)\n",
      "  }\n",
      "  for (i_1: int32, 0, m) {\n",
      "    C[(i_1*stride_2)] = (B[(i_1*stride_1)]*2f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_compute_root():\n",
    "  A = te.placeholder((m,), name=\"A\")\n",
    "  B = te.compute((m,), lambda i: A[i] + 1, name=\"B\")\n",
    "  C = te.compute((m,), lambda i: B[i] * 2, name=\"C\")\n",
    "  s = te.create_schedule(C.op)\n",
    "  s[B].compute_at(s[C], C.op.axis[0])\n",
    "  # move computation of one stage to the root\n",
    "  s[B].compute_root()\n",
    "  print(tvm.lower(s, [A, B, C], simple_mode=True))\n",
    "\n",
    "test_compute_root()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e5d003a34d96501b815919cdb43990f793a06da60803b68ddd0500cde71cfe2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
