{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [How to do reduction in TVM](https://tvm.apache.org/docs/how_to/work_with_schedules/reduction.html#sphx-glr-how-to-work-with-schedules-reduction-py)\n",
    "\n",
    "## SchedulePrimitives::Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "n = te.var(\"n\")\n",
    "m = te.var(\"m\")\n",
    "\n",
    "def test_reduce():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split reduce axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_reduce():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  xo, xi = s[B].split(B.op.axis[0], factor=32)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_split_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bind row in GPU kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reduce_bind():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  xo, xi = s[B].split(B.op.axis[0], factor=32)\n",
    "  s[B].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
    "  s[B].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_reduce_bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Refactor\n",
    "\n",
    "构建归约的一个问题是我们不能简单地在归约轴上并行化，需要划分归约的计算，将局部归约结果存储在临时数组中，然后再对临时数组进行归约。为了简化这个问题，引入 ___rfactor___ 原语对计算进行重写。下面这个这个列子对reduce维度进行了rfactor操作，目的是想让 _16_ 个 thread 能够同时进行规约计算，然后再使用 _1_ 个 thread 对 _16_ 个中间结果最后进行一次 _16_ 元素的规约，以便于GPU处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reduce_rfactor():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  BF = s.rfactor(B, ki)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  print(s[B].op.body)\n",
    "\n",
    "test_reduce_rfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用 C 语言描述上面的计算过程:\n",
    "\n",
    "```C++\n",
    "#include <array>\n",
    "\n",
    "// Row\n",
    "#define N 100\n",
    "// Col\n",
    "#define M 256\n",
    "// Parallel \n",
    "#define K 16\n",
    "\n",
    "int main(void) {\n",
    "  float A[N][M];\n",
    "  float B[N];\n",
    "  float B_rf[K][N];\n",
    "\n",
    "  for(int k_inner = 0; k_inner < K; ++k_inner) {\n",
    "    for (int i = 0; i < N; ++i) {\n",
    "      B_rf[k_inner][i] = 0.0F;\n",
    "      for(int k_outer = 0; k_outer < (M + 15) >> 4; ++k_outer) {\n",
    "        if (k_outer * K + k_inner < M) {\n",
    "          B_rf[k_inner][i] += A[i][k_outer * K + k_inner];\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    B[i] = 0.0F;\n",
    "    for (int k = 0 ; k < K; ++k) {\n",
    "      B[i] += B_rf[k];\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::set_store_predicate\n",
    "\n",
    "Cross Thread Reduction\n",
    "这个例子展示了 ___rfactor___ 后的规约并行计算如何和 CUDA 编程模型进行 _bind_。这里我们在 ___set_store_predicate___ 的前后进行了结果对比，观察其行为。 ___set_store_predicate___ 设置或者不设置都不影响结果的正确性，它的语义是指定并行 axis 上的哪个 index (thread) 完成 store 的处理，如果没有指定的话那么16个thread会同时写结果到正确的位置上，不保证写的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_thread_reduction(show_cuda_code):\n",
    "  A = te.placeholder((n, m), dtype='float32', name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  BF = s.rfactor(B, ki)\n",
    "  \n",
    "  xo, xi = s[B].split(s[B].op.axis[0], factor=32)\n",
    "  s[B].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
    "  s[B].bind(xi, te.thread_axis(\"threadIdx.y\"))\n",
    "  tx = te.thread_axis(\"threadIdx.x\")\n",
    "  s[B].bind(s[B].op.reduce_axis[0], tx)\n",
    "  s[BF].compute_at(s[B], s[B].op.reduce_axis[0])\n",
    "  s[B].set_store_predicate(tx.var.equal(0))\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  fcuda = tvm.build(s, [A, B], \"cuda\")\n",
    "  if show_cuda_code:\n",
    "    print('*'*64)\n",
    "    print('CUDA Source Code')\n",
    "    print('*'*64)\n",
    "    print(fcuda.imported_modules[0].get_source())\n",
    "  return A, B, fcuda\n",
    "\n",
    "results = cross_thread_reduction(show_cuda_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_gpu(nn, A, B, fcuda):\n",
    "  dev = tvm.cuda(0)\n",
    "  a = tvm.nd.array(np.random.uniform(size=(nn, nn)).astype(A.dtype), dev)\n",
    "  b = tvm.nd.array(np.zeros(nn, dtype=B.dtype), dev)\n",
    "  fcuda(a, b)\n",
    "  tvm.testing.assert_allclose(b.numpy(), np.sum(a.numpy(), axis=1), rtol=1e-6)\n",
    "\n",
    "test_with_gpu(4096, results[0], results[1], results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 ___reduce___ 实现一维卷积的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv1d():\n",
    "  n = te.var(\"n\")\n",
    "  Input = te.placeholder((n, n), name=\"Input\")\n",
    "  Filter = te.placeholder((3, 3), name=\"Filter\")\n",
    "  di = te.reduce_axis((0, 3), name=\"di\")\n",
    "  dj = te.reduce_axis((0, 3), name=\"dj\")\n",
    "  Output = te.compute(\n",
    "      (n - 2, n - 2),\n",
    "      lambda i, j: te.sum(Input[i + di, j + dj] * Filter[di, dj], axis=[di, dj]),\n",
    "      name=\"Output\",\n",
    "  )\n",
    "  s = te.create_schedule(Output.op)\n",
    "  print(tvm.lower(s, [Input, Filter, Output], simple_mode=True))\n",
    "\n",
    "test_conv1d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::comm_reducer\n",
    "\n",
    "除了使用 tvm 内置的规约操作例如 te.sum, te.min, te.max 以外，也可以通过 ___reducer___ 自定义规约操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reducer():\n",
    "  n = te.var(\"n\")\n",
    "  m = te.var(\"m\")\n",
    "  product = te.comm_reducer(lambda x, y: x * y, lambda t: tvm.tir.const(1, dtype=t), name=\"product\")\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), name=\"k\")\n",
    "  B = te.compute((n,), lambda i: product(A[i, k], axis=k), name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_reducer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "221bae69870c1675c4cfc152c4d60c3a5cfbb0e1cf1a5072332d9dded6c10f66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
