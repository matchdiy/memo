{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [How to do reduction in TVM](https://tvm.apache.org/docs/how_to/work_with_schedules/reduction.html#sphx-glr-how-to-work-with-schedules-reduction-py)\n",
    "\n",
    "## SchedulePrimitives::Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n], [stride_1], type=\"auto\")} {\n",
      "  for (i: int32, 0, n) {\n",
      "    B[(i*stride_1)] = 0f32\n",
      "    for (k: int32, 0, m) {\n",
      "      B[(i*stride_1)] = (B[(i*stride_1)] + A[((i*stride) + (k*stride_2))])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "n = te.var(\"n\")\n",
    "m = te.var(\"m\")\n",
    "\n",
    "def test_reduce():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split reduce axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n], [stride_1], type=\"auto\")} {\n",
      "  for (i.outer: int32, 0, floordiv((n + 31), 32)) {\n",
      "    for (i.inner: int32, 0, 32) {\n",
      "      if @tir.likely((((i.outer*32) + i.inner) < n), dtype=bool) {\n",
      "        B[(((i.outer*32) + i.inner)*stride_1)] = 0f32\n",
      "      }\n",
      "      if @tir.likely((((i.outer*32) + i.inner) < n), dtype=bool) {\n",
      "        for (k.outer: int32, 0, floordiv((m + 15), 16)) {\n",
      "          for (k.inner: int32, 0, 16) {\n",
      "            if @tir.likely((((k.outer*16) + k.inner) < m), dtype=bool) {\n",
      "              let cse_var_1: int32 = ((i.outer*32) + i.inner)\n",
      "              B[(cse_var_1*stride_1)] = (B[(cse_var_1*stride_1)] + A[((cse_var_1*stride) + (((k.outer*16) + k.inner)*stride_2))])\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_split_reduce():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  xo, xi = s[B].split(B.op.axis[0], factor=32)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_split_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bind row in GPU kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n], [stride_1], type=\"auto\")} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = floordiv((n + 31), 32);\n",
      "  attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32 {\n",
      "    if @tir.likely((((blockIdx.x*32) + threadIdx.x) < n), dtype=bool) {\n",
      "      B[(((blockIdx.x*32) + threadIdx.x)*stride_1)] = 0f32\n",
      "    }\n",
      "    for (k.outer: int32, 0, floordiv((m + 15), 16)) {\n",
      "      for (k.inner: int32, 0, 16) {\n",
      "        if @tir.likely((((blockIdx.x*32) + threadIdx.x) < n), dtype=bool) {\n",
      "          if @tir.likely((((k.outer*16) + k.inner) < m), dtype=bool) {\n",
      "            B[(((blockIdx.x*32) + threadIdx.x)*stride_1)] = (B[(((blockIdx.x*32) + threadIdx.x)*stride_1)] + A[((((blockIdx.x*32) + threadIdx.x)*stride) + (((k.outer*16) + k.inner)*stride_2))])\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_reduce_bind():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  xo, xi = s[B].split(B.op.axis[0], factor=32)\n",
    "  s[B].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
    "  s[B].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "\n",
    "test_reduce_bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::Refactor\n",
    "\n",
    "构建归约的一个问题是我们不能简单地在归约轴上并行化，需要划分归约的计算，将局部归约结果存储在临时数组中，然后再对临时数组进行归约。为了简化这个问题，引入 ___rfactor___ 原语对计算进行重写。下面这个这个列子对reduce维度进行了rfactor操作，目的是想让 _16_ 个 thread 能够同时进行规约计算，然后再使用 _1_ 个 thread 对 _16_ 个中间结果最后进行一次 _16_ 元素的规约，以便于GPU处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n], [stride_1], type=\"auto\")} {\n",
      "  allocate(B.rf: Pointer(global float32), float32, [(n*16)]), storage_scope = global {\n",
      "    for (k.inner: int32, 0, 16) {\n",
      "      for (i: int32, 0, n) {\n",
      "        B.rf_1: Buffer(B.rf, float32, [(16*n)], [])[((k.inner*n) + i)] = 0f32\n",
      "        for (k.outer: int32, 0, floordiv((m + 15), 16)) {\n",
      "          if @tir.likely((((k.outer*16) + k.inner) < m), dtype=bool) {\n",
      "            B.rf_1[((k.inner*n) + i)] = (B.rf_1[((k.inner*n) + i)] + A[((i*stride) + (((k.outer*16) + k.inner)*stride_2))])\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (ax0: int32, 0, n) {\n",
      "      B[(ax0*stride_1)] = 0f32\n",
      "      for (k.inner.v: int32, 0, 16) {\n",
      "        B[(ax0*stride_1)] = (B[(ax0*stride_1)] + B.rf_1[((k.inner.v*n) + ax0)])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[B.rf[k.inner.v, ax0]], init=[], axis=[iter_var(k.inner.v, range(min=0, ext=16))], where=(bool)1, value_index=0)]\n"
     ]
    }
   ],
   "source": [
    "def test_reduce_rfactor():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  BF = s.rfactor(B, ki)\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  print(s[B].op.body)\n",
    "\n",
    "test_reduce_rfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用 C 语言描述上面的计算过程:\n",
    "\n",
    "```C++\n",
    "#include <array>\n",
    "\n",
    "// Row\n",
    "#define N 100\n",
    "// Col\n",
    "#define M 256\n",
    "// Parallel \n",
    "#define K 16\n",
    "\n",
    "int main(void) {\n",
    "  float A[N][M];\n",
    "  float B[N];\n",
    "  float B_rf[K][N];\n",
    "\n",
    "  for(int k_inner = 0; k_inner < K; ++k_inner) {\n",
    "    for (int i = 0; i < N; ++i) {\n",
    "      B_rf[k_inner][i] = 0.0F;\n",
    "      for(int k_outer = 0; k_outer < (M + 15) >> 4; ++k_outer) {\n",
    "        if (k_outer * K + k_inner < M) {\n",
    "          B_rf[k_inner][i] += A[i][k_outer * K + k_inner];\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    B[i] = 0.0F;\n",
    "    for (int k = 0 ; k < K; ++k) {\n",
    "      B[i] += B_rf[k];\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchedulePrimitives::set_store_predicate\n",
    "\n",
    "Cross Thread Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\"),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [(stride_1: int32*n)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m: int32], [stride, stride_2: int32], type=\"auto\"), B_1: B_3: Buffer(B_2, float32, [n], [stride_1], type=\"auto\")} {\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = floordiv((n + 31), 32);\n",
      "  allocate(B.rf: Pointer(local float32), float32, [1]), storage_scope = local;\n",
      "  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;\n",
      "  attr [IterVar(threadIdx.y: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 32;\n",
      "  attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16 {\n",
      "    B.rf_1: Buffer(B.rf, float32, [1], [], scope=\"local\", align=4)[0] = 0f32\n",
      "    for (k.outer: int32, 0, floordiv((m + 15), 16)) {\n",
      "      if @tir.likely((((blockIdx.x*32) + threadIdx.y) < n), dtype=bool) {\n",
      "        if @tir.likely((((k.outer*16) + threadIdx.x) < m), dtype=bool) {\n",
      "          B.rf_1[0] = (B.rf_1[0] + A[((((blockIdx.x*32) + threadIdx.y)*stride) + (((k.outer*16) + threadIdx.x)*stride_2))])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    attr [meta[tir.CommReducer][0]] \"reduce_scope\" = @tir.reinterpret(0u64, dtype=handle);\n",
      "    @tir.tvm_thread_allreduce(1u32, B.rf_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope=\"local\")[0], threadIdx.x, dtype=handle)\n",
      "    if (threadIdx.x == 0) {\n",
      "      B[(((blockIdx.x*32) + threadIdx.y)*stride_1)] = reduce_temp0_1[0]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "****************************************************************\n",
      "CUDA Source Code\n",
      "****************************************************************\n",
      "\n",
      "#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n",
      "#define __shfl_sync(mask, var, lane, width) \\\n",
      "        __shfl((var), (lane), (width))\n",
      "\n",
      "#define __shfl_down_sync(mask, var, offset, width) \\\n",
      "        __shfl_down((var), (offset), (width))\n",
      "\n",
      "#define __shfl_up_sync(mask, var, offset, width) \\\n",
      "        __shfl_up((var), (offset), (width))\n",
      "#endif\n",
      "\n",
      "\n",
      "#ifdef _WIN32\n",
      "  using uint = unsigned int;\n",
      "  using uchar = unsigned char;\n",
      "  using ushort = unsigned short;\n",
      "  using int64_t = long long;\n",
      "  using uint64_t = unsigned long long;\n",
      "#else\n",
      "  #define uint unsigned int\n",
      "  #define uchar unsigned char\n",
      "  #define ushort unsigned short\n",
      "  #define int64_t long long\n",
      "  #define uint64_t unsigned long long\n",
      "#endif\n",
      "extern \"C\" __global__ void __launch_bounds__(512) default_function_kernel0(float* __restrict__ A, float* __restrict__ B, int m, int n, int stride, int stride_1, int stride_2) {\n",
      "  float B_rf[1];\n",
      "  float red_buf0[1];\n",
      "  B_rf[0] = 0.000000e+00f;\n",
      "  for (int k_outer = 0; k_outer < (m >> 4); ++k_outer) {\n",
      "    if (((((int)blockIdx.x) * 32) + ((int)threadIdx.y)) < n) {\n",
      "      B_rf[0] = (B_rf[0] + A[((((((int)blockIdx.x) * 32) + ((int)threadIdx.y)) * stride) + (((k_outer * 16) + ((int)threadIdx.x)) * stride_1))]);\n",
      "    }\n",
      "  }\n",
      "  for (int k_outer_1 = 0; k_outer_1 < (((m & 15) + 15) >> 4); ++k_outer_1) {\n",
      "    if (((((int)blockIdx.x) * 32) + ((int)threadIdx.y)) < n) {\n",
      "      if (((((m >> 4) * 16) + (k_outer_1 * 16)) + ((int)threadIdx.x)) < m) {\n",
      "        B_rf[0] = (B_rf[0] + A[((((((int)blockIdx.x) * 32) + ((int)threadIdx.y)) * stride) + (((((m >> 4) * 16) + (k_outer_1 * 16)) + ((int)threadIdx.x)) * stride_1))]);\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  uint mask[1];\n",
      "  float t0[1];\n",
      "  red_buf0[0] = B_rf[0];\n",
      "  mask[0] = (__activemask() & ((uint)(65535 << (((int)threadIdx.y) * 16))));\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 16), 32);\n",
      "  if (((int)threadIdx.x) == 0) {\n",
      "    B[(((((int)blockIdx.x) * 32) + ((int)threadIdx.y)) * stride_2)] = red_buf0[0];\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cross_thread_reduction():\n",
    "  A = te.placeholder((n, m), name=\"A\")\n",
    "  k = te.reduce_axis((0, m), \"k\")\n",
    "  B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k) , name=\"B\")\n",
    "  s = te.create_schedule(B.op)\n",
    "  ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)\n",
    "  BF = s.rfactor(B, ki)\n",
    "  \n",
    "  xo, xi = s[B].split(s[B].op.axis[0], factor=32)\n",
    "  s[B].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
    "  s[B].bind(xi, te.thread_axis(\"threadIdx.y\"))\n",
    "  tx = te.thread_axis(\"threadIdx.x\")\n",
    "  s[B].bind(s[B].op.reduce_axis[0], tx)\n",
    "  s[BF].compute_at(s[B], s[B].op.reduce_axis[0])\n",
    "  s[B].set_store_predicate(tx.var.equal(0))\n",
    "  print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "  fcuda = tvm.build(s, [A, B], \"cuda\")\n",
    "  print('*'*64)\n",
    "  print('CUDA Source Code')\n",
    "  print('*'*64)\n",
    "  print(fcuda.imported_modules[0].get_source())\n",
    "\n",
    "cross_thread_reduction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "221bae69870c1675c4cfc152c4d60c3a5cfbb0e1cf1a5072332d9dded6c10f66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
