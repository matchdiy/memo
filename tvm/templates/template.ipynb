{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul Dataflow\n",
    "\n",
    "* 这个例子中假设我们的设备信息是：\n",
    "  * (a) 处理器上支持的矩阵乘指令是 _8x16x8_\n",
    "  * (b) ___[K]___ L1 tile 需要循环 5x5 次 KernelFunctionCall \n",
    "  * (c) ___[S]___ L2 tile 需要循环 4x4 次 DMA C2S\n",
    "  * (d) ___[C]___ L3 tile 需要循环 3x3 次 DMA D2C\n",
    "  * (e) ___[P]___ L3 tile view：无论是 _4C_ 还是 _1C_ 都不需要进行 SliceD2D\n",
    "    * _4C_ 的时候需要在算子进行计算之前把每个C的亲和 Tensor准备好，不需要将这个逻辑集成到每一个算子中。\n",
    "    * _1C_ 的时候比如有4个SIP，那么这里只是确定每个SIP的负载，只是一个View而不需要真正的切分。\n",
    "\n",
    "\n",
    "* 图示\n",
    "\n",
    "```\n",
    "假设每个Cluster有4个SIP，这里是 _1C_ 分到的计算:\n",
    "--------------------------------------------------------------------------------\n",
    "Global:\n",
    "  Global-M = PM x 2 = 960\n",
    "  Global-N = PN x 2 = 1920\n",
    "  Global-K = PK x 1 = 160\n",
    "            ________________________________________________\n",
    "           |                        |                       |\n",
    "           |________________________|_______________________|\n",
    "                                 B[K, N]\n",
    "      Global(L3)\n",
    "    __      ________________________________________________ \n",
    "   |  |    |PX-L3                   |PX-L3                  |\n",
    "   |  |    |                        |                       |\n",
    "   |  |    |                        |                       |\n",
    "   |  |    |                        |                       |\n",
    "   |  |    |                        |                       |\n",
    "   |__|    |________________________|_______________________|\n",
    "   |  |    |PX-L3                   |PX-L3                  |\n",
    "   |  |    |                        |                       |\n",
    "   |  |    |                        |                       |\n",
    "   |  |    |                        |                       |\n",
    "   |  |    |                        |                       |\n",
    "   |__|    |________________________|_______________________|\n",
    "  A[M,K]                        C[K, N]\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "L3:\n",
    "  PM = CM x 3 = 480\n",
    "  PN = CN x 3 = 960\n",
    "  PK = CK x 1 = 160\n",
    "            __________________________\n",
    "           |        |        |        |\n",
    "           |________|________|________|\n",
    "                   [PK, PN]\n",
    "\n",
    "    __      __________________________\n",
    "   |  |    |CX-L2   |        |        |\n",
    "   |__|    |________|________|________|\n",
    "   |  |    |        |        |        |\n",
    "   |__|    |________|________|________|\n",
    "   |  |    |        |        |        |\n",
    "   |__|    |________|________|________|\n",
    "[PM, PK]            [PM, PN]\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "L2:（放大图像）\n",
    "  CM = SM x 4 = 160\n",
    "  CN = SN x 4 = 320\n",
    "  CK = SK x 4 = 160\n",
    "                  ___________________\n",
    "                 |_SN_|_SN_|_SN_|_SN_| SK0\n",
    "                 |____|____|____|____| SK1\n",
    "                 |____|____|____|____| SK2\n",
    "                 |____|____|____|____| SK3\n",
    "                       [CK, CN]\n",
    "              (CX-L2)\n",
    " ____________     ___________________\n",
    "|SK|SK|SK|SK|    |_SX_|____|____|____| SM0\n",
    "|__|__|__|__|    |____|____|____|____| SM1\n",
    "|__|__|__|__|    |____|_L1_|____|____| SM2\n",
    "|__|__|__|__|    |____|____|____|____| SM3\n",
    "  [CM,CK]              [CM, CN]\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "L1:（放大图像）\n",
    "  SM = KM * 5 = 40\n",
    "  SN = KN * 5 = 80\n",
    "  SK = KK * 5 = 40\n",
    "                     ________________________\n",
    "                    | KK0|    |    |    |    |\n",
    "                    | KK1|    |    |    |    |\n",
    "                    | KK2|    |    |    |    |\n",
    "                    | KK3|    |    |    |    |\n",
    "                    |_KK4|____|____|____|____|\n",
    "                             [SK, CN]\n",
    "           SX-L1\n",
    " ______________      ________________________\n",
    "|KK|KK|KK|KK|KK|    |_KN0|_KN1|_KN2|_KN3|_KN4| KM0\n",
    "|  |  |  |  |  |    |____|____|____|____|____| KM1\n",
    "|  |  |  |  |  |    |____|____|_L0_|____|____| KM2\n",
    "|  |  |  |  |  |    |____|____|____|____|____| KM3\n",
    "|__|__|__|__|__|    |____|____|____|____|____| KM4\n",
    "    [SM, SK]                  [SM, SN]\n",
    "\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Instruction: GEMM_8_16_8:\n",
    "\n",
    "   c[8, 16] = a[8, 8] * b[8, 16] + c[8, 16]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Base Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te \n",
    "\n",
    "# ---------------\n",
    "# op config\n",
    "# ---------------\n",
    "\n",
    "# dtu.instruction\n",
    "KM = 8\n",
    "KN = 16\n",
    "KK = 8\n",
    "\n",
    "# dtu.sdb (tile) = local\n",
    "SM = KM * 5\n",
    "SN = KN * 5\n",
    "SK = KK * 5\n",
    "\n",
    "# dtu.csb (tile) = global/shared (use global) = CX\n",
    "CM = SM * 4\n",
    "CN = SN * 4\n",
    "CK = SK * 4\n",
    "\n",
    "# dtu.sip (tile) = processor = PX = tvm.parallel\n",
    "PM = CM * 3\n",
    "PN = CN * 3\n",
    "PK = CK\n",
    "\n",
    "# feature map\n",
    "M = PM * 2\n",
    "N = PN * 2\n",
    "K = PK\n",
    "\n",
    "print('*'*64)\n",
    "print('X: {}'.format([M, N, K]))\n",
    "print('PX:{}'.format([PM, PN, PK]))\n",
    "print('CX:{}'.format([CM, CN, CK]))\n",
    "print('SX:{}'.format([SM, SN, SK]))\n",
    "print('KX:{}'.format([KM, KN, KK]))\n",
    "print('*'*64 + '\\n')\n",
    "\n",
    "# ---------------\n",
    "# define compute\n",
    "# ---------------\n",
    "# define a reduce axis\n",
    "k = te.reduce_axis((0, K), \"k\") \n",
    "\n",
    "# input tensors\n",
    "l = te.placeholder((M, K), name=\"l\")\n",
    "r = te.placeholder((K, N), name=\"r\")\n",
    "# compute\n",
    "o = te.compute((M, N), lambda m, n: te.sum(l[m, k] * r[k, n], axis=k), name=\"o\")\n",
    "\n",
    "# ---------------\n",
    "# schedule op\n",
    "# ---------------\n",
    "# create a schedule\n",
    "s = te.create_schedule(o.op)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add New Storage\n",
    "\n",
    "* `cache_read`: This will mutate the body of the readers. A new cache stage will be created for the tensor. ___Call this before doing any split/fuse schedule___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add d2c\n",
    "# l_l2 = s.cache_read(l, \"global\", [o])\n",
    "# r_l2 = s.cache_read(r, \"global\", [o])\n",
    "l_l2 = s.cache_read(l, \"shared\", [o])\n",
    "r_l2 = s.cache_read(r, \"shared\", [o])\n",
    "\n",
    "# add c2s\n",
    "l_l1 = s.cache_read(l_l2, \"local\", [o])\n",
    "r_l1 = s.cache_read(r_l2, \"local\", [o])\n",
    "\n",
    "# add c2d\n",
    "o_l2 = s.cache_write(o, \"shared\")\n",
    "\n",
    "# add s2c\n",
    "o_l1 = s.cache_write(o_l2, \"local\")\n",
    "\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile output and insert output-L2 to output-L3\n",
    "\n",
    "_PM_，_PN_，_CM_，_CN_ 只是对 output 进行 tiling 的参数，它们并不能描述其具体对应到什么存储层级、并行层级。下面这段代码进行了两个主要的操作：\n",
    "* 对 `s[o]` 节点进行两次切分，生成 `pm`，`pn`，`cm`，`cn`\n",
    "* 将 `s[o_l2]` 合并到 `s[o]`的 `pn`，这会触发 pass 对 `s[o_l2]` 做上面同样的两次切分。\n",
    "* output tlie x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# axis\n",
    "m, n = o.op.axis\n",
    "# tile output firstly\n",
    "m, n, pm, pn = s[o].tile(m, n, PM, PN)\n",
    "# tile output secondly\n",
    "pm, pn, cm, cn = s[o].tile(pm, pn, CM, CN)\n",
    "\n",
    "s[o_l2].compute_at(s[o], pn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile output-L2 and insert output-L1 to output-L2\n",
    "\n",
    "上面的操作是通过对`s[o]`的切分然后 `s[o_l2]` 合并到其中完成了同样的切分，下面的操作是在这个基础上，对`s[o_l2]`再进行一次切分，然后将`s[o_l1]`合并到`s[l_l2]`中同样会自动完成`s[o_l1]`的切分。这里的操作不会对 `s[o]` 产生影响， 从而实现了不同的切分。\n",
    "\n",
    "* * output tlie x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, cn = o_l2.op.axis\n",
    "print(cm)\n",
    "print(cn)\n",
    "cm, cn, sm, sn = s[o_l2].tile(cm, cn, SM, SN)\n",
    "\n",
    "# insert tensors\n",
    "s[o_l1].compute_at(s[o_l2], cn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile output-L0\n",
    "\n",
    "对`s[o_l1]`进行进一步切分，并且将 `s[l_l1]`, `s[l_l2]`, `s[r_l1]` 以及 `s[r_2]` 合并到 `s[o_l1]` 中。\n",
    "\n",
    "* output tlie x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tile - sdb & kernel - o_l1\n",
    "sm, sn = o_l1.op.axis\n",
    "print(sm)\n",
    "print(sn)\n",
    "\n",
    "sm, sn, km, kn = s[o_l1].tile(sm, sn, KM, KN)\n",
    "\n",
    "# k\n",
    "pk, ck = s[o_l1].split(k, CK)\n",
    "ck, sk = s[o_l1].split(ck, SK)\n",
    "sk, kk = s[o_l1].split(sk, KK)\n",
    "# reorder \n",
    "s[o_l1].reorder(pk, ck, sm, sn, sk, km, kn, kk)\n",
    "\n",
    "# insert tensors\n",
    "s[l_l1].compute_at(s[o_l1], ck)\n",
    "s[r_l1].compute_at(s[o_l1], ck)\n",
    "s[l_l2].compute_at(s[o_l1], pk)\n",
    "s[r_l2].compute_at(s[o_l1], pk)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('*'*64)\n",
    "print('X: {}'.format([M, N, K]))\n",
    "print('PX:{}'.format([PM, PN, PK]))\n",
    "print('CX:{}'.format([CM, CN, CK]))\n",
    "print('SX:{}'.format([SM, SN, SK]))\n",
    "print('KX:{}'.format([KM, KN, KK]))\n",
    "print('*'*64 + '\\n')\n",
    "\n",
    "# parallelize\n",
    "s[o].parallel(m)\n",
    "s[o].parallel(n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "221bae69870c1675c4cfc152c4d60c3a5cfbb0e1cf1a5072332d9dded6c10f66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
