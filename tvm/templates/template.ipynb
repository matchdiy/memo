{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul Dataflow\n",
    "\n",
    "这个例子中假设我们的设备信息是：\n",
    "* (a) 处理器上支持的矩阵乘指令是 _16x16x8_\n",
    "* (b) L1 tile 需要循环 5x5 次 KernelFunctionCall \n",
    "* (c) L2 tile 需要循环 4x4 次 DMA C2S\n",
    "* (d) L3 tile 需要循环 3x3 次 DMA D2C\n",
    "* (e) L3 tile 需要循环 2x2 次 DMA D2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te \n",
    "# ---------------\n",
    "# op config\n",
    "# ---------------\n",
    "\n",
    "# dtu.kernel\n",
    "KM = 16\n",
    "KN = 16\n",
    "KK = 8\n",
    "\n",
    "# dtu.sdb (tile) = local\n",
    "SM = KM * 5\n",
    "SN = KN * 5\n",
    "SK = KK * 5\n",
    "\n",
    "# dtu.csb (tile) = global/shared (use global) = CX\n",
    "CM = SM * 4\n",
    "CN = SN * 4\n",
    "CK = SK * 4\n",
    "\n",
    "# dtu.sip (tile) = processor = PX = tvm.parallel\n",
    "PM = CM * 3\n",
    "PN = CN * 3\n",
    "PK = CK\n",
    "\n",
    "# feature map\n",
    "M = PM * 2\n",
    "N = PN * 2\n",
    "K = PK\n",
    "\n",
    "print('*'*64)\n",
    "print('X: {}'.format([M, N, K]))\n",
    "print('PX:{}'.format([PM, PN, PK]))\n",
    "print('CX:{}'.format([CM, CN, CK]))\n",
    "print('SX:{}'.format([SM, SN, SK]))\n",
    "print('KX:{}'.format([KM, KN, KK]))\n",
    "print('*'*64 + '\\n')\n",
    "\n",
    "# ---------------\n",
    "# define compute\n",
    "# ---------------\n",
    "# define a reduce axis\n",
    "k = te.reduce_axis((0, K), \"k\") \n",
    "\n",
    "# input tensors\n",
    "l = te.placeholder((M, K), name=\"l\")\n",
    "r = te.placeholder((K, N), name=\"r\")\n",
    "# compute\n",
    "o = te.compute((M, N), lambda m, n: te.sum(l[m, k] * r[k, n], axis=k), name=\"o\")\n",
    "\n",
    "# ---------------\n",
    "# schedule op\n",
    "# ---------------\n",
    "# create a schedule\n",
    "s = te.create_schedule(o.op)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add New Storage\n",
    "\n",
    "* `cache_read`: This will mutate the body of the readers. A new cache stage will be created for the tensor. ___Call this before doing any split/fuse schedule___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add d2c\n",
    "# l_l2 = s.cache_read(l, \"global\", [o])\n",
    "# r_l2 = s.cache_read(r, \"global\", [o])\n",
    "l_l2 = s.cache_read(l, \"shared\", [o])\n",
    "r_l2 = s.cache_read(r, \"shared\", [o])\n",
    "\n",
    "# add c2s\n",
    "l_l1 = s.cache_read(l_l2, \"local\", [o])\n",
    "r_l1 = s.cache_read(r_l2, \"local\", [o])\n",
    "\n",
    "# add c2d\n",
    "o_l2 = s.cache_write(o, \"shared\")\n",
    "\n",
    "# add s2c\n",
    "o_l1 = s.cache_write(o_l2, \"local\")\n",
    "\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile output and insert output-L2 to output-L3\n",
    "\n",
    "_PM_，_PN_，_CM_，_CN_ 只是对 output 进行 tiling 的参数，它们并不能描述其具体对应到什么存储层级、并行层级。下面这段代码进行了两个主要的操作：\n",
    "* 对 `s[o]` 节点进行两次切分，生成 `pm`，`pn`，`cm`，`cn`\n",
    "* 将 `s[o_l2]` 合并到 `s[o]`的 `pn`，这会触发 pass 对 `s[o_l2]` 做上面同样的两次切分。\n",
    "* output tlie x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# axis\n",
    "m, n = o.op.axis\n",
    "# tile output firstly\n",
    "m, n, pm, pn = s[o].tile(m, n, PM, PN)\n",
    "# tile output secondly\n",
    "pm, pn, cm, cn = s[o].tile(pm, pn, CM, CN)\n",
    "\n",
    "s[o_l2].compute_at(s[o], pn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile output-L2 and insert output-L1 to output-L2\n",
    "\n",
    "上面的操作是通过对`s[o]`的切分然后 `s[o_l2]` 合并到其中完成了同样的切分，下面的操作是在这个基础上，对`s[o_l2]`再进行一次切分，然后将`s[o_l1]`合并到`s[l_l2]`中同样会自动完成`s[o_l1]`的切分。\n",
    "\n",
    "这里的操作不会对 `s[o]` 产生影响， 从而实现了不同的切分。\n",
    "\n",
    "* * output tlie x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, cn = o_l2.op.axis\n",
    "print(cm)\n",
    "print(cn)\n",
    "cm, cn, sm, sn = s[o_l2].tile(cm, cn, SM, SN)\n",
    "\n",
    "# insert tensors\n",
    "s[o_l1].compute_at(s[o_l2], cn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile output-L0\n",
    "\n",
    "对`s[o_l1]`进行进一步切分，并且将 `s[l_l1]`, `s[l_l2]`, `s[r_l1]` 以及 `s[r_2]` 合并到 `s[o_l1]` 中。\n",
    "\n",
    "* output tlie x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tile - sdb & kernel - o_l1\n",
    "sm, sn = o_l1.op.axis\n",
    "print(sm)\n",
    "print(sn)\n",
    "\n",
    "sm, sn, km, kn = s[o_l1].tile(sm, sn, KM, KN)\n",
    "\n",
    "# k\n",
    "pk, ck = s[o_l1].split(k, CK)\n",
    "ck, sk = s[o_l1].split(ck, SK)\n",
    "sk, kk = s[o_l1].split(sk, KK)\n",
    "# reorder \n",
    "s[o_l1].reorder(pk, ck, sm, sn, sk, km, kn, kk)\n",
    "\n",
    "# insert tensors\n",
    "s[l_l1].compute_at(s[o_l1], ck)\n",
    "s[r_l1].compute_at(s[o_l1], ck)\n",
    "s[l_l2].compute_at(s[o_l1], pk)\n",
    "s[r_l2].compute_at(s[o_l1], pk)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "print(n)\n",
    "\n",
    "# parallelize\n",
    "s[o].parallel(m)\n",
    "s[o].parallel(n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "221bae69870c1675c4cfc152c4d60c3a5cfbb0e1cf1a5072332d9dded6c10f66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
