{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul Template 111\n",
    "\n",
    "我们先预热一下两个简单的Case，他们是从一个SIP的视角出发，对L2和L1上的数据搬运的Loop次数进行计数。\n",
    "```C++\n",
    "  int func(std::array<int, 3> &loop_times) {\n",
    "    int sum = 0;\n",
    "    for (int i = 0 ; i < loop_times.size(); ++i) {\n",
    "      if (loop_times[i] > 1) { sum += 1; }\n",
    "    }\n",
    "    if (sum ==0) { sum = 1; }\n",
    "    return sum;\n",
    "  }\n",
    "```\n",
    "\n",
    "\n",
    "|Hierarchy|LHS|RHS|OUT|\n",
    "|---------|---|---|---|\n",
    "|L3       | 1 | 1 | 1 |\n",
    "|L2       | 1 | 1 | 1 |\n",
    "|L1       | 1 | 1 | 1 |\n",
    "\n",
    "TODO：怎么表达会比较清晰呢？\n",
    "\n",
    "\n",
    "## Implement1: no split reduction axis\n",
    "不切分 reduce axis 的实现。\n",
    "\n",
    "### Implement1 orignal dataflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te \n",
    "# ---------------\n",
    "# op config\n",
    "# ---------------\n",
    "M = 256\n",
    "N = 128\n",
    "K = 512\n",
    "# ---------------\n",
    "# tile size\n",
    "# NOTE: Tunable params\n",
    "# ---------------\n",
    "# dtu.sip (tile) = processor = PX = tvm.parallel\n",
    "PM = M // 2      # 128\n",
    "PN = N // 2      # 64\n",
    "PK = K           # 512\n",
    "\n",
    "# dtu.kernel\n",
    "KM = 16          # 16\n",
    "KN = 8           # 8 \n",
    "KK = 32          # 32\n",
    "\n",
    "\n",
    "# ---------------\n",
    "# define compute\n",
    "# ---------------\n",
    "def matmul(dump=False):\n",
    "  # define a reduce axis\n",
    "  k = te.reduce_axis((0, K), \"k\") \n",
    "\n",
    "  # input tensors\n",
    "  l = te.placeholder((M, K), name=\"l\")\n",
    "  r = te.placeholder((K, N), name=\"r\")\n",
    "  # compute\n",
    "  o = te.compute((M, N), lambda m, n: te.sum(l[m, k] * r[k, n], axis=k), name=\"o\")\n",
    "  # ---------------\n",
    "  # schedule op\n",
    "  # ---------------\n",
    "  # create a schedule\n",
    "  s = te.create_schedule(o.op)\n",
    "  if dump:\n",
    "    print(tvm.lower(s, [l, r, o], simple_mode=True))\n",
    "  \n",
    "  return k, l, r, o, s\n",
    "\n",
    "k, l, r, o, s = matmul(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Implement1: Add DMA Ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add d2c\n",
    "# l_l2 = s.cache_read(l, \"global\", [o])\n",
    "# r_l2 = s.cache_read(r, \"global\", [o])\n",
    "l_l2 = s.cache_read(l, \"shared\", [o])\n",
    "r_l2 = s.cache_read(r, \"shared\", [o])\n",
    "\n",
    "# add c2s\n",
    "# XC - X in cache: global, shared, local\n",
    "l_l1 = s.cache_read(l_l2, \"local\", [o])\n",
    "r_l1 = s.cache_read(r_l2, \"local\", [o])\n",
    "\n",
    "# add c2d\n",
    "o_l2 = s.cache_write(o, \"shared\")\n",
    "\n",
    "# add s2c\n",
    "o_l1 = s.cache_write(o_l2, \"local\")\n",
    "\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement1: Tile output-L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get axes of o\n",
    "m, n = o_l1.op.axis # o has no k dim now\n",
    "\n",
    "# tile m and n for sip \n",
    "m, n, pm, pn = s[o_l1].tile(m, n, PM, PN)\n",
    "\n",
    "## tile equal splitx2 + reorder\n",
    "# m, pm = s[o_l1].split(m, PM)\n",
    "# n, pn = s[o_l1].split(n, PN)\n",
    "# s[o_l1].reorder(m, n, pm, pn)\n",
    "\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement1: Insert Inputs-L1 to Output-L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert l_l1 & r_l1 to o_l1\n",
    "s[l_l1].compute_at(s[o_l1], pn)\n",
    "s[r_l1].compute_at(s[o_l1], pn)\n",
    "# print(tvm.lower(s, [l, r, o], simple_mode=True))\n",
    "# insert l_l2 & r_l2 to o_l1\n",
    "s[l_l2].compute_at(s[o_l1], pn)\n",
    "s[r_l2].compute_at(s[o_l1], pn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement1: Tile output-L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile o_l2 \n",
    "# get axes of o\n",
    "m, n = o_l2.op.axis # o has no k dim now\n",
    "m, n, pm, pn = s[o_l2].tile(m, n, PM, PN)\n",
    "## split m and n for sip \n",
    "# m, pm = s[o_l2].split(m, PM)\n",
    "# n, pn = s[o_l2].split(n, PN)\n",
    "# s[o_l2].reorder(m, n, pm, pn)\n",
    "\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement1: Insert output-L1 to output-L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert o_l1 to o_l2\n",
    "s[o_l1].compute_at(s[o_l2], n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement1: Tile output-L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tile o\n",
    "# get axes of o\n",
    "m, n = o.op.axis # o has no k dim now\n",
    "\n",
    "m, n, pm, pn = s[o].tile(m, n, PM, PN)\n",
    "### split m and n for sip \n",
    "# m, pm = s[o].split(m, PM)\n",
    "# n, pn = s[o].split(n, PN)\n",
    "# s[o].reorder(m, n, pm, pn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement1: Insert output-L2 to output-L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[o_l2].compute_at(s[o], n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement1: Parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize outer m, n loops\n",
    "s[o].parallel(m)\n",
    "s[o].parallel(n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n",
    "print('Implement1 Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement2\n",
    "\n",
    "### Implement2: Split output-L1\n",
    "\n",
    "在Implement1基础上添加了Kernel层，并且在Kernel层实现了对reduce axis的切分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement2: Insert Inputs-L1 and Inputs-L2 to output-L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, l, r, o, s = matmul()\n",
    "\n",
    "l_l2 = s.cache_read(l, \"shared\", [o])\n",
    "r_l2 = s.cache_read(r, \"shared\", [o])\n",
    "\n",
    "# add c2s\n",
    "# XC - X in cache: global, shared, local\n",
    "l_l1 = s.cache_read(l_l2, \"local\", [o])\n",
    "r_l1 = s.cache_read(r_l2, \"local\", [o])\n",
    "\n",
    "# add c2d\n",
    "o_l2 = s.cache_write(o, \"shared\")\n",
    "\n",
    "# add s2c\n",
    "o_l1 = s.cache_write(o_l2, \"local\")\n",
    "\n",
    "m, n = o_l1.op.axis # o has no k dim now\n",
    "\n",
    "# insert l_l1 & r_l1 to o_l1\n",
    "s[l_l1].compute_at(s[o_l1], n)\n",
    "s[r_l1].compute_at(s[o_l1], n)\n",
    "# insert l_l2 & r_l2 to o_l1\n",
    "s[l_l2].compute_at(s[o_l1], n)\n",
    "s[r_l2].compute_at(s[o_l1], n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement2: Split output-L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tile o_l2 \n",
    "# get axes of o\n",
    "m, n = o_l2.op.axis # o has no k dim now\n",
    "\n",
    "# split m and n for sip \n",
    "m, pm = s[o_l2].split(m, PM)\n",
    "n, pn = s[o_l2].split(n, PN)\n",
    "# reorder \n",
    "s[o_l2].reorder(m, n, pm, pn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement2: Insert output-L1 to output-L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# insert o_l1 to o_l2\n",
    "s[o_l1].compute_at(s[o_l2], n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement2: Split output-L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile o\n",
    "# get axes of o\n",
    "m, n = o.op.axis # o has no k dim now\n",
    "\n",
    "# split m and n for sip \n",
    "m, pm = s[o].split(m, PM)\n",
    "n, pn = s[o].split(n, PN)\n",
    "# reorder \n",
    "s[o].reorder(m, n, pm, pn)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement2: Insert output-L2 to output-L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[o_l2].compute_at(s[o], n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement2: Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[o].parallel(m)\n",
    "s[o].parallel(n)\n",
    "print(tvm.lower(s, [l, r, o], simple_mode=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "221bae69870c1675c4cfc152c4d60c3a5cfbb0e1cf1a5072332d9dded6c10f66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
